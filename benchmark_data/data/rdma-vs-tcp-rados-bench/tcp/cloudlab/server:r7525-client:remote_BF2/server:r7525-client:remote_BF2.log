

[1;7;39;49m[2021-05-17T23:46:23,024080024-07:00][RUNNING][ROUND 1/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T23:46:23,030544355-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T23:46:23,047565660-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40235\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.685079\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 4ec42d22-9c99-423a-90f4-6cfdd5860f7e\nsetting min_mon_release = octopus\nepoch 0\nfsid 4ec42d22-9c99-423a-90f4-6cfdd5860f7e\nlast_changed 2021-05-17T23:46:32.750777-0700\ncreated 2021-05-17T23:46:32.750777-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40235/0,v1:10.10.1.2:40236/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.685079 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 81ea3e2c-57b1-4120-bc8b-ba2074f5e4cc\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 c160e345-0559-4a7f-83a4-6b958101cf62\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 3f8587c7-8f87-494b-9f64-edf44177e0c0\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42235\n'
10.10.1.2: b'  w/ user/pass: admin / 8f9e0a61-3a16-44f1-8dee-6ec7b1c27940\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 23:46:48 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40235
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.685079
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 4ec42d22-9c99-423a-90f4-6cfdd5860f7e
setting min_mon_release = octopus
epoch 0
fsid 4ec42d22-9c99-423a-90f4-6cfdd5860f7e
last_changed 2021-05-17T23:46:32.750777-0700
created 2021-05-17T23:46:32.750777-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40235/0,v1:10.10.1.2:40236/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.685079 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 81ea3e2c-57b1-4120-bc8b-ba2074f5e4cc
0
start osd.0
add osd1 c160e345-0559-4a7f-83a4-6b958101cf62
1
start osd.1
add osd2 3f8587c7-8f87-494b-9f64-edf44177e0c0
2
start osd.2


restful urls: https://10.10.1.2:42235
  w/ user/pass: admin / 8f9e0a61-3a16-44f1-8dee-6ec7b1c27940


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T23:46:24.072-0700 7f3266f2c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:46:24.072-0700 7f3266f2c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:46:24.088-0700 7fa0b5e321c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:46:24.088-0700 7fa0b5e321c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40235,v1:10.10.1.2:40236] --print /tmp/ceph_monmap.685079 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.685079 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.685079 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42235 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.yzKOmjmseF 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 81ea3e2c-57b1-4120-bc8b-ba2074f5e4cc -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDRYqNgcGhNOBAA0HKxCV8qVkFQ2uVZjltc2Q== --osd-uuid 81ea3e2c-57b1-4120-bc8b-ba2074f5e4cc 
2021-05-17T23:46:42.316-0700 7f2bb52a2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T23:46:42.320-0700 7f2bb52a2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T23:46:42.320-0700 7f2bb52a2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T23:46:42.364-0700 7f2bb52a2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c160e345-0559-4a7f-83a4-6b958101cf62 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T23:46:42.644-0700 7fdad761af00 -1 Falling back to public interface
2021-05-17T23:46:42.656-0700 7fdad761af00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDSYqNgrXNvJhAAPilYIFMEUliZOTZ5dQHDKg== --osd-uuid c160e345-0559-4a7f-83a4-6b958101cf62 
2021-05-17T23:46:43.016-0700 7feef565df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T23:46:43.016-0700 7feef565df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T23:46:43.016-0700 7feef565df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T23:46:43.092-0700 7feef565df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3f8587c7-8f87-494b-9f64-edf44177e0c0 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T23:46:43.476-0700 7fb317acbf00 -1 Falling back to public interface
2021-05-17T23:46:43.492-0700 7fb317acbf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDTYqNgdbOpHBAAHokclRONx0YFMtx1GqQhVA== --osd-uuid 3f8587c7-8f87-494b-9f64-edf44177e0c0 
2021-05-17T23:46:43.884-0700 7fa86f223f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T23:46:43.884-0700 7fa86f223f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T23:46:43.884-0700 7fa86f223f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T23:46:43.936-0700 7fa86f223f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T23:46:44.208-0700 7f1f3cf08f00 -1 Falling back to public interface
2021-05-17T23:46:44.224-0700 7f1f3cf08f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T23:46:48,536576009-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T23:46:48,541392193-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T23:46:48,630819333-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:46:48,637521305-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f3099f9-21f6-4855-a1b4-11d2d0a79517', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f3099f9-21f6-4855-a1b4-11d2d0a79517 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T23:46:53,891129457-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:46:53,897581367-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f3099f9-21f6-4855-a1b4-11d2d0a79517', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f3099f9-21f6-4855-a1b4-11d2d0a79517 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T23:46:56,903522176-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:46:56,910199079-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f3099f9-21f6-4855-a1b4-11d2d0a79517', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f3099f9-21f6-4855-a1b4-11d2d0a79517 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T23:46:59,770756462-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:46:59,777266221-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f3099f9-21f6-4855-a1b4-11d2d0a79517', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f3099f9-21f6-4855-a1b4-11d2d0a79517 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T23:47:05,394576888-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:47:05,401340941-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f3099f9-21f6-4855-a1b4-11d2d0a79517', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f3099f9-21f6-4855-a1b4-11d2d0a79517 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T23:47:08,835845103-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:47:08,842250400-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f3099f9-21f6-4855-a1b4-11d2d0a79517', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f3099f9-21f6-4855-a1b4-11d2d0a79517 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T23:47:12,008150086-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:47:12,014401292-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f3099f9-21f6-4855-a1b4-11d2d0a79517', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f3099f9-21f6-4855-a1b4-11d2d0a79517 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T23:47:15,466148983-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:47:15,472606609-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f3099f9-21f6-4855-a1b4-11d2d0a79517', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f3099f9-21f6-4855-a1b4-11d2d0a79517 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T23:47:18,652700882-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:47:18,659357081-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f3099f9-21f6-4855-a1b4-11d2d0a79517', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f3099f9-21f6-4855-a1b4-11d2d0a79517 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T23:47:22,125984831-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:47:22,132385778-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f3099f9-21f6-4855-a1b4-11d2d0a79517', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f3099f9-21f6-4855-a1b4-11d2d0a79517 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T23:47:25,854453159-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:47:25,860757805-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f3099f9-21f6-4855-a1b4-11d2d0a79517', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f3099f9-21f6-4855-a1b4-11d2d0a79517 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T23:47:28,735117407-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:47:28,741534437-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f3099f9-21f6-4855-a1b4-11d2d0a79517', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f3099f9-21f6-4855-a1b4-11d2d0a79517 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T23:47:31,384460186-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T23:47:54,331427788-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   213 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (5s)
      [=========...................] (remaining: 10s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:48:02,263466424-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:48:10,211475748-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:48:18,250857157-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:48:26,514173126-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:48:34,392733907-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:48:42,306458095-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:48:50,312888372-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:48:50,324229984-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:48:58,483058419-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:48:58,494200348-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:49:06,417669656-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:49:06,429068953-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:49:14,376184791-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:49:14,387711940-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:49:22,319588377-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:49:22,331121947-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:49:22,339633693-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T23:49:22,344605787-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T23:49:22,355911439-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2340171
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T23:49:22,365609435-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T23:49:22,404993583-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:49:22,411350020-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f3099f9-21f6-4855-a1b4-11d2d0a79517', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f3099f9-21f6-4855-a1b4-11d2d0a79517 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T06:49:23.980+0000 ffffa6582010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T06:49:23.988+0000 ffffa6582010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T06:49:23.988+0000 ffffa6582010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T06:49:24.013715+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-18T06:49:24.013764+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T06:49:24.016066+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T06:49:24.016066+0000     0       0         0         0         0         0           -           0
2021-05-18T06:49:25.016235+0000     1     255     18060     17805   69.5458   69.5508  0.00274159   0.0136279
2021-05-18T06:49:26.016413+0000     2     255     36853     36598   71.4715   73.4102  0.00456928   0.0137383
2021-05-18T06:49:27.016590+0000     3     255     56760     56505   73.5637   77.7617  0.00427659   0.0134643
2021-05-18T06:49:28.016958+0000     4     255     76863     76608   74.7976   78.5273   0.0502517   0.0132924
2021-05-18T06:49:29.017169+0000     5     255     95578     95323   74.4561   73.1055   0.0132428   0.0133943
2021-05-18T06:49:30.017368+0000     6     255    115313    115058   74.8925   77.0898   0.0284708   0.0133193
2021-05-18T06:49:31.017730+0000     7     255    134838    134583   75.0853   76.2695   0.0128279   0.0132916
2021-05-18T06:49:32.018064+0000     8     255    153893    153638   75.0007   74.4336  0.00925277   0.0133089
2021-05-18T06:49:33.018359+0000     9     255    174166    173911   75.4638   79.1914  0.00338315   0.0132113
2021-05-18T06:49:34.018668+0000    10     255    193788    193533   75.5799   76.6484   0.0228536   0.0131907
2021-05-18T06:49:35.018887+0000    11     255    213581    213326   75.7362   77.3164  0.00622026   0.0131557
2021-05-18T06:49:36.019106+0000    12     255    232915    232660   75.7171   75.5234  0.00221137   0.0131729
2021-05-18T06:49:37.019289+0000    13     255    252612    252357   75.8102   76.9414    0.049634   0.0131585
2021-05-18T06:49:38.019539+0000    14     255    271798    271543   75.7471   74.9453  0.00430265   0.0131679
2021-05-18T06:49:39.019717+0000    15     255    289708    289453   75.3605   69.9609   0.0114826   0.0132538
2021-05-18T06:49:40.019959+0000    16     255    307992    307737   75.1133   71.4219  0.00733067   0.0132991
2021-05-18T06:49:41.020162+0000    17     255    326440    326185    74.933   72.0625   0.0192449   0.0133255
2021-05-18T06:49:42.020405+0000    18     255    344946    344691   74.7851   72.2891  0.00727417   0.0133526
2021-05-18T06:49:43.022780+0000    19     255    363454    363199   74.6448   72.2969  0.00695571   0.0133813
2021-05-18T06:49:44.022929+0000 min lat: 0.000705104 max lat: 0.141506 avg lat: 0.0134197
2021-05-18T06:49:44.022929+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T06:49:44.022929+0000    20     102    381178    381076   74.4037    69.832  0.00984274   0.0134197
2021-05-18T06:49:45.023179+0000 Total time run:         20.0474
Total writes made:      381178
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     74.2728
Stddev Bandwidth:       3.02222
Max bandwidth (MB/sec): 79.1914
Min bandwidth (MB/sec): 69.5508
Average IOPS:           19013
Stddev IOPS:            773.688
Max IOPS:               20273
Min IOPS:               17805
Average Latency(s):     0.0134325
Stddev Latency(s):      0.0177074
Max latency(s):         0.141506
Min latency(s):         0.000705104

[1;32mlocalhost.localdomain	[2021-05-17T23:49:45,330945003-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2340171


[1;33mlocalhost.localdomain	[2021-05-17T23:49:45,336491049-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T23:50:08,261734095-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 381.18k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:50:08,273520985-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:50:16,464046549-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 381.18k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:50:16,475965761-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:50:24,507760883-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 381.18k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:50:24,519589166-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:50:32,321082927-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 381.18k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:50:32,332918036-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:50:40,757986212-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 381.18k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:50:40,769686869-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:50:40,778218525-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T23:50:40,783526031-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T23:50:40,795154225-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2343480
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T23:50:40,807585580-07:00] INFO: > Run rados bench[0m
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.1
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
[1;30mlocalhost.localdomain	[2021-05-17T23:50:40,846137892-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:50:40,852534417-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f3099f9-21f6-4855-a1b4-11d2d0a79517', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f3099f9-21f6-4855-a1b4-11d2d0a79517 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ttJ8sg:/tmp/ceph-asok.ttJ8sg -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T06:50:42.317+0000 ffffb7e5d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T06:50:42.325+0000 ffffb7e5d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T06:50:42.325+0000 ffffb7e5d010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T06:50:42.343271+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T06:50:42.343271+0000     0       0         0         0         0         0           -           0
2021-05-18T06:50:43.343441+0000     1     255     26977     26722   104.353   104.383   0.0120411  0.00949083
2021-05-18T06:50:44.343608+0000     2     255     51583     51328   100.227   96.1172  0.00796621   0.0099371
2021-05-18T06:50:45.343795+0000     3     255     80779     80524   104.827   114.047  0.00783083  0.00951167
2021-05-18T06:50:46.343977+0000     4     255    110767    110512     107.9   117.141  0.00804095  0.00924527
2021-05-18T06:50:47.344158+0000     5     255    144124    143869   112.375   130.301  0.00476967  0.00887892
2021-05-18T06:50:48.344340+0000     6     255    177359    177104   115.279   129.824  0.00444953  0.00865674
2021-05-18T06:50:49.344520+0000     7     256    203125    202869   113.186   100.645 0.000318579  0.00877155
2021-05-18T06:50:50.344709+0000     8     255    227105    226850   110.745   93.6758  0.00101069  0.00892423
2021-05-18T06:50:51.344920+0000     9     255    254288    254033   110.236   106.184   0.0199793  0.00905406
2021-05-18T06:50:52.345114+0000    10     255    284773    284518   111.118   119.082  0.00498633   0.0089852
2021-05-18T06:50:53.345274+0000    11     256    310298    310042   110.079   99.7031 0.000553998  0.00905664
2021-05-18T06:50:54.345442+0000    12     255    331458    331203   107.793   82.6602   0.0073597  0.00926277
2021-05-18T06:50:55.345630+0000    13     255    354959    354704   106.561   91.8008   0.0555553  0.00933855
2021-05-18T06:50:56.345872+0000 Total time run:       13.9244
Total reads made:     381178
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   106.933
Average IOPS:         27374
Stddev IOPS:          3754.16
Max IOPS:             33357
Min IOPS:             21161
Average Latency(s):   0.00934165
Max latency(s):       0.202451
Min latency(s):       0.000300951

[1;32mlocalhost.localdomain	[2021-05-17T23:50:56,815354963-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2343480


[1;33mlocalhost.localdomain	[2021-05-17T23:50:56,822146205-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T23:51:20,515769155-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 381.18k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:51:20,529683451-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:51:28,561876193-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 381.18k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:51:28,575719903-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:51:36,487225110-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 381.18k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:51:36,501370611-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:51:44,730303249-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 381.18k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:51:44,744342007-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:51:53,049842667-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 381.18k objects, 1.5 GiB
    usage:   2.9 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:51:53,063693774-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:51:53,074299622-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T23:51:53,078242859-07:00][RUNNING][ROUND 2/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T23:51:53,084650817-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T23:51:53,100925344-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40688\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.686541\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b06685de-26b6-44cd-8e95-7c9243b4a887\nsetting min_mon_release = octopus\nepoch 0\nfsid b06685de-26b6-44cd-8e95-7c9243b4a887\nlast_changed 2021-05-17T23:52:10.992910-0700\ncreated 2021-05-17T23:52:10.992910-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40688/0,v1:10.10.1.2:40689/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.686541 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 67066770-a0c1-43d5-be26-25dae18910cf\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 fa22648c-be3d-4546-9f3f-8644d44b4733\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 041a692c-2e19-4c56-97c5-ba05058f290f\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42688\n  w/ user/pass: admin / bfe86861-0414-42b6-bed9-88bf79764969\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 23:52:25 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40688
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.686541
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b06685de-26b6-44cd-8e95-7c9243b4a887
setting min_mon_release = octopus
epoch 0
fsid b06685de-26b6-44cd-8e95-7c9243b4a887
last_changed 2021-05-17T23:52:10.992910-0700
created 2021-05-17T23:52:10.992910-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40688/0,v1:10.10.1.2:40689/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.686541 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 67066770-a0c1-43d5-be26-25dae18910cf
0
start osd.0
add osd1 fa22648c-be3d-4546-9f3f-8644d44b4733
1
start osd.1
add osd2 041a692c-2e19-4c56-97c5-ba05058f290f
2
start osd.2


restful urls: https://10.10.1.2:42688
  w/ user/pass: admin / bfe86861-0414-42b6-bed9-88bf79764969


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T23:51:54.803-0700 7f56935f91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:51:54.803-0700 7f56935f91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:51:54.823-0700 7fc61a3131c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:51:54.823-0700 7fc61a3131c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd ceph-mon still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40688,v1:10.10.1.2:40689] --print /tmp/ceph_monmap.686541 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.686541 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.686541 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42688 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.kw0vSqNJ41 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 67066770-a0c1-43d5-be26-25dae18910cf -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAjZKNgCSmhOhAAej9G0EC4V3J/oSBr8tr0lA== --osd-uuid 67066770-a0c1-43d5-be26-25dae18910cf 
2021-05-17T23:52:20.351-0700 7ff98a0a9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T23:52:20.355-0700 7ff98a0a9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T23:52:20.355-0700 7ff98a0a9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T23:52:20.395-0700 7ff98a0a9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new fa22648c-be3d-4546-9f3f-8644d44b4733 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T23:52:20.647-0700 7ff528745f00 -1 Falling back to public interface
2021-05-17T23:52:20.663-0700 7ff528745f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAkZKNg4HOMJhAAyU4cONLYsGOX6t2QMRVUQA== --osd-uuid fa22648c-be3d-4546-9f3f-8644d44b4733 
2021-05-17T23:52:20.979-0700 7fc25f2faf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T23:52:20.979-0700 7fc25f2faf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T23:52:20.979-0700 7fc25f2faf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T23:52:21.023-0700 7fc25f2faf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 041a692c-2e19-4c56-97c5-ba05058f290f -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T23:52:21.331-0700 7f6ca9094f00 -1 Falling back to public interface
2021-05-17T23:52:21.347-0700 7f6ca9094f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAlZKNgrd0DExAA021IHn8LAQ22NNy/9wKJkg== --osd-uuid 041a692c-2e19-4c56-97c5-ba05058f290f 
2021-05-17T23:52:21.667-0700 7fec97f09f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T23:52:21.667-0700 7fec97f09f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T23:52:21.667-0700 7fec97f09f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T23:52:21.755-0700 7fec97f09f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T23:52:22.007-0700 7fb065132f00 -1 Falling back to public interface
2021-05-17T23:52:22.023-0700 7fb065132f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T23:52:25,980041874-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T23:52:25,986273088-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T23:52:26,069109450-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:52:26,075531497-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e657430b-e889-4a95-be6b-e83046ad68d8', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e657430b-e889-4a95-be6b-e83046ad68d8 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T23:52:28,772686273-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:52:28,779081359-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e657430b-e889-4a95-be6b-e83046ad68d8', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e657430b-e889-4a95-be6b-e83046ad68d8 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T23:52:31,566322716-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:52:31,572664664-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e657430b-e889-4a95-be6b-e83046ad68d8', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e657430b-e889-4a95-be6b-e83046ad68d8 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T23:52:34,381547978-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:52:34,389699667-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e657430b-e889-4a95-be6b-e83046ad68d8', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e657430b-e889-4a95-be6b-e83046ad68d8 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T23:52:40,049855944-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:52:40,056223306-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e657430b-e889-4a95-be6b-e83046ad68d8', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e657430b-e889-4a95-be6b-e83046ad68d8 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T23:52:43,925592153-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:52:43,931905432-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e657430b-e889-4a95-be6b-e83046ad68d8', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e657430b-e889-4a95-be6b-e83046ad68d8 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T23:52:47,464534291-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:52:47,471114690-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e657430b-e889-4a95-be6b-e83046ad68d8', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e657430b-e889-4a95-be6b-e83046ad68d8 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T23:52:50,548044660-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:52:50,554490756-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e657430b-e889-4a95-be6b-e83046ad68d8', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e657430b-e889-4a95-be6b-e83046ad68d8 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T23:52:53,997407423-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:52:54,003933257-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e657430b-e889-4a95-be6b-e83046ad68d8', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e657430b-e889-4a95-be6b-e83046ad68d8 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T23:52:57,276793366-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:52:57,283469256-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e657430b-e889-4a95-be6b-e83046ad68d8', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e657430b-e889-4a95-be6b-e83046ad68d8 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T23:53:00,543165154-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:53:00,549498911-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e657430b-e889-4a95-be6b-e83046ad68d8', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e657430b-e889-4a95-be6b-e83046ad68d8 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T23:53:03,227023194-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:53:03,233499054-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e657430b-e889-4a95-be6b-e83046ad68d8', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e657430b-e889-4a95-be6b-e83046ad68d8 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T23:53:05,875227114-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T23:53:28,823047197-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   202 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:53:36,844274699-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:53:44,749842698-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:53:52,793324698-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:54:00,835699448-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 59s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:54:08,737487215-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 77s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:54:16,581329173-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 88s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:54:24,863716044-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   233 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 43s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:54:32,824942502-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:54:32,840851767-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:54:40,790786235-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:54:40,805100355-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:54:48,779579567-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:54:48,793370429-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:54:56,844532449-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:54:56,859080894-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:55:04,920768562-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:55:04,935319573-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:55:04,946037164-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T23:55:04,952705164-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T23:55:04,967052830-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2356872
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T23:55:04,979107433-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T23:55:05,018872376-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:55:05,025243641-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e657430b-e889-4a95-be6b-e83046ad68d8', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e657430b-e889-4a95-be6b-e83046ad68d8 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T06:55:06.538+0000 ffffaeea2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T06:55:06.546+0000 ffffaeea2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T06:55:06.546+0000 ffffaeea2010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T06:55:06.568666+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-18T06:55:06.568729+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T06:55:06.571087+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T06:55:06.571087+0000     0       0         0         0         0         0           -           0
2021-05-18T06:55:07.571261+0000     1     255     18938     18683   72.9766   72.9805  0.00184521   0.0132753
2021-05-18T06:55:08.571469+0000     2     255     37472     37217     72.68   72.3984    0.112073     0.01336
2021-05-18T06:55:09.571664+0000     3     256     55541     55285   71.9747   70.5781  0.00271091   0.0136466
2021-05-18T06:55:10.571816+0000     4     255     73050     72795   71.0781   68.3984  0.00126935   0.0139314
2021-05-18T06:55:11.572017+0000     5     256     90435     90179   70.4409   67.9062     0.01867   0.0140572
2021-05-18T06:55:12.572191+0000     6     255    107586    107331   69.8655        67  0.00152786   0.0141744
2021-05-18T06:55:13.572439+0000     7     255    124856    124601   69.5196   67.4609  0.00634577   0.0143215
2021-05-18T06:55:14.572729+0000     8     255    142755    142500   69.5669    69.918  0.00125262   0.0143022
2021-05-18T06:55:15.572897+0000     9     255    160080    159825   69.3555   67.6758  0.00186061    0.014368
2021-05-18T06:55:16.573044+0000    10     255    177455    177200    69.206   67.8711  0.00132333   0.0143758
2021-05-18T06:55:17.573249+0000    11     255    194543    194288   68.9815     66.75  0.00138206   0.0144295
2021-05-18T06:55:18.573455+0000    12     256    212593    212337   69.1072   70.5039  0.00109429   0.0144291
2021-05-18T06:55:19.573698+0000    13     255    230155    229900   69.0673   68.6055   0.0036531    0.014422
2021-05-18T06:55:20.573922+0000    14     255    247442    247187   68.9562   67.5273   0.0223232   0.0144696
2021-05-18T06:55:21.574098+0000    15     255    265202    264947   68.9833    69.375   0.0910546   0.0144527
2021-05-18T06:55:22.574331+0000    16     255    282607    282352   68.9201   67.9883  0.00224012   0.0144548
2021-05-18T06:55:23.574503+0000    17     255    299693    299438   68.7914   66.7422   0.0927774   0.0144998
2021-05-18T06:55:24.574678+0000    18     255    316782    316527   68.6775   66.7539   0.0201203   0.0145285
2021-05-18T06:55:25.574843+0000    19     256    333860    333604   68.5732    66.707    0.110604   0.0145364
2021-05-18T06:55:26.575030+0000 min lat: 0.000688602 max lat: 0.12402 avg lat: 0.0145698
2021-05-18T06:55:26.575030+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T06:55:26.575030+0000    20     213    350834    350621   68.4676   66.4727    0.092486   0.0145698
2021-05-18T06:55:27.575289+0000 Total time run:         20.0728
Total writes made:      350834
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     68.2737
Stddev Bandwidth:       1.90412
Max bandwidth (MB/sec): 72.9805
Min bandwidth (MB/sec): 66.4727
Average IOPS:           17478
Stddev IOPS:            487.455
Max IOPS:               18683
Min IOPS:               17017
Average Latency(s):     0.0146059
Stddev Latency(s):      0.0274304
Max latency(s):         0.12402
Min latency(s):         0.000688602

[1;32mlocalhost.localdomain	[2021-05-17T23:55:27,884561389-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2356872


[1;33mlocalhost.localdomain	[2021-05-17T23:55:27,891858551-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T23:55:50,941754192-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.83k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:55:50,956240963-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:55:59,144493288-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.83k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:55:59,159228366-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:56:07,750376708-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.83k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:56:07,764588303-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:56:16,171757523-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.83k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:56:16,186075456-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:56:24,327097318-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.83k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:56:24,343867477-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:56:24,356948120-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T23:56:24,365354510-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T23:56:24,381650589-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2360274
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T23:56:24,395166411-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T23:56:24,435308493-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:56:24,442001158-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e657430b-e889-4a95-be6b-e83046ad68d8', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e657430b-e889-4a95-be6b-e83046ad68d8 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4vlEhn:/tmp/ceph-asok.4vlEhn -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T06:56:25.944+0000 ffffa2b83010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T06:56:25.948+0000 ffffa2b83010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T06:56:25.952+0000 ffffa2b83010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T06:56:25.970174+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T06:56:25.970174+0000     0       0         0         0         0         0           -           0
2021-05-18T06:56:26.970337+0000     1     256     23444     23188   90.5518   90.5781 0.000414611   0.0106584
2021-05-18T06:56:27.970513+0000     2     256     49756     49500   96.6571   102.781 0.000621436   0.0102662
2021-05-18T06:56:28.970695+0000     3     255     75402     75147   97.8265   100.184  0.00892864     0.01019
2021-05-18T06:56:29.970851+0000     4     255    102431    102176   99.7612   105.582  0.00584751  0.00999856
2021-05-18T06:56:30.971013+0000     5     255    131159    130904   102.249   112.219   0.0133246  0.00975739
2021-05-18T06:56:31.971189+0000     6     255    159337    159082   103.549    110.07  0.00759535   0.0096388
2021-05-18T06:56:32.971380+0000     7     255    185033    184778   103.093   100.375   0.0109862  0.00968312
2021-05-18T06:56:33.971553+0000     8     255    212237    211982   103.487   106.266  0.00752522  0.00964755
2021-05-18T06:56:34.971759+0000     9     255    234906    234651   101.826   88.5508  0.00345969  0.00980547
2021-05-18T06:56:35.971957+0000    10     255    262058    261803   102.247   106.062  0.00967994  0.00976491
2021-05-18T06:56:36.972138+0000    11     255    285672    285417   101.336   92.2422  0.00799366  0.00985339
2021-05-18T06:56:37.972731+0000    12     255    310595    310340       101   97.3555  0.00704839  0.00988831
2021-05-18T06:56:38.972937+0000    13     256    336891    336635    101.13   102.715  0.00268453   0.0098351
2021-05-18T06:56:39.973185+0000 Total time run:       13.5501
Total reads made:     350834
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   101.139
Average IOPS:         25891
Stddev IOPS:          1873.84
Max IOPS:             28728
Min IOPS:             22669
Average Latency(s):   0.00987476
Max latency(s):       0.153824
Min latency(s):       0.000313973

[1;32mlocalhost.localdomain	[2021-05-17T23:56:40,282698922-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2360274


[1;33mlocalhost.localdomain	[2021-05-17T23:56:40,289698584-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T23:57:03,599815426-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.83k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:57:03,614458795-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:57:12,024069820-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.83k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:57:12,039316201-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:57:20,256253243-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.83k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:57:20,270837382-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:57:28,346417825-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.83k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:57:28,360942509-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:57:36,524816685-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.83k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:57:36,538935776-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T23:57:36,550279531-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T23:57:36,554390738-07:00][RUNNING][ROUND 3/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T23:57:36,560925217-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T23:57:36,577137688-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40245\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.687710\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 16dbd035-6569-4218-9331-f5635ed00202\nsetting min_mon_release = octopus\nepoch 0\nfsid 16dbd035-6569-4218-9331-f5635ed00202\nlast_changed 2021-05-17T23:58:01.331763-0700\ncreated 2021-05-17T23:58:01.331763-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40245/0,v1:10.10.1.2:40246/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.687710 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 a662d547-89ce-469f-97fc-97ad79803e01\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 d401ac55-e201-49b7-ba0e-ee1fe45b98fc\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 e17b7a4c-2075-41bb-8d99-1f18c0aee341\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42245\n  w/ user/pass: admin / 7f3025bf-f63b-43a5-a110-7a1323a2da83\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 23:58:16 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40245
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.687710
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 16dbd035-6569-4218-9331-f5635ed00202
setting min_mon_release = octopus
epoch 0
fsid 16dbd035-6569-4218-9331-f5635ed00202
last_changed 2021-05-17T23:58:01.331763-0700
created 2021-05-17T23:58:01.331763-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40245/0,v1:10.10.1.2:40246/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.687710 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 a662d547-89ce-469f-97fc-97ad79803e01
0
start osd.0
add osd1 d401ac55-e201-49b7-ba0e-ee1fe45b98fc
1
start osd.1
add osd2 e17b7a4c-2075-41bb-8d99-1f18c0aee341
2
start osd.2


restful urls: https://10.10.1.2:42245
  w/ user/pass: admin / 7f3025bf-f63b-43a5-a110-7a1323a2da83


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T23:57:37.586-0700 7f3444b7a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:57:37.586-0700 7f3444b7a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:57:37.602-0700 7f0a7a3061c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:57:37.602-0700 7f0a7a3061c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40245,v1:10.10.1.2:40246] --print /tmp/ceph_monmap.687710 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.687710 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.687710 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42245 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.DLFCK4Txoc 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a662d547-89ce-469f-97fc-97ad79803e01 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCCZaNgsKmXDhAA/77szLG2MjsagpnLLbl/rw== --osd-uuid a662d547-89ce-469f-97fc-97ad79803e01 
2021-05-17T23:58:10.618-0700 7f12d424bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T23:58:10.622-0700 7f12d424bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T23:58:10.622-0700 7f12d424bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T23:58:10.666-0700 7f12d424bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d401ac55-e201-49b7-ba0e-ee1fe45b98fc -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T23:58:10.918-0700 7f7edea33f00 -1 Falling back to public interface
2021-05-17T23:58:10.930-0700 7f7edea33f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCCZaNgV6+1NhAAl72sSYky4slWCui7uluJJQ== --osd-uuid d401ac55-e201-49b7-ba0e-ee1fe45b98fc 
2021-05-17T23:58:11.274-0700 7fb82059ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T23:58:11.274-0700 7fb82059ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T23:58:11.274-0700 7fb82059ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T23:58:11.334-0700 7fb82059ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e17b7a4c-2075-41bb-8d99-1f18c0aee341 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T23:58:11.714-0700 7f4620ffff00 -1 Falling back to public interface
2021-05-17T23:58:11.726-0700 7f4620ffff00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCDZaNg7KyWKhAAIQqImAuybCRrekv8muNB9Q== --osd-uuid e17b7a4c-2075-41bb-8d99-1f18c0aee341 
2021-05-17T23:58:12.042-0700 7f92fb514f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T23:58:12.042-0700 7f92fb514f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T23:58:12.042-0700 7f92fb514f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T23:58:12.154-0700 7f92fb514f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T23:58:12.406-0700 7fe5f6b7df00 -1 Falling back to public interface
2021-05-17T23:58:12.422-0700 7fe5f6b7df00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T23:58:16,410924544-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T23:58:16,417778712-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T23:58:16,503585460-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:58:16,510160581-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ab38bee-681a-42ce-bd08-a74f65174602', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ab38bee-681a-42ce-bd08-a74f65174602 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T23:58:19,435367706-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:58:19,441723925-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ab38bee-681a-42ce-bd08-a74f65174602', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ab38bee-681a-42ce-bd08-a74f65174602 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T23:58:22,184460417-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:58:22,190901355-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ab38bee-681a-42ce-bd08-a74f65174602', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ab38bee-681a-42ce-bd08-a74f65174602 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T23:58:25,048913759-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:58:25,055403410-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ab38bee-681a-42ce-bd08-a74f65174602', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ab38bee-681a-42ce-bd08-a74f65174602 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T23:58:30,583837367-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:58:30,590357563-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ab38bee-681a-42ce-bd08-a74f65174602', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ab38bee-681a-42ce-bd08-a74f65174602 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T23:58:34,682400741-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:58:34,688966620-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ab38bee-681a-42ce-bd08-a74f65174602', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ab38bee-681a-42ce-bd08-a74f65174602 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T23:58:37,766371801-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:58:37,772885532-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ab38bee-681a-42ce-bd08-a74f65174602', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ab38bee-681a-42ce-bd08-a74f65174602 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T23:58:40,953085636-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:58:40,959610048-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ab38bee-681a-42ce-bd08-a74f65174602', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ab38bee-681a-42ce-bd08-a74f65174602 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T23:58:44,390424147-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:58:44,396945454-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ab38bee-681a-42ce-bd08-a74f65174602', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ab38bee-681a-42ce-bd08-a74f65174602 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T23:58:47,835701388-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:58:47,841983645-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ab38bee-681a-42ce-bd08-a74f65174602', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ab38bee-681a-42ce-bd08-a74f65174602 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T23:58:51,108378108-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:58:51,115015880-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ab38bee-681a-42ce-bd08-a74f65174602', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ab38bee-681a-42ce-bd08-a74f65174602 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T23:58:53,860939409-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T23:58:53,867410577-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ab38bee-681a-42ce-bd08-a74f65174602', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ab38bee-681a-42ce-bd08-a74f65174602 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T23:58:56,700365823-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T23:59:19,714266421-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   196 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:59:27,664954716-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:59:35,789825916-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:59:43,768400617-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:59:51,957856675-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (30s)
      [========....................] (remaining: 72s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T23:59:59,861986992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 75s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:00:08,164074167-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (45s)
      [=========...................] (remaining: 94s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:00:16,093475805-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 41s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:00:24,189616445-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:00:24,203998622-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:00:32,039138550-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:00:32,053607367-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:00:40,159859219-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:00:40,174253594-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:00:48,360272999-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:00:48,376512207-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:00:56,485597804-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:00:56,500229684-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:00:56,511407243-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:00:56,518219540-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:00:56,532901484-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2373947
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-18T00:00:56,544888160-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-18T00:00:56,584811336-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:00:56,591166926-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ab38bee-681a-42ce-bd08-a74f65174602', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ab38bee-681a-42ce-bd08-a74f65174602 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:00:58.086+0000 ffffab296010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:00:58.094+0000 ffffab296010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:00:58.094+0000 ffffab296010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:00:58.107204+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-18T07:00:58.107244+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T07:00:58.109484+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:00:58.109484+0000     0       0         0         0         0         0           -           0
2021-05-18T07:00:59.109653+0000     1     255     17506     17251   67.3827   67.3867   0.0691895   0.0144495
2021-05-18T07:01:00.109812+0000     2     255     36091     35836   69.9845   72.5977    0.118985   0.0138273
2021-05-18T07:01:01.109975+0000     3     255     53971     53716   69.9338   69.8438  0.00322726   0.0140329
2021-05-18T07:01:02.110170+0000     4     255     71932     71677    69.987   70.1602   0.0131879   0.0141913
2021-05-18T07:01:03.110382+0000     5     255     91090     90835   70.9537   74.8359  0.00576309   0.0139891
2021-05-18T07:01:04.110545+0000     6     255    110224    109969   71.5831   74.7422  0.00154211   0.0138686
2021-05-18T07:01:05.110800+0000     7     255    127763    127508   71.1418   68.5117  0.00202135   0.0139142
2021-05-18T07:01:06.111029+0000     8     255    145284    145029   70.8022   68.4414   0.0674699   0.0140521
2021-05-18T07:01:07.111261+0000     9     256    163188    162932   70.7039   69.9336   0.0850812   0.0140749
2021-05-18T07:01:08.111464+0000    10     255    180932    180677   70.5638   69.3164   0.0327221   0.0141029
2021-05-18T07:01:09.111673+0000    11     255    198832    198577   70.5041   69.9219  0.00809699   0.0141389
2021-05-18T07:01:10.111885+0000    12     255    216183    215928   70.2756   67.7773   0.0422153   0.0141996
2021-05-18T07:01:11.112207+0000    13     255    232819    232564    69.867   64.9844  0.00168879   0.0142762
2021-05-18T07:01:12.112472+0000    14     256    250227    249971    69.732   67.9961   0.0444414   0.0142804
2021-05-18T07:01:13.112632+0000    15     256    267428    267172    69.562   67.1914   0.0258771   0.0143339
2021-05-18T07:01:14.112807+0000    16     255    283912    283657   69.2383   64.3945  0.00271041   0.0144094
2021-05-18T07:01:15.113038+0000    17     255    300466    300211   68.9683   64.6641   0.0271824   0.0144683
2021-05-18T07:01:16.113262+0000    18     256    316128    315872   68.5346   61.1758  0.00503895   0.0145632
2021-05-18T07:01:17.113421+0000    19     256    333172    332916   68.4311   66.5781   0.0042065   0.0145544
2021-05-18T07:01:18.113576+0000 min lat: 0.000684747 max lat: 0.227363 avg lat: 0.0147054
2021-05-18T07:01:18.113576+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:01:18.113576+0000    20     180    347275    347095   67.7785   55.3867  0.00555466   0.0147054
2021-05-18T07:01:19.113827+0000 Total time run:         20.0676
Total writes made:      347275
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     67.5985
Stddev Bandwidth:       4.42562
Max bandwidth (MB/sec): 74.8359
Min bandwidth (MB/sec): 55.3867
Average IOPS:           17305
Stddev IOPS:            1132.96
Max IOPS:               19158
Min IOPS:               14179
Average Latency(s):     0.0147508
Stddev Latency(s):      0.024409
Max latency(s):         0.227363
Min latency(s):         0.000684747

[1;32mlocalhost.localdomain	[2021-05-18T00:01:19,428041189-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2373947


[1;33mlocalhost.localdomain	[2021-05-18T00:01:19,435108649-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:01:42,785768687-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 347.28k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:01:42,800086069-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:01:50,992545954-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 347.28k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:01:51,007006896-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:01:58,928544767-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 347.28k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:01:58,942927456-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:02:06,963823294-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 347.28k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:02:06,978164531-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:02:15,598374360-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 347.28k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:02:15,612653111-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:02:15,624068701-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:02:15,630998603-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:02:15,646109606-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2377259
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-18T00:02:15,658910546-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-18T00:02:15,697613327-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:02:15,704127104-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ab38bee-681a-42ce-bd08-a74f65174602', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ab38bee-681a-42ce-bd08-a74f65174602 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vyAcxe:/tmp/ceph-asok.vyAcxe -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:02:17.219+0000 ffffa31ec010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:02:17.227+0000 ffffa31ec010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:02:17.227+0000 ffffa31ec010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:02:17.247548+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:02:17.247548+0000     0       0         0         0         0         0           -           0
2021-05-18T07:02:18.247710+0000     1     255     25511     25256   98.6263   98.6562    0.010009   0.0100501
2021-05-18T07:02:19.247889+0000     2     255     47545     47290   92.3409   86.0703  0.00909241   0.0107768
2021-05-18T07:02:20.248149+0000     3     255     73048     72793   94.7591   99.6211   0.0111445   0.0105142
2021-05-18T07:02:21.248407+0000     4     255     94638     94383   92.1478   84.3359  0.00309839   0.0108152
2021-05-18T07:02:22.248950+0000     5     256    122863    122607   95.7571    110.25  0.00168842   0.0104134
2021-05-18T07:02:23.249239+0000     6     256    147439    147183    95.793        96   0.0093597   0.0104157
2021-05-18T07:02:24.249436+0000     7     255    172113    171858   95.8751   96.3867  0.00670207   0.0104102
2021-05-18T07:02:25.249663+0000     8     255    194933    194678   95.0308   89.1406  0.00735194   0.0105058
2021-05-18T07:02:26.249964+0000     9     255    221723    221468   96.0959   104.648   0.0105867   0.0103894
2021-05-18T07:02:27.250258+0000    10     255    246683    246428   96.2335      97.5  0.00722571   0.0103763
2021-05-18T07:02:28.250537+0000    11     255    272480    272225   96.6433    100.77   0.0129932   0.0103315
2021-05-18T07:02:29.250725+0000    12     255    298484    298229   97.0529   101.578  0.00352929   0.0102893
2021-05-18T07:02:30.250945+0000    13     255    324437    324182   97.3839   101.379  0.00872568   0.0102543
2021-05-18T07:02:31.251247+0000 Total time run:       13.8028
Total reads made:     347275
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   98.2805
Average IOPS:         25159
Stddev IOPS:          1868.19
Max IOPS:             28224
Min IOPS:             21590
Average Latency(s):   0.010161
Max latency(s):       0.180251
Min latency(s):       0.00029891

[1;32mlocalhost.localdomain	[2021-05-18T00:02:31,580518318-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2377259


[1;33mlocalhost.localdomain	[2021-05-18T00:02:31,587630952-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:02:54,507298964-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 347.28k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:02:54,521734207-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:03:02,977320353-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 347.28k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:03:02,991662460-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:03:11,535154817-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 347.28k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:03:11,549136473-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:03:19,523994638-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 347.28k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:03:19,538907788-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:03:27,677853636-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 347.28k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:03:27,692042207-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:03:27,703212206-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T00:03:27,707206650-07:00][RUNNING][ROUND 4/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:03:27,713782402-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T00:03:27,730515346-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40198\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.688983\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 020fa4aa-1291-41df-9e9a-bdad21248efd\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 020fa4aa-1291-41df-9e9a-bdad21248efd\nlast_changed 2021-05-18T00:03:53.198634-0700\ncreated 2021-05-18T00:03:53.198634-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40198/0,v1:10.10.1.2:40199/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.688983 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 8d9f4d90-6963-45f0-abf3-de22e1a788d9\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 7988f464-47b2-42a4-9720-b0e83755dd79\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 66177609-9644-429f-9621-839503c6ce3c\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42198\n  w/ user/pass: admin / c2a053e0-8355-4598-8d70-204c2c2fef43\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:04:08 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40198
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.688983
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 020fa4aa-1291-41df-9e9a-bdad21248efd
setting min_mon_release = octopus
epoch 0
fsid 020fa4aa-1291-41df-9e9a-bdad21248efd
last_changed 2021-05-18T00:03:53.198634-0700
created 2021-05-18T00:03:53.198634-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40198/0,v1:10.10.1.2:40199/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.688983 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 8d9f4d90-6963-45f0-abf3-de22e1a788d9
0
start osd.0
add osd1 7988f464-47b2-42a4-9720-b0e83755dd79
1
start osd.1
add osd2 66177609-9644-429f-9621-839503c6ce3c
2
start osd.2


restful urls: https://10.10.1.2:42198
  w/ user/pass: admin / c2a053e0-8355-4598-8d70-204c2c2fef43


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T00:03:29.149-0700 7f1202aea1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:03:29.149-0700 7f1202aea1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:03:29.165-0700 7f05dac881c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:03:29.165-0700 7f05dac881c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40198,v1:10.10.1.2:40199] --print /tmp/ceph_monmap.688983 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.688983 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.688983 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42198 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.uaSIuSohwd 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8d9f4d90-6963-45f0-abf3-de22e1a788d9 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDiZqNgNKNuNRAAxunaUvYavh/XbRbUMvZ5eQ== --osd-uuid 8d9f4d90-6963-45f0-abf3-de22e1a788d9 
2021-05-18T00:04:03.237-0700 7ff631c03f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:04:03.237-0700 7ff631c03f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:04:03.237-0700 7ff631c03f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:04:03.305-0700 7ff631c03f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7988f464-47b2-42a4-9720-b0e83755dd79 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T00:04:03.549-0700 7f29a5a98f00 -1 Falling back to public interface
2021-05-18T00:04:03.565-0700 7f29a5a98f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDjZqNgKrH6IBAA/Hmy6g1dBC2+1vUCYKCzeQ== --osd-uuid 7988f464-47b2-42a4-9720-b0e83755dd79 
2021-05-18T00:04:03.905-0700 7f1e8b275f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:04:03.905-0700 7f1e8b275f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:04:03.905-0700 7f1e8b275f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:04:03.985-0700 7f1e8b275f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 66177609-9644-429f-9621-839503c6ce3c -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T00:04:04.309-0700 7fe0ddc6cf00 -1 Falling back to public interface
2021-05-18T00:04:04.321-0700 7fe0ddc6cf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDkZqNgD5egEBAAHMUpGuQDNNkoQkGoHdFRcQ== --osd-uuid 66177609-9644-429f-9621-839503c6ce3c 
2021-05-18T00:04:04.625-0700 7f848009af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:04:04.625-0700 7f848009af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:04:04.625-0700 7f848009af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:04:04.665-0700 7f848009af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T00:04:04.993-0700 7f3ead823f00 -1 Falling back to public interface
2021-05-18T00:04:05.005-0700 7f3ead823f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T00:04:08,934828841-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:04:08,941819624-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T00:04:09,026517572-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:04:09,032874272-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd2234f9-e4bb-430a-b8fa-5560effb6a0f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd2234f9-e4bb-430a-b8fa-5560effb6a0f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T00:04:11,867215154-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:04:11,873691740-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd2234f9-e4bb-430a-b8fa-5560effb6a0f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd2234f9-e4bb-430a-b8fa-5560effb6a0f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T00:04:14,806467405-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:04:14,812914136-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd2234f9-e4bb-430a-b8fa-5560effb6a0f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd2234f9-e4bb-430a-b8fa-5560effb6a0f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T00:04:17,439610999-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:04:17,446110524-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd2234f9-e4bb-430a-b8fa-5560effb6a0f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd2234f9-e4bb-430a-b8fa-5560effb6a0f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T00:04:23,171422610-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:04:23,178045726-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd2234f9-e4bb-430a-b8fa-5560effb6a0f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd2234f9-e4bb-430a-b8fa-5560effb6a0f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T00:04:26,805309871-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:04:26,811795279-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd2234f9-e4bb-430a-b8fa-5560effb6a0f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd2234f9-e4bb-430a-b8fa-5560effb6a0f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T00:04:30,602927489-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:04:30,609533592-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd2234f9-e4bb-430a-b8fa-5560effb6a0f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd2234f9-e4bb-430a-b8fa-5560effb6a0f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T00:04:33,850004485-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:04:33,856545957-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd2234f9-e4bb-430a-b8fa-5560effb6a0f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd2234f9-e4bb-430a-b8fa-5560effb6a0f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:04:37,524973566-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:04:37,531480097-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd2234f9-e4bb-430a-b8fa-5560effb6a0f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd2234f9-e4bb-430a-b8fa-5560effb6a0f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:04:40,944393964-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:04:40,950903345-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd2234f9-e4bb-430a-b8fa-5560effb6a0f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd2234f9-e4bb-430a-b8fa-5560effb6a0f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T00:04:44,272124644-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:04:44,278550822-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd2234f9-e4bb-430a-b8fa-5560effb6a0f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd2234f9-e4bb-430a-b8fa-5560effb6a0f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T00:04:47,147920582-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:04:47,154144840-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd2234f9-e4bb-430a-b8fa-5560effb6a0f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd2234f9-e4bb-430a-b8fa-5560effb6a0f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  146 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  146 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.99   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.05   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.97   70      up          osd.2  
                       TOTAL  300 GiB  148 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.97/1.05  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T00:04:49,732824479-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:05:12,505937752-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=========...................] (remaining: 9s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:05:20,561065159-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [===================.........] (remaining: 4s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:05:28,517914996-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:05:36,467109737-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:05:44,330420153-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:05:52,432788537-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:06:00,419093180-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:06:08,320600626-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:06:08,335168854-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:06:16,280302051-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:06:16,296609539-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:06:24,227562906-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:06:24,242116371-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:06:32,331294128-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:06:32,345691836-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:06:40,287275315-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:06:40,302045132-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:06:40,313528085-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:06:40,320482413-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:06:40,335877371-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2390641
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-18T00:06:40,348286506-07:00] INFO: > Run rados bench[0m
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.4
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mlocalhost.localdomain	[2021-05-18T00:06:40,388022954-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:06:40,394516898-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd2234f9-e4bb-430a-b8fa-5560effb6a0f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd2234f9-e4bb-430a-b8fa-5560effb6a0f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:06:41.992+0000 ffffb09c5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:06:42.000+0000 ffffb09c5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:06:42.000+0000 ffffb09c5010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:06:42.015621+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-18T07:06:42.015666+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T07:06:42.017846+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:06:42.017846+0000     0       0         0         0         0         0           -           0
2021-05-18T07:06:43.018019+0000     1     255     20042     19787   77.2891    77.293  0.00193674   0.0124736
2021-05-18T07:06:44.018188+0000     2     255     38578     38323   74.8414   72.4062  0.00434925    0.013175
2021-05-18T07:06:45.018348+0000     3     255     57457     57202   74.4723   73.7461  0.00135908   0.0131241
2021-05-18T07:06:46.018497+0000     4     255     75721     75466   73.6875   71.3438  0.00115054   0.0134416
2021-05-18T07:06:47.018667+0000     5     255     94286     94031   73.4515   72.5195  0.00313051   0.0135345
2021-05-18T07:06:48.018833+0000     6     256    110090    109834   71.4962   61.7305 0.000718427   0.0137869
2021-05-18T07:06:49.018990+0000     7     255    125355    125100   69.8001   59.6328   0.0110876   0.0142962
2021-05-18T07:06:50.019150+0000     8     255    143611    143356   69.9877   71.3125  0.00450062    0.014252
2021-05-18T07:06:51.019306+0000     9     255    161808    161553   70.1081    71.082    0.010354   0.0142379
2021-05-18T07:06:52.019476+0000    10     255    180166    179911   70.2671   71.7109  0.00931462   0.0141759
2021-05-18T07:06:53.019672+0000    11     256    198049    197793   70.2281   69.8516  0.00117569   0.0141814
2021-05-18T07:06:54.019847+0000    12     256    212037    211781   68.9283   54.6406 0.000681401    0.014406
2021-05-18T07:06:55.020024+0000    13     255    229272    229017   68.8043   67.3281   0.0165531   0.0145164
2021-05-18T07:06:56.020366+0000    14     255    246780    246525   68.7731   68.3906  0.00518638   0.0145248
2021-05-18T07:06:57.020606+0000    15     255    263455    263200   68.5296   65.1367   0.0110272   0.0145741
2021-05-18T07:06:58.020820+0000    16     255    279183    278928   68.0855   61.4375  0.00826699   0.0146725
2021-05-18T07:06:59.021039+0000    17     255    293130    292875   67.2845   54.4805   0.0122557   0.0148486
2021-05-18T07:07:00.021235+0000    18     255    306623    306368    66.474    52.707  0.00308172   0.0150158
2021-05-18T07:07:01.021444+0000    19     255    321507    321252   66.0348   58.1406  0.00700676    0.015128
2021-05-18T07:07:02.021662+0000 min lat: 0.000677815 max lat: 0.302147 avg lat: 0.0151619
2021-05-18T07:07:02.021662+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:07:02.021662+0000    20     117    337550    337433   65.8927    63.207  0.00680454   0.0151619
2021-05-18T07:07:03.021924+0000 Total time run:         20.035
Total writes made:      337550
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     65.8127
Stddev Bandwidth:       7.24304
Max bandwidth (MB/sec): 77.293
Min bandwidth (MB/sec): 52.707
Average IOPS:           16848
Stddev IOPS:            1854.22
Max IOPS:               19787
Min IOPS:               13493
Average Latency(s):     0.0151671
Stddev Latency(s):      0.0261116
Max latency(s):         0.302147
Min latency(s):         0.000677815

[1;32mlocalhost.localdomain	[2021-05-18T00:07:03,309060303-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2390641


[1;33mlocalhost.localdomain	[2021-05-18T00:07:03,316237764-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:07:26,230007257-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 337.55k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:07:26,244323487-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:07:34,710492266-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 337.55k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:07:34,725082428-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:07:43,198981298-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 337.55k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:07:43,213142176-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:07:51,130205356-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 337.55k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:07:51,146598269-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:07:59,422010758-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 337.55k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:07:59,436512765-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:07:59,447745056-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:07:59,454523644-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:07:59,469624027-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2394017
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-18T00:07:59,482530137-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-18T00:07:59,521647716-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:07:59,528061097-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd2234f9-e4bb-430a-b8fa-5560effb6a0f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd2234f9-e4bb-430a-b8fa-5560effb6a0f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6ctk9E:/tmp/ceph-asok.6ctk9E -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:08:01.082+0000 ffffa2743010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:08:01.090+0000 ffffa2743010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:08:01.090+0000 ffffa2743010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:08:01.108347+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:08:01.108347+0000     0       0         0         0         0         0           -           0
2021-05-18T07:08:02.108537+0000     1     255     30306     30051   117.351   117.387  0.00226878  0.00842314
2021-05-18T07:08:03.108728+0000     2     256     56040     55784   108.926    100.52 0.000403209  0.00887072
2021-05-18T07:08:04.108896+0000     3     255     85421     85166   110.869   114.773  0.00174376  0.00898047
2021-05-18T07:08:05.109060+0000     4     255    116392    116137   113.392    120.98  0.00775956  0.00879721
2021-05-18T07:08:06.109255+0000     5     255    141976    141721   110.697   99.9375  0.00472313  0.00901287
2021-05-18T07:08:07.109448+0000     6     256    164000    163744   106.583   86.0273  0.00742041  0.00936075
2021-05-18T07:08:08.109668+0000     7     255    189472    189217   105.568   99.5039  0.00647935  0.00945642
2021-05-18T07:08:09.109872+0000     8     255    215220    214965   104.942   100.578  0.00280605  0.00951161
2021-05-18T07:08:10.110042+0000     9     255    240983    240728   104.462   100.637    0.005475   0.0095586
2021-05-18T07:08:11.110211+0000    10     255    272788    272533   106.437   124.238  0.00562284   0.0093819
2021-05-18T07:08:12.110389+0000    11     255    301521    301266   106.963   112.238   0.0120084  0.00933618
2021-05-18T07:08:13.112455+0000    12     255    327170    326915    106.38   100.191   0.0118944  0.00938601
2021-05-18T07:08:14.112873+0000 Total time run:       12.4196
Total reads made:     337550
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   106.167
Average IOPS:         27178
Stddev IOPS:          2888.38
Max IOPS:             31805
Min IOPS:             22023
Average Latency(s):   0.00940492
Max latency(s):       0.158576
Min latency(s):       0.000299916

[1;32mlocalhost.localdomain	[2021-05-18T00:08:14,427419367-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2394017


[1;33mlocalhost.localdomain	[2021-05-18T00:08:14,434971997-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:08:37,571612933-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 337.55k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:08:37,586418487-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:08:45,704110131-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 337.55k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:08:45,719063115-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:08:54,375389127-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 337.55k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:08:54,389677017-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:09:02,481206354-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 337.55k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:09:02,495836738-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:09:10,515871733-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 337.55k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:09:10,530769973-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:09:10,542298609-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T00:09:10,546450655-07:00][RUNNING][ROUND 5/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:09:10,553228178-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T00:09:10,569496841-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40616\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.690266\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid d669b841-c85d-4cfe-ba78-0d3b8629a173\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid d669b841-c85d-4cfe-ba78-0d3b8629a173\nlast_changed 2021-05-18T00:09:35.544751-0700\ncreated 2021-05-18T00:09:35.544751-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40616/0,v1:10.10.1.2:40617/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.690266 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 c81f1013-3be3-4e8b-a83a-253dfcf24562\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 4fe90723-75ee-4cf4-832e-8b69faeb8fb7\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 a88437fe-f2a2-47f4-adcc-d921ce844835\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42616\n  w/ user/pass: admin / a3110d07-8d57-4a6c-abe6-ed53c21ffe78\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:09:50 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40616
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.690266
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid d669b841-c85d-4cfe-ba78-0d3b8629a173
setting min_mon_release = octopus
epoch 0
fsid d669b841-c85d-4cfe-ba78-0d3b8629a173
last_changed 2021-05-18T00:09:35.544751-0700
created 2021-05-18T00:09:35.544751-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40616/0,v1:10.10.1.2:40617/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.690266 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 c81f1013-3be3-4e8b-a83a-253dfcf24562
0
start osd.0
add osd1 4fe90723-75ee-4cf4-832e-8b69faeb8fb7
1
start osd.1
add osd2 a88437fe-f2a2-47f4-adcc-d921ce844835
2
start osd.2


restful urls: https://10.10.1.2:42616
  w/ user/pass: admin / a3110d07-8d57-4a6c-abe6-ed53c21ffe78


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T00:09:12.180-0700 7f04f86f81c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:09:12.180-0700 7f04f86f81c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:09:12.196-0700 7fd816c5d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:09:12.196-0700 7fd816c5d1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40616,v1:10.10.1.2:40617] --print /tmp/ceph_monmap.690266 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.690266 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.690266 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42616 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.LNXRRW2mYG 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c81f1013-3be3-4e8b-a83a-253dfcf24562 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA4aKNgioXUHRAAMsthvvfKQ0R+oL9oSaWkyA== --osd-uuid c81f1013-3be3-4e8b-a83a-253dfcf24562 
2021-05-18T00:09:44.857-0700 7f7b236e2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:09:44.857-0700 7f7b236e2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:09:44.857-0700 7f7b236e2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:09:44.901-0700 7f7b236e2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4fe90723-75ee-4cf4-832e-8b69faeb8fb7 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T00:09:45.189-0700 7f67aa128f00 -1 Falling back to public interface
2021-05-18T00:09:45.201-0700 7f67aa128f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA5aKNg3dthCxAAPDLB0W9URWZ/LDkTUJUwhg== --osd-uuid 4fe90723-75ee-4cf4-832e-8b69faeb8fb7 
2021-05-18T00:09:45.553-0700 7f6805d55f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:09:45.553-0700 7f6805d55f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:09:45.553-0700 7f6805d55f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:09:45.597-0700 7f6805d55f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a88437fe-f2a2-47f4-adcc-d921ce844835 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T00:09:46.009-0700 7f4802cf6f00 -1 Falling back to public interface
2021-05-18T00:09:46.021-0700 7f4802cf6f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA6aKNgtVicABAAhPaGRk/E1uf15yIyFVgtGQ== --osd-uuid a88437fe-f2a2-47f4-adcc-d921ce844835 
2021-05-18T00:09:46.389-0700 7f4571270f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:09:46.389-0700 7f4571270f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:09:46.389-0700 7f4571270f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:09:46.453-0700 7f4571270f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T00:09:46.729-0700 7f3668be2f00 -1 Falling back to public interface
2021-05-18T00:09:46.745-0700 7f3668be2f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T00:09:50,795249273-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:09:50,802090767-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T00:09:50,883792687-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:09:50,891223136-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ada154b6-767b-453c-909b-d55d3f0aaaa1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ada154b6-767b-453c-909b-d55d3f0aaaa1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T00:09:53,825642216-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:09:53,832063353-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ada154b6-767b-453c-909b-d55d3f0aaaa1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ada154b6-767b-453c-909b-d55d3f0aaaa1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T00:09:56,606004484-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:09:56,612372797-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ada154b6-767b-453c-909b-d55d3f0aaaa1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ada154b6-767b-453c-909b-d55d3f0aaaa1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T00:09:59,350411742-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:09:59,357740234-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ada154b6-767b-453c-909b-d55d3f0aaaa1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ada154b6-767b-453c-909b-d55d3f0aaaa1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T00:10:05,157228077-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:10:05,163718016-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ada154b6-767b-453c-909b-d55d3f0aaaa1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ada154b6-767b-453c-909b-d55d3f0aaaa1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T00:10:09,149824494-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:10:09,156455997-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ada154b6-767b-453c-909b-d55d3f0aaaa1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ada154b6-767b-453c-909b-d55d3f0aaaa1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T00:10:12,940529811-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:10:12,947371965-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ada154b6-767b-453c-909b-d55d3f0aaaa1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ada154b6-767b-453c-909b-d55d3f0aaaa1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T00:10:16,256345808-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:10:16,262898698-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ada154b6-767b-453c-909b-d55d3f0aaaa1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ada154b6-767b-453c-909b-d55d3f0aaaa1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:10:19,847289251-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:10:19,855066829-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ada154b6-767b-453c-909b-d55d3f0aaaa1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ada154b6-767b-453c-909b-d55d3f0aaaa1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:10:23,072148574-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:10:23,079861026-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ada154b6-767b-453c-909b-d55d3f0aaaa1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ada154b6-767b-453c-909b-d55d3f0aaaa1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T00:10:26,574905025-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:10:26,582575185-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ada154b6-767b-453c-909b-d55d3f0aaaa1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ada154b6-767b-453c-909b-d55d3f0aaaa1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T00:10:29,270863197-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:10:29,277304123-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ada154b6-767b-453c-909b-d55d3f0aaaa1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ada154b6-767b-453c-909b-d55d3f0aaaa1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T00:10:32,079791777-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:10:55,064617906-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=========...................] (remaining: 9s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:11:02,896744198-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:11:10,871490742-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:11:19,044947941-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:11:26,980436600-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:11:35,067583500-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:11:42,951293773-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:11:51,077206023-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:11:51,092024179-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:11:59,061535971-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:11:59,076086527-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:12:07,209273424-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:12:07,224045703-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:12:15,125272458-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:12:15,140156941-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:12:23,113832582-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:12:23,129152214-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:12:23,140548558-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:12:23,147386512-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:12:23,162753207-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2407227
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-18T00:12:23,175743167-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-18T00:12:23,215775679-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:12:23,222263878-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ada154b6-767b-453c-909b-d55d3f0aaaa1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ada154b6-767b-453c-909b-d55d3f0aaaa1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:12:25.043+0000 ffffbb586010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:12:25.051+0000 ffffbb586010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:12:25.051+0000 ffffbb586010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:12:25.069454+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-18T07:12:25.069493+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T07:12:25.071770+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:12:25.071770+0000     0       0         0         0         0         0           -           0
2021-05-18T07:12:26.072021+0000     1     255     17805     17550   68.5454   68.5547  0.00214202   0.0141954
2021-05-18T07:12:27.072203+0000     2     255     35193     34938   68.2274   67.9219  0.00609204   0.0144506
2021-05-18T07:12:28.072457+0000     3     255     52155     51900   67.5652   66.2578  0.00608511   0.0147338
2021-05-18T07:12:29.072733+0000     4     256     69097     68841   67.2133   66.1758   0.0125613   0.0148134
2021-05-18T07:12:30.073118+0000     5     255     87049     86794   67.7911   70.1289  0.00733007   0.0146642
2021-05-18T07:12:31.073312+0000     6     255    104603    104348   67.9187   68.5703   0.0094877   0.0146445
2021-05-18T07:12:32.073575+0000     7     255    121707    121452   67.7582   66.8125   0.0141545   0.0147022
2021-05-18T07:12:33.073896+0000     8     255    138788    138533   67.6261   66.7227  0.00828114   0.0147414
2021-05-18T07:12:34.074096+0000     9     255    155660    155405   67.4335   65.9062   0.0311712   0.0147966
2021-05-18T07:12:35.074287+0000    10     256    172693    172437    67.342   66.5312  0.00862152   0.0148219
2021-05-18T07:12:36.074540+0000    11     255    190091    189836   67.3971   67.9648    0.115584   0.0147578
2021-05-18T07:12:37.074729+0000    12     255    206982    206727    67.278   65.9805  0.00718253   0.0148143
2021-05-18T07:12:38.074919+0000    13     256    224473    224217   67.3572   68.3203  0.00310694   0.0147816
2021-05-18T07:12:39.075082+0000    14     255    241548    241293   67.3097   66.7031   0.0840618   0.0148219
2021-05-18T07:12:40.075336+0000    15     255    258620    258365   67.2671   66.6875  0.00358234   0.0148145
2021-05-18T07:12:41.075546+0000    16     255    275210    274955   67.1123   64.8047  0.00190349   0.0148366
2021-05-18T07:12:42.075736+0000    17     255    291375    291120   66.8782   63.1445  0.00513032   0.0149148
2021-05-18T07:12:43.075969+0000    18     255    308257    308002   66.8255   65.9453  0.00209607   0.0149195
2021-05-18T07:12:44.076154+0000    19     255    324790    324535   66.7068    64.582   0.0049037   0.0149574
2021-05-18T07:12:45.076342+0000 min lat: 0.000699344 max lat: 0.141891 avg lat: 0.0149984
2021-05-18T07:12:45.076342+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:12:45.076342+0000    20     164    340836    340672   66.5227   63.0352  0.00454814   0.0149984
2021-05-18T07:12:46.076600+0000 Total time run:         20.0677
Total writes made:      340836
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     66.3448
Stddev Bandwidth:       1.78097
Max bandwidth (MB/sec): 70.1289
Min bandwidth (MB/sec): 63.0352
Average IOPS:           16984
Stddev IOPS:            455.929
Max IOPS:               17953
Min IOPS:               16137
Average Latency(s):     0.0150321
Stddev Latency(s):      0.0240563
Max latency(s):         0.141891
Min latency(s):         0.000699344

[1;32mlocalhost.localdomain	[2021-05-18T00:12:46,382856825-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2407227


[1;33mlocalhost.localdomain	[2021-05-18T00:12:46,390238892-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:13:09,632520220-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.84k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:13:09,647707407-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:13:17,841265393-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.84k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:13:17,856412177-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:13:26,403109262-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.84k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:13:26,418213663-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:13:34,870416972-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.84k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:13:34,885227650-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:13:43,234042140-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.84k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:13:43,249240650-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:13:43,261215427-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:13:43,268616249-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:13:43,284411594-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2410672
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-18T00:13:43,297704441-07:00] INFO: > Run rados bench[0m
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.5
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
[1;30mlocalhost.localdomain	[2021-05-18T00:13:43,337949030-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:13:43,344523419-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ada154b6-767b-453c-909b-d55d3f0aaaa1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ada154b6-767b-453c-909b-d55d3f0aaaa1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.82Iq2U:/tmp/ceph-asok.82Iq2U -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:13:44.920+0000 ffff83d92010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:13:44.940+0000 ffff83d92010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:13:44.940+0000 ffff83d92010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:13:44.959985+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:13:44.959985+0000     0       0         0         0         0         0           -           0
2021-05-18T07:13:45.960168+0000     1     255     24722     24467   95.5439   95.5742  0.00995628   0.0103759
2021-05-18T07:13:46.960355+0000     2     255     47972     47717   93.1738   90.8203  0.00751895   0.0106899
2021-05-18T07:13:47.960636+0000     3     255     74877     74622   97.1386   105.098  0.00781831   0.0102639
2021-05-18T07:13:48.960912+0000     4     256    101745    101489   99.0841   104.949   0.0143867   0.0100648
2021-05-18T07:13:49.961119+0000     5     255    127297    127042   99.2264   99.8164   0.0688103  0.00996517
2021-05-18T07:13:50.961410+0000     6     255    157766    157511    102.52    119.02  0.00590929  0.00973525
2021-05-18T07:13:51.961660+0000     7     255    187081    186826   104.229   114.512  0.00509849  0.00957667
2021-05-18T07:13:52.962696+0000     8     255    217137    216882   105.862   117.406   0.0154561  0.00942733
2021-05-18T07:13:53.962878+0000     9     256    239998    239742    104.02   89.2969  0.00303647  0.00959663
2021-05-18T07:13:54.963065+0000    10     255    264489    264234   103.183   95.6719  0.00288155  0.00967539
2021-05-18T07:13:55.963243+0000    11     255    293260    293005   104.018   112.387  0.00243374  0.00960023
2021-05-18T07:13:56.963534+0000    12     255    315711    315456   102.656   87.6992    0.151603  0.00971739
2021-05-18T07:13:57.963798+0000    13     256    339312    339056   101.849   92.1875   0.0188911  0.00980252
2021-05-18T07:13:58.964026+0000 Total time run:       13.068
Total reads made:     340836
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   101.882
Average IOPS:         26081
Stddev IOPS:          2845.72
Max IOPS:             30469
Min IOPS:             22451
Average Latency(s):   0.00980269
Max latency(s):       0.181882
Min latency(s):       0.000304928

[1;32mlocalhost.localdomain	[2021-05-18T00:13:59,301298066-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2410672


[1;33mlocalhost.localdomain	[2021-05-18T00:13:59,308796765-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:14:22,551317798-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.84k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:14:22,566502966-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:14:30,476536467-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.84k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:14:30,491734686-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:14:38,959648712-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.84k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:14:38,974961146-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:14:47,176119160-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.84k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:14:47,191379369-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:14:55,109670945-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.84k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:14:55,124870422-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:14:55,136582137-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T00:14:55,143940109-07:00][RUNNING][ROUND 1/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:14:55,150935137-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T00:14:55,167111194-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40305\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.691542\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 5a84ef34-63ea-4211-89de-88008e4a1b37\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 5a84ef34-63ea-4211-89de-88008e4a1b37\nlast_changed 2021-05-18T00:15:19.634093-0700\ncreated 2021-05-18T00:15:19.634093-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40305/0,v1:10.10.1.2:40306/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.691542 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 75d098a2-abd3-497a-be08-cd1b76bd4f17\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 4753e1ef-04be-49ae-b336-cec1ce30e305\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 49bb788e-7f29-4b5a-ac51-5a99aea93d47\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42305\n  w/ user/pass: admin / 7456aa1d-52f5-4d53-9545-d2bb24ce5f4d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:15:35 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40305
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.691542
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 5a84ef34-63ea-4211-89de-88008e4a1b37
setting min_mon_release = octopus
epoch 0
fsid 5a84ef34-63ea-4211-89de-88008e4a1b37
last_changed 2021-05-18T00:15:19.634093-0700
created 2021-05-18T00:15:19.634093-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40305/0,v1:10.10.1.2:40306/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.691542 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 75d098a2-abd3-497a-be08-cd1b76bd4f17
0
start osd.0
add osd1 4753e1ef-04be-49ae-b336-cec1ce30e305
1
start osd.1
add osd2 49bb788e-7f29-4b5a-ac51-5a99aea93d47
2
start osd.2


restful urls: https://10.10.1.2:42305
  w/ user/pass: admin / 7456aa1d-52f5-4d53-9545-d2bb24ce5f4d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T00:14:56.175-0700 7f5d6ce4e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:14:56.175-0700 7f5d6ce4e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:14:56.191-0700 7f722a4511c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:14:56.191-0700 7f722a4511c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40305,v1:10.10.1.2:40306] --print /tmp/ceph_monmap.691542 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.691542 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.691542 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42305 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.4WADn6zKzo 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 75d098a2-abd3-497a-be08-cd1b76bd4f17 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCRaaNgH76TExAAzWBV+ZxMkNRxrsclmwYl6g== --osd-uuid 75d098a2-abd3-497a-be08-cd1b76bd4f17 
2021-05-18T00:15:29.676-0700 7fbb239c6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:15:29.676-0700 7fbb239c6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:15:29.676-0700 7fbb239c6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:15:29.776-0700 7fbb239c6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4753e1ef-04be-49ae-b336-cec1ce30e305 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T00:15:30.052-0700 7fccf655bf00 -1 Falling back to public interface
2021-05-18T00:15:30.064-0700 7fccf655bf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCSaaNgHZA/AxAA/H3G6mfSBR7h1wIG1Q7W8g== --osd-uuid 4753e1ef-04be-49ae-b336-cec1ce30e305 
2021-05-18T00:15:30.416-0700 7f567cba0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:15:30.416-0700 7f567cba0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:15:30.416-0700 7f567cba0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:15:30.492-0700 7f567cba0f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 49bb788e-7f29-4b5a-ac51-5a99aea93d47 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T00:15:30.804-0700 7f5d50f3df00 -1 Falling back to public interface
2021-05-18T00:15:30.820-0700 7f5d50f3df00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCSaaNg7CUTMBAALFZKOkTB9jm0WXO7dyjR7w== --osd-uuid 49bb788e-7f29-4b5a-ac51-5a99aea93d47 
2021-05-18T00:15:31.188-0700 7f9f9480bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:15:31.188-0700 7f9f9480bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:15:31.188-0700 7f9f9480bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:15:31.248-0700 7f9f9480bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T00:15:31.528-0700 7f105fccbf00 -1 Falling back to public interface
2021-05-18T00:15:31.540-0700 7f105fccbf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T00:15:35,551139084-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:15:35,558228327-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T00:15:35,641016850-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:15:35,647299868-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '905fa268-42e1-43de-bd72-346facef396a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 905fa268-42e1-43de-bd72-346facef396a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T00:15:38,580061508-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:15:38,586720626-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '905fa268-42e1-43de-bd72-346facef396a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 905fa268-42e1-43de-bd72-346facef396a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T00:15:41,405136276-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:15:41,411388778-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '905fa268-42e1-43de-bd72-346facef396a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 905fa268-42e1-43de-bd72-346facef396a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T00:15:44,377843068-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:15:44,384230684-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '905fa268-42e1-43de-bd72-346facef396a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 905fa268-42e1-43de-bd72-346facef396a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T00:15:50,045270083-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:15:50,051743663-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '905fa268-42e1-43de-bd72-346facef396a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 905fa268-42e1-43de-bd72-346facef396a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T00:15:53,096704376-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:15:53,103386177-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '905fa268-42e1-43de-bd72-346facef396a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 905fa268-42e1-43de-bd72-346facef396a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T00:15:56,994111245-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:15:57,000851902-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '905fa268-42e1-43de-bd72-346facef396a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 905fa268-42e1-43de-bd72-346facef396a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T00:16:00,148745766-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:16:00,155285808-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '905fa268-42e1-43de-bd72-346facef396a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 905fa268-42e1-43de-bd72-346facef396a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:16:04,055918572-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:16:04,062379895-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '905fa268-42e1-43de-bd72-346facef396a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 905fa268-42e1-43de-bd72-346facef396a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:16:07,384973656-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:16:07,391222721-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '905fa268-42e1-43de-bd72-346facef396a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 905fa268-42e1-43de-bd72-346facef396a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T00:16:10,629206897-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:16:10,635410714-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '905fa268-42e1-43de-bd72-346facef396a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 905fa268-42e1-43de-bd72-346facef396a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T00:16:13,511413667-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:16:13,517742261-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '905fa268-42e1-43de-bd72-346facef396a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 905fa268-42e1-43de-bd72-346facef396a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  145 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  145 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.97   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.06   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.97   70      up          osd.2  
                       TOTAL  300 GiB  147 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.97/1.06  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T00:16:16,241053958-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:16:39,366629533-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   198 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (5s)
      [=========...................] (remaining: 10s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:16:47,427417453-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:16:55,364204303-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:17:03,328136638-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:17:11,407951032-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:17:19,294529006-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:17:27,215217467-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:17:35,164850254-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:17:35,179952005-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:17:43,270546159-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:17:43,285997891-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:17:51,179232224-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:17:51,194493467-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:17:59,136766228-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:17:59,156065480-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:18:07,108217460-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:18:07,123640715-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:18:07,135354653-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:18:07,142454392-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:18:07,158730530-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2423895
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-18T00:18:07,172097424-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-18T00:18:07,212183403-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:18:07,218748659-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '905fa268-42e1-43de-bd72-346facef396a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 905fa268-42e1-43de-bd72-346facef396a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:18:08.818+0000 ffff98310010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:18:08.826+0000 ffff98310010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:18:08.826+0000 ffff98310010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:18:08.845733+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-18T07:18:08.845778+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-18T07:18:08.850600+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:18:08.850600+0000     0       1         1         0         0         0           -           0
2021-05-18T07:18:09.850810+0000     1     255     17040     16785   262.142   262.266  0.00680857   0.0150482
2021-05-18T07:18:10.851088+0000     2     255     36145     35890   280.286   298.516   0.0392302   0.0141383
2021-05-18T07:18:11.851359+0000     3     256     54090     53834    280.29   280.375  0.00274552   0.0141879
2021-05-18T07:18:12.851725+0000     4     255     71839     71584   279.528   277.344    0.019835   0.0142425
2021-05-18T07:18:13.851915+0000     5     255     85321     85066   265.748   210.656  0.00349663   0.0148582
2021-05-18T07:18:14.852206+0000     6     255    102771    102516   266.886   272.656  0.00277694   0.0148612
2021-05-18T07:18:15.852453+0000     7     255    120157    119902   267.558   271.656   0.0025779   0.0148258
2021-05-18T07:18:16.852660+0000     8     255    136846    136591   266.702   260.766  0.00328606   0.0149347
2021-05-18T07:18:17.852877+0000     9     255    153693    153438    266.31   263.234  0.00207831   0.0149272
2021-05-18T07:18:18.853089+0000    10     255    169827    169572   264.883   252.094   0.0112338   0.0150403
2021-05-18T07:18:19.853325+0000    11     255    186077    185822    263.88   253.906   0.0137617   0.0151313
2021-05-18T07:18:20.853543+0000    12     255    202593    202338   263.391   258.062   0.0148042   0.0151584
2021-05-18T07:18:21.853735+0000    13     255    218108    217853   261.774   242.422  0.00359672   0.0152492
2021-05-18T07:18:22.853954+0000    14     255    235248    234993   262.201   267.812   0.0174354   0.0152295
2021-05-18T07:18:23.854125+0000    15     255    250108    249853   260.198   232.188  0.00704247   0.0153535
2021-05-18T07:18:24.854330+0000    16     255    266516    266261   259.956   256.375  0.00921881   0.0153704
2021-05-18T07:18:25.854648+0000    17     255    283151    282896   259.949   259.922   0.0131299   0.0153615
2021-05-18T07:18:26.854850+0000    18     255    295322    295067    256.07   190.172   0.0171599   0.0156023
2021-05-18T07:18:27.855080+0000    19     255    310918    310663   255.416   243.688   0.0159136   0.0156299
2021-05-18T07:18:28.855413+0000 min lat: 0.000796923 max lat: 0.194916 avg lat: 0.015718
2021-05-18T07:18:28.855413+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:18:28.855413+0000    20       2    325558    325556   254.276   232.703  0.00581111    0.015718
2021-05-18T07:18:29.855698+0000 Total time run:         20.0052
Total writes made:      325558
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     254.276
Stddev Bandwidth:       24.5519
Max bandwidth (MB/sec): 298.516
Min bandwidth (MB/sec): 190.172
Average IOPS:           16273
Stddev IOPS:            1571.32
Max IOPS:               19105
Min IOPS:               12171
Average Latency(s):     0.015718
Stddev Latency(s):      0.0205773
Max latency(s):         0.194916
Min latency(s):         0.000796923

[1;32mlocalhost.localdomain	[2021-05-18T00:18:30,325003355-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2423895


[1;33mlocalhost.localdomain	[2021-05-18T00:18:30,332372840-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:18:53,250898178-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 325.56k objects, 5.0 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:18:53,266451263-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:19:01,312021625-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 325.56k objects, 5.0 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:19:01,327280478-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:19:09,491809933-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 325.56k objects, 5.0 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:19:09,507464223-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:19:17,710974931-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 325.56k objects, 5.0 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:19:17,726251261-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:19:25,953562085-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 325.56k objects, 5.0 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:19:25,968837274-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:19:25,981167635-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:19:25,988511629-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:19:26,004231435-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2427207
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-18T00:19:26,017204882-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-18T00:19:26,056566012-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:19:26,062948760-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '905fa268-42e1-43de-bd72-346facef396a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 905fa268-42e1-43de-bd72-346facef396a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SnqCvn:/tmp/ceph-asok.SnqCvn -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:19:27.784+0000 ffff9657f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:19:27.792+0000 ffff9657f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:19:27.792+0000 ffff9657f010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:19:27.812828+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:19:27.812828+0000     0       0         0         0         0         0           -           0
2021-05-18T07:19:28.813031+0000     1     255     20350     20095   313.873   313.984  0.00154092   0.0125625
2021-05-18T07:19:29.813242+0000     2     255     44626     44371    346.55   379.312   0.0019214   0.0114773
2021-05-18T07:19:30.813424+0000     3     255     63323     63068   328.397   292.141   0.0429859   0.0120596
2021-05-18T07:19:31.813621+0000     4     255     82687     82432   321.924   302.562  0.00979038   0.0123906
2021-05-18T07:19:32.813836+0000     5     255    108797    108542   339.115   407.969   0.0196232   0.0117588
2021-05-18T07:19:33.814006+0000     6     256    132735    132479   344.921   374.016   0.0003453   0.0115435
2021-05-18T07:19:34.814177+0000     7     255    148732    148477   331.351   249.969  0.00142507     0.01189
2021-05-18T07:19:35.814349+0000     8     256    173692    173436   338.671   389.984 0.000369664   0.0117214
2021-05-18T07:19:36.814513+0000     9     255    194264    194009   336.752   321.453   0.0117626   0.0118554
2021-05-18T07:19:37.816881+0000    10     256    215334    215078   335.918   329.203  0.00678222   0.0118844
2021-05-18T07:19:38.817070+0000    11     256    236587    236331   335.563   332.078  0.00824886   0.0118973
2021-05-18T07:19:39.817329+0000    12     256    257125    256869   334.335   320.906   0.0214451   0.0119426
2021-05-18T07:19:40.817525+0000    13     256    274070    273814   328.981   264.766   0.0246371   0.0120437
2021-05-18T07:19:41.817714+0000    14     255    292055    291800   325.552   281.031  0.00319908   0.0122678
2021-05-18T07:19:42.817921+0000    15     255    315592    315337   328.361   367.766  0.00468051   0.0121634
2021-05-18T07:19:43.818156+0000 Total time run:       15.516
Total reads made:     325558
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   327.844
Average IOPS:         20982
Stddev IOPS:          3016.43
Max IOPS:             26110
Min IOPS:             15998
Average Latency(s):   0.0121807
Max latency(s):       0.208538
Min latency(s):       0.000319255

[1;32mlocalhost.localdomain	[2021-05-18T00:19:44,137826615-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2427207


[1;33mlocalhost.localdomain	[2021-05-18T00:19:44,145431773-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:20:07,308492314-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 325.56k objects, 5.0 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:20:07,323799081-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:20:15,249151982-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 325.56k objects, 5.0 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:20:15,264595633-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:20:23,109699537-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 325.56k objects, 5.0 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:20:23,125016956-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:20:31,583197379-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 325.56k objects, 5.0 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:20:31,600569311-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:20:39,567071647-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 325.56k objects, 5.0 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:20:39,582080476-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:20:39,594254401-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T00:20:39,598905084-07:00][RUNNING][ROUND 2/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:20:39,605987220-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T00:20:39,622988123-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40138\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.692842\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid da71af2c-a6b5-4462-ac63-648078435d14\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid da71af2c-a6b5-4462-ac63-648078435d14\nlast_changed 2021-05-18T00:21:06.378440-0700\ncreated 2021-05-18T00:21:06.378440-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40138/0,v1:10.10.1.2:40139/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.692842 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 6e8607fb-d07b-47ee-bfaf-d7add1d6d4b7\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 544a20ec-a1f1-4a18-83fd-f8b4ec559223\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 d7ef8702-f9d1-4460-a0e2-d07bbc246fb2\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42138\n  w/ user/pass: admin / fcd856e2-d14c-49a6-a7c2-8d58982bac56\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:21:21 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40138
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.692842
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid da71af2c-a6b5-4462-ac63-648078435d14
setting min_mon_release = octopus
epoch 0
fsid da71af2c-a6b5-4462-ac63-648078435d14
last_changed 2021-05-18T00:21:06.378440-0700
created 2021-05-18T00:21:06.378440-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40138/0,v1:10.10.1.2:40139/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.692842 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 6e8607fb-d07b-47ee-bfaf-d7add1d6d4b7
0
start osd.0
add osd1 544a20ec-a1f1-4a18-83fd-f8b4ec559223
1
start osd.1
add osd2 d7ef8702-f9d1-4460-a0e2-d07bbc246fb2
2
start osd.2


restful urls: https://10.10.1.2:42138
  w/ user/pass: admin / fcd856e2-d14c-49a6-a7c2-8d58982bac56


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T00:20:40.615-0700 7fd43da031c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:20:40.615-0700 7fd43da031c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:20:40.631-0700 7f244f04d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:20:40.631-0700 7f244f04d1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40138,v1:10.10.1.2:40139] --print /tmp/ceph_monmap.692842 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.692842 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.692842 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42138 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.V6xxwaasiD 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6e8607fb-d07b-47ee-bfaf-d7add1d6d4b7 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDraqNgVt4hFRAAyFur3IWXkaSIHfzakayLDw== --osd-uuid 6e8607fb-d07b-47ee-bfaf-d7add1d6d4b7 
2021-05-18T00:21:15.779-0700 7f3ae74e0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:21:15.783-0700 7f3ae74e0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:21:15.783-0700 7f3ae74e0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:21:15.827-0700 7f3ae74e0f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 544a20ec-a1f1-4a18-83fd-f8b4ec559223 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T00:21:16.119-0700 7fe579102f00 -1 Falling back to public interface
2021-05-18T00:21:16.131-0700 7fe579102f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDsaqNgABEnBxAADOvggQQK5gbN+0M5Ph5lqw== --osd-uuid 544a20ec-a1f1-4a18-83fd-f8b4ec559223 
2021-05-18T00:21:16.491-0700 7f98bad44f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:21:16.491-0700 7f98bad44f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:21:16.491-0700 7f98bad44f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:21:16.547-0700 7f98bad44f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d7ef8702-f9d1-4460-a0e2-d07bbc246fb2 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T00:21:16.939-0700 7f21f037bf00 -1 Falling back to public interface
2021-05-18T00:21:16.955-0700 7f21f037bf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDsaqNg5Z0EOBAAcPNPyuFuBgIYV2jBEhjA9A== --osd-uuid d7ef8702-f9d1-4460-a0e2-d07bbc246fb2 
2021-05-18T00:21:17.295-0700 7f672f75ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:21:17.295-0700 7f672f75ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:21:17.295-0700 7f672f75ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:21:17.351-0700 7f672f75ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T00:21:17.611-0700 7f8d6d120f00 -1 Falling back to public interface
2021-05-18T00:21:17.623-0700 7f8d6d120f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T00:21:21,613661315-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:21:21,621566057-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T00:21:21,716292521-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:21:21,724512318-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62853715-853a-441d-943d-8a9043090294', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62853715-853a-441d-943d-8a9043090294 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T00:21:24,781153908-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:21:24,787754836-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62853715-853a-441d-943d-8a9043090294', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62853715-853a-441d-943d-8a9043090294 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T00:21:27,505176393-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:21:27,511620874-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62853715-853a-441d-943d-8a9043090294', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62853715-853a-441d-943d-8a9043090294 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T00:21:30,461021766-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:21:30,467609342-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62853715-853a-441d-943d-8a9043090294', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62853715-853a-441d-943d-8a9043090294 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T00:21:36,095522818-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:21:36,102163757-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62853715-853a-441d-943d-8a9043090294', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62853715-853a-441d-943d-8a9043090294 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T00:21:39,040305598-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:21:39,046811214-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62853715-853a-441d-943d-8a9043090294', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62853715-853a-441d-943d-8a9043090294 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T00:21:42,383824510-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:21:42,390163328-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62853715-853a-441d-943d-8a9043090294', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62853715-853a-441d-943d-8a9043090294 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T00:21:45,224842230-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:21:45,232020022-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62853715-853a-441d-943d-8a9043090294', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62853715-853a-441d-943d-8a9043090294 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:21:48,728947715-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:21:48,737255712-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62853715-853a-441d-943d-8a9043090294', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62853715-853a-441d-943d-8a9043090294 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:21:51,959020557-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:21:51,965940469-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62853715-853a-441d-943d-8a9043090294', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62853715-853a-441d-943d-8a9043090294 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T00:21:55,352266549-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:21:55,359423037-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62853715-853a-441d-943d-8a9043090294', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62853715-853a-441d-943d-8a9043090294 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T00:21:58,069359107-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:21:58,076019850-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62853715-853a-441d-943d-8a9043090294', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62853715-853a-441d-943d-8a9043090294 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T00:22:00,853870092-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:22:23,864193788-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   188 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=====.......................] (remaining: 18s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:22:31,670152210-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:22:39,657238104-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:22:47,842360258-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:22:56,025188941-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:23:03,899787427-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:23:12,098285550-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:23:20,139934984-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:23:20,155068222-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:23:30,329551683-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:23:30,344999976-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:23:38,298798036-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:23:38,314160438-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:23:46,278612520-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:23:46,294021655-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:23:54,492009509-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:23:54,507188895-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:23:54,519083615-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:23:54,525890931-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:23:54,541581049-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2440568
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-18T00:23:54,555240313-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-18T00:23:54,595917783-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:23:54,602416961-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62853715-853a-441d-943d-8a9043090294', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62853715-853a-441d-943d-8a9043090294 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:23:56.306+0000 ffff824ac010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:23:56.314+0000 ffff824ac010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:23:56.314+0000 ffff824ac010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:23:56.334909+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-18T07:23:56.334959+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T07:23:56.339421+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:23:56.339421+0000     0       0         0         0         0         0           -           0
2021-05-18T07:23:57.339606+0000     1     255     18511     18256   285.231    285.25  0.00353609   0.0137883
2021-05-18T07:23:58.339781+0000     2     255     32589     32334   252.579   219.969  0.00433663    0.014337
2021-05-18T07:23:59.339982+0000     3     256     46551     46295   241.084   218.141   0.0339129   0.0164702
2021-05-18T07:24:00.340170+0000     4     255     64420     64165   250.605   279.219  0.00562964   0.0158959
2021-05-18T07:24:01.340373+0000     5     255     82508     82253   256.998   282.625  0.00214561   0.0154363
2021-05-18T07:24:02.340544+0000     6     255    100734    100479    261.62   284.781   0.0179479   0.0152294
2021-05-18T07:24:03.340751+0000     7     255    117581    117326   261.843   263.234    0.020688   0.0152351
2021-05-18T07:24:04.341051+0000     8     255    136193    135938   265.454   290.812  0.00363367   0.0150321
2021-05-18T07:24:05.341430+0000     9     255    150167    149912   260.209   218.344   0.0141718   0.0153452
2021-05-18T07:24:06.341750+0000    10     255    165785    165530   258.583   244.031   0.0150375   0.0154401
2021-05-18T07:24:07.341931+0000    11     256    182420    182164   258.699   259.906   0.0212223     0.01544
2021-05-18T07:24:08.342113+0000    12     255    199069    198814   258.817   260.156   0.0230442    0.015435
2021-05-18T07:24:09.342322+0000    13     256    214106    213850   256.976   234.938   0.0026506   0.0155277
2021-05-18T07:24:10.342531+0000    14     255    231432    231177   257.955   270.734   0.0080621   0.0154897
2021-05-18T07:24:11.342921+0000    15     255    247288    247033   257.268    247.75   0.0233094   0.0155295
2021-05-18T07:24:12.343124+0000    16     255    263181    262926   256.706   248.328   0.0109568   0.0155665
2021-05-18T07:24:13.343305+0000    17     255    279653    279398   256.743   257.375  0.00194286   0.0155593
2021-05-18T07:24:14.343472+0000    18     255    296345    296090   256.966   260.812  0.00171729   0.0155474
2021-05-18T07:24:15.343645+0000    19     256    312742    312486   256.923   256.188  0.00401742   0.0155163
2021-05-18T07:24:16.343802+0000 min lat: 0.000740753 max lat: 0.35078 avg lat: 0.0156147
2021-05-18T07:24:16.343802+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:24:16.343802+0000    20      50    327775    327725   255.981   238.109  0.00403395   0.0156147
2021-05-18T07:24:17.344046+0000 Total time run:         20.0091
Total writes made:      327775
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     255.957
Stddev Bandwidth:       22.5416
Max bandwidth (MB/sec): 290.812
Min bandwidth (MB/sec): 218.141
Average IOPS:           16381
Stddev IOPS:            1442.66
Max IOPS:               18612
Min IOPS:               13961
Average Latency(s):     0.0156138
Stddev Latency(s):      0.0175507
Max latency(s):         0.35078
Min latency(s):         0.000740753

[1;32mlocalhost.localdomain	[2021-05-18T00:24:17,639960275-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2440568


[1;33mlocalhost.localdomain	[2021-05-18T00:24:17,647733637-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:24:40,586464705-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 327.78k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:24:40,602132978-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:24:48,881648724-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 327.78k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:24:48,897275409-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:24:57,127881753-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 327.78k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:24:57,143954045-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:25:05,165413440-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 327.78k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:25:05,181455351-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:25:13,081590844-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 327.78k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:25:13,097354172-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:25:13,109550262-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:25:13,116719229-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:25:13,132407334-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2443887
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-18T00:25:13,146191132-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-18T00:25:13,186159949-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:25:13,192522767-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62853715-853a-441d-943d-8a9043090294', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62853715-853a-441d-943d-8a9043090294 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AQYigP:/tmp/ceph-asok.AQYigP -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:25:14.896+0000 ffffbe7f5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:25:14.904+0000 ffffbe7f5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:25:14.904+0000 ffffbe7f5010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:25:14.923675+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:25:14.923675+0000     0       0         0         0         0         0           -           0
2021-05-18T07:25:15.923900+0000     1     255     24630     24375   380.717   380.859  0.00269095   0.0103778
2021-05-18T07:25:16.924449+0000     2     255     48701     48446    378.31   376.109   0.0102399    0.010511
2021-05-18T07:25:17.924618+0000     3     255     71873     71618   372.875   362.062  0.00857102   0.0106682
2021-05-18T07:25:18.924804+0000     4     255     89296     89041   347.705   272.234   0.0320075   0.0114503
2021-05-18T07:25:19.925020+0000     5     256    107216    106960    334.15   279.984  0.00383127   0.0119249
2021-05-18T07:25:20.925289+0000     6     255    126243    125988   327.997   297.312  0.00625968   0.0121618
2021-05-18T07:25:21.925504+0000     7     255    145863    145608   324.926   306.562  0.00238305   0.0122713
2021-05-18T07:25:22.925738+0000     8     255    164686    164431   321.066   294.109  0.00227175    0.012419
2021-05-18T07:25:23.925945+0000     9     255    183231    182976   317.581   289.766  0.00326566   0.0125672
2021-05-18T07:25:24.926264+0000    10     256    202488    202232   315.901   300.875   0.0153043   0.0126371
2021-05-18T07:25:25.926459+0000    11     256    222599    222343   315.744   314.234  0.00046616   0.0126273
2021-05-18T07:25:26.926659+0000    12     256    242250    241994   315.014   307.047  0.00491786   0.0126671
2021-05-18T07:25:27.926904+0000    13     255    258468    258213   310.272   253.422   0.0277589   0.0128653
2021-05-18T07:25:28.927145+0000    14     255    277269    277014   309.088   293.766  0.00296976   0.0129176
2021-05-18T07:25:29.927417+0000    15     255    299879    299624   312.027   353.281    0.019195   0.0127983
2021-05-18T07:25:30.927641+0000    16     255    324930    324675   316.984   391.422  0.00158687   0.0125957
2021-05-18T07:25:31.927920+0000 Total time run:       16.1494
Total reads made:     327775
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   317.132
Average IOPS:         20296
Stddev IOPS:          2692.19
Max IOPS:             25051
Min IOPS:             16219
Average Latency(s):   0.0125917
Max latency(s):       0.13127
Min latency(s):       0.000313163

[1;32mlocalhost.localdomain	[2021-05-18T00:25:32,268994479-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2443887


[1;33mlocalhost.localdomain	[2021-05-18T00:25:32,276773255-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:25:55,135915936-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 327.78k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:25:55,151307604-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:26:03,397086428-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 327.78k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:26:03,412514373-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:26:11,356114028-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 327.78k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:26:11,371711711-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:26:19,242481266-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 327.78k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:26:19,258524390-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:26:27,402749531-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 327.78k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:26:27,418212415-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:26:27,430362401-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T00:26:27,434915307-07:00][RUNNING][ROUND 3/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:26:27,442030309-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T00:26:27,458534897-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40591\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.694173\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 922fb710-3c6f-45dc-b9c9-c1c60fe48699\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 922fb710-3c6f-45dc-b9c9-c1c60fe48699\nlast_changed 2021-05-18T00:26:54.267549-0700\ncreated 2021-05-18T00:26:54.267549-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40591/0,v1:10.10.1.2:40592/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.694173 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 5689e835-b112-4e42-b894-9ba7c45fb8e1\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 101cd606-fc0e-4711-8e89-bc013e421312\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 53c7559e-4ea1-41b3-b31a-51b33a5b2945\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42591\n  w/ user/pass: admin / 174828f5-a88d-4e61-822b-d855be079dd4\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:27:10 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40591
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.694173
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 922fb710-3c6f-45dc-b9c9-c1c60fe48699
setting min_mon_release = octopus
epoch 0
fsid 922fb710-3c6f-45dc-b9c9-c1c60fe48699
last_changed 2021-05-18T00:26:54.267549-0700
created 2021-05-18T00:26:54.267549-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40591/0,v1:10.10.1.2:40592/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.694173 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 5689e835-b112-4e42-b894-9ba7c45fb8e1
0
start osd.0
add osd1 101cd606-fc0e-4711-8e89-bc013e421312
1
start osd.1
add osd2 53c7559e-4ea1-41b3-b31a-51b33a5b2945
2
start osd.2


restful urls: https://10.10.1.2:42591
  w/ user/pass: admin / 174828f5-a88d-4e61-822b-d855be079dd4


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T00:26:28.458-0700 7f4903d2d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:26:28.458-0700 7f4903d2d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:26:28.478-0700 7f89293931c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:26:28.478-0700 7f89293931c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40591,v1:10.10.1.2:40592] --print /tmp/ceph_monmap.694173 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.694173 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.694173 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42591 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.ewLONUWySf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5689e835-b112-4e42-b894-9ba7c45fb8e1 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBHbKNgAtaSOxAABRB3Gcxxpoi8FntN5569og== --osd-uuid 5689e835-b112-4e42-b894-9ba7c45fb8e1 
2021-05-18T00:27:04.334-0700 7fd45e958f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:27:04.334-0700 7fd45e958f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:27:04.334-0700 7fd45e958f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:27:04.398-0700 7fd45e958f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 101cd606-fc0e-4711-8e89-bc013e421312 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T00:27:04.674-0700 7fa262777f00 -1 Falling back to public interface
2021-05-18T00:27:04.686-0700 7fa262777f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBIbKNgeogtKBAAim+Dmiq0QU25BrrkQoYzSQ== --osd-uuid 101cd606-fc0e-4711-8e89-bc013e421312 
2021-05-18T00:27:05.030-0700 7f716746af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:27:05.030-0700 7f716746af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:27:05.030-0700 7f716746af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:27:05.106-0700 7f716746af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 53c7559e-4ea1-41b3-b31a-51b33a5b2945 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T00:27:05.446-0700 7f36cca55f00 -1 Falling back to public interface
2021-05-18T00:27:05.458-0700 7f36cca55f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBJbKNgC9t9GhAAE2hBNWgYQNT8taHoVZi4wQ== --osd-uuid 53c7559e-4ea1-41b3-b31a-51b33a5b2945 
2021-05-18T00:27:05.798-0700 7f1bd734af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:27:05.798-0700 7f1bd734af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:27:05.798-0700 7f1bd734af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:27:05.878-0700 7f1bd734af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T00:27:06.230-0700 7fea1ecd0f00 -1 Falling back to public interface
2021-05-18T00:27:06.246-0700 7fea1ecd0f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T00:27:10,185455559-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:27:10,192822964-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T00:27:10,274542796-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:27:10,280966163-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '22dbc45e-ebff-427a-bff6-1cc517f99b27', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 22dbc45e-ebff-427a-bff6-1cc517f99b27 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T00:27:13,085484912-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:27:13,091785078-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '22dbc45e-ebff-427a-bff6-1cc517f99b27', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 22dbc45e-ebff-427a-bff6-1cc517f99b27 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T00:27:15,840483226-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:27:15,846914574-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '22dbc45e-ebff-427a-bff6-1cc517f99b27', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 22dbc45e-ebff-427a-bff6-1cc517f99b27 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T00:27:18,520201251-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:27:18,526372935-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '22dbc45e-ebff-427a-bff6-1cc517f99b27', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 22dbc45e-ebff-427a-bff6-1cc517f99b27 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T00:27:24,146822806-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:27:24,153309874-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '22dbc45e-ebff-427a-bff6-1cc517f99b27', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 22dbc45e-ebff-427a-bff6-1cc517f99b27 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T00:27:28,268656170-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:27:28,274892064-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '22dbc45e-ebff-427a-bff6-1cc517f99b27', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 22dbc45e-ebff-427a-bff6-1cc517f99b27 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T00:27:31,699060439-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:27:31,705553013-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '22dbc45e-ebff-427a-bff6-1cc517f99b27', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 22dbc45e-ebff-427a-bff6-1cc517f99b27 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T00:27:34,908335530-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:27:34,915041215-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '22dbc45e-ebff-427a-bff6-1cc517f99b27', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 22dbc45e-ebff-427a-bff6-1cc517f99b27 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:27:38,753797325-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:27:38,760327450-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '22dbc45e-ebff-427a-bff6-1cc517f99b27', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 22dbc45e-ebff-427a-bff6-1cc517f99b27 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:27:42,112728650-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:27:42,119193439-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '22dbc45e-ebff-427a-bff6-1cc517f99b27', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 22dbc45e-ebff-427a-bff6-1cc517f99b27 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T00:27:45,302235550-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:27:45,308429392-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '22dbc45e-ebff-427a-bff6-1cc517f99b27', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 22dbc45e-ebff-427a-bff6-1cc517f99b27 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T00:27:48,008700825-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:27:48,015117755-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '22dbc45e-ebff-427a-bff6-1cc517f99b27', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 22dbc45e-ebff-427a-bff6-1cc517f99b27 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T00:27:50,726698267-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:28:13,634427836-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   198 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=====================.......] (remaining: 1s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:28:21,686029526-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:28:29,483443920-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:28:37,355329952-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:28:45,439955138-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:28:53,353151293-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:29:01,304053256-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:29:09,453019064-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:29:09,468689565-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:29:17,477108767-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:29:17,493145781-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:29:25,417235615-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:29:25,432612531-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:29:33,305298048-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:29:33,320603626-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:29:41,246252228-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:29:41,262311145-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:29:41,274854210-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:29:41,282006310-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:29:41,298507654-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2457296
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-18T00:29:41,312137985-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-18T00:29:41,352023206-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:29:41,358340608-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '22dbc45e-ebff-427a-bff6-1cc517f99b27', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 22dbc45e-ebff-427a-bff6-1cc517f99b27 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:29:42.954+0000 ffff822be010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:29:42.962+0000 ffff822be010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:29:42.962+0000 ffff822be010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:29:42.976089+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-18T07:29:42.976136+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T07:29:42.980646+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:29:42.980646+0000     0       0         0         0         0         0           -           0
2021-05-18T07:29:43.980823+0000     1     255     19180     18925   295.682   295.703   0.0137764   0.0133504
2021-05-18T07:29:44.981002+0000     2     256     35820     35564   277.809   259.984  0.00681115   0.0142407
2021-05-18T07:29:45.981188+0000     3     255     53058     52803   274.976   269.359   0.0148667   0.0144685
2021-05-18T07:29:46.981365+0000     4     255     72125     71870   280.699   297.922   0.0116859   0.0141953
2021-05-18T07:29:47.981541+0000     5     255     90042     89787    280.54   279.953   0.0108494   0.0142086
2021-05-18T07:29:48.982507+0000     6     255    106064    105809   275.464   250.344  0.00398248   0.0144739
2021-05-18T07:29:49.982691+0000     7     255    122170    121915   272.056   251.656    0.029397   0.0146619
2021-05-18T07:29:50.982860+0000     8     255    137703    137448   268.382   242.703   0.0275894   0.0148621
2021-05-18T07:29:51.983036+0000     9     255    152969    152714   265.061   238.531  0.00898791   0.0150556
2021-05-18T07:29:52.983253+0000    10     255    166977    166722   260.438   218.875   0.0170569   0.0153317
2021-05-18T07:29:53.983441+0000    11     256    182578    182322   258.917    243.75   0.0159444   0.0154254
2021-05-18T07:29:54.983718+0000    12     255    196623    196368   255.624   219.469   0.0151589    0.015621
2021-05-18T07:29:55.983949+0000    13     255    212124    211869   254.588   242.203  0.00467156   0.0156929
2021-05-18T07:29:56.984878+0000    14     255    226153    225898   252.044   219.203  0.00849144   0.0158467
2021-05-18T07:29:57.985098+0000    15     255    240527    240272   250.211   224.594   0.0296732   0.0159607
2021-05-18T07:29:58.985383+0000    16     255    253043    252788   246.792   195.562     0.03173   0.0161854
2021-05-18T07:29:59.985618+0000    17     255    268154    267899    246.16   236.109   0.0156912   0.0162309
2021-05-18T07:30:00.985790+0000    18     255    282149    281894   244.631   218.672   0.0330963   0.0163301
2021-05-18T07:30:01.986015+0000    19     256    296915    296659   243.895   230.703  0.00105621    0.016379
2021-05-18T07:30:02.986172+0000 min lat: 0.0007603 max lat: 0.128221 avg lat: 0.0164504
2021-05-18T07:30:02.986172+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:30:02.986172+0000    20      88    311162    311074   242.961   225.234   0.0177098   0.0164504
2021-05-18T07:30:03.986435+0000 Total time run:         20.0696
Total writes made:      311162
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     242.252
Stddev Bandwidth:       26.8429
Max bandwidth (MB/sec): 297.922
Min bandwidth (MB/sec): 195.562
Average IOPS:           15504
Stddev IOPS:            1717.94
Max IOPS:               19067
Min IOPS:               12516
Average Latency(s):     0.0164527
Stddev Latency(s):      0.0108291
Max latency(s):         0.128221
Min latency(s):         0.0007603

[1;32mlocalhost.localdomain	[2021-05-18T00:30:04,262794644-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2457296


[1;33mlocalhost.localdomain	[2021-05-18T00:30:04,270675539-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:30:27,245363253-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 311.16k objects, 4.7 GiB
    usage:   9.5 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:30:27,261239873-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:30:35,237210623-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 311.16k objects, 4.7 GiB
    usage:   9.5 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:30:35,252904602-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:30:43,839067700-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 311.16k objects, 4.7 GiB
    usage:   9.5 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:30:43,855030194-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:30:52,376051861-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 311.16k objects, 4.7 GiB
    usage:   9.5 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:30:52,391824963-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:31:00,529941603-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 311.16k objects, 4.7 GiB
    usage:   9.5 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:31:00,545824809-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:31:00,558418371-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:31:00,565797775-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:31:00,582366463-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2460596
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-18T00:31:00,596452436-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-18T00:31:00,636187733-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:31:00,642684401-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '22dbc45e-ebff-427a-bff6-1cc517f99b27', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 22dbc45e-ebff-427a-bff6-1cc517f99b27 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.a1gJtP:/tmp/ceph-asok.a1gJtP -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:31:02.271+0000 ffff87703010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:31:02.279+0000 ffff87703010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:31:02.279+0000 ffff87703010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:31:02.685426+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:31:02.685426+0000     0      19        19         0         0         0           -           0
2021-05-18T07:31:03.685587+0000     1     255     26112     25857   403.699   404.016   0.0018256  0.00975765
2021-05-18T07:31:04.685767+0000     2     256     50623     50367   393.303   382.969  0.00257612   0.0100649
2021-05-18T07:31:05.685964+0000     3     255     72325     72070   375.219   339.109  0.00613113   0.0106172
2021-05-18T07:31:06.686142+0000     4     255     95837     95582   373.242   367.375  0.00246401   0.0106702
2021-05-18T07:31:07.686324+0000     5     255    117893    117638   367.507   344.625  0.00315533   0.0108401
2021-05-18T07:31:08.686541+0000     6     255    137275    137020    356.72   302.844  0.00355724    0.011185
2021-05-18T07:31:09.686758+0000     7     256    154724    154468   344.698   272.625  0.00318991   0.0115667
2021-05-18T07:31:10.686941+0000     8     255    173315    173060   337.918     290.5  0.00744329   0.0118116
2021-05-18T07:31:11.687123+0000     9     255    192415    192160   333.525   298.438  0.00264101   0.0119667
2021-05-18T07:31:12.687313+0000    10     255    208331    208076   325.037   248.688  0.00829911   0.0122832
2021-05-18T07:31:13.687493+0000    11     255    227385    227130   322.549   297.719   0.0100579   0.0123783
2021-05-18T07:31:14.687753+0000    12     256    249255    248999   324.138   341.703  0.00931948   0.0123201
2021-05-18T07:31:15.687998+0000    13     256    273014    272758   327.754   371.234    0.021462   0.0121817
2021-05-18T07:31:16.688253+0000    14     256    298486    298230   332.764       398  0.00321907   0.0119984
2021-05-18T07:31:17.688480+0000 Total time run:       14.5147
Total reads made:     311162
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   334.964
Average IOPS:         21437
Stddev IOPS:          3103.29
Max IOPS:             25857
Min IOPS:             15916
Average Latency(s):   0.0119204
Max latency(s):       0.131335
Min latency(s):       0.000322871

[1;32mlocalhost.localdomain	[2021-05-18T00:31:18,006888252-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2460596


[1;33mlocalhost.localdomain	[2021-05-18T00:31:18,014734761-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:31:40,970963600-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 311.16k objects, 4.7 GiB
    usage:   9.5 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:31:40,987482329-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:31:48,953813738-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 311.16k objects, 4.7 GiB
    usage:   9.5 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:31:48,969849039-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:31:57,037717688-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 311.16k objects, 4.7 GiB
    usage:   9.5 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:31:57,053816870-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:32:05,034121317-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 311.16k objects, 4.7 GiB
    usage:   9.5 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:32:05,050213672-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:32:12,840440914-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 311.16k objects, 4.7 GiB
    usage:   9.5 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:32:12,858581930-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:32:12,872834761-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T00:32:12,877898683-07:00][RUNNING][ROUND 4/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:32:12,886117661-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T00:32:12,904482155-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40861\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.695476\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a836865f-82b7-4a80-9d6f-29a545723440\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid a836865f-82b7-4a80-9d6f-29a545723440\nlast_changed 2021-05-18T00:32:40.012346-0700\ncreated 2021-05-18T00:32:40.012346-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40861/0,v1:10.10.1.2:40862/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.695476 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 6a09fcf5-c091-429e-a187-b00fcb4c2f66\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 d1281571-ab69-4299-b0fa-37b80749e6aa\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 f084381a-1658-4cf5-a2ee-f324e89d9bae\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42861\n  w/ user/pass: admin / 8846118c-515b-4b5a-8c4b-350f7af4ae5f\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:32:55 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40861
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.695476
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a836865f-82b7-4a80-9d6f-29a545723440
setting min_mon_release = octopus
epoch 0
fsid a836865f-82b7-4a80-9d6f-29a545723440
last_changed 2021-05-18T00:32:40.012346-0700
created 2021-05-18T00:32:40.012346-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40861/0,v1:10.10.1.2:40862/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.695476 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 6a09fcf5-c091-429e-a187-b00fcb4c2f66
0
start osd.0
add osd1 d1281571-ab69-4299-b0fa-37b80749e6aa
1
start osd.1
add osd2 f084381a-1658-4cf5-a2ee-f324e89d9bae
2
start osd.2


restful urls: https://10.10.1.2:42861
  w/ user/pass: admin / 8846118c-515b-4b5a-8c4b-350f7af4ae5f


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T00:32:13.913-0700 7f61b04d11c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:32:13.913-0700 7f61b04d11c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:32:13.929-0700 7f4a49aeb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:32:13.929-0700 7f4a49aeb1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40861,v1:10.10.1.2:40862] --print /tmp/ceph_monmap.695476 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.695476 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.695476 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42861 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.1u2JK7ZjqX 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6a09fcf5-c091-429e-a187-b00fcb4c2f66 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQChbaNgNZdqBxAAEpLxzcwsmlDAASIzubyvhQ== --osd-uuid 6a09fcf5-c091-429e-a187-b00fcb4c2f66 
2021-05-18T00:32:49.458-0700 7f03b2015f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:32:49.458-0700 7f03b2015f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:32:49.458-0700 7f03b2015f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:32:49.502-0700 7f03b2015f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d1281571-ab69-4299-b0fa-37b80749e6aa -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T00:32:49.790-0700 7ff79857cf00 -1 Falling back to public interface
2021-05-18T00:32:49.802-0700 7ff79857cf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQChbaNgzA8ULxAAhuikRhxnwFFG0MawxaBWlg== --osd-uuid d1281571-ab69-4299-b0fa-37b80749e6aa 
2021-05-18T00:32:50.150-0700 7f685f6aaf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:32:50.150-0700 7f685f6aaf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:32:50.150-0700 7f685f6aaf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:32:50.206-0700 7f685f6aaf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f084381a-1658-4cf5-a2ee-f324e89d9bae -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T00:32:50.590-0700 7f423808cf00 -1 Falling back to public interface
2021-05-18T00:32:50.606-0700 7f423808cf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCibaNg4clBIxAA2WhBqzy88H0wgUk9TI9xjA== --osd-uuid f084381a-1658-4cf5-a2ee-f324e89d9bae 
2021-05-18T00:32:50.938-0700 7f1e32e1af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:32:50.938-0700 7f1e32e1af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:32:50.938-0700 7f1e32e1af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:32:51.034-0700 7f1e32e1af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T00:32:51.318-0700 7f999f014f00 -1 Falling back to public interface
2021-05-18T00:32:51.334-0700 7f999f014f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T00:32:55,221826904-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:32:55,229040904-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T00:32:55,311575749-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:32:55,318242785-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afb95fed-adca-49af-a0bf-70690c996692', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afb95fed-adca-49af-a0bf-70690c996692 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T00:32:58,192958770-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:32:58,199545423-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afb95fed-adca-49af-a0bf-70690c996692', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afb95fed-adca-49af-a0bf-70690c996692 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T00:33:01,076620614-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:33:01,083000817-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afb95fed-adca-49af-a0bf-70690c996692', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afb95fed-adca-49af-a0bf-70690c996692 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T00:33:03,781580784-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:33:03,788361780-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afb95fed-adca-49af-a0bf-70690c996692', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afb95fed-adca-49af-a0bf-70690c996692 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T00:33:09,354358232-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:33:09,361041606-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afb95fed-adca-49af-a0bf-70690c996692', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afb95fed-adca-49af-a0bf-70690c996692 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T00:33:13,002680552-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:33:13,009192942-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afb95fed-adca-49af-a0bf-70690c996692', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afb95fed-adca-49af-a0bf-70690c996692 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T00:33:16,717866938-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:33:16,724252151-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afb95fed-adca-49af-a0bf-70690c996692', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afb95fed-adca-49af-a0bf-70690c996692 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T00:33:20,016739728-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:33:20,023460429-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afb95fed-adca-49af-a0bf-70690c996692', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afb95fed-adca-49af-a0bf-70690c996692 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:33:23,768023683-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:33:23,776181510-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afb95fed-adca-49af-a0bf-70690c996692', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afb95fed-adca-49af-a0bf-70690c996692 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:33:27,857063939-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:33:27,863371709-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afb95fed-adca-49af-a0bf-70690c996692', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afb95fed-adca-49af-a0bf-70690c996692 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T00:33:30,931455076-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:33:30,937937461-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afb95fed-adca-49af-a0bf-70690c996692', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afb95fed-adca-49af-a0bf-70690c996692 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T00:33:33,598365041-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:33:33,605009498-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afb95fed-adca-49af-a0bf-70690c996692', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afb95fed-adca-49af-a0bf-70690c996692 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  156 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T00:33:38,333589814-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:34:01,246394491-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (4s)
      [=========...................] (remaining: 9s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:34:09,430300206-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:34:17,306389119-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:34:25,243865015-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:34:33,296181052-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:34:41,197984495-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:34:49,138011798-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:34:49,153969159-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:34:57,330231120-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:34:57,345979299-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:35:05,379474404-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:35:05,395127032-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:35:13,553942695-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:35:13,569808000-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:35:21,562531313-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:35:21,578282463-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:35:21,591001250-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:35:21,598310770-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:35:21,614933989-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2473538
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-18T00:35:21,628571218-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-18T00:35:21,668755848-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:35:21,675060032-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afb95fed-adca-49af-a0bf-70690c996692', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afb95fed-adca-49af-a0bf-70690c996692 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:35:23.204+0000 ffff80656010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:35:23.212+0000 ffff80656010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:35:23.212+0000 ffff80656010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:35:23.229724+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-18T07:35:23.229778+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T07:35:23.234330+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:35:23.234330+0000     0       0         0         0         0         0           -           0
2021-05-18T07:35:24.234513+0000     1     255     16848     16593   259.248   259.266  0.00334272   0.0145041
2021-05-18T07:35:25.234697+0000     2     255     32917     32662    255.14   251.078  0.00244129   0.0152134
2021-05-18T07:35:26.234862+0000     3     255     48880     48625    253.22   249.422  0.00206752   0.0155839
2021-05-18T07:35:27.235036+0000     4     255     64849     64594   252.283   249.516  0.00975739   0.0155691
2021-05-18T07:35:28.235195+0000     5     256     80736     80480   251.462   248.219  0.00342995   0.0157317
2021-05-18T07:35:29.235364+0000     6     255     96169     95914   249.738   241.156   0.0149779    0.015967
2021-05-18T07:35:30.235554+0000     7     255    110669    110414   246.421   226.562   0.0091329   0.0161911
2021-05-18T07:35:31.236034+0000     8     255    126307    126052   246.146   244.344   0.0172705   0.0162243
2021-05-18T07:35:32.236816+0000     9     255    140986    140731    244.26   229.359   0.0105776   0.0163449
2021-05-18T07:35:33.237093+0000    10     255    155667    155412   242.767   229.391    0.020949   0.0164504
2021-05-18T07:35:34.237496+0000    11     256    171293    171037   242.883   244.141   0.0123961    0.016434
2021-05-18T07:35:35.237947+0000    12     255    187421    187166   243.635   252.016   0.0104348   0.0163968
2021-05-18T07:35:36.238373+0000    13     256    202673    202417   243.216   238.297  0.00652528   0.0164211
2021-05-18T07:35:37.238591+0000    14     255    219105    218850    244.18   256.766   0.0223617   0.0163559
2021-05-18T07:35:38.238779+0000    15     255    234228    233973   243.651   236.297   0.0114189   0.0163941
2021-05-18T07:35:39.241262+0000    16     255    248346    248091   242.173   220.594   0.0190249   0.0164949
2021-05-18T07:35:40.241502+0000    17     255    262819    262564   241.227   226.141   0.0119184   0.0165636
2021-05-18T07:35:41.241701+0000    18     255    278222    277967   241.194   240.672   0.0184741   0.0165676
2021-05-18T07:35:42.241932+0000    19     255    293209    292954   240.821   234.172  0.00765126   0.0165916
2021-05-18T07:35:43.242114+0000 min lat: 0.000765836 max lat: 0.168592 avg lat: 0.0166875
2021-05-18T07:35:43.242114+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:35:43.242114+0000    20     256    306087    305831   238.839   201.203  0.00078852   0.0166875
2021-05-18T07:35:44.242353+0000 Total time run:         20.1167
Total writes made:      306087
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     237.744
Stddev Bandwidth:       13.9773
Max bandwidth (MB/sec): 259.266
Min bandwidth (MB/sec): 201.203
Average IOPS:           15215
Stddev IOPS:            894.549
Max IOPS:               16593
Min IOPS:               12877
Average Latency(s):     0.0168073
Stddev Latency(s):      0.0212837
Max latency(s):         0.176832
Min latency(s):         0.000765836

[1;32mlocalhost.localdomain	[2021-05-18T00:35:44,534585513-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2473538


[1;33mlocalhost.localdomain	[2021-05-18T00:35:44,542540489-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:36:07,654257838-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306.09k objects, 4.7 GiB
    usage:   9.3 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:36:07,670197390-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:36:15,656905326-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306.09k objects, 4.7 GiB
    usage:   9.3 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:36:15,672902473-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:36:23,656542515-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306.09k objects, 4.7 GiB
    usage:   9.3 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:36:23,672602702-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:36:31,704485938-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306.09k objects, 4.7 GiB
    usage:   9.3 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:36:31,720765267-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:36:40,177379852-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306.09k objects, 4.7 GiB
    usage:   9.3 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:36:40,196327808-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:36:40,210143371-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:36:40,217636208-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:36:40,233950884-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2476937
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-18T00:36:40,248658365-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-18T00:36:40,288058644-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:36:40,294546579-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afb95fed-adca-49af-a0bf-70690c996692', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afb95fed-adca-49af-a0bf-70690c996692 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rdYNTu:/tmp/ceph-asok.rdYNTu -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:36:41.757+0000 ffffab49d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:36:41.801+0000 ffffab49d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:36:41.801+0000 ffffab49d010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:36:41.820896+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:36:41.820896+0000     0       0         0         0         0         0           -           0
2021-05-18T07:36:42.821071+0000     1     255     23652     23397    365.47   365.578  0.00198505   0.0107806
2021-05-18T07:36:43.821246+0000     2     256     47441     47185   368.546   371.688   0.0195028   0.0107539
2021-05-18T07:36:44.821426+0000     3     256     69263     69007   359.333   340.969 0.000447317   0.0107751
2021-05-18T07:36:45.821608+0000     4     255     90714     90459   353.282   335.188   0.0294778   0.0112646
2021-05-18T07:36:46.821801+0000     5     256    113468    113212   353.715   355.516  0.00359035   0.0112739
2021-05-18T07:36:47.822027+0000     6     255    134211    133956   348.771   324.125  0.00626987   0.0114401
2021-05-18T07:36:48.822227+0000     7     255    152228    151973   339.155   281.516   0.0475835   0.0117376
2021-05-18T07:36:49.822422+0000     8     256    167353    167097   326.294   236.312 0.000507417   0.0122158
2021-05-18T07:36:50.822998+0000     9     255    188936    188681    327.49    337.25  0.00673732   0.0121887
2021-05-18T07:36:51.823465+0000    10     256    209499    209243   326.854   321.281  0.00562658   0.0122126
2021-05-18T07:36:52.823664+0000    11     255    231849    231594   328.882   349.234   0.0263676   0.0121384
2021-05-18T07:36:53.823893+0000    12     255    253889    253634   330.167   344.375  0.00197554   0.0120911
2021-05-18T07:36:54.824076+0000    13     255    276606    276351   332.068   354.953   0.0288472     0.01202
2021-05-18T07:36:55.824261+0000    14     255    298403    298148   332.672   340.578  0.00203961   0.0120009
2021-05-18T07:36:56.824548+0000 Total time run:       14.3533
Total reads made:     306087
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   333.207
Average IOPS:         21325
Stddev IOPS:          2266.08
Max IOPS:             23788
Min IOPS:             15124
Average Latency(s):   0.0119844
Max latency(s):       0.160279
Min latency(s):       0.000323801

[1;32mlocalhost.localdomain	[2021-05-18T00:36:57,176508745-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2476937


[1;33mlocalhost.localdomain	[2021-05-18T00:36:57,185281240-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:37:20,130830144-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306.09k objects, 4.7 GiB
    usage:   9.3 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:37:20,147427033-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:37:27,951090649-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306.09k objects, 4.7 GiB
    usage:   9.3 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:37:27,967380930-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:37:36,046080181-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306.09k objects, 4.7 GiB
    usage:   9.3 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:37:36,062319634-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:37:43,984760815-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306.09k objects, 4.7 GiB
    usage:   9.3 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:37:44,000709398-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:37:52,020146970-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306.09k objects, 4.7 GiB
    usage:   9.3 GiB used, 291 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:37:52,036326907-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:37:52,049436074-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T00:37:52,054552399-07:00][RUNNING][ROUND 5/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:37:52,062140531-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T00:37:52,078876028-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40316\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.696681\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 73ea87c5-590b-4244-a1f3-535d2b8fd73d\nsetting min_mon_release = octopus\nepoch 0\nfsid 73ea87c5-590b-4244-a1f3-535d2b8fd73d\nlast_changed 2021-05-18T00:38:20.825178-0700\ncreated 2021-05-18T00:38:20.825178-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40316/0,v1:10.10.1.2:40317/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.696681 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 cdd0d4e7-1ba8-4b21-aef6-1f83f949d56e\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 f154ce2c-30a3-4cc9-ac8d-ed56063551fa\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 052fe6e2-f6f0-4e52-aa03-a1a54ac5e28c\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42316\n  w/ user/pass: admin / 60062e02-49fe-40aa-ac2c-2a34162c3fdd\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:38:35 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40316
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.696681
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 73ea87c5-590b-4244-a1f3-535d2b8fd73d
setting min_mon_release = octopus
epoch 0
fsid 73ea87c5-590b-4244-a1f3-535d2b8fd73d
last_changed 2021-05-18T00:38:20.825178-0700
created 2021-05-18T00:38:20.825178-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40316/0,v1:10.10.1.2:40317/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.696681 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 cdd0d4e7-1ba8-4b21-aef6-1f83f949d56e
0
start osd.0
add osd1 f154ce2c-30a3-4cc9-ac8d-ed56063551fa
1
start osd.1
add osd2 052fe6e2-f6f0-4e52-aa03-a1a54ac5e28c
2
start osd.2


restful urls: https://10.10.1.2:42316
  w/ user/pass: admin / 60062e02-49fe-40aa-ac2c-2a34162c3fdd


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T00:37:53.084-0700 7f12566bd1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:37:53.084-0700 7f12566bd1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:37:53.104-0700 7ff5488271c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:37:53.104-0700 7ff5488271c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40316,v1:10.10.1.2:40317] --print /tmp/ceph_monmap.696681 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.696681 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.696681 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42316 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.WTQIgLDry9 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new cdd0d4e7-1ba8-4b21-aef6-1f83f949d56e -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD1bqNg0pwKLBAAtrjRV7m8HiDkQerW4yMmLA== --osd-uuid cdd0d4e7-1ba8-4b21-aef6-1f83f949d56e 
2021-05-18T00:38:30.089-0700 7f1e2ac68f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:38:30.089-0700 7f1e2ac68f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:38:30.089-0700 7f1e2ac68f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:38:30.177-0700 7f1e2ac68f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f154ce2c-30a3-4cc9-ac8d-ed56063551fa -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T00:38:30.469-0700 7f494b93af00 -1 Falling back to public interface
2021-05-18T00:38:30.481-0700 7f494b93af00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD2bqNgAeMiHBAA98JyMYAVdvZjBaHn/5ovGw== --osd-uuid f154ce2c-30a3-4cc9-ac8d-ed56063551fa 
2021-05-18T00:38:30.817-0700 7f7970c2df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:38:30.817-0700 7f7970c2df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:38:30.817-0700 7f7970c2df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:38:30.861-0700 7f7970c2df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 052fe6e2-f6f0-4e52-aa03-a1a54ac5e28c -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T00:38:31.141-0700 7f92e9a6df00 -1 Falling back to public interface
2021-05-18T00:38:31.157-0700 7f92e9a6df00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD3bqNgT31eCBAAJTdgdAWu6DEy8tnrDkH8oA== --osd-uuid 052fe6e2-f6f0-4e52-aa03-a1a54ac5e28c 
2021-05-18T00:38:31.489-0700 7f1c8fab1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:38:31.489-0700 7f1c8fab1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:38:31.489-0700 7f1c8fab1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:38:31.533-0700 7f1c8fab1f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T00:38:31.961-0700 7f74fec74f00 -1 Falling back to public interface
2021-05-18T00:38:31.973-0700 7f74fec74f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T00:38:35,821987157-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:38:35,829585926-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T00:38:35,911789828-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:38:35,918364838-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5916260-ec80-49eb-ab53-3888d70cf6a5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5916260-ec80-49eb-ab53-3888d70cf6a5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T00:38:38,717230522-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:38:38,723702883-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5916260-ec80-49eb-ab53-3888d70cf6a5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5916260-ec80-49eb-ab53-3888d70cf6a5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T00:38:41,602186774-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:38:41,610263195-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5916260-ec80-49eb-ab53-3888d70cf6a5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5916260-ec80-49eb-ab53-3888d70cf6a5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T00:38:44,354620622-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:38:44,361058973-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5916260-ec80-49eb-ab53-3888d70cf6a5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5916260-ec80-49eb-ab53-3888d70cf6a5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T00:38:50,050299576-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:38:50,056824072-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5916260-ec80-49eb-ab53-3888d70cf6a5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5916260-ec80-49eb-ab53-3888d70cf6a5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T00:38:53,650322819-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:38:53,656657652-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5916260-ec80-49eb-ab53-3888d70cf6a5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5916260-ec80-49eb-ab53-3888d70cf6a5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T00:38:57,364005072-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:38:57,370394725-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5916260-ec80-49eb-ab53-3888d70cf6a5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5916260-ec80-49eb-ab53-3888d70cf6a5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T00:39:00,507680647-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:39:00,514073946-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5916260-ec80-49eb-ab53-3888d70cf6a5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5916260-ec80-49eb-ab53-3888d70cf6a5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:39:03,951415056-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:39:03,957849387-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5916260-ec80-49eb-ab53-3888d70cf6a5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5916260-ec80-49eb-ab53-3888d70cf6a5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:39:07,259789025-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:39:07,266462827-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5916260-ec80-49eb-ab53-3888d70cf6a5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5916260-ec80-49eb-ab53-3888d70cf6a5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T00:39:10,463701118-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:39:10,470189998-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5916260-ec80-49eb-ab53-3888d70cf6a5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5916260-ec80-49eb-ab53-3888d70cf6a5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T00:39:13,164816710-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:39:13,171129474-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5916260-ec80-49eb-ab53-3888d70cf6a5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5916260-ec80-49eb-ab53-3888d70cf6a5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T00:39:15,843074780-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:39:38,889800166-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:39:46,853211366-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:39:54,826671562-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:40:02,791969095-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:40:10,949996141-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 56s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:40:18,940311367-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 71s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:40:26,791152946-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 81s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:40:34,970059627-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 43s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:40:43,210871203-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:40:43,228817781-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:40:51,176130051-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:40:51,192464288-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:40:59,281558291-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:40:59,297697765-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:41:07,169403130-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:41:07,185384041-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:41:15,309249208-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:41:15,325503932-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:41:15,338975770-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:41:15,346715533-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:41:15,363802284-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2490683
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-18T00:41:15,378045824-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-18T00:41:15,418720732-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:41:15,425092891-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5916260-ec80-49eb-ab53-3888d70cf6a5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5916260-ec80-49eb-ab53-3888d70cf6a5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:41:16.900+0000 ffff83502010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:41:16.904+0000 ffff83502010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:41:16.904+0000 ffff83502010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:41:16.924804+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-18T07:41:16.924843+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T07:41:16.929485+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:41:16.929485+0000     0       0         0         0         0         0           -           0
2021-05-18T07:41:17.929683+0000     1     255     17860     17605   275.059   275.078   0.0278754    0.014191
2021-05-18T07:41:18.929889+0000     2     255     35099     34844   272.181   269.359   0.0134624   0.0145139
2021-05-18T07:41:19.930166+0000     3     255     52822     52567   273.736   276.922  0.00261508   0.0144822
2021-05-18T07:41:20.930413+0000     4     255     69459     69204   270.274   259.953   0.0282832   0.0147042
2021-05-18T07:41:21.930583+0000     5     255     86583     86328   269.723   267.562  0.00906812   0.0147561
2021-05-18T07:41:22.930816+0000     6     255    105082    104827   272.932   289.047   0.0294402   0.0145256
2021-05-18T07:41:23.931009+0000     7     255    122402    122147   272.595   270.625   0.0340619   0.0146139
2021-05-18T07:41:24.931329+0000     8     255    140789    140534   274.422   287.297   0.0189863   0.0145288
2021-05-18T07:41:25.931569+0000     9     255    159450    159195    276.32   291.578   0.0209578   0.0144348
2021-05-18T07:41:26.931830+0000    10     255    177339    177084   276.632   279.516  0.00433337   0.0144257
2021-05-18T07:41:27.932079+0000    11     255    192375    192120   272.837   234.938   0.0273131   0.0146201
2021-05-18T07:41:28.932264+0000    12     256    208261    208005    270.78   248.203    0.010172   0.0147398
2021-05-18T07:41:29.932494+0000    13     255    224291    224036   269.214   250.484  0.00542319   0.0148295
2021-05-18T07:41:30.932741+0000    14     255    239816    239561   267.308   242.578  0.00920779   0.0149391
2021-05-18T07:41:31.932950+0000    15     255    256965    256710   267.347   267.953  0.00914717   0.0149087
2021-05-18T07:41:32.933233+0000    16     255    273278    273023   266.564   254.891   0.0136537   0.0149797
2021-05-18T07:41:33.933457+0000    17     255    288474    288219   264.847   237.438   0.0100141   0.0150753
2021-05-18T07:41:34.933667+0000    18     255    302904    302649   262.657   225.469   0.0157922   0.0152051
2021-05-18T07:41:35.934069+0000    19     255    316415    316160   259.939   211.109   0.0101496   0.0153642
2021-05-18T07:41:36.934288+0000 min lat: 0.000756624 max lat: 0.159784 avg lat: 0.0154234
2021-05-18T07:41:36.934288+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:41:36.934288+0000    20      86    331640    331554   258.966   240.531  0.00726711   0.0154234
2021-05-18T07:41:37.934578+0000 Total time run:         20.0333
Total writes made:      331640
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     258.663
Stddev Bandwidth:       22.2909
Max bandwidth (MB/sec): 291.578
Min bandwidth (MB/sec): 211.109
Average IOPS:           16554
Stddev IOPS:            1426.62
Max IOPS:               18661
Min IOPS:               13511
Average Latency(s):     0.0154296
Stddev Latency(s):      0.0139481
Max latency(s):         0.159784
Min latency(s):         0.000756624

[1;32mlocalhost.localdomain	[2021-05-18T00:41:38,253037966-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2490683


[1;33mlocalhost.localdomain	[2021-05-18T00:41:38,260925354-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:42:01,388342604-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 331.64k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:42:01,404691978-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:42:09,636194507-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 331.64k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:42:09,652001794-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:42:17,921289312-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 331.64k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:42:17,939108443-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:42:26,363476431-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 331.64k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:42:26,379766576-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:42:34,355113766-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 331.64k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:42:34,371199591-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:42:34,384047502-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:42:34,391645610-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:42:34,408541123-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2493962
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-18T00:42:34,422985442-07:00] INFO: > Run rados bench[0m
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.5
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
[1;30mlocalhost.localdomain	[2021-05-18T00:42:34,463155633-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:42:34,472142904-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5916260-ec80-49eb-ab53-3888d70cf6a5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5916260-ec80-49eb-ab53-3888d70cf6a5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.p6gnDa:/tmp/ceph-asok.p6gnDa -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:42:35.974+0000 ffff86aa4010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:42:35.982+0000 ffff86aa4010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:42:35.982+0000 ffff86aa4010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:42:35.999242+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:42:35.999242+0000     0       0         0         0         0         0           -           0
2021-05-18T07:42:36.999408+0000     1     255     22631     22376   349.522   349.625  0.00185912    0.011283
2021-05-18T07:42:37.999621+0000     2     255     43434     43179    337.25   325.047  0.00134168   0.0117485
2021-05-18T07:42:38.999859+0000     3     255     62925     62670   326.325   304.547    0.027578    0.012189
2021-05-18T07:42:40.000216+0000     4     255     85892     85637   334.427   358.859  0.00122103   0.0119001
2021-05-18T07:42:41.000475+0000     5     256    106593    106337   332.213   323.438  0.00235876   0.0119923
2021-05-18T07:42:42.001116+0000     6     255    124677    124422   323.908   282.578   0.0364284   0.0122979
2021-05-18T07:42:43.001362+0000     7     255    142476    142221   317.356   278.109   0.0192906   0.0125739
2021-05-18T07:42:44.001536+0000     8     256    162110    161854   316.025   306.766   0.0227443   0.0125588
2021-05-18T07:42:45.002437+0000     9     255    182346    182091   316.014   316.203  0.00144007     0.01263
2021-05-18T07:42:46.002829+0000    10     255    203157    202902   316.917   325.172   0.0272751   0.0125899
2021-05-18T07:42:47.003014+0000    11     255    224278    224023   318.102   330.016   0.0253151   0.0125494
2021-05-18T07:42:48.003214+0000    12     255    244139    243884   317.449   310.328  0.00193074   0.0125741
2021-05-18T07:42:49.003506+0000    13     255    263843    263588   316.705   307.875   0.0119998   0.0126086
2021-05-18T07:42:50.003819+0000    14     256    284575    284319   317.214   323.922  0.00543618   0.0125903
2021-05-18T07:42:51.004042+0000    15     256    306126    305870    318.51   336.734  0.00204266   0.0125374
2021-05-18T07:42:52.006640+0000    16     255    326019    325764   317.979   310.844   0.0147815   0.0125611
2021-05-18T07:42:53.006895+0000 Total time run:       16.2517
Total reads made:     331640
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   318.851
Average IOPS:         20406
Stddev IOPS:          1352.55
Max IOPS:             22967
Min IOPS:             17799
Average Latency(s):   0.0125245
Max latency(s):       0.144947
Min latency(s):       0.00032125

[1;32mlocalhost.localdomain	[2021-05-18T00:42:53,322077181-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2493962


[1;33mlocalhost.localdomain	[2021-05-18T00:42:53,330153272-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:43:16,147965406-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 331.64k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:43:16,164060022-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:43:24,383675128-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 331.64k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:43:24,400338791-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:43:32,184888313-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 331.64k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:43:32,204901583-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:43:40,185786948-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 331.64k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:43:40,202057603-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:43:48,253795517-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 331.64k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:43:48,270162159-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:43:48,283471459-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T00:43:48,291508497-07:00][RUNNING][ROUND 1/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:43:48,299040280-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T00:43:48,315955161-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40655\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.697842\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 270654fd-6bba-4c30-9335-2811ce861065\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 270654fd-6bba-4c30-9335-2811ce861065\nlast_changed 2021-05-18T00:44:15.377967-0700\ncreated 2021-05-18T00:44:15.377967-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40655/0,v1:10.10.1.2:40656/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.697842 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 40eb2934-e52a-4342-bfab-015ea8a86ab8\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 7de06998-967d-4d4d-9d8a-783fa7636098\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 ab0dfad1-1821-4eeb-8dca-ebb2dff61f1d\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42655\n  w/ user/pass: admin / d722b989-4477-4d73-84d9-6a4aced47595\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:44:30 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40655
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.697842
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 270654fd-6bba-4c30-9335-2811ce861065
setting min_mon_release = octopus
epoch 0
fsid 270654fd-6bba-4c30-9335-2811ce861065
last_changed 2021-05-18T00:44:15.377967-0700
created 2021-05-18T00:44:15.377967-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40655/0,v1:10.10.1.2:40656/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.697842 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 40eb2934-e52a-4342-bfab-015ea8a86ab8
0
start osd.0
add osd1 7de06998-967d-4d4d-9d8a-783fa7636098
1
start osd.1
add osd2 ab0dfad1-1821-4eeb-8dca-ebb2dff61f1d
2
start osd.2


restful urls: https://10.10.1.2:42655
  w/ user/pass: admin / d722b989-4477-4d73-84d9-6a4aced47595


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T00:43:49.316-0700 7f8419c4a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:43:49.316-0700 7f8419c4a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:43:49.336-0700 7f8a48baf1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:43:49.336-0700 7f8a48baf1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40655,v1:10.10.1.2:40656] --print /tmp/ceph_monmap.697842 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.697842 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.697842 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42655 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.RptE58UoaT 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 40eb2934-e52a-4342-bfab-015ea8a86ab8 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBYcKNg3CQzFRAAZD4I+QLPL8BFUfvFIViudQ== --osd-uuid 40eb2934-e52a-4342-bfab-015ea8a86ab8 
2021-05-18T00:44:24.696-0700 7fe783267f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:44:24.696-0700 7fe783267f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:44:24.696-0700 7fe783267f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:44:24.768-0700 7fe783267f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7de06998-967d-4d4d-9d8a-783fa7636098 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T00:44:25.036-0700 7f976f228f00 -1 Falling back to public interface
2021-05-18T00:44:25.052-0700 7f976f228f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBZcKNgSyVVAhAAGWZ7aQ2b7Imxrl52YLGGpg== --osd-uuid 7de06998-967d-4d4d-9d8a-783fa7636098 
2021-05-18T00:44:25.384-0700 7f5f63d05f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:44:25.384-0700 7f5f63d05f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:44:25.384-0700 7f5f63d05f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:44:25.436-0700 7f5f63d05f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ab0dfad1-1821-4eeb-8dca-ebb2dff61f1d -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T00:44:25.732-0700 7f6371745f00 -1 Falling back to public interface
2021-05-18T00:44:25.744-0700 7f6371745f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBZcKNgrhi9KxAA74tAcDFNCxP53pzXFRi/oA== --osd-uuid ab0dfad1-1821-4eeb-8dca-ebb2dff61f1d 
2021-05-18T00:44:26.092-0700 7f67fb537f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:44:26.096-0700 7f67fb537f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:44:26.096-0700 7f67fb537f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:44:26.208-0700 7f67fb537f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T00:44:26.544-0700 7f16f575ff00 -1 Falling back to public interface
2021-05-18T00:44:26.560-0700 7f16f575ff00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T00:44:30,445130551-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:44:30,452804872-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T00:44:30,534651585-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:44:30,541174895-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T00:44:33,324061115-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:44:33,330543182-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T00:44:36,305310978-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:44:36,311824311-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T00:44:39,884374198-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:44:39,891804653-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T00:44:46,551490555-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:44:46,558882333-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T00:44:50,425166211-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:44:50,431546936-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T00:44:54,012920227-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:44:54,019250333-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T00:44:57,011136222-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:44:57,020045689-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:45:00,413562527-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:45:00,421238948-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:45:04,113037192-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:45:04,119814811-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T00:45:07,550142504-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:45:07,556441150-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T00:45:10,200372539-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:45:10,206980763-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T00:45:12,877420300-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:45:36,011817658-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   196 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (4s)
      [=========...................] (remaining: 9s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:45:44,301030155-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:45:52,367674852-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:46:00,380868988-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:46:08,222055151-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:46:16,539764131-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:46:24,450905156-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:46:32,322151399-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:46:32,338495732-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:46:40,324027353-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:46:40,340792163-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:46:48,338629086-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:46:48,357844145-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:46:56,345914855-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:46:56,362607232-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:47:04,335793304-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:47:04,352345197-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:47:04,365340552-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:47:04,372948202-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:47:04,390782575-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2507355
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-18T00:47:04,405381644-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-18T00:47:04,446498030-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:47:04,453206696-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:47:06.044+0000 ffffac187010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:47:06.052+0000 ffffac187010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:47:06.052+0000 ffffac187010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:47:06.067491+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-18T07:47:06.067537+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T07:47:06.081193+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:47:06.081193+0000     0       0         0         0         0         0           -           0
2021-05-18T07:47:07.081386+0000     1     255      8629      8374   523.337   523.375  0.00403465   0.0292581
2021-05-18T07:47:08.081557+0000     2     255     17129     16874   527.248    531.25   0.0272946   0.0299541
2021-05-18T07:47:09.081723+0000     3     255     25839     25584   532.927   544.375  0.00939022   0.0296805
2021-05-18T07:47:10.081959+0000     4     256     34472     34216   534.539     539.5   0.0129771   0.0298008
2021-05-18T07:47:11.082214+0000     5     256     42101     41845   522.968   476.812   0.0028093   0.0302865
2021-05-18T07:47:12.082502+0000     6     256     49789     49533   515.867     480.5   0.0155766   0.0309208
2021-05-18T07:47:13.083101+0000     7     255     57218     56963   508.468   464.375   0.0317795   0.0313754
2021-05-18T07:47:14.083493+0000     8     256     64764     64508   503.832   471.562   0.0188341   0.0316738
2021-05-18T07:47:15.083813+0000     9     255     72387     72132   500.778     476.5   0.0183593   0.0318866
2021-05-18T07:47:16.083998+0000    10     255     79837     79582   497.254   465.625   0.0202559   0.0320771
2021-05-18T07:47:17.084219+0000    11     255     87903     87648   497.869   504.125    0.012271   0.0320449
2021-05-18T07:47:18.084388+0000    12     255     95853     95598   497.779   496.875      0.0347   0.0320691
2021-05-18T07:47:19.084630+0000    13     256    103360    103104   495.566   469.125   0.0102804    0.032221
2021-05-18T07:47:20.084831+0000    14     256    109869    109613   489.221   406.812   0.0213344   0.0326224
2021-05-18T07:47:21.085119+0000    15     255    116813    116558   485.535   434.062  0.00958272   0.0328852
2021-05-18T07:47:22.085386+0000    16     255    124653    124398   485.806       490   0.0398644   0.0328661
2021-05-18T07:47:23.085557+0000    17     255    132538    132283   486.213   492.812    0.080313   0.0328367
2021-05-18T07:47:24.085736+0000    18     255    140575    140320   487.103   502.312  0.00541112   0.0327412
2021-05-18T07:47:25.085955+0000    19     255    148955    148700   489.025    523.75   0.0130268   0.0326699
2021-05-18T07:47:26.086207+0000 min lat: 0.00102509 max lat: 0.157364 avg lat: 0.0327289
2021-05-18T07:47:26.086207+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:47:26.086207+0000    20     154    156492    156338   488.437   477.375   0.0156844   0.0327289
2021-05-18T07:47:27.086499+0000 Total time run:         20.0151
Total writes made:      156492
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     488.669
Stddev Bandwidth:       34.3415
Max bandwidth (MB/sec): 544.375
Min bandwidth (MB/sec): 406.812
Average IOPS:           7818
Stddev IOPS:            549.464
Max IOPS:               8710
Min IOPS:               6509
Average Latency(s):     0.0327171
Stddev Latency(s):      0.0218929
Max latency(s):         0.157364
Min latency(s):         0.00102509

[1;32mlocalhost.localdomain	[2021-05-18T00:47:27,383444811-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2507355


[1;33mlocalhost.localdomain	[2021-05-18T00:47:27,391949374-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:47:50,628817383-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.49k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:47:50,644806432-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:47:58,811563428-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.49k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:47:58,827702128-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:48:06,818814984-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.49k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:48:06,835059197-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:48:15,120664467-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.49k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:48:15,136781894-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:48:23,072330187-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.49k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:48:23,088625230-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:48:23,101440556-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:48:23,109204725-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:48:23,126690729-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2510651
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-18T00:48:23,140884097-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-18T00:48:23,180174688-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:48:23,186583438-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5f06cd39-bcd7-4b23-86f3-a604e4ce0ddd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gWaXVp:/tmp/ceph-asok.gWaXVp -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:48:24.721+0000 ffff970d6010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:48:24.729+0000 ffff970d6010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:48:24.729+0000 ffff970d6010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:48:24.748251+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:48:24.748251+0000     0       0         0         0         0         0           -           0
2021-05-18T07:48:25.748482+0000     1     255     13060     12805   800.026   800.312   0.0376565   0.0196251
2021-05-18T07:48:26.748695+0000     2     255     24777     24522   766.094   732.312   0.0492932   0.0206279
2021-05-18T07:48:27.749043+0000     3     255     36043     35788   745.355   704.125   0.0238584   0.0213329
2021-05-18T07:48:28.749392+0000     4     256     45949     45693   713.727   619.062  0.00765477   0.0222751
2021-05-18T07:48:29.749681+0000     5     256     57164     56908   711.129   700.938  0.00107966   0.0223194
2021-05-18T07:48:30.749882+0000     6     255     68691     68436   712.666     720.5    0.019649    0.022356
2021-05-18T07:48:31.750091+0000     7     256     81024     80768    720.94    770.75   0.0557401   0.0220766
2021-05-18T07:48:32.750288+0000     8     256     91092     90836   709.464    629.25   0.0876428   0.0223955
2021-05-18T07:48:33.750526+0000     9     256    103135    102879   714.247   752.688  0.00322647   0.0222871
2021-05-18T07:48:34.750761+0000    10     255    113268    113013   706.145   633.375   0.0022859   0.0225456
2021-05-18T07:48:35.750961+0000    11     256    125241    124985   709.959    748.25  0.00587235   0.0224475
2021-05-18T07:48:36.751173+0000    12     255    135710    135455   705.316   654.375  0.00113094   0.0225875
2021-05-18T07:48:37.751499+0000    13     255    147378    147123   707.139    729.25   0.0261131   0.0225685
2021-05-18T07:48:38.751760+0000 Total time run:       13.8432
Total reads made:     156492
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   706.536
Average IOPS:         11304
Stddev IOPS:          922.489
Max IOPS:             12805
Min IOPS:             9905
Average Latency(s):   0.0225932
Max latency(s):       0.130339
Min latency(s):       0.000448815

[1;32mlocalhost.localdomain	[2021-05-18T00:48:39,110922682-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2510651


[1;33mlocalhost.localdomain	[2021-05-18T00:48:39,118986482-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:49:02,542299311-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.49k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:49:02,558757433-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:49:10,548762103-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.49k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:49:10,565329279-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:49:18,416762618-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.49k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:49:18,433298455-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:49:26,531828733-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.49k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:49:26,548230492-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:49:34,484476007-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.49k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:49:34,502681716-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:49:34,516946830-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T00:49:34,522527577-07:00][RUNNING][ROUND 2/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:49:34,531520456-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T00:49:34,552522487-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40510\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.698965\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a23ae276-4687-4cc9-b6bd-7fe7acece6e3\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid a23ae276-4687-4cc9-b6bd-7fe7acece6e3\nlast_changed 2021-05-18T00:50:02.591259-0700\ncreated 2021-05-18T00:50:02.591259-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40510/0,v1:10.10.1.2:40511/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.698965 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 db239957-f2c5-44fd-8223-d057bfa1d3ab\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 23be01ab-a40d-4044-8de9-b79464ec17a2\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 f4cbc297-2b34-4b71-9bc0-1a0728667391\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42510\n  w/ user/pass: admin / 5b9cdee1-e5d1-4b4e-aacf-9e5f4191caf2\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:50:18 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40510
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.698965
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a23ae276-4687-4cc9-b6bd-7fe7acece6e3
setting min_mon_release = octopus
epoch 0
fsid a23ae276-4687-4cc9-b6bd-7fe7acece6e3
last_changed 2021-05-18T00:50:02.591259-0700
created 2021-05-18T00:50:02.591259-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40510/0,v1:10.10.1.2:40511/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.698965 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 db239957-f2c5-44fd-8223-d057bfa1d3ab
0
start osd.0
add osd1 23be01ab-a40d-4044-8de9-b79464ec17a2
1
start osd.1
add osd2 f4cbc297-2b34-4b71-9bc0-1a0728667391
2
start osd.2


restful urls: https://10.10.1.2:42510
  w/ user/pass: admin / 5b9cdee1-e5d1-4b4e-aacf-9e5f4191caf2


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T00:49:35.559-0700 7f033718e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:49:35.559-0700 7f033718e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:49:35.575-0700 7f2c3a37f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:49:35.575-0700 7f2c3a37f1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40510,v1:10.10.1.2:40511] --print /tmp/ceph_monmap.698965 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.698965 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.698965 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42510 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.rS0NJKV3Xw 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new db239957-f2c5-44fd-8223-d057bfa1d3ab -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC0caNgiXJXDxAAsDvqLCL1xOChXq3y9pQ5aw== --osd-uuid db239957-f2c5-44fd-8223-d057bfa1d3ab 
2021-05-18T00:50:12.587-0700 7fea55a14f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:50:12.587-0700 7fea55a14f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:50:12.587-0700 7fea55a14f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:50:12.659-0700 7fea55a14f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 23be01ab-a40d-4044-8de9-b79464ec17a2 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T00:50:12.943-0700 7fb6c0403f00 -1 Falling back to public interface
2021-05-18T00:50:12.955-0700 7fb6c0403f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC0caNgEFpoOBAAXdGRkQ7pfcXMbqamIZMYCg== --osd-uuid 23be01ab-a40d-4044-8de9-b79464ec17a2 
2021-05-18T00:50:13.291-0700 7fbdd4be8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:50:13.291-0700 7fbdd4be8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:50:13.291-0700 7fbdd4be8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:50:13.371-0700 7fbdd4be8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f4cbc297-2b34-4b71-9bc0-1a0728667391 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T00:50:13.703-0700 7f5bf3c34f00 -1 Falling back to public interface
2021-05-18T00:50:13.715-0700 7f5bf3c34f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC1caNg6kG/KRAA5V9V1K0dMZMbDSTflYyBsQ== --osd-uuid f4cbc297-2b34-4b71-9bc0-1a0728667391 
2021-05-18T00:50:14.027-0700 7feaefcf8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:50:14.027-0700 7feaefcf8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:50:14.027-0700 7feaefcf8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:50:14.083-0700 7feaefcf8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T00:50:14.427-0700 7ff7e653af00 -1 Falling back to public interface
2021-05-18T00:50:14.439-0700 7ff7e653af00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T00:50:18,413623597-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:50:18,421689845-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T00:50:18,505380272-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:50:18,512156693-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T00:50:21,271654149-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:50:21,278256990-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T00:50:24,081083696-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:50:24,087766620-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T00:50:26,992932049-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:50:26,999598636-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T00:50:32,508555414-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:50:32,514754057-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T00:50:35,392513191-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:50:35,398919304-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T00:50:38,788643141-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:50:38,795212826-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T00:50:42,246190909-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:50:42,252812879-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:50:45,378524679-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:50:45,386729236-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:50:49,029013815-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:50:49,035758025-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T00:50:52,347195794-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:50:52,353913150-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T00:50:55,125485967-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:50:55,132023488-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T00:50:57,765326735-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:51:20,691820638-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   189 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (0s)
      [............................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:51:28,676797482-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:51:36,564830870-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:51:44,786309602-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:51:52,776178835-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:52:00,712767765-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:52:08,808209953-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:52:16,665123402-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:52:16,684315101-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:52:24,507258106-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:52:24,523718803-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:52:32,521565203-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:52:32,537738977-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:52:40,542007053-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:52:40,558475657-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:52:49,007696381-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:52:49,023962676-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:52:49,036854885-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:52:49,044621760-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:52:49,062050062-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2524146
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-18T00:52:49,076910828-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-18T00:52:49,117022672-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:52:49,123675098-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82 -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:52:50.826+0000 ffffa5ea3010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:52:50.834+0000 ffffa5ea3010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:52:50.838+0000 ffffa5ea3010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:52:50.855217+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-18T07:52:50.855259+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-18T07:52:50.868906+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:52:50.868906+0000     0       0         0         0         0         0           -           0
2021-05-18T07:52:51.869168+0000     1     256      7373      7117   444.749   444.812  0.00168054   0.0341015
2021-05-18T07:52:52.869891+0000     2     255     15507     15252   476.419   508.438    0.015907   0.0332411
2021-05-18T07:52:53.870257+0000     3     255     23667     23412    487.55       510   0.0680843    0.032457
2021-05-18T07:52:54.870527+0000     4     255     33333     33078    516.65   604.125   0.0188609   0.0308428
2021-05-18T07:52:55.870750+0000     5     255     42880     42625   532.629   596.688   0.0433957   0.0298893
2021-05-18T07:52:56.870925+0000     6     256     51651     51395   535.195   548.125   0.0225216   0.0297916
2021-05-18T07:52:57.871117+0000     7     256     60064     59808   533.841   525.812   0.0207083    0.029875
2021-05-18T07:52:58.871296+0000     8     255     67606     67351    526.03   471.438   0.0124729   0.0302747
2021-05-18T07:52:59.871500+0000     9     256     75008     74752   518.968   462.562  0.00115215   0.0306308
2021-05-18T07:53:00.871696+0000    10     255     82589     82334    514.45   473.875   0.0256807   0.0309731
2021-05-18T07:53:01.871886+0000    11     256     90004     89748   509.799   463.375  0.00161181   0.0312873
2021-05-18T07:53:02.872066+0000    12     255     97335     97080   505.497    458.25   0.0378703   0.0315704
2021-05-18T07:53:03.872328+0000    13     255    104541    104286   501.248   450.375   0.0292667   0.0318621
2021-05-18T07:53:04.872554+0000    14     255    111625    111370   497.062    442.75   0.0212194   0.0321373
2021-05-18T07:53:05.872811+0000    15     255    119236    118981   495.629   475.688    0.022566   0.0322425
2021-05-18T07:53:06.873649+0000    16     256    126442    126186   492.772   450.312   0.0181313   0.0323958
2021-05-18T07:53:07.873873+0000    17     256    133927    133671   491.297   467.812    0.035075   0.0325282
2021-05-18T07:53:08.874200+0000    18     255    141879    141624   491.609   497.062   0.0166947   0.0325091
2021-05-18T07:53:09.874497+0000    19     256    149329    149073   490.231   465.562   0.0125566      0.0326
2021-05-18T07:53:10.874805+0000 min lat: 0.00105241 max lat: 0.185841 avg lat: 0.0326711
2021-05-18T07:53:10.874805+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:53:10.874805+0000    20      30    156693    156663    489.43   474.375  0.00512923   0.0326711
2021-05-18T07:53:11.875059+0000 Total time run:         20.0104
Total writes made:      156693
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     489.41
Stddev Bandwidth:       46.9526
Max bandwidth (MB/sec): 604.125
Min bandwidth (MB/sec): 442.75
Average IOPS:           7830
Stddev IOPS:            751.242
Max IOPS:               9666
Min IOPS:               7084
Average Latency(s):     0.032667
Stddev Latency(s):      0.0234856
Max latency(s):         0.185841
Min latency(s):         0.00105241

[1;32mlocalhost.localdomain	[2021-05-18T00:53:12,173844158-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2524146


[1;33mlocalhost.localdomain	[2021-05-18T00:53:12,182130046-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:53:35,290656747-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.69k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:53:35,307594692-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:53:43,498896117-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.69k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:53:43,515918347-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:53:51,455971223-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.69k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:53:51,474788327-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:53:59,435832368-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.69k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:53:59,452521347-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:54:07,421416095-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.69k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:54:07,438409626-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:54:07,451797790-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:54:07,459875264-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:54:07,477904226-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2527455
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-18T00:54:07,492473810-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-18T00:54:07,532633563-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:54:07,539093088-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8a20f0f-39ed-4cb4-8116-6ffc03a1bf0d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RNRC82:/tmp/ceph-asok.RNRC82 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:54:09.068+0000 ffffb0b59010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:54:09.076+0000 ffffb0b59010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:54:09.076+0000 ffffb0b59010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:54:09.093933+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:54:09.093933+0000     0       0         0         0         0         0           -           0
2021-05-18T07:54:10.094111+0000     1     256     12325     12069   754.085   754.312  0.00313802   0.0206268
2021-05-18T07:54:11.094318+0000     2     256     23468     23212   725.191   696.438   0.0511777   0.0216966
2021-05-18T07:54:12.094647+0000     3     255     35899     35644   742.376       777  0.00751923   0.0212962
2021-05-18T07:54:13.094847+0000     4     255     48293     48038   750.399   774.625   0.0163241   0.0211888
2021-05-18T07:54:14.095052+0000     5     256     61721     61465   768.122   839.188  0.00160442   0.0206817
2021-05-18T07:54:15.095364+0000     6     255     74977     74722   778.152   828.562  0.00143499   0.0204239
2021-05-18T07:54:16.095570+0000     7     256     86537     86281   770.172   722.438  0.00408414   0.0206336
2021-05-18T07:54:17.096062+0000     8     255     98226     97971   765.183   730.625  0.00577245   0.0208089
2021-05-18T07:54:18.096339+0000     9     256    109453    109197   758.099   701.625   0.0590103   0.0210077
2021-05-18T07:54:19.096765+0000    10     256    120455    120199   751.022   687.625   0.0657853   0.0211942
2021-05-18T07:54:20.097006+0000    11     256    131670    131414   746.454   700.938   0.0619945   0.0213256
2021-05-18T07:54:21.097213+0000    12     256    144151    143895   749.241   780.062  0.00254604   0.0212645
2021-05-18T07:54:22.097607+0000    13     255    155149    154894   744.465   687.438    0.047358   0.0214175
2021-05-18T07:54:23.097912+0000 Total time run:       13.1578
Total reads made:     156693
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   744.298
Average IOPS:         11908
Stddev IOPS:          833.597
Max IOPS:             13427
Min IOPS:             10999
Average Latency(s):   0.0214229
Max latency(s):       0.104915
Min latency(s):       0.00047234

[1;32mlocalhost.localdomain	[2021-05-18T00:54:23,427766688-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2527455


[1;33mlocalhost.localdomain	[2021-05-18T00:54:23,436126015-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:54:46,330868760-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.69k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:54:46,347966755-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:54:54,542202198-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.69k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:54:54,559117851-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:55:02,403836305-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.69k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:55:02,420619487-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:55:10,277366841-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.69k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:55:10,293644227-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:55:18,638538498-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 156.69k objects, 9.6 GiB
    usage:   19 GiB used, 281 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:55:18,655415326-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:55:18,668666009-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T00:55:18,673593775-07:00][RUNNING][ROUND 3/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:55:18,681031481-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T00:55:18,698215366-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40562\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.700081\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 9ec927a5-dbe7-4f32-977b-c4a0a362fe2b\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 9ec927a5-dbe7-4f32-977b-c4a0a362fe2b\nlast_changed 2021-05-18T00:55:48.819215-0700\ncreated 2021-05-18T00:55:48.819215-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40562/0,v1:10.10.1.2:40563/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.700081 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 33f968b4-7b7a-4ad2-bba4-795e934a9408\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 0728a3b4-b170-4026-8475-eb14dd0fe644\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 ed8b7968-9846-44c3-8f59-14b82aa97e16\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42562\n  w/ user/pass: admin / 77730e98-c0c7-49ba-a609-c1464942256b\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:56:03 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40562
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.700081
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 9ec927a5-dbe7-4f32-977b-c4a0a362fe2b
setting min_mon_release = octopus
epoch 0
fsid 9ec927a5-dbe7-4f32-977b-c4a0a362fe2b
last_changed 2021-05-18T00:55:48.819215-0700
created 2021-05-18T00:55:48.819215-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40562/0,v1:10.10.1.2:40563/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.700081 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 33f968b4-7b7a-4ad2-bba4-795e934a9408
0
start osd.0
add osd1 0728a3b4-b170-4026-8475-eb14dd0fe644
1
start osd.1
add osd2 ed8b7968-9846-44c3-8f59-14b82aa97e16
2
start osd.2


restful urls: https://10.10.1.2:42562
  w/ user/pass: admin / 77730e98-c0c7-49ba-a609-c1464942256b


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T00:55:19.686-0700 7fde2b4c91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:55:19.686-0700 7fde2b4c91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:55:19.702-0700 7f2d76b5e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:55:19.702-0700 7f2d76b5e1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40562,v1:10.10.1.2:40563] --print /tmp/ceph_monmap.700081 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.700081 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.700081 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42562 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.GxXoGsMBaj 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 33f968b4-7b7a-4ad2-bba4-795e934a9408 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQANc6Ngj0IUJxAAPnpFmAQnfJzYdHvHMNmU5g== --osd-uuid 33f968b4-7b7a-4ad2-bba4-795e934a9408 
2021-05-18T00:55:58.006-0700 7f83e664cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:55:58.006-0700 7f83e664cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:55:58.006-0700 7f83e664cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T00:55:58.150-0700 7f83e664cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0728a3b4-b170-4026-8475-eb14dd0fe644 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T00:55:58.426-0700 7f1867227f00 -1 Falling back to public interface
2021-05-18T00:55:58.438-0700 7f1867227f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAOc6NgE2qZGRAAEsrQvLcS7CG85omctqdiuw== --osd-uuid 0728a3b4-b170-4026-8475-eb14dd0fe644 
2021-05-18T00:55:58.766-0700 7f7b1e548f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:55:58.770-0700 7f7b1e548f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:55:58.770-0700 7f7b1e548f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T00:55:58.822-0700 7f7b1e548f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ed8b7968-9846-44c3-8f59-14b82aa97e16 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T00:55:59.226-0700 7f3d64a13f00 -1 Falling back to public interface
2021-05-18T00:55:59.242-0700 7f3d64a13f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAPc6Ng471HDRAAhEXdce8O3lJDZPS9RnQ0Lg== --osd-uuid ed8b7968-9846-44c3-8f59-14b82aa97e16 
2021-05-18T00:55:59.554-0700 7f49a249df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:55:59.554-0700 7f49a249df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:55:59.554-0700 7f49a249df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T00:55:59.642-0700 7f49a249df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T00:55:59.922-0700 7fcd3c626f00 -1 Falling back to public interface
2021-05-18T00:55:59.934-0700 7fcd3c626f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T00:56:03,896428118-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:56:03,904081892-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T00:56:03,988718470-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:56:03,995333512-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01c33444-90b7-4a9e-9130-6016923722c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01c33444-90b7-4a9e-9130-6016923722c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T00:56:06,769825698-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:56:06,776487788-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01c33444-90b7-4a9e-9130-6016923722c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01c33444-90b7-4a9e-9130-6016923722c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T00:56:09,523126963-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:56:09,529762769-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01c33444-90b7-4a9e-9130-6016923722c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01c33444-90b7-4a9e-9130-6016923722c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T00:56:12,633311986-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:56:12,639792951-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01c33444-90b7-4a9e-9130-6016923722c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01c33444-90b7-4a9e-9130-6016923722c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T00:56:18,424851137-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:56:18,431277132-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01c33444-90b7-4a9e-9130-6016923722c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01c33444-90b7-4a9e-9130-6016923722c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T00:56:21,653088115-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:56:21,659834790-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01c33444-90b7-4a9e-9130-6016923722c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01c33444-90b7-4a9e-9130-6016923722c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T00:56:24,914671461-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:56:24,921156747-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01c33444-90b7-4a9e-9130-6016923722c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01c33444-90b7-4a9e-9130-6016923722c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T00:56:28,347496513-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:56:28,354419783-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01c33444-90b7-4a9e-9130-6016923722c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01c33444-90b7-4a9e-9130-6016923722c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:56:31,613363747-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:56:31,620021967-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01c33444-90b7-4a9e-9130-6016923722c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01c33444-90b7-4a9e-9130-6016923722c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T00:56:34,831850913-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:56:34,838457315-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01c33444-90b7-4a9e-9130-6016923722c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01c33444-90b7-4a9e-9130-6016923722c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T00:56:38,380490331-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:56:38,386760525-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01c33444-90b7-4a9e-9130-6016923722c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01c33444-90b7-4a9e-9130-6016923722c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T00:56:41,218272977-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:56:41,224894306-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01c33444-90b7-4a9e-9130-6016923722c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01c33444-90b7-4a9e-9130-6016923722c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T00:56:43,824423453-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:57:06,975511424-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:57:15,156304109-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:57:23,361911256-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 29s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:57:31,323052798-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 56s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:57:39,148854692-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 60s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:57:47,448279641-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 78s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:57:55,427914677-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (45s)
      [=========...................] (remaining: 92s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:58:03,472815401-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   227 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [=========...................] (remaining: 102s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:58:11,329337062-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:58:11,346042099-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:58:19,411082404-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:58:19,427536149-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:58:27,409475204-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:58:27,426211013-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:58:35,379382885-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:58:35,399233537-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:58:43,374086440-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:58:43,391230978-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:58:43,404716229-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T00:58:43,412628273-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:58:43,430676280-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2541254
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-18T00:58:43,445543295-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-18T00:58:43,486366446-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T00:58:43,492910003-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01c33444-90b7-4a9e-9130-6016923722c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01c33444-90b7-4a9e-9130-6016923722c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T07:58:45.142+0000 ffff8bafd010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:58:45.150+0000 ffff8bafd010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T07:58:45.150+0000 ffff8bafd010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T07:58:45.169937+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-18T07:58:45.169978+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T07:58:45.183654+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:58:45.183654+0000     0       0         0         0         0         0           -           0
2021-05-18T07:58:46.183861+0000     1     256      8578      8322   520.094   520.125  0.00129922   0.0295828
2021-05-18T07:58:47.184029+0000     2     256     17200     16944    529.44   538.875   0.0234102   0.0300073
2021-05-18T07:58:48.184206+0000     3     256     25351     25095   522.742   509.438  0.00122098   0.0299965
2021-05-18T07:58:49.184372+0000     4     255     33591     33336   520.801   515.062  0.00872265   0.0305339
2021-05-18T07:58:50.184576+0000     5     256     42102     41846   522.994   531.875   0.0011428    0.030351
2021-05-18T07:58:51.184750+0000     6     256     50678     50422   525.146       536   0.0012485   0.0302912
2021-05-18T07:58:52.185058+0000     7     255     59164     58909   525.879   530.438   0.0457158   0.0302642
2021-05-18T07:58:53.185285+0000     8     255     67573     67318   525.824   525.562  0.00720367   0.0302783
2021-05-18T07:58:54.185466+0000     9     256     75817     75561   524.632   515.188   0.0205916   0.0303757
2021-05-18T07:58:55.185647+0000    10     256     83570     83314   520.616   484.562   0.0163063   0.0306823
2021-05-18T07:58:56.186043+0000    11     256     91647     91391   519.161   504.812  0.00617521   0.0307473
2021-05-18T07:58:57.186320+0000    12     255     99842     99587   518.573    512.25   0.0182173   0.0307684
2021-05-18T07:58:58.186573+0000    13     256    108043    107787   518.096     512.5  0.00110557   0.0307053
2021-05-18T07:58:59.186743+0000    14     255    116025    115770   516.722   498.938   0.0274976   0.0308773
2021-05-18T07:59:00.186981+0000    15     255    123946    123691    515.27   495.062   0.0906037   0.0309508
2021-05-18T07:59:01.187153+0000    16     256    131897    131641   514.115   496.875  0.00620555   0.0310654
2021-05-18T07:59:02.187327+0000    17     255    139750    139495   512.743   490.875   0.0148033   0.0311607
2021-05-18T07:59:03.187675+0000    18     255    148395    148140   514.264   540.312   0.0328177   0.0310737
2021-05-18T07:59:04.188016+0000    19     256    156622    156366   514.248   514.125  0.00346088   0.0310569
2021-05-18T07:59:05.188250+0000 min lat: 0.000989418 max lat: 0.174017 avg lat: 0.0309655
2021-05-18T07:59:05.188250+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T07:59:05.188250+0000    20     204    165257    165053   515.676   542.938   0.0557928   0.0309655
2021-05-18T07:59:06.188545+0000 Total time run:         20.0153
Total writes made:      165257
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     516.033
Stddev Bandwidth:       17.3546
Max bandwidth (MB/sec): 542.938
Min bandwidth (MB/sec): 484.562
Average IOPS:           8256
Stddev IOPS:            277.674
Max IOPS:               8687
Min IOPS:               7753
Average Latency(s):     0.0309797
Stddev Latency(s):      0.0259215
Max latency(s):         0.174017
Min latency(s):         0.000989418

[1;32mlocalhost.localdomain	[2021-05-18T00:59:06,567662675-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2541254


[1;33mlocalhost.localdomain	[2021-05-18T00:59:06,576976123-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T00:59:29,502081418-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 165.26k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:59:29,519312026-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:59:37,471718654-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 165.26k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:59:37,488743187-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:59:45,781035115-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 165.26k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:59:45,797994912-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T00:59:53,880283851-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 165.26k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T00:59:53,897195746-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:00:01,985224585-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 165.26k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:00:02,002358353-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:00:02,015866319-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:00:02,023919552-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:00:02,041712907-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2544564
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-18T01:00:02,056385641-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-18T01:00:02,095565340-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:00:02,101958122-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01c33444-90b7-4a9e-9130-6016923722c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01c33444-90b7-4a9e-9130-6016923722c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YBsHj:/tmp/ceph-asok.3YBsHj -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:00:03.720+0000 ffffb9c8c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:00:03.728+0000 ffffb9c8c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:00:03.728+0000 ffffb9c8c010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:00:03.748690+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:00:03.748690+0000     0       0         0         0         0         0           -           0
2021-05-18T08:00:04.748943+0000     1     255     12893     12638   789.565   789.875  0.00362271   0.0197225
2021-05-18T08:00:05.749153+0000     2     255     25012     24757   773.424   757.438  0.00260464   0.0204326
2021-05-18T08:00:06.750720+0000     3     256     38986     38730   806.292   873.312  0.00352224   0.0196974
2021-05-18T08:00:07.751335+0000     4     255     51747     51492   804.003   797.625   0.0650504   0.0197324
2021-05-18T08:00:08.751589+0000     5     255     63759     63504   793.318    750.75  0.00168623   0.0200195
2021-05-18T08:00:09.751855+0000     6     255     75741     75486    785.88   748.875   0.0131678    0.020273
2021-05-18T08:00:10.752069+0000     7     255     87212     86957   776.012   716.938  0.00920138   0.0205452
2021-05-18T08:00:11.752323+0000     8     256     99627     99371    775.97   775.875   0.0348253   0.0205492
2021-05-18T08:00:12.752620+0000     9     255    111301    111046   770.804   729.688  0.00597038   0.0206516
2021-05-18T08:00:13.752964+0000    10     256    122456    122200   763.413   697.125  0.00870783    0.020905
2021-05-18T08:00:14.753183+0000    11     256    133629    133373   757.482   698.312   0.0115177   0.0210567
2021-05-18T08:00:15.753431+0000    12     255    144941    144686   753.267   707.062    0.115263   0.0211404
2021-05-18T08:00:16.753638+0000    13     255    155117    154862   744.238       636    0.130556   0.0214167
2021-05-18T08:00:17.753896+0000 Total time run:       13.8843
Total reads made:     165257
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   743.903
Average IOPS:         11902
Stddev IOPS:          937.678
Max IOPS:             13973
Min IOPS:             10176
Average Latency(s):   0.0214609
Max latency(s):       0.133171
Min latency(s):       0.000443221

[1;32mlocalhost.localdomain	[2021-05-18T01:00:18,070322519-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2544564


[1;33mlocalhost.localdomain	[2021-05-18T01:00:18,078595285-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:00:41,046480612-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 165.26k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:00:41,064789859-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:00:48,991207654-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 165.26k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:00:49,008119114-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:00:56,961202915-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 165.26k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:00:56,978039197-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:01:05,020663418-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 165.26k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:01:05,037503001-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:01:13,184239306-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 165.26k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:01:13,200930544-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:01:13,214389332-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T01:01:13,219491280-07:00][RUNNING][ROUND 4/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:01:13,227229956-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T01:01:13,244640821-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40482\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.701193\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 5cfef230-deb2-4899-8f8d-486ede92936c\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 5cfef230-deb2-4899-8f8d-486ede92936c\nlast_changed 2021-05-18T01:01:40.876642-0700\ncreated 2021-05-18T01:01:40.876642-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40482/0,v1:10.10.1.2:40483/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.701193 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 e12642f9-4e03-48e2-a424-c96084617a79\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 79dafc9d-6906-4edf-9ee2-dccb816cca92\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 6380e297-e973-4360-8f8d-af6d2423ffd9\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42482\n  w/ user/pass: admin / ea5bb8a0-b844-4d49-afa4-664c4da0231d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:01:55 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40482
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.701193
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 5cfef230-deb2-4899-8f8d-486ede92936c
setting min_mon_release = octopus
epoch 0
fsid 5cfef230-deb2-4899-8f8d-486ede92936c
last_changed 2021-05-18T01:01:40.876642-0700
created 2021-05-18T01:01:40.876642-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40482/0,v1:10.10.1.2:40483/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.701193 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 e12642f9-4e03-48e2-a424-c96084617a79
0
start osd.0
add osd1 79dafc9d-6906-4edf-9ee2-dccb816cca92
1
start osd.1
add osd2 6380e297-e973-4360-8f8d-af6d2423ffd9
2
start osd.2


restful urls: https://10.10.1.2:42482
  w/ user/pass: admin / ea5bb8a0-b844-4d49-afa4-664c4da0231d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T01:01:14.253-0700 7ff1e528a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:01:14.257-0700 7ff1e528a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:01:14.273-0700 7fc215a8e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:01:14.273-0700 7fc215a8e1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40482,v1:10.10.1.2:40483] --print /tmp/ceph_monmap.701193 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.701193 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.701193 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42482 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.BJFpv1Tf89 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e12642f9-4e03-48e2-a424-c96084617a79 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBtdKNgeV9xMRAAECLjRprvAZCe5H6em/hKyg== --osd-uuid e12642f9-4e03-48e2-a424-c96084617a79 
2021-05-18T01:01:50.174-0700 7fc32a9acf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:01:50.174-0700 7fc32a9acf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:01:50.174-0700 7fc32a9acf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:01:50.258-0700 7fc32a9acf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 79dafc9d-6906-4edf-9ee2-dccb816cca92 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T01:01:50.538-0700 7efc7b598f00 -1 Falling back to public interface
2021-05-18T01:01:50.550-0700 7efc7b598f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBudKNgV4ACIBAAxE/b6Y6p5DcMQa5EbC5a8g== --osd-uuid 79dafc9d-6906-4edf-9ee2-dccb816cca92 
2021-05-18T01:01:50.874-0700 7f1172c1ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:01:50.878-0700 7f1172c1ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:01:50.878-0700 7f1172c1ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:01:50.930-0700 7f1172c1ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6380e297-e973-4360-8f8d-af6d2423ffd9 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T01:01:51.222-0700 7f16b9c12f00 -1 Falling back to public interface
2021-05-18T01:01:51.238-0700 7f16b9c12f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBvdKNgMk5bDRAAONqURKdYx9x2StXOxSIPqQ== --osd-uuid 6380e297-e973-4360-8f8d-af6d2423ffd9 
2021-05-18T01:01:51.570-0700 7fb83f2a1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:01:51.570-0700 7fb83f2a1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:01:51.570-0700 7fb83f2a1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:01:51.626-0700 7fb83f2a1f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T01:01:52.050-0700 7fe22f075f00 -1 Falling back to public interface
2021-05-18T01:01:52.066-0700 7fe22f075f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T01:01:55,869280541-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:01:55,877332860-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T01:01:55,960874643-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:01:55,967427847-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb598b3a-572d-4d39-8507-6d62f68e7b43', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb598b3a-572d-4d39-8507-6d62f68e7b43 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T01:01:58,825368895-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:01:58,831873731-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb598b3a-572d-4d39-8507-6d62f68e7b43', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb598b3a-572d-4d39-8507-6d62f68e7b43 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T01:02:01,884886135-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:02:01,891464724-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb598b3a-572d-4d39-8507-6d62f68e7b43', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb598b3a-572d-4d39-8507-6d62f68e7b43 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T01:02:04,557747004-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:02:04,563987180-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb598b3a-572d-4d39-8507-6d62f68e7b43', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb598b3a-572d-4d39-8507-6d62f68e7b43 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T01:02:10,403517271-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:02:10,409855984-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb598b3a-572d-4d39-8507-6d62f68e7b43', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb598b3a-572d-4d39-8507-6d62f68e7b43 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T01:02:14,212524251-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:02:14,219197701-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb598b3a-572d-4d39-8507-6d62f68e7b43', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb598b3a-572d-4d39-8507-6d62f68e7b43 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T01:02:17,535754006-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:02:17,542201008-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb598b3a-572d-4d39-8507-6d62f68e7b43', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb598b3a-572d-4d39-8507-6d62f68e7b43 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T01:02:20,705709060-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:02:20,712205630-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb598b3a-572d-4d39-8507-6d62f68e7b43', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb598b3a-572d-4d39-8507-6d62f68e7b43 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:02:24,123777745-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:02:24,130231108-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb598b3a-572d-4d39-8507-6d62f68e7b43', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb598b3a-572d-4d39-8507-6d62f68e7b43 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:02:27,407965587-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:02:27,414506055-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb598b3a-572d-4d39-8507-6d62f68e7b43', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb598b3a-572d-4d39-8507-6d62f68e7b43 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T01:02:30,607461047-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:02:30,613733404-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb598b3a-572d-4d39-8507-6d62f68e7b43', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb598b3a-572d-4d39-8507-6d62f68e7b43 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T01:02:33,425399375-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:02:33,431756796-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb598b3a-572d-4d39-8507-6d62f68e7b43', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb598b3a-572d-4d39-8507-6d62f68e7b43 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T01:02:36,074874008-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:02:58,828374204-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:03:06,764303150-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:03:14,831874880-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 28s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:03:22,822472231-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 51s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:03:30,666897592-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 59s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:03:38,662357121-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 71s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:03:46,901965811-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 81s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:03:54,845416653-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 42s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:04:02,718736688-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:04:02,735750843-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:04:10,701586329-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:04:10,718252905-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:04:18,625277775-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:04:18,642173335-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:04:26,573759593-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:04:26,590477193-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:04:34,404508206-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:04:34,421399536-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:04:34,434990123-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:04:34,442833443-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:04:34,460675814-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2558240
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-18T01:04:34,476029306-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-18T01:04:34,517076756-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:04:34,523662847-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb598b3a-572d-4d39-8507-6d62f68e7b43', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb598b3a-572d-4d39-8507-6d62f68e7b43 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:04:36.006+0000 ffffa163b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:04:36.014+0000 ffffa163b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:04:36.026+0000 ffffa163b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:04:36.062762+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-18T08:04:36.062808+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T08:04:36.076709+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:04:36.076709+0000     0       0         0         0         0         0           -           0
2021-05-18T08:04:37.076914+0000     1     255      9745      9490   593.077   593.125   0.0161946   0.0262653
2021-05-18T08:04:38.077170+0000     2     256     18905     18649   582.683   572.438   0.0100386   0.0271604
2021-05-18T08:04:39.077374+0000     3     256     27509     27253   567.669    537.75   0.0138186   0.0278158
2021-05-18T08:04:40.077631+0000     4     255     35386     35131   548.812   492.375   0.0520298   0.0289755
2021-05-18T08:04:41.077829+0000     5     256     44743     44487   555.977    584.75    0.017478   0.0286332
2021-05-18T08:04:42.078173+0000     6     255     53845     53590   558.105   568.938   0.0226681   0.0284993
2021-05-18T08:04:43.078429+0000     7     256     62092     61836   551.981   515.375  0.00499671   0.0288002
2021-05-18T08:04:44.078731+0000     8     255     70673     70418    550.01   536.375    0.010327   0.0289465
2021-05-18T08:04:45.078921+0000     9     256     78765     78509   545.075   505.688     0.01675   0.0292911
2021-05-18T08:04:46.079110+0000    10     256     86680     86424   540.027   494.688  0.00280899   0.0294513
2021-05-18T08:04:47.079291+0000    11     255     95008     94753   538.249   520.562   0.0390961   0.0296477
2021-05-18T08:04:48.079571+0000    12     256    104038    103782   540.408   564.312  0.00524821   0.0295495
2021-05-18T08:04:49.079779+0000    13     255    112045    111790    537.33     500.5   0.0225886   0.0297112
2021-05-18T08:04:50.079963+0000    14     255    120191    119936   535.309   509.125    0.049406   0.0298325
2021-05-18T08:04:51.080163+0000    15     255    128591    128336   534.615       525   0.0119445   0.0298732
2021-05-18T08:04:52.080439+0000    16     255    136943    136688   533.817       522   0.0101534   0.0299307
2021-05-18T08:04:53.080732+0000    17     256    145639    145383   534.374   543.438   0.0216317   0.0299146
2021-05-18T08:04:54.081012+0000    18     256    154351    154095   534.928     544.5     0.01467   0.0298738
2021-05-18T08:04:55.081439+0000    19     255    162553    162298   533.746   512.688   0.0309874   0.0299356
2021-05-18T08:04:56.081762+0000 min lat: 0.00102725 max lat: 0.13238 avg lat: 0.0298868
2021-05-18T08:04:56.081762+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:04:56.081762+0000    20     143    171367    171224   534.943   557.875    0.014311   0.0298868
2021-05-18T08:04:57.082267+0000 Total time run:         20.0181
Total writes made:      171367
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     535.037
Stddev Bandwidth:       30.3329
Max bandwidth (MB/sec): 593.125
Min bandwidth (MB/sec): 492.375
Average IOPS:           8560
Stddev IOPS:            485.326
Max IOPS:               9490
Min IOPS:               7878
Average Latency(s):     0.0298753
Stddev Latency(s):      0.0209563
Max latency(s):         0.13238
Min latency(s):         0.00102725

[1;32mlocalhost.localdomain	[2021-05-18T01:04:57,403365578-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2558240


[1;33mlocalhost.localdomain	[2021-05-18T01:04:57,412006178-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:05:20,573449397-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.37k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:05:20,590401712-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:05:28,785858184-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.37k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:05:28,802755514-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:05:36,912875919-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.37k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:05:36,929500729-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:05:44,844879005-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.37k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:05:44,861616664-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:05:52,685433201-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.37k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:05:52,702620561-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:05:52,716127600-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:05:52,724076853-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:05:52,742334900-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2561566
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-18T01:05:52,757511963-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-18T01:05:52,797550148-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:05:52,803963229-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb598b3a-572d-4d39-8507-6d62f68e7b43', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb598b3a-572d-4d39-8507-6d62f68e7b43 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.x0CIMz:/tmp/ceph-asok.x0CIMz -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:05:54.379+0000 ffffa8895010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:05:54.387+0000 ffffa8895010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:05:54.387+0000 ffffa8895010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:05:54.408142+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:05:54.408142+0000     0       0         0         0         0         0           -           0
2021-05-18T08:05:55.408673+0000     1     256     12452     12196   761.746    762.25  0.00504176   0.0201795
2021-05-18T08:05:56.409026+0000     2     255     24265     24010   749.932   738.375   0.0605819   0.0210203
2021-05-18T08:05:57.409311+0000     3     256     37213     36957   769.604   809.188 0.000647736   0.0205563
2021-05-18T08:05:58.409745+0000     4     255     49329     49074   766.449   757.312 0.000892937   0.0206957
2021-05-18T08:05:59.409969+0000     5     255     60809     60554   756.629     717.5   0.0495231   0.0209933
2021-05-18T08:06:00.410295+0000     6     256     70717     70461    733.69   619.188   0.0728297   0.0216719
2021-05-18T08:06:01.410536+0000     7     255     80947     80692   720.205   639.438  0.00278543    0.022069
2021-05-18T08:06:02.410952+0000     8     255     90550     90295   705.171   600.188  0.00895169   0.0225714
2021-05-18T08:06:03.411142+0000     9     255    102308    102053   708.455   734.875  0.00197579   0.0224876
2021-05-18T08:06:04.411781+0000    10     256    115069    114813   717.311     797.5   0.0661241   0.0222104
2021-05-18T08:06:05.412053+0000    11     256    125052    124796   708.808   623.938   0.0445817   0.0224898
2021-05-18T08:06:06.412401+0000    12     256    134951    134695    701.28   618.688 0.000550624   0.0226814
2021-05-18T08:06:07.412583+0000    13     256    145454    145198   697.822   656.438   0.0522686   0.0228508
2021-05-18T08:06:08.412775+0000    14     255    156495    156240   697.263   690.125  0.00776067   0.0228179
2021-05-18T08:06:09.413017+0000    15     256    165977    165721   690.274   592.562   0.0510692   0.0231009
2021-05-18T08:06:10.413337+0000 Total time run:       15.6094
Total reads made:     171367
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   686.153
Average IOPS:         10978
Stddev IOPS:          1181.39
Max IOPS:             12947
Min IOPS:             9481
Average Latency(s):   0.0232546
Max latency(s):       0.16234
Min latency(s):       0.000440625

[1;32mlocalhost.localdomain	[2021-05-18T01:06:10,742990384-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2561566


[1;33mlocalhost.localdomain	[2021-05-18T01:06:10,751871370-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:06:33,705046126-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.37k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:06:33,721970222-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:06:41,848558728-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.37k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:06:41,865364739-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:06:49,854945091-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.37k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:06:49,875947091-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:06:57,834874101-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.37k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:06:57,851970876-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:07:05,851404822-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.37k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:07:05,868509038-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:07:05,881855295-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T01:07:05,887196399-07:00][RUNNING][ROUND 5/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:07:05,894905402-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T01:07:05,912072974-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40303\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.702373\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f9026665-0310-4907-9dba-0c6631dfb2dc\nsetting min_mon_release = octopus\nepoch 0\nfsid f9026665-0310-4907-9dba-0c6631dfb2dc\nlast_changed 2021-05-18T01:07:33.835928-0700\ncreated 2021-05-18T01:07:33.835928-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40303/0,v1:10.10.1.2:40304/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.702373 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 39899daa-429e-4ad7-ad89-a5af0d2911ad\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 f07bf417-39fb-4c22-8400-0526fc2c8e41\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 53e8ad6d-fa7b-4cfe-9ce0-8ab7f0feab15\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42303\n  w/ user/pass: admin / 3027bad0-8527-4fd8-bf95-a98958a56941\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:07:48 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40303
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.702373
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f9026665-0310-4907-9dba-0c6631dfb2dc
setting min_mon_release = octopus
epoch 0
fsid f9026665-0310-4907-9dba-0c6631dfb2dc
last_changed 2021-05-18T01:07:33.835928-0700
created 2021-05-18T01:07:33.835928-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40303/0,v1:10.10.1.2:40304/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.702373 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 39899daa-429e-4ad7-ad89-a5af0d2911ad
0
start osd.0
add osd1 f07bf417-39fb-4c22-8400-0526fc2c8e41
1
start osd.1
add osd2 53e8ad6d-fa7b-4cfe-9ce0-8ab7f0feab15
2
start osd.2


restful urls: https://10.10.1.2:42303
  w/ user/pass: admin / 3027bad0-8527-4fd8-bf95-a98958a56941


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T01:07:06.905-0700 7f91bcf3b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:07:06.905-0700 7f91bcf3b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:07:06.921-0700 7f8eaf5bb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:07:06.921-0700 7f8eaf5bb1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40303,v1:10.10.1.2:40304] --print /tmp/ceph_monmap.702373 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.702373 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.702373 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42303 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.A1vDdSoIef 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 39899daa-429e-4ad7-ad89-a5af0d2911ad -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDOdaNg6bXOLRAAJME+JnDXjgUU02r3qtqN/A== --osd-uuid 39899daa-429e-4ad7-ad89-a5af0d2911ad 
2021-05-18T01:07:43.177-0700 7fa47fc27f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:07:43.177-0700 7fa47fc27f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:07:43.177-0700 7fa47fc27f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:07:43.229-0700 7fa47fc27f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f07bf417-39fb-4c22-8400-0526fc2c8e41 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T01:07:43.533-0700 7f23abfeff00 -1 Falling back to public interface
2021-05-18T01:07:43.545-0700 7f23abfeff00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDPdaNgl0XNHxAASQc9scjVbyhx8IFOmzM15w== --osd-uuid f07bf417-39fb-4c22-8400-0526fc2c8e41 
2021-05-18T01:07:43.861-0700 7fb0503b0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:07:43.861-0700 7fb0503b0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:07:43.861-0700 7fb0503b0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:07:43.917-0700 7fb0503b0f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 53e8ad6d-fa7b-4cfe-9ce0-8ab7f0feab15 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T01:07:44.245-0700 7f676f4dff00 -1 Falling back to public interface
2021-05-18T01:07:44.257-0700 7f676f4dff00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDQdaNg8UeRDhAAxPd86JjCquhQPeG632fyoA== --osd-uuid 53e8ad6d-fa7b-4cfe-9ce0-8ab7f0feab15 
2021-05-18T01:07:44.597-0700 7fd6cf389f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:07:44.597-0700 7fd6cf389f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:07:44.597-0700 7fd6cf389f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:07:44.669-0700 7fd6cf389f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T01:07:44.969-0700 7ff671a0af00 -1 Falling back to public interface
2021-05-18T01:07:44.985-0700 7ff671a0af00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T01:07:48,973874846-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:07:48,982213161-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T01:07:49,065013067-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:07:49,071339824-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '964fb6cd-5984-4f7a-a428-9f8555094b22', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 964fb6cd-5984-4f7a-a428-9f8555094b22 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T01:07:51,870022703-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:07:51,876538762-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '964fb6cd-5984-4f7a-a428-9f8555094b22', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 964fb6cd-5984-4f7a-a428-9f8555094b22 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T01:07:54,636131859-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:07:54,642608176-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '964fb6cd-5984-4f7a-a428-9f8555094b22', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 964fb6cd-5984-4f7a-a428-9f8555094b22 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T01:07:57,588574375-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:07:57,595309742-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '964fb6cd-5984-4f7a-a428-9f8555094b22', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 964fb6cd-5984-4f7a-a428-9f8555094b22 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T01:08:03,243317796-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:08:03,249829444-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '964fb6cd-5984-4f7a-a428-9f8555094b22', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 964fb6cd-5984-4f7a-a428-9f8555094b22 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T01:08:06,235170831-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:08:06,241699598-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '964fb6cd-5984-4f7a-a428-9f8555094b22', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 964fb6cd-5984-4f7a-a428-9f8555094b22 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T01:08:09,635613664-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:08:09,642108360-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '964fb6cd-5984-4f7a-a428-9f8555094b22', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 964fb6cd-5984-4f7a-a428-9f8555094b22 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T01:08:13,282443929-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:08:13,289117185-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '964fb6cd-5984-4f7a-a428-9f8555094b22', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 964fb6cd-5984-4f7a-a428-9f8555094b22 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:08:16,780579173-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:08:16,786980853-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '964fb6cd-5984-4f7a-a428-9f8555094b22', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 964fb6cd-5984-4f7a-a428-9f8555094b22 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:08:19,888079607-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:08:19,894770251-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '964fb6cd-5984-4f7a-a428-9f8555094b22', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 964fb6cd-5984-4f7a-a428-9f8555094b22 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T01:08:23,424831809-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:08:23,431301795-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '964fb6cd-5984-4f7a-a428-9f8555094b22', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 964fb6cd-5984-4f7a-a428-9f8555094b22 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T01:08:26,240911233-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:08:26,247533900-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '964fb6cd-5984-4f7a-a428-9f8555094b22', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 964fb6cd-5984-4f7a-a428-9f8555094b22 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T01:08:28,824633183-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:08:51,800235533-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   198 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:08:59,762968063-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:09:07,651526439-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [========....................] (remaining: 24s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:09:15,606051681-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 47s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:09:23,848017353-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 53s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:09:31,702282902-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 71s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:09:39,639072197-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 81s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:09:47,604099889-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [=========...................] (remaining: 102s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:09:55,718820180-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:09:55,738237761-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:10:04,446814204-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:10:04,464064681-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:10:12,358979970-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:10:12,379129751-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:10:20,354014150-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:10:20,371133970-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:10:28,355658202-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:10:28,372742707-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:10:28,386508556-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:10:28,394595038-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:10:28,412830763-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2575264
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-18T01:10:28,428139775-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-18T01:10:28,469189136-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:10:28,475811459-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '964fb6cd-5984-4f7a-a428-9f8555094b22', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 964fb6cd-5984-4f7a-a428-9f8555094b22 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27 -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:10:29.982+0000 ffffae30b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:10:29.990+0000 ffffae30b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:10:29.990+0000 ffffae30b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:10:30.008316+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-18T08:10:30.008360+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T08:10:30.022356+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:10:30.022356+0000     0       0         0         0         0         0           -           0
2021-05-18T08:10:31.022591+0000     1     255      6571      6316   394.708    394.75   0.0397421   0.0388241
2021-05-18T08:10:32.022799+0000     2     255     13438     13183   411.904   429.188   0.0203245   0.0383707
2021-05-18T08:10:33.023317+0000     3     256     20166     19910   414.676   420.438  0.00108566   0.0380549
2021-05-18T08:10:34.023507+0000     4     256     27151     26895   420.127   436.562  0.00611933   0.0377307
2021-05-18T08:10:35.023732+0000     5     255     34111     33856   423.094   435.062   0.0373529   0.0374811
2021-05-18T08:10:36.024016+0000     6     255     41100     40845    425.36   436.812  0.00389573   0.0373739
2021-05-18T08:10:37.024245+0000     7     256     48106     47850   427.125   437.812  0.00255114   0.0372667
2021-05-18T08:10:38.024528+0000     8     255     55166     54911   428.883   441.312   0.0354687    0.037134
2021-05-18T08:10:39.024750+0000     9     255     62087     61832   429.281   432.562   0.0318729   0.0371481
2021-05-18T08:10:40.024929+0000    10     256     68789     68533   428.227   418.812   0.0222164   0.0372566
2021-05-18T08:10:41.025133+0000    11     256     74981     74725   424.472       387   0.0131485   0.0376213
2021-05-18T08:10:42.025395+0000    12     256     81761     81505   424.402    423.75   0.0350382     0.03762
2021-05-18T08:10:43.025768+0000    13     255     87933     87678   421.422   385.812   0.0389346   0.0378582
2021-05-18T08:10:44.025999+0000    14     256     93946     93690   418.154    375.75  0.00119245   0.0381109
2021-05-18T08:10:45.026167+0000    15     255    100457    100202   417.406       407   0.0257857   0.0382582
2021-05-18T08:10:46.026337+0000    16     255    106668    106413   415.576   388.188  0.00679622   0.0384252
2021-05-18T08:10:47.026756+0000    17     256    112871    112615   413.922   387.625  0.00129391   0.0385748
2021-05-18T08:10:48.026917+0000    18     255    119897    119642   415.321   439.188  0.00152786    0.038448
2021-05-18T08:10:49.027137+0000    19     256    126128    125872   413.951   389.375   0.0248809   0.0385994
2021-05-18T08:10:50.027319+0000 min lat: 0.00097023 max lat: 0.195369 avg lat: 0.0385247
2021-05-18T08:10:50.027319+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:10:50.027319+0000    20     176    132963    132787   414.859   432.188    0.004569   0.0385247
2021-05-18T08:10:51.027575+0000 Total time run:         20.0352
Total writes made:      132963
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     414.78
Stddev Bandwidth:       22.7599
Max bandwidth (MB/sec): 441.312
Min bandwidth (MB/sec): 375.75
Average IOPS:           6636
Stddev IOPS:            364.159
Max IOPS:               7061
Min IOPS:               6012
Average Latency(s):     0.0385319
Stddev Latency(s):      0.027605
Max latency(s):         0.195369
Min latency(s):         0.00097023

[1;32mlocalhost.localdomain	[2021-05-18T01:10:51,329200168-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2575264


[1;33mlocalhost.localdomain	[2021-05-18T01:10:51,337718618-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:11:14,164186049-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 132.96k objects, 8.1 GiB
    usage:   16 GiB used, 284 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:11:14,181379697-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:11:22,284261926-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 132.96k objects, 8.1 GiB
    usage:   16 GiB used, 284 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:11:22,301662535-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:11:30,099448527-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 132.96k objects, 8.1 GiB
    usage:   16 GiB used, 284 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:11:30,118006253-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:11:38,205449563-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 132.96k objects, 8.1 GiB
    usage:   16 GiB used, 284 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:11:38,223235032-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:11:46,200751574-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 132.96k objects, 8.1 GiB
    usage:   16 GiB used, 284 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:11:46,218059287-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:11:46,231994455-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:11:46,240102031-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:11:46,258697728-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2578593
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-18T01:11:46,274168003-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-18T01:11:46,314029477-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:11:46,320535740-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '964fb6cd-5984-4f7a-a428-9f8555094b22', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 964fb6cd-5984-4f7a-a428-9f8555094b22 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NRBg27:/tmp/ceph-asok.NRBg27 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:11:47.884+0000 ffffa6749010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:11:47.892+0000 ffffa6749010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:11:47.892+0000 ffffa6749010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:11:47.912575+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:11:47.912575+0000     0       0         0         0         0         0           -           0
2021-05-18T08:11:48.912749+0000     1     256     12276     12020   751.032    751.25  0.00715559   0.0206053
2021-05-18T08:11:49.912938+0000     2     256     21315     21059   657.936   564.938  0.00666161   0.0239503
2021-05-18T08:11:50.913131+0000     3     255     33544     33289   693.365   764.375    0.033873   0.0228845
2021-05-18T08:11:51.913325+0000     4     255     45631     45376   708.846   755.438   0.0100086   0.0224276
2021-05-18T08:11:52.913538+0000     5     256     57545     57289   715.958   744.562   0.0371605   0.0222258
2021-05-18T08:11:53.913814+0000     6     255     68785     68530   713.693   702.562   0.0498876    0.022298
2021-05-18T08:11:54.914028+0000     7     255     81141     80886   722.035    772.25   0.0488003   0.0220413
2021-05-18T08:11:55.914289+0000     8     255     93447     93192   727.896   769.125   0.0418163   0.0218893
2021-05-18T08:11:56.914574+0000     9     256    105823    105567   732.932   773.438   0.0203582   0.0217456
2021-05-18T08:11:57.914797+0000    10     255    118436    118181   738.459   788.375   0.0315562   0.0215871
2021-05-18T08:11:58.915912+0000    11     256    129937    129681   736.593    718.75   0.0112465   0.0216573
2021-05-18T08:11:59.916165+0000 Total time run:       11.3032
Total reads made:     132963
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   735.205
Average IOPS:         11763
Stddev IOPS:          995.547
Max IOPS:             12614
Min IOPS:             9039
Average Latency(s):   0.0216815
Max latency(s):       0.109136
Min latency(s):       0.000467585

[1;32mlocalhost.localdomain	[2021-05-18T01:12:00,251994336-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2578593


[1;33mlocalhost.localdomain	[2021-05-18T01:12:00,260463894-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:12:23,663637641-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 132.96k objects, 8.1 GiB
    usage:   16 GiB used, 284 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:12:23,681614213-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:12:31,709750717-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 132.96k objects, 8.1 GiB
    usage:   16 GiB used, 284 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:12:31,727180235-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:12:39,759933819-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 132.96k objects, 8.1 GiB
    usage:   16 GiB used, 284 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:12:39,777421563-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:12:47,835907618-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 132.96k objects, 8.1 GiB
    usage:   16 GiB used, 284 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:12:47,855756344-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:12:55,781598294-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 132.96k objects, 8.1 GiB
    usage:   16 GiB used, 284 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:12:55,799150623-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:12:55,813357863-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T01:12:55,821862812-07:00][RUNNING][ROUND 1/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:12:55,829942993-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T01:12:55,847323514-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40463\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.703486\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 34f3dff4-26a7-4163-b453-34898d9b370f\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 34f3dff4-26a7-4163-b453-34898d9b370f\nlast_changed 2021-05-18T01:13:27.479177-0700\ncreated 2021-05-18T01:13:27.479177-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40463/0,v1:10.10.1.2:40464/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.703486 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 a3b52a1e-1e32-4c74-b588-0381ce9469b1\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 b0450799-9ccf-4257-af16-8fe325993a04\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 28fdabaf-3d81-43b3-b744-19fabbd3d39f\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42463\n  w/ user/pass: admin / 07ddc542-7d0e-4467-b049-b4b2cf75df5c\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:13:42 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40463
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.703486
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 34f3dff4-26a7-4163-b453-34898d9b370f
setting min_mon_release = octopus
epoch 0
fsid 34f3dff4-26a7-4163-b453-34898d9b370f
last_changed 2021-05-18T01:13:27.479177-0700
created 2021-05-18T01:13:27.479177-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40463/0,v1:10.10.1.2:40464/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.703486 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 a3b52a1e-1e32-4c74-b588-0381ce9469b1
0
start osd.0
add osd1 b0450799-9ccf-4257-af16-8fe325993a04
1
start osd.1
add osd2 28fdabaf-3d81-43b3-b744-19fabbd3d39f
2
start osd.2


restful urls: https://10.10.1.2:42463
  w/ user/pass: admin / 07ddc542-7d0e-4467-b049-b4b2cf75df5c


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T01:12:56.864-0700 7fe5b77e21c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:12:56.864-0700 7fe5b77e21c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:12:56.880-0700 7fc3b0c881c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:12:56.880-0700 7fc3b0c881c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40463,v1:10.10.1.2:40464] --print /tmp/ceph_monmap.703486 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.703486 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.703486 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42463 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.ukx7waneed 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a3b52a1e-1e32-4c74-b588-0381ce9469b1 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAwd6NgCnmOFhAA2Zr1jstCkrEpH4mR2YGPAA== --osd-uuid a3b52a1e-1e32-4c74-b588-0381ce9469b1 
2021-05-18T01:13:36.784-0700 7f351f9adf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:13:36.784-0700 7f351f9adf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:13:36.784-0700 7f351f9adf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:13:36.828-0700 7f351f9adf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b0450799-9ccf-4257-af16-8fe325993a04 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T01:13:37.132-0700 7f4ddeeeaf00 -1 Falling back to public interface
2021-05-18T01:13:37.144-0700 7f4ddeeeaf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAxd6NgwacOCBAAN43i7Gj6/dgsbazbbOr66Q== --osd-uuid b0450799-9ccf-4257-af16-8fe325993a04 
2021-05-18T01:13:37.456-0700 7f3e82dbcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:13:37.456-0700 7f3e82dbcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:13:37.456-0700 7f3e82dbcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:13:37.500-0700 7f3e82dbcf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 28fdabaf-3d81-43b3-b744-19fabbd3d39f -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T01:13:37.876-0700 7f5d4342af00 -1 Falling back to public interface
2021-05-18T01:13:37.888-0700 7f5d4342af00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAxd6Ng4nFqNBAAHhKj+a8xGYLCNCxOzjn+RQ== --osd-uuid 28fdabaf-3d81-43b3-b744-19fabbd3d39f 
2021-05-18T01:13:38.216-0700 7f786439bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:13:38.216-0700 7f786439bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:13:38.216-0700 7f786439bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:13:38.304-0700 7f786439bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T01:13:38.624-0700 7fd215443f00 -1 Falling back to public interface
2021-05-18T01:13:38.636-0700 7fd215443f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T01:13:42,544711730-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:13:42,553221900-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T01:13:42,637902339-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:13:42,644266648-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c262ade-74e6-43a8-91a3-2629c34db856', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c262ade-74e6-43a8-91a3-2629c34db856 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T01:13:45,391046043-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:13:45,397625654-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c262ade-74e6-43a8-91a3-2629c34db856', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c262ade-74e6-43a8-91a3-2629c34db856 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T01:13:48,150209629-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:13:48,159718819-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c262ade-74e6-43a8-91a3-2629c34db856', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c262ade-74e6-43a8-91a3-2629c34db856 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T01:13:51,064086665-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:13:51,070401436-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c262ade-74e6-43a8-91a3-2629c34db856', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c262ade-74e6-43a8-91a3-2629c34db856 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T01:13:57,015719889-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:13:57,022491142-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c262ade-74e6-43a8-91a3-2629c34db856', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c262ade-74e6-43a8-91a3-2629c34db856 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T01:14:01,015280688-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:14:01,021886868-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c262ade-74e6-43a8-91a3-2629c34db856', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c262ade-74e6-43a8-91a3-2629c34db856 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T01:14:04,893084409-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:14:04,899357097-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c262ade-74e6-43a8-91a3-2629c34db856', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c262ade-74e6-43a8-91a3-2629c34db856 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T01:14:07,911731875-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:14:07,918223871-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c262ade-74e6-43a8-91a3-2629c34db856', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c262ade-74e6-43a8-91a3-2629c34db856 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:14:11,239735160-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:14:11,246064664-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c262ade-74e6-43a8-91a3-2629c34db856', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c262ade-74e6-43a8-91a3-2629c34db856 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:14:15,375422783-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:14:15,381753171-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c262ade-74e6-43a8-91a3-2629c34db856', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c262ade-74e6-43a8-91a3-2629c34db856 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T01:14:18,305460060-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:14:18,311908009-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c262ade-74e6-43a8-91a3-2629c34db856', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c262ade-74e6-43a8-91a3-2629c34db856 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T01:14:21,225001972-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:14:21,231332435-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c262ade-74e6-43a8-91a3-2629c34db856', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c262ade-74e6-43a8-91a3-2629c34db856 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T01:14:23,792980702-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:14:46,632342373-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   206 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=========...................] (remaining: 9s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:14:54,788286369-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:15:02,768719047-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:15:10,632011059-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:15:18,759260253-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:15:26,689358365-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:15:34,655967606-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:15:42,822769267-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:15:42,840274714-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:15:50,908555665-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:15:50,926085626-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:15:58,963867605-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:15:58,981444240-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:16:06,874239143-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:16:06,892074796-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:16:14,885445537-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:16:14,903012240-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:16:14,917324918-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:16:14,925572468-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:16:14,944671532-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2592024
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-18T01:16:14,960179869-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-18T01:16:15,002252977-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:16:15,008909251-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c262ade-74e6-43a8-91a3-2629c34db856', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c262ade-74e6-43a8-91a3-2629c34db856 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:16:16.613+0000 ffffa914e010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:16:16.621+0000 ffffa914e010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:16:16.621+0000 ffffa914e010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:16:16.635515+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-18T08:16:16.635570+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T08:16:16.686345+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:16:16.686345+0000     0       0         0         0         0         0           -           0
2021-05-18T08:16:17.686599+0000     1     255      2675      2420   604.932       605   0.0922691   0.0980784
2021-05-18T08:16:18.686841+0000     2     255      5214      4959   619.765    634.75   0.0884048   0.0998581
2021-05-18T08:16:19.687552+0000     3     255      7988      7733   644.188     693.5   0.0731271   0.0966687
2021-05-18T08:16:20.687783+0000     4     255     10324     10069   629.109       584   0.0796321   0.0995769
2021-05-18T08:16:21.688170+0000     5     255     12847     12592   629.388    630.75    0.155221   0.0994007
2021-05-18T08:16:22.689495+0000     6     255     15481     15226   634.099     658.5   0.0590415   0.0998167
2021-05-18T08:16:23.689956+0000     7     255     18105     17850   637.184       656    0.151692   0.0992912
2021-05-18T08:16:24.690349+0000     8     255     20488     20233   631.976    595.75   0.0875431   0.0996451
2021-05-18T08:16:25.690545+0000     9     255     22935     22680   629.716    611.75    0.135738     0.10038
2021-05-18T08:16:26.690742+0000    10     255     25648     25393   634.555    678.25   0.0666088    0.100072
2021-05-18T08:16:27.690942+0000    11     255     27989     27734   630.063    585.25     0.15022    0.100114
2021-05-18T08:16:28.691203+0000    12     255     30429     30174   628.378       610    0.138941    0.101054
2021-05-18T08:16:29.691511+0000    13     255     33039     32784   630.218     652.5   0.0712201    0.101005
2021-05-18T08:16:30.691783+0000    14     255     35544     35289   629.922    626.25   0.0809037    0.100967
2021-05-18T08:16:31.692009+0000    15     255     37917     37662   627.469    593.25   0.0778349    0.101441
2021-05-18T08:16:32.692233+0000    16     255     40477     40222   628.243       640   0.0629234    0.101395
2021-05-18T08:16:33.692463+0000    17     255     43122     42867   630.176    661.25   0.0797939    0.101169
2021-05-18T08:16:34.692742+0000    18     255     45381     45126   626.532    564.75   0.0687362     0.10171
2021-05-18T08:16:35.693666+0000    19     255     47665     47410    623.58       571     0.28008    0.102077
2021-05-18T08:16:36.693887+0000 min lat: 0.0161355 max lat: 0.343189 avg lat: 0.102528
2021-05-18T08:16:36.693887+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:16:36.693887+0000    20      57     49891     49834   622.695       606   0.0261174    0.102528
2021-05-18T08:16:37.694123+0000 Total time run:         20.0197
Total writes made:      49891
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     623.024
Stddev Bandwidth:       36.1155
Max bandwidth (MB/sec): 693.5
Min bandwidth (MB/sec): 564.75
Average IOPS:           2492
Stddev IOPS:            144.462
Max IOPS:               2774
Min IOPS:               2259
Average Latency(s):     0.102527
Stddev Latency(s):      0.0450561
Max latency(s):         0.343189
Min latency(s):         0.0161355

[1;32mlocalhost.localdomain	[2021-05-18T01:16:38,008222799-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2592024


[1;33mlocalhost.localdomain	[2021-05-18T01:16:38,016808822-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:17:00,998686454-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.89k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:17:01,016588329-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:17:08,972250486-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.89k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:17:08,989968264-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:17:17,226184377-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.89k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:17:17,243986200-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:17:25,411829463-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.89k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:17:25,432278682-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:17:33,428127969-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.89k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:17:33,445919080-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:17:33,460248111-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:17:33,468551126-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:17:33,487629621-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2595336
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-18T01:17:33,503679250-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-18T01:17:33,543271521-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:17:33,549630114-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c262ade-74e6-43a8-91a3-2629c34db856', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c262ade-74e6-43a8-91a3-2629c34db856 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ypc2mk:/tmp/ceph-asok.Ypc2mk -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:17:35.103+0000 ffffaf6c2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:17:35.111+0000 ffffaf6c2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:17:35.111+0000 ffffaf6c2010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:17:35.132147+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:17:35.132147+0000     0       0         0         0         0         0           -           0
2021-05-18T08:17:36.137596+0000     1     255      4171      3916   973.554       979    0.114984   0.0623603
2021-05-18T08:17:37.137922+0000     2     255      8826      8571   1068.21   1163.75   0.0834384   0.0586557
2021-05-18T08:17:38.138355+0000     3     255     13411     13156   1094.02   1146.25  0.00902375    0.057414
2021-05-18T08:17:39.138988+0000     4     255     17620     17365   1083.42   1052.25    0.123478   0.0582722
2021-05-18T08:17:40.139337+0000     5     255     22195     21940   1095.39   1143.75  0.00210027   0.0575949
2021-05-18T08:17:41.139786+0000     6     255     26535     26280   1093.58      1085   0.0183101   0.0578393
2021-05-18T08:17:42.140876+0000     7     256     30593     30337   1082.09   1014.25   0.0327545    0.058464
2021-05-18T08:17:43.142080+0000     8     256     34600     34344    1071.9   1001.75   0.0457067   0.0592323
2021-05-18T08:17:44.142418+0000     9     255     38831     38576   1070.32      1058   0.0224494   0.0593269
2021-05-18T08:17:45.142728+0000    10     256     42908     42652   1065.16      1019   0.0383276   0.0597214
2021-05-18T08:17:46.142998+0000    11     255     46937     46682    1059.9    1007.5   0.0266473   0.0600375
2021-05-18T08:17:47.143246+0000 Total time run:       11.9096
Total reads made:     49891
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1047.28
Average IOPS:         4189
Stddev IOPS:          260.402
Max IOPS:             4655
Min IOPS:             3916
Average Latency(s):   0.060699
Max latency(s):       0.265047
Min latency(s):       0.000948386

[1;32mlocalhost.localdomain	[2021-05-18T01:17:47,474351895-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2595336


[1;33mlocalhost.localdomain	[2021-05-18T01:17:47,483688106-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:18:10,421344826-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.89k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:18:10,439425651-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:18:18,529557732-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.89k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:18:18,547531754-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:18:26,477891525-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.89k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:18:26,498989890-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:18:34,534787377-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.89k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:18:34,552966469-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:18:42,524717075-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.89k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:18:42,542456158-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:18:42,556684687-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T01:18:42,562005809-07:00][RUNNING][ROUND 2/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:18:42,570265646-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T01:18:42,587786681-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40652\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.704598\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c8d203f9-5e5f-4002-bcdb-a052c7d7bdcc\nsetting min_mon_release = octopus\nepoch 0\nfsid c8d203f9-5e5f-4002-bcdb-a052c7d7bdcc\nlast_changed 2021-05-18T01:19:10.190234-0700\ncreated 2021-05-18T01:19:10.190234-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40652/0,v1:10.10.1.2:40653/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.704598 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 a1be56ac-3076-4a96-8bf9-8ecafc33afe2\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 0d1668f0-2381-4862-bb07-e13d962bad83\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 3b4a7179-bb17-4f03-8e39-81f9e36351bd\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42652\n  w/ user/pass: admin / 6504fae8-d006-474d-bd73-4f1b60a4bdbb\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:19:25 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40652
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.704598
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c8d203f9-5e5f-4002-bcdb-a052c7d7bdcc
setting min_mon_release = octopus
epoch 0
fsid c8d203f9-5e5f-4002-bcdb-a052c7d7bdcc
last_changed 2021-05-18T01:19:10.190234-0700
created 2021-05-18T01:19:10.190234-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40652/0,v1:10.10.1.2:40653/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.704598 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 a1be56ac-3076-4a96-8bf9-8ecafc33afe2
0
start osd.0
add osd1 0d1668f0-2381-4862-bb07-e13d962bad83
1
start osd.1
add osd2 3b4a7179-bb17-4f03-8e39-81f9e36351bd
2
start osd.2


restful urls: https://10.10.1.2:42652
  w/ user/pass: admin / 6504fae8-d006-474d-bd73-4f1b60a4bdbb


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T01:18:43.575-0700 7f4f04ea01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:18:43.575-0700 7f4f04ea01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:18:43.591-0700 7f52af2fa1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:18:43.591-0700 7f52af2fa1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40652,v1:10.10.1.2:40653] --print /tmp/ceph_monmap.704598 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.704598 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.704598 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42652 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.AO5RtAGBsU 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a1be56ac-3076-4a96-8bf9-8ecafc33afe2 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCHeKNg132fBBAAPx8xM/KJ37SxqXL2OkRvtQ== --osd-uuid a1be56ac-3076-4a96-8bf9-8ecafc33afe2 
2021-05-18T01:19:19.408-0700 7fe344089f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:19:19.408-0700 7fe344089f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:19:19.408-0700 7fe344089f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:19:19.472-0700 7fe344089f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0d1668f0-2381-4862-bb07-e13d962bad83 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T01:19:19.724-0700 7ff0dee9ef00 -1 Falling back to public interface
2021-05-18T01:19:19.736-0700 7ff0dee9ef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCHeKNgbRtDKxAAX0S23T5Y29F+q+AIQ8t0BA== --osd-uuid 0d1668f0-2381-4862-bb07-e13d962bad83 
2021-05-18T01:19:20.060-0700 7f1993541f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:19:20.064-0700 7f1993541f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:19:20.064-0700 7f1993541f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:19:20.108-0700 7f1993541f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3b4a7179-bb17-4f03-8e39-81f9e36351bd -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T01:19:20.400-0700 7f5045687f00 -1 Falling back to public interface
2021-05-18T01:19:20.416-0700 7f5045687f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCIeKNgLBkUGBAAwQdDJVIVHJ4soq0sqjitvw== --osd-uuid 3b4a7179-bb17-4f03-8e39-81f9e36351bd 
2021-05-18T01:19:20.788-0700 7f7eccaa9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:19:20.792-0700 7f7eccaa9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:19:20.792-0700 7f7eccaa9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:19:20.844-0700 7f7eccaa9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T01:19:21.196-0700 7f1bf6c40f00 -1 Falling back to public interface
2021-05-18T01:19:21.212-0700 7f1bf6c40f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T01:19:25,116242447-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:19:25,124897467-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T01:19:25,207547119-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:19:25,213985031-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c60f9cd5-6db3-40d8-91c4-5b9912672570', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c60f9cd5-6db3-40d8-91c4-5b9912672570 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T01:19:27,944731348-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:19:27,951459964-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c60f9cd5-6db3-40d8-91c4-5b9912672570', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c60f9cd5-6db3-40d8-91c4-5b9912672570 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T01:19:30,810927719-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:19:30,817067860-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c60f9cd5-6db3-40d8-91c4-5b9912672570', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c60f9cd5-6db3-40d8-91c4-5b9912672570 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T01:19:33,558124295-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:19:33,565137105-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c60f9cd5-6db3-40d8-91c4-5b9912672570', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c60f9cd5-6db3-40d8-91c4-5b9912672570 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T01:19:39,265956072-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:19:39,272513869-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c60f9cd5-6db3-40d8-91c4-5b9912672570', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c60f9cd5-6db3-40d8-91c4-5b9912672570 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T01:19:43,135605996-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:19:43,142029985-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c60f9cd5-6db3-40d8-91c4-5b9912672570', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c60f9cd5-6db3-40d8-91c4-5b9912672570 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T01:19:46,234632727-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:19:46,240966477-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c60f9cd5-6db3-40d8-91c4-5b9912672570', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c60f9cd5-6db3-40d8-91c4-5b9912672570 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T01:19:49,772395474-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:19:49,779153991-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c60f9cd5-6db3-40d8-91c4-5b9912672570', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c60f9cd5-6db3-40d8-91c4-5b9912672570 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:19:53,059536380-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:19:53,065975267-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c60f9cd5-6db3-40d8-91c4-5b9912672570', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c60f9cd5-6db3-40d8-91c4-5b9912672570 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:19:56,351732368-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:19:56,357979807-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c60f9cd5-6db3-40d8-91c4-5b9912672570', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c60f9cd5-6db3-40d8-91c4-5b9912672570 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T01:19:59,707339181-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:19:59,713862247-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c60f9cd5-6db3-40d8-91c4-5b9912672570', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c60f9cd5-6db3-40d8-91c4-5b9912672570 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T01:20:02,464416163-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:20:02,470690742-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c60f9cd5-6db3-40d8-91c4-5b9912672570', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c60f9cd5-6db3-40d8-91c4-5b9912672570 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T01:20:05,104415732-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:20:28,057725463-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   188 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:20:36,081394913-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:20:44,175558770-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:20:52,150312185-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 51s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:21:00,329096501-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 55s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:21:08,342083492-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 75s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:21:16,444061135-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 85s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:21:24,605079214-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   224 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 42s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:21:32,508713831-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:21:32,526514964-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:21:40,627489266-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:21:40,645496835-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:21:48,550256832-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:21:48,568309258-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:21:56,542931174-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:21:56,561218488-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:22:04,615216924-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:22:04,632813563-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:22:04,647125937-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:22:04,655502054-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:22:04,674934380-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2608940
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-18T01:22:04,690614782-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-18T01:22:04,731735527-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:22:04,738076375-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c60f9cd5-6db3-40d8-91c4-5b9912672570', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c60f9cd5-6db3-40d8-91c4-5b9912672570 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:22:06.369+0000 ffffae1bd010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:22:06.377+0000 ffffae1bd010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:22:06.377+0000 ffffae1bd010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:22:06.392910+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-18T08:22:06.392968+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T08:22:06.443668+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:22:06.443668+0000     0       0         0         0         0         0           -           0
2021-05-18T08:22:07.443913+0000     1     255      3190      2935   733.679    733.75   0.0344814   0.0822988
2021-05-18T08:22:08.444390+0000     2     255      6230      5975   746.661       760   0.0962324   0.0825635
2021-05-18T08:22:09.444604+0000     3     255      9324      9069   755.552     773.5   0.0630625   0.0830361
2021-05-18T08:22:10.444855+0000     4     255     12371     12116   757.053    761.75   0.0583758   0.0830864
2021-05-18T08:22:11.445138+0000     5     255     15491     15236   761.599       780   0.0547532   0.0829596
2021-05-18T08:22:12.445330+0000     6     255     18464     18209   758.517    743.25   0.0813774   0.0832668
2021-05-18T08:22:13.445991+0000     7     255     21653     21398   763.977    797.25    0.122331   0.0831634
2021-05-18T08:22:14.446268+0000     8     255     24802     24547   766.859    787.25   0.0825588   0.0827078
2021-05-18T08:22:15.446485+0000     9     255     28043     27788    771.66    810.25   0.0758168   0.0823312
2021-05-18T08:22:16.446730+0000    10     255     30670     30415   760.154    656.75     0.16661    0.082604
2021-05-18T08:22:17.446964+0000    11     255     33501     33246   755.375    707.75   0.0420765    0.084237
2021-05-18T08:22:18.447619+0000    12     255     36417     36162   753.136       729   0.0743015   0.0845316
2021-05-18T08:22:19.447898+0000    13     255     39324     39069   751.091    726.75   0.0814751   0.0846803
2021-05-18T08:22:20.448092+0000    14     255     42301     42046   750.592    744.25   0.0505867   0.0849266
2021-05-18T08:22:21.449424+0000    15     255     45190     44935   748.637    722.25    0.117568   0.0848956
2021-05-18T08:22:22.449781+0000    16     255     48035     47780   746.284    711.25   0.0571874   0.0854131
2021-05-18T08:22:23.450733+0000    17     255     50530     50275   739.038    623.75    0.126907   0.0858851
2021-05-18T08:22:24.450967+0000    18     255     53045     52790   732.903    628.75   0.0836131   0.0868605
2021-05-18T08:22:25.451257+0000    19     255     55910     55655   732.016    716.25   0.0630419   0.0870372
2021-05-18T08:22:26.451665+0000 min lat: 0.0240711 max lat: 0.320686 avg lat: 0.0872399
2021-05-18T08:22:26.451665+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:22:26.451665+0000    20     126     58727     58601   732.225     736.5   0.0609793   0.0872399
2021-05-18T08:22:27.452082+0000 Total time run:         20.0321
Total writes made:      58727
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     732.91
Stddev Bandwidth:       50.7121
Max bandwidth (MB/sec): 810.25
Min bandwidth (MB/sec): 623.75
Average IOPS:           2931
Stddev IOPS:            202.849
Max IOPS:               3241
Min IOPS:               2495
Average Latency(s):     0.0871515
Stddev Latency(s):      0.0340741
Max latency(s):         0.320686
Min latency(s):         0.0240711

[1;32mlocalhost.localdomain	[2021-05-18T01:22:27,789698183-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2608940


[1;33mlocalhost.localdomain	[2021-05-18T01:22:27,798805480-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:22:50,768397412-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 58.73k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:22:50,786205208-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:22:58,907570571-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 58.73k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:22:58,925516436-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:23:07,088894324-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 58.73k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:23:07,109158614-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:23:15,072954853-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 58.73k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:23:15,090903627-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:23:23,065910792-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 58.73k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:23:23,083754202-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:23:23,097919682-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:23:23,106325305-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:23:23,125614065-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2612207
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-18T01:23:23,141489270-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-18T01:23:23,181396230-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:23:23,187697874-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c60f9cd5-6db3-40d8-91c4-5b9912672570', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c60f9cd5-6db3-40d8-91c4-5b9912672570 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3VBJFH:/tmp/ceph-asok.3VBJFH -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:23:24.875+0000 ffff85760010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:23:24.883+0000 ffff85760010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:23:24.883+0000 ffff85760010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:23:24.903933+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:23:24.903933+0000     0       0         0         0         0         0           -           0
2021-05-18T08:23:25.904132+0000     1     255      3596      3341   834.957    835.25    0.050572   0.0698897
2021-05-18T08:23:26.904456+0000     2     256      7038      6782   847.464    860.25    0.171772   0.0719872
2021-05-18T08:23:27.904801+0000     3     256     10334     10078   839.548       824  0.00132327   0.0738827
2021-05-18T08:23:28.905019+0000     4     256     14884     14628   913.967    1137.5  0.00265213   0.0689001
2021-05-18T08:23:29.905400+0000     5     255     19317     19062   952.792    1108.5   0.0331351   0.0660519
2021-05-18T08:23:30.905616+0000     6     255     23539     23284    969.87    1055.5   0.0219018    0.064952
2021-05-18T08:23:31.905939+0000     7     255     28062     27807   992.801   1130.75  0.00288332    0.063724
2021-05-18T08:23:32.906432+0000     8     256     32659     32403   1012.26      1149   0.0685308   0.0626688
2021-05-18T08:23:33.906788+0000     9     256     37159     36903   1024.74      1125   0.0330391   0.0618634
2021-05-18T08:23:34.907013+0000    10     255     41362     41107   1027.34      1051    0.121118   0.0617942
2021-05-18T08:23:35.907472+0000    11     256     45391     45135   1025.45      1007   0.0483646   0.0619796
2021-05-18T08:23:36.907915+0000    12     256     49667     49411   1029.04      1069    0.065743   0.0617879
2021-05-18T08:23:37.908639+0000    13     255     53890     53635   1031.06      1056    0.025302   0.0615564
2021-05-18T08:23:38.908887+0000    14     256     57823     57567   1027.61       983    0.104764   0.0619202
2021-05-18T08:23:39.909181+0000 Total time run:       14.2747
Total reads made:     58727
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1028.52
Average IOPS:         4114
Stddev IOPS:          452.28
Max IOPS:             4596
Min IOPS:             3296
Average Latency(s):   0.0618417
Max latency(s):       0.248795
Min latency(s):       0.000788987

[1;32mlocalhost.localdomain	[2021-05-18T01:23:40,215314899-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2612207


[1;33mlocalhost.localdomain	[2021-05-18T01:23:40,224378131-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:24:03,129594470-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 58.73k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:24:03,147722507-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:24:11,091262887-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 58.73k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:24:11,109049719-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:24:19,080414682-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 58.73k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:24:19,098325915-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:24:27,296047228-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 58.73k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:24:27,313955265-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:24:35,148197065-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 58.73k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:24:35,171440457-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:24:35,187817959-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T01:24:35,193886267-07:00][RUNNING][ROUND 3/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:24:35,203231905-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T01:24:35,222820576-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40510\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.705732\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid bdc97a13-76b1-4967-8602-9f6d482aff0d\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid bdc97a13-76b1-4967-8602-9f6d482aff0d\nlast_changed 2021-05-18T01:25:03.572261-0700\ncreated 2021-05-18T01:25:03.572261-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40510/0,v1:10.10.1.2:40511/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.705732 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 33ac9417-5486-4f6c-95c1-82e0c20f8eeb\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 bc787700-60d3-49e7-9d1b-4c0275dd570b\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 490d9097-4d24-418d-9d5e-a1a051324098\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42510\n  w/ user/pass: admin / d3b8c9ae-0f2b-444d-ac48-b2bce42bd8c2\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:25:18 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40510
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.705732
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid bdc97a13-76b1-4967-8602-9f6d482aff0d
setting min_mon_release = octopus
epoch 0
fsid bdc97a13-76b1-4967-8602-9f6d482aff0d
last_changed 2021-05-18T01:25:03.572261-0700
created 2021-05-18T01:25:03.572261-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40510/0,v1:10.10.1.2:40511/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.705732 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 33ac9417-5486-4f6c-95c1-82e0c20f8eeb
0
start osd.0
add osd1 bc787700-60d3-49e7-9d1b-4c0275dd570b
1
start osd.1
add osd2 490d9097-4d24-418d-9d5e-a1a051324098
2
start osd.2


restful urls: https://10.10.1.2:42510
  w/ user/pass: admin / d3b8c9ae-0f2b-444d-ac48-b2bce42bd8c2


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T01:24:36.242-0700 7ff6132221c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:24:36.242-0700 7ff6132221c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:24:36.258-0700 7ff2a00b31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:24:36.258-0700 7ff2a00b31c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40510,v1:10.10.1.2:40511] --print /tmp/ceph_monmap.705732 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.705732 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.705732 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42510 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.towj9Yb7VP 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 33ac9417-5486-4f6c-95c1-82e0c20f8eeb -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDoeaNgASilGhAA4y3vPiDkaOqGtYMFD+Wrhw== --osd-uuid 33ac9417-5486-4f6c-95c1-82e0c20f8eeb 
2021-05-18T01:25:12.863-0700 7f32cbc8cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:25:12.867-0700 7f32cbc8cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:25:12.867-0700 7f32cbc8cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:25:12.911-0700 7f32cbc8cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new bc787700-60d3-49e7-9d1b-4c0275dd570b -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T01:25:13.147-0700 7f1929c22f00 -1 Falling back to public interface
2021-05-18T01:25:13.159-0700 7f1929c22f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDpeaNg0Zm9CBAAV/Ej2cF6NnJ/L5zO55fSAQ== --osd-uuid bc787700-60d3-49e7-9d1b-4c0275dd570b 
2021-05-18T01:25:13.467-0700 7f8777667f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:25:13.467-0700 7f8777667f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:25:13.467-0700 7f8777667f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:25:13.523-0700 7f8777667f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 490d9097-4d24-418d-9d5e-a1a051324098 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T01:25:13.839-0700 7eff3515cf00 -1 Falling back to public interface
2021-05-18T01:25:13.855-0700 7eff3515cf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDpeaNgDHsWMhAAW/MpeCySl1fVas6Z17PZUg== --osd-uuid 490d9097-4d24-418d-9d5e-a1a051324098 
2021-05-18T01:25:14.171-0700 7f0034c7df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:25:14.171-0700 7f0034c7df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:25:14.171-0700 7f0034c7df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:25:14.227-0700 7f0034c7df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T01:25:14.627-0700 7fcd82450f00 -1 Falling back to public interface
2021-05-18T01:25:14.643-0700 7fcd82450f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T01:25:18,449119620-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:25:18,457382445-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T01:25:18,539717291-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:25:18,546092325-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9844244-0aab-4bb0-b45b-79bc998299cb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9844244-0aab-4bb0-b45b-79bc998299cb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T01:25:21,348295146-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:25:21,354949168-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9844244-0aab-4bb0-b45b-79bc998299cb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9844244-0aab-4bb0-b45b-79bc998299cb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T01:25:24,368902393-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:25:24,375391492-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9844244-0aab-4bb0-b45b-79bc998299cb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9844244-0aab-4bb0-b45b-79bc998299cb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T01:25:27,260341867-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:25:27,266797616-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9844244-0aab-4bb0-b45b-79bc998299cb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9844244-0aab-4bb0-b45b-79bc998299cb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T01:25:33,114598090-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:25:33,121225242-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9844244-0aab-4bb0-b45b-79bc998299cb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9844244-0aab-4bb0-b45b-79bc998299cb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T01:25:36,639611399-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:25:36,645830827-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9844244-0aab-4bb0-b45b-79bc998299cb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9844244-0aab-4bb0-b45b-79bc998299cb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T01:25:40,270737197-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:25:40,277221390-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9844244-0aab-4bb0-b45b-79bc998299cb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9844244-0aab-4bb0-b45b-79bc998299cb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T01:25:43,572048123-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:25:43,578492154-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9844244-0aab-4bb0-b45b-79bc998299cb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9844244-0aab-4bb0-b45b-79bc998299cb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:25:47,279518970-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:25:47,286224586-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9844244-0aab-4bb0-b45b-79bc998299cb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9844244-0aab-4bb0-b45b-79bc998299cb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:25:50,613396325-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:25:50,619803269-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9844244-0aab-4bb0-b45b-79bc998299cb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9844244-0aab-4bb0-b45b-79bc998299cb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T01:25:53,842972608-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:25:53,849167146-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9844244-0aab-4bb0-b45b-79bc998299cb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9844244-0aab-4bb0-b45b-79bc998299cb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T01:25:56,676613687-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:25:56,683176223-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9844244-0aab-4bb0-b45b-79bc998299cb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9844244-0aab-4bb0-b45b-79bc998299cb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T01:25:59,393090714-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:26:22,370951338-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   206 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=========...................] (remaining: 9s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:26:30,229493408-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:26:38,379640832-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:26:46,477666390-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:26:54,420820038-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:27:02,418823343-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:27:10,396910245-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:27:18,339901645-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:27:18,357957822-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:27:26,378365031-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:27:26,396426534-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:27:34,452037567-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:27:34,469758527-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:27:42,624629590-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:27:42,642697169-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:27:50,587241016-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:27:50,607265081-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:27:50,624756965-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:27:50,635118842-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:27:50,656972307-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2625586
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-18T01:27:50,685100016-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-18T01:27:50,730796812-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:27:50,737333991-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9844244-0aab-4bb0-b45b-79bc998299cb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9844244-0aab-4bb0-b45b-79bc998299cb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:27:52.237+0000 ffff94420010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:27:52.245+0000 ffff94420010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:27:52.245+0000 ffff94420010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:27:52.261048+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-18T08:27:52.261105+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T08:27:52.312204+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:27:52.312204+0000     0       0         0         0         0         0           -           0
2021-05-18T08:27:53.312526+0000     1     255      2759      2504   625.888       626   0.0601084   0.0945531
2021-05-18T08:27:54.314591+0000     2     255      5269      5014   626.047     627.5   0.0775679   0.0977757
2021-05-18T08:27:55.316336+0000     3     255      7797      7542   627.665       632    0.172332   0.0992455
2021-05-18T08:27:56.318485+0000     4     255     10362     10107    630.72    641.25   0.0907943   0.0992982
2021-05-18T08:27:57.318845+0000     5     255     13032     12777   638.021     667.5   0.0832446   0.0988129
2021-05-18T08:27:58.322695+0000     6     255     15674     15419   641.352     660.5   0.0790344   0.0983263
2021-05-18T08:27:59.322978+0000     7     255     17860     17605   627.797     546.5   0.0838318    0.100428
2021-05-18T08:28:00.323153+0000     8     255     20489     20234    631.46    657.25     0.11759   0.0995977
2021-05-18T08:28:01.323344+0000     9     255     22829     22574    626.29       585   0.0661082    0.101267
2021-05-18T08:28:02.323954+0000    10     255     25504     25249   630.493    668.75   0.0658489    0.100479
2021-05-18T08:28:03.324156+0000    11     255     27855     27600     626.6    587.75    0.152348    0.101315
2021-05-18T08:28:04.324883+0000    12     255     30381     30126    626.97     631.5   0.0658043    0.101331
2021-05-18T08:28:05.325130+0000    13     255     32974     32719   628.593    648.25   0.0672265    0.101245
2021-05-18T08:28:06.325551+0000    14     255     35606     35351   630.673       658   0.0663965    0.100786
2021-05-18T08:28:07.325982+0000    15     255     38319     38064   633.824    678.25   0.0671887    0.100494
2021-05-18T08:28:08.326496+0000    16     255     41078     40823   637.296    689.75   0.0663138   0.0999086
2021-05-18T08:28:09.326907+0000    17     255     43716     43461   638.585     659.5    0.127655   0.0997726
2021-05-18T08:28:10.327466+0000    18     255     46177     45922    637.27    615.25   0.0692542    0.100038
2021-05-18T08:28:11.327652+0000    19     255     48855     48600   638.959     669.5    0.072897   0.0997328
2021-05-18T08:28:12.327819+0000 min lat: 0.0364496 max lat: 0.367832 avg lat: 0.0997842
2021-05-18T08:28:12.327819+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:28:12.327819+0000    20      62     51237     51175   639.193    643.75    0.168834   0.0997842
2021-05-18T08:28:13.328055+0000 Total time run:         20.0268
Total writes made:      51237
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     639.604
Stddev Bandwidth:       35.1638
Max bandwidth (MB/sec): 689.75
Min bandwidth (MB/sec): 546.5
Average IOPS:           2558
Stddev IOPS:            140.655
Max IOPS:               2759
Min IOPS:               2186
Average Latency(s):     0.0998217
Stddev Latency(s):      0.044743
Max latency(s):         0.367832
Min latency(s):         0.0364496

[1;32mlocalhost.localdomain	[2021-05-18T01:28:13,659416668-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2625586


[1;33mlocalhost.localdomain	[2021-05-18T01:28:13,668443255-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:28:36,724301462-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.24k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:28:36,742493215-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:28:44,756335800-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.24k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:28:44,774397000-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:28:52,822239035-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.24k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:28:52,840991058-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:29:00,835163235-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.24k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:29:00,853486859-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:29:08,673632757-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.24k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:29:08,696254875-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:29:08,712878196-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:29:08,722982088-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:29:08,745006307-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2628899
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-18T01:29:08,763934280-07:00] INFO: > Run rados bench[0m
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.3
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
[1;30mlocalhost.localdomain	[2021-05-18T01:29:08,809437790-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:29:08,816741855-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9844244-0aab-4bb0-b45b-79bc998299cb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9844244-0aab-4bb0-b45b-79bc998299cb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SEwCUX:/tmp/ceph-asok.SEwCUX -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:29:10.318+0000 ffff9ecb8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:29:10.326+0000 ffff9ecb8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:29:10.326+0000 ffff9ecb8010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:29:10.350213+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:29:10.350213+0000     0       0         0         0         0         0           -           0
2021-05-18T08:29:11.350392+0000     1     255      3911      3656   913.724       914   0.0286353   0.0642502
2021-05-18T08:29:12.350692+0000     2     256      7224      6968   870.738       828  0.00824375   0.0699771
2021-05-18T08:29:13.350971+0000     3     255     10996     10741   894.821    943.25  0.00387482   0.0691655
2021-05-18T08:29:14.351305+0000     4     255     14654     14399   899.664     914.5    0.101083   0.0700945
2021-05-18T08:29:15.351506+0000     5     255     18847     18592   929.337   1048.25   0.0995176   0.0681352
2021-05-18T08:29:16.351706+0000     6     256     22122     21866   910.838     818.5    0.208888   0.0691423
2021-05-18T08:29:17.351989+0000     7     256     25724     25468   909.325     900.5   0.0495958   0.0697151
2021-05-18T08:29:18.352402+0000     8     256     29921     29665   926.763   1049.25    0.165794   0.0684377
2021-05-18T08:29:19.352686+0000     9     255     34358     34103   947.032    1109.5   0.0069805    0.066917
2021-05-18T08:29:20.352908+0000    10     256     38759     38503   962.304      1100   0.0135405   0.0658991
2021-05-18T08:29:21.353280+0000    11     256     43222     42966   976.217   1115.75   0.0191502   0.0650226
2021-05-18T08:29:22.353640+0000    12     256     47685     47429   987.812   1115.75   0.0181542   0.0643349
2021-05-18T08:29:23.353905+0000 Total time run:       12.9388
Total reads made:     51237
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   989.99
Average IOPS:         3959
Stddev IOPS:          454.785
Max IOPS:             4463
Min IOPS:             3274
Average Latency(s):   0.0642519
Max latency(s):       0.294137
Min latency(s):       0.000937842

[1;32mlocalhost.localdomain	[2021-05-18T01:29:23,697694966-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2628899


[1;33mlocalhost.localdomain	[2021-05-18T01:29:23,706845038-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:29:46,756826865-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.24k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:29:46,774948358-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:29:54,826738940-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.24k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:29:54,844786035-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:30:02,840243661-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.24k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:30:02,858205255-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:30:10,805322312-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.24k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:30:10,823789208-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:30:18,872262168-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.24k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:30:18,890328150-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:30:18,905411125-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T01:30:18,911063916-07:00][RUNNING][ROUND 4/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:30:18,919387930-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T01:30:18,936940629-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40595\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.706841\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 279b1e47-a5ed-438b-99c7-95b62f2d219b\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 279b1e47-a5ed-438b-99c7-95b62f2d219b\nlast_changed 2021-05-18T01:30:47.246911-0700\ncreated 2021-05-18T01:30:47.246911-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40595/0,v1:10.10.1.2:40596/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.706841 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 b39f4feb-3ab0-4786-8453-232c0bb47a78\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 ecf43e3a-d32b-4b27-8f46-85fd69ab7902\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 a16c0fd4-84ca-4a59-9d90-6afcb9d45795\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42595\n  w/ user/pass: admin / b0e4d839-c17b-41cc-ac41-bec7a7137253\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:31:02 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40595
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.706841
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 279b1e47-a5ed-438b-99c7-95b62f2d219b
setting min_mon_release = octopus
epoch 0
fsid 279b1e47-a5ed-438b-99c7-95b62f2d219b
last_changed 2021-05-18T01:30:47.246911-0700
created 2021-05-18T01:30:47.246911-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40595/0,v1:10.10.1.2:40596/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.706841 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 b39f4feb-3ab0-4786-8453-232c0bb47a78
0
start osd.0
add osd1 ecf43e3a-d32b-4b27-8f46-85fd69ab7902
1
start osd.1
add osd2 a16c0fd4-84ca-4a59-9d90-6afcb9d45795
2
start osd.2


restful urls: https://10.10.1.2:42595
  w/ user/pass: admin / b0e4d839-c17b-41cc-ac41-bec7a7137253


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T01:30:19.922-0700 7f4896ec71c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:30:19.922-0700 7f4896ec71c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:30:19.938-0700 7f4ea0a601c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:30:19.938-0700 7f4ea0a601c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40595,v1:10.10.1.2:40596] --print /tmp/ceph_monmap.706841 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.706841 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.706841 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42595 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.UYtfWHkFG9 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b39f4feb-3ab0-4786-8453-232c0bb47a78 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBAe6NgvbiABhAA85sbhvYc7q+jvYLKBc09Gw== --osd-uuid b39f4feb-3ab0-4786-8453-232c0bb47a78 
2021-05-18T01:30:56.502-0700 7fc78da6af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:30:56.502-0700 7fc78da6af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:30:56.502-0700 7fc78da6af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:30:56.554-0700 7fc78da6af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T01:30:56.858-0700 7f4778915f00 -1 Falling back to public interface
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ecf43e3a-d32b-4b27-8f46-85fd69ab7902 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T01:30:56.870-0700 7f4778915f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBAe6Ng5oMvMxAAyvzcMJnJ95DkY14rtFGHKQ== --osd-uuid ecf43e3a-d32b-4b27-8f46-85fd69ab7902 
2021-05-18T01:30:57.194-0700 7f2fcc0d5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:30:57.194-0700 7f2fcc0d5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:30:57.194-0700 7f2fcc0d5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:30:57.242-0700 7f2fcc0d5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a16c0fd4-84ca-4a59-9d90-6afcb9d45795 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T01:30:57.610-0700 7f9b81670f00 -1 Falling back to public interface
2021-05-18T01:30:57.622-0700 7f9b81670f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBBe6NgkZdfJBAAFIjWzBpkoMX6D78AL913ZA== --osd-uuid a16c0fd4-84ca-4a59-9d90-6afcb9d45795 
2021-05-18T01:30:57.938-0700 7f914367ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:30:57.938-0700 7f914367ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:30:57.938-0700 7f914367ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:30:58.010-0700 7f914367ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T01:30:58.354-0700 7f452ccd7f00 -1 Falling back to public interface
2021-05-18T01:30:58.370-0700 7f452ccd7f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T01:31:02,347068698-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:31:02,356867585-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T01:31:02,440980766-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:31:02,447453582-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a86f552-d51b-490b-85b8-7c89c9c3ca4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a86f552-d51b-490b-85b8-7c89c9c3ca4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T01:31:05,299196427-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:31:05,305744555-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a86f552-d51b-490b-85b8-7c89c9c3ca4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a86f552-d51b-490b-85b8-7c89c9c3ca4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T01:31:08,055606207-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:31:08,062145124-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a86f552-d51b-490b-85b8-7c89c9c3ca4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a86f552-d51b-490b-85b8-7c89c9c3ca4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T01:31:11,003421282-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:31:11,009727179-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a86f552-d51b-490b-85b8-7c89c9c3ca4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a86f552-d51b-490b-85b8-7c89c9c3ca4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T01:31:16,631549198-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:31:16,638130768-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a86f552-d51b-490b-85b8-7c89c9c3ca4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a86f552-d51b-490b-85b8-7c89c9c3ca4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T01:31:19,744857014-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:31:19,751228367-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a86f552-d51b-490b-85b8-7c89c9c3ca4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a86f552-d51b-490b-85b8-7c89c9c3ca4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T01:31:23,297808278-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:31:23,304250999-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a86f552-d51b-490b-85b8-7c89c9c3ca4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a86f552-d51b-490b-85b8-7c89c9c3ca4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T01:31:26,710152605-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:31:26,716501935-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a86f552-d51b-490b-85b8-7c89c9c3ca4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a86f552-d51b-490b-85b8-7c89c9c3ca4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:31:30,352504468-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:31:30,359200042-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a86f552-d51b-490b-85b8-7c89c9c3ca4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a86f552-d51b-490b-85b8-7c89c9c3ca4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:31:33,672455584-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:31:33,678972177-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a86f552-d51b-490b-85b8-7c89c9c3ca4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a86f552-d51b-490b-85b8-7c89c9c3ca4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T01:31:36,778696231-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:31:36,785231232-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a86f552-d51b-490b-85b8-7c89c9c3ca4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a86f552-d51b-490b-85b8-7c89c9c3ca4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T01:31:39,542033666-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:31:39,548389327-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a86f552-d51b-490b-85b8-7c89c9c3ca4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a86f552-d51b-490b-85b8-7c89c9c3ca4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T01:31:42,311733846-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:32:05,383145768-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   188 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:32:13,444619050-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:32:21,527062621-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:32:29,512525550-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 48s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:32:37,485118115-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [=========...................] (remaining: 52s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:32:45,428472676-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 71s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:32:53,554102101-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 81s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:33:01,580547091-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   223 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [==============..............] (remaining: 44s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:33:09,450747107-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:33:09,468845369-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:33:17,739154741-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:33:17,757258764-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:33:25,686900195-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:33:25,706758574-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:33:33,694298133-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:33:33,712433211-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:33:41,631946574-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:33:41,650197772-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:33:41,665093060-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:33:41,673766737-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:33:41,693510901-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2642624
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-18T01:33:41,709735507-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-18T01:33:41,750246485-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:33:41,756511634-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a86f552-d51b-490b-85b8-7c89c9c3ca4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a86f552-d51b-490b-85b8-7c89c9c3ca4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:33:43.509+0000 ffff9fdb8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:33:43.521+0000 ffff9fdb8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:33:43.521+0000 ffff9fdb8010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:33:43.535583+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-18T08:33:43.535747+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T08:33:43.586544+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:33:43.586544+0000     0       0         0         0         0         0           -           0
2021-05-18T08:33:44.586784+0000     1     255      2839      2584   645.951       646   0.0596768   0.0922389
2021-05-18T08:33:45.587072+0000     2     255      5259      5004   625.386       605    0.137432   0.0983618
2021-05-18T08:33:46.587497+0000     3     255      7696      7441    619.92    609.25    0.148619    0.100248
2021-05-18T08:33:47.587691+0000     4     255     10186      9931   620.535     622.5   0.0818555      0.1004
2021-05-18T08:33:48.587901+0000     5     255     12578     12323   616.003       598    0.136484    0.102147
2021-05-18T08:33:49.588096+0000     6     255     14987     14732   613.691    602.25   0.0719835    0.102946
2021-05-18T08:33:50.588365+0000     7     255     17629     17374   620.353     660.5    0.139084    0.102002
2021-05-18T08:33:51.588609+0000     8     255     20233     19978   624.164       651   0.0622801    0.101534
2021-05-18T08:33:52.588954+0000     9     255     22586     22331   620.151    588.25   0.0917635    0.102211
2021-05-18T08:33:53.589193+0000    10     255     25393     25138   628.294    701.75    0.130426    0.101055
2021-05-18T08:33:54.589422+0000    11     255     27726     27471   624.187    583.25   0.0754403    0.101747
2021-05-18T08:33:55.589686+0000    12     255     30058     29803   620.742       583   0.0957231    0.102267
2021-05-18T08:33:56.589876+0000    13     255     32688     32433    623.56     657.5   0.0660157    0.102063
2021-05-18T08:33:57.590119+0000    14     255     35001     34746   620.313    578.25   0.0757187     0.10261
2021-05-18T08:33:58.590315+0000    15     255     37568     37313   621.734    641.75    0.139718    0.102394
2021-05-18T08:33:59.590527+0000    16     255     39789     39534   617.571    555.25   0.0829865    0.103089
2021-05-18T08:34:00.590995+0000    17     255     42059     41804    614.61     567.5     0.12527    0.103496
2021-05-18T08:34:01.591186+0000    18     255     44453     44198   613.708     598.5   0.0897017    0.103796
2021-05-18T08:34:02.591413+0000    19     255     46840     46585   612.809    596.75    0.093961    0.104006
2021-05-18T08:34:03.591698+0000 min lat: 0.0362686 max lat: 0.379335 avg lat: 0.104237
2021-05-18T08:34:03.591698+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:34:03.591698+0000    20     124     49127     49003   612.385     604.5   0.0777437    0.104237
2021-05-18T08:34:04.592065+0000 Total time run:         20.0596
Total writes made:      49127
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     612.262
Stddev Bandwidth:       36.6594
Max bandwidth (MB/sec): 701.75
Min bandwidth (MB/sec): 555.25
Average IOPS:           2449
Stddev IOPS:            146.637
Max IOPS:               2807
Min IOPS:               2221
Average Latency(s):     0.104127
Stddev Latency(s):      0.0445964
Max latency(s):         0.379335
Min latency(s):         0.0119594

[1;32mlocalhost.localdomain	[2021-05-18T01:34:04,896408748-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2642624


[1;33mlocalhost.localdomain	[2021-05-18T01:34:04,905419037-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:34:27,887660487-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.13k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:34:27,906476336-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:34:35,969018319-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.13k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:34:35,987630613-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:34:43,937282051-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.13k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:34:43,956278366-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:34:51,875539358-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.13k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:34:51,894365378-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:35:00,059847205-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.13k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:35:00,078725929-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:35:00,093559900-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:35:00,102090646-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:35:00,122240628-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2645919
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-18T01:35:00,138709774-07:00] INFO: > Run rados bench[0m
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.4
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
[1;30mlocalhost.localdomain	[2021-05-18T01:35:00,178853096-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:35:00,185379139-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2a86f552-d51b-490b-85b8-7c89c9c3ca4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2a86f552-d51b-490b-85b8-7c89c9c3ca4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go7bqS:/tmp/ceph-asok.go7bqS -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:35:01.810+0000 ffff9732c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:35:01.818+0000 ffff9732c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:35:01.818+0000 ffff9732c010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:35:01.841455+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:35:01.841455+0000     0       0         0         0         0         0           -           0
2021-05-18T08:35:02.841715+0000     1     256      4993      4737   1183.79   1184.25   0.0479919   0.0516751
2021-05-18T08:35:03.842034+0000     2     255      9508      9253   1156.21      1129    0.132519   0.0536654
2021-05-18T08:35:04.842424+0000     3     255     13640     13385   1115.01      1033    0.164429   0.0558314
2021-05-18T08:35:05.842738+0000     4     255     18020     17765   1109.92      1095  0.00320339   0.0564518
2021-05-18T08:35:06.842960+0000     5     256     22435     22179   1108.59    1103.5  0.00233539   0.0566655
2021-05-18T08:35:07.843246+0000     6     255     26831     26576   1106.98   1099.25    0.154514   0.0570142
2021-05-18T08:35:08.843670+0000     7     256     30804     30548   1090.63       993    0.137023   0.0579611
2021-05-18T08:35:09.843946+0000     8     255     34964     34709    1084.3   1040.25   0.0340254   0.0584611
2021-05-18T08:35:10.844303+0000     9     255     39190     38935   1081.17    1056.5   0.0110069    0.058792
2021-05-18T08:35:11.844526+0000    10     255     43664     43409   1084.88    1118.5   0.0951453   0.0586045
2021-05-18T08:35:12.845179+0000    11     256     47940     47684   1083.35   1068.75   0.0850315   0.0587354
2021-05-18T08:35:13.845468+0000 Total time run:       11.3717
Total reads made:     49127
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1080.03
Average IOPS:         4320
Stddev IOPS:          210.839
Max IOPS:             4737
Min IOPS:             3972
Average Latency(s):   0.0588361
Max latency(s):       0.219217
Min latency(s):       0.000960194

[1;32mlocalhost.localdomain	[2021-05-18T01:35:14,183114982-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2645919


[1;33mlocalhost.localdomain	[2021-05-18T01:35:14,192312368-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:35:37,172200103-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.13k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:35:37,191011360-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:35:45,136011999-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.13k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:35:45,154995755-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:35:53,245312882-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.13k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:35:53,264156904-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:36:01,207577827-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.13k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:36:01,226669872-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:36:09,187201479-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.13k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:36:09,209633606-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:36:09,227040741-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T01:36:09,233207772-07:00][RUNNING][ROUND 5/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:36:09,242882269-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T01:36:09,260897744-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40991\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.707954\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid dc73f389-a7b2-4d96-bb5b-dea964ddcfb8\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid dc73f389-a7b2-4d96-bb5b-dea964ddcfb8\nlast_changed 2021-05-18T01:36:37.341053-0700\ncreated 2021-05-18T01:36:37.341053-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40991/0,v1:10.10.1.2:40992/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.707954 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 ec4e8a17-a0c6-4a64-b24d-dd21b5d85a1e\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 0c22b5f2-0307-4dbf-b3fe-261809a2ac99\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 51745285-22ee-49c7-baee-21b6f63b7fc1\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42991\n  w/ user/pass: admin / 04764b2b-b161-4bd8-95a7-f1adb1ffb1e3\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:36:52 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40991
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.707954
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid dc73f389-a7b2-4d96-bb5b-dea964ddcfb8
setting min_mon_release = octopus
epoch 0
fsid dc73f389-a7b2-4d96-bb5b-dea964ddcfb8
last_changed 2021-05-18T01:36:37.341053-0700
created 2021-05-18T01:36:37.341053-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40991/0,v1:10.10.1.2:40992/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.707954 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 ec4e8a17-a0c6-4a64-b24d-dd21b5d85a1e
0
start osd.0
add osd1 0c22b5f2-0307-4dbf-b3fe-261809a2ac99
1
start osd.1
add osd2 51745285-22ee-49c7-baee-21b6f63b7fc1
2
start osd.2


restful urls: https://10.10.1.2:42991
  w/ user/pass: admin / 04764b2b-b161-4bd8-95a7-f1adb1ffb1e3


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T01:36:10.257-0700 7f52f14341c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:36:10.257-0700 7f52f14341c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:36:10.273-0700 7f03873ba1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:36:10.273-0700 7f03873ba1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40991,v1:10.10.1.2:40992] --print /tmp/ceph_monmap.707954 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.707954 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.707954 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42991 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.dzfdLZZc6Z 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ec4e8a17-a0c6-4a64-b24d-dd21b5d85a1e -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCefKNg3KbeCxAAt0yVPjZI6JHwR/189ItR3Q== --osd-uuid ec4e8a17-a0c6-4a64-b24d-dd21b5d85a1e 
2021-05-18T01:36:46.589-0700 7fa92520df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:36:46.593-0700 7fa92520df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:36:46.593-0700 7fa92520df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:36:46.645-0700 7fa92520df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0c22b5f2-0307-4dbf-b3fe-261809a2ac99 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T01:36:46.937-0700 7fa2274a4f00 -1 Falling back to public interface
2021-05-18T01:36:46.949-0700 7fa2274a4f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCefKNgg9L3NxAAuELFoHbjzslWtTo9nfF2KQ== --osd-uuid 0c22b5f2-0307-4dbf-b3fe-261809a2ac99 
2021-05-18T01:36:47.269-0700 7f3422e4ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:36:47.269-0700 7f3422e4ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:36:47.269-0700 7f3422e4ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:36:47.329-0700 7f3422e4ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 51745285-22ee-49c7-baee-21b6f63b7fc1 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T01:36:47.701-0700 7f1fea00cf00 -1 Falling back to public interface
2021-05-18T01:36:47.713-0700 7f1fea00cf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCffKNgdIHDKRAABeYM8ZUrxPEC+Q2kYFnoJA== --osd-uuid 51745285-22ee-49c7-baee-21b6f63b7fc1 
2021-05-18T01:36:48.029-0700 7ff57ca02f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:36:48.029-0700 7ff57ca02f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:36:48.029-0700 7ff57ca02f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:36:48.185-0700 7ff57ca02f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T01:36:48.461-0700 7f5be790ef00 -1 Falling back to public interface
2021-05-18T01:36:48.477-0700 7f5be790ef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T01:36:52,351226496-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:36:52,360144593-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T01:36:52,446242262-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:36:52,452601100-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e51f3e4-4cf8-47a6-8228-192bf0d46c03', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e51f3e4-4cf8-47a6-8228-192bf0d46c03 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T01:36:55,274257130-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:36:55,280897552-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e51f3e4-4cf8-47a6-8228-192bf0d46c03', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e51f3e4-4cf8-47a6-8228-192bf0d46c03 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T01:36:58,292299651-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:36:58,298786027-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e51f3e4-4cf8-47a6-8228-192bf0d46c03', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e51f3e4-4cf8-47a6-8228-192bf0d46c03 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T01:37:01,124355612-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:37:01,130800910-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e51f3e4-4cf8-47a6-8228-192bf0d46c03', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e51f3e4-4cf8-47a6-8228-192bf0d46c03 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T01:37:06,674913329-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:37:06,682116002-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e51f3e4-4cf8-47a6-8228-192bf0d46c03', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e51f3e4-4cf8-47a6-8228-192bf0d46c03 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T01:37:10,360721689-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:37:10,367271240-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e51f3e4-4cf8-47a6-8228-192bf0d46c03', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e51f3e4-4cf8-47a6-8228-192bf0d46c03 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T01:37:13,661493913-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:37:13,667778279-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e51f3e4-4cf8-47a6-8228-192bf0d46c03', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e51f3e4-4cf8-47a6-8228-192bf0d46c03 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T01:37:17,004543860-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:37:17,010925743-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e51f3e4-4cf8-47a6-8228-192bf0d46c03', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e51f3e4-4cf8-47a6-8228-192bf0d46c03 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:37:20,359617876-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:37:20,367123556-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e51f3e4-4cf8-47a6-8228-192bf0d46c03', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e51f3e4-4cf8-47a6-8228-192bf0d46c03 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:37:23,504709476-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:37:23,512165092-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e51f3e4-4cf8-47a6-8228-192bf0d46c03', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e51f3e4-4cf8-47a6-8228-192bf0d46c03 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T01:37:26,903645852-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:37:26,911345200-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e51f3e4-4cf8-47a6-8228-192bf0d46c03', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e51f3e4-4cf8-47a6-8228-192bf0d46c03 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T01:37:29,600238759-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:37:29,606918398-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e51f3e4-4cf8-47a6-8228-192bf0d46c03', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e51f3e4-4cf8-47a6-8228-192bf0d46c03 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T01:37:32,446747070-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:37:55,341879975-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:38:03,323558235-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:38:11,313527511-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 28s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:38:19,584118261-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:38:28,058385629-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 56s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:38:35,991511790-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 77s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:38:43,892947774-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (45s)
      [=========...................] (remaining: 90s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:38:51,919692849-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 42s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:39:00,048255427-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:39:00,066482165-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:39:08,047588145-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:39:08,069269734-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:39:15,896335602-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:39:15,915252114-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:39:24,035175409-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:39:24,053739074-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:39:32,115820952-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:39:32,134392269-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:39:32,149445829-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:39:32,158133020-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:39:32,178206169-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2659623
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-18T01:39:32,194685243-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-18T01:39:32,235834628-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:39:32,242412683-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e51f3e4-4cf8-47a6-8228-192bf0d46c03', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e51f3e4-4cf8-47a6-8228-192bf0d46c03 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:39:33.777+0000 ffff93135010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:39:33.785+0000 ffff93135010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:39:33.785+0000 ffff93135010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:39:33.802037+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-18T08:39:33.802112+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T08:39:33.852904+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:39:33.852904+0000     0       0         0         0         0         0           -           0
2021-05-18T08:39:34.853130+0000     1     255      3138      2883   720.691    720.75   0.0942529   0.0822509
2021-05-18T08:39:35.853363+0000     2     255      6295      6040   754.881    789.25    0.135275   0.0816832
2021-05-18T08:39:36.853705+0000     3     255      9334      9079   756.418    759.75     0.13645   0.0820776
2021-05-18T08:39:37.853918+0000     4     255     12535     12280   767.333    800.25    0.112675   0.0818836
2021-05-18T08:39:38.855049+0000     5     255     15551     15296   764.494       754   0.0623455   0.0825102
2021-05-18T08:39:39.855411+0000     6     255     18702     18447   768.322    787.75   0.0654585   0.0822656
2021-05-18T08:39:40.855713+0000     7     255     21843     21588   770.707    785.25   0.0741114   0.0821308
2021-05-18T08:39:41.856122+0000     8     255     24894     24639   769.673    762.75   0.0728886    0.082387
2021-05-18T08:39:42.856528+0000     9     255     27448     27193   755.069     638.5    0.093226   0.0840501
2021-05-18T08:39:43.856871+0000    10     255     29840     29585   739.342       598    0.118511   0.0857357
2021-05-18T08:39:44.857083+0000    11     255     32290     32035   727.801     612.5    0.133803   0.0871251
2021-05-18T08:39:45.857355+0000    12     255     34692     34437    717.18     600.5   0.0831085    0.088599
2021-05-18T08:39:46.857831+0000    13     255     37334     37079   712.795     660.5    0.168097   0.0892232
2021-05-18T08:39:47.858210+0000    14     255     40056     39801    710.47     680.5    0.102144   0.0895984
2021-05-18T08:39:48.858964+0000    15     255     42628     42373   705.938       643    0.164099   0.0900746
2021-05-18T08:39:49.859348+0000    16     255     44894     44639    697.21     566.5   0.0592683    0.091318
2021-05-18T08:39:50.859621+0000    17     255     47494     47239   694.423       650   0.0614035   0.0916796
2021-05-18T08:39:51.860093+0000    18     255     50130     49875   692.437       659    0.165914     0.09203
2021-05-18T08:39:52.860817+0000    19     255     52633     52378   688.903    625.75   0.0522182      0.0925
2021-05-18T08:39:53.861330+0000 min lat: 0.0251121 max lat: 0.26806 avg lat: 0.0927398
2021-05-18T08:39:53.861330+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:39:53.861330+0000    20     255     55260     55005   687.278    656.75   0.0746633   0.0927398
2021-05-18T08:39:54.861653+0000 Total time run:         20.0375
Total writes made:      55261
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     689.468
Stddev Bandwidth:       75.0658
Max bandwidth (MB/sec): 800.25
Min bandwidth (MB/sec): 566.5
Average IOPS:           2757
Stddev IOPS:            300.263
Max IOPS:               3201
Min IOPS:               2266
Average Latency(s):     0.0925937
Stddev Latency(s):      0.0328433
Max latency(s):         0.26806
Min latency(s):         0.0133519

[1;32mlocalhost.localdomain	[2021-05-18T01:39:55,168267117-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2659623


[1;33mlocalhost.localdomain	[2021-05-18T01:39:55,177511653-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:40:18,313771552-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 55.26k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:40:18,332552515-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:40:26,481381920-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 55.26k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:40:26,502251405-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:40:34,568821612-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 55.26k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:40:34,587805919-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:40:42,593196407-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 55.26k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:40:42,612221911-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:40:50,695647054-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 55.26k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:40:50,714677494-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:40:50,730140474-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:40:50,739083623-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:40:50,759411890-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2662945
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-18T01:40:50,776298658-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-18T01:40:50,816748576-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:40:50,823235686-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e51f3e4-4cf8-47a6-8228-192bf0d46c03', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e51f3e4-4cf8-47a6-8228-192bf0d46c03 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3GwfpV:/tmp/ceph-asok.3GwfpV -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:40:52.274+0000 ffffb20d2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:40:52.282+0000 ffffb20d2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:40:52.282+0000 ffffb20d2010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:40:52.306343+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:40:52.306343+0000     0       0         0         0         0         0           -           0
2021-05-18T08:40:53.306630+0000     1     255      4562      4307   1076.31   1076.75   0.0915814   0.0567611
2021-05-18T08:40:54.306833+0000     2     255      9208      8953   1118.79    1161.5   0.0704518   0.0561706
2021-05-18T08:40:55.307052+0000     3     255     14014     13759   1146.27    1201.5   0.0859739   0.0549771
2021-05-18T08:40:56.307594+0000     4     255     18641     18386   1148.73   1156.75   0.0728794    0.054904
2021-05-18T08:40:57.307832+0000     5     255     22797     22542   1126.74      1039   0.0994883   0.0562113
2021-05-18T08:40:58.308065+0000     6     256     27246     26990   1124.24      1112  0.00213459   0.0563278
2021-05-18T08:40:59.308395+0000     7     255     31418     31163   1112.62   1043.25   0.0721947   0.0570903
2021-05-18T08:41:00.308874+0000     8     255     35898     35643   1113.48      1120   0.0327823   0.0570688
2021-05-18T08:41:01.309149+0000     9     256     40293     40037   1111.78    1098.5   0.0979867   0.0571706
2021-05-18T08:41:02.309615+0000    10     256     44196     43940   1098.13    975.75   0.0766426    0.057905
2021-05-18T08:41:03.310005+0000    11     255     48307     48052   1091.72      1028   0.0929866   0.0583331
2021-05-18T08:41:04.310416+0000    12     255     52348     52093   1084.89   1010.25    0.111316   0.0586762
2021-05-18T08:41:05.310684+0000 Total time run:       12.8067
Total reads made:     55261
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1078.75
Average IOPS:         4314
Stddev IOPS:          273.449
Max IOPS:             4806
Min IOPS:             3903
Average Latency(s):   0.0590211
Max latency(s):       0.214101
Min latency(s):       0.000869954

[1;32mlocalhost.localdomain	[2021-05-18T01:41:05,657627828-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2662945


[1;33mlocalhost.localdomain	[2021-05-18T01:41:05,667278543-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:41:28,567816782-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 55.26k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:41:28,586686199-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:41:36,604201322-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 55.26k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:41:36,623202041-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:41:44,692882958-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 55.26k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:41:44,713514216-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:41:52,511451653-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 55.26k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:41:52,530762760-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:42:00,418508795-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 55.26k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:42:00,437316011-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:42:00,452714029-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T01:42:00,461991795-07:00][RUNNING][ROUND 1/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:42:00,470782129-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T01:42:00,488409108-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40704\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.709060\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 44f653c7-a50b-4cc7-82d4-84ed9615bb66\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 44f653c7-a50b-4cc7-82d4-84ed9615bb66\nlast_changed 2021-05-18T01:42:29.479283-0700\ncreated 2021-05-18T01:42:29.479283-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40704/0,v1:10.10.1.2:40705/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.709060 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 7ae57458-f73c-4bbe-a81f-e10fc5ae4fcf\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 a225bac6-8d07-4ff5-9289-58de1fa8172c\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 8bee2e0a-4199-4550-a3dc-d5723182123e\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42704\n  w/ user/pass: admin / 35bb303b-6eac-4471-81f5-df16bdc19beb\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:42:44 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40704
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.709060
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 44f653c7-a50b-4cc7-82d4-84ed9615bb66
setting min_mon_release = octopus
epoch 0
fsid 44f653c7-a50b-4cc7-82d4-84ed9615bb66
last_changed 2021-05-18T01:42:29.479283-0700
created 2021-05-18T01:42:29.479283-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40704/0,v1:10.10.1.2:40705/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.709060 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 7ae57458-f73c-4bbe-a81f-e10fc5ae4fcf
0
start osd.0
add osd1 a225bac6-8d07-4ff5-9289-58de1fa8172c
1
start osd.1
add osd2 8bee2e0a-4199-4550-a3dc-d5723182123e
2
start osd.2


restful urls: https://10.10.1.2:42704
  w/ user/pass: admin / 35bb303b-6eac-4471-81f5-df16bdc19beb


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T01:42:01.468-0700 7f6a267801c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:42:01.468-0700 7f6a267801c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:42:01.484-0700 7f21781b21c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:42:01.484-0700 7f21781b21c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40704,v1:10.10.1.2:40705] --print /tmp/ceph_monmap.709060 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.709060 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.709060 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42704 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.tju1CvNyzY 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7ae57458-f73c-4bbe-a81f-e10fc5ae4fcf -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD+faNg96WzFhAAvouIA9edSghKCb/ZHFfEBA== --osd-uuid 7ae57458-f73c-4bbe-a81f-e10fc5ae4fcf 
2021-05-18T01:42:38.761-0700 7fd1363d2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:42:38.765-0700 7fd1363d2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:42:38.765-0700 7fd1363d2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:42:38.817-0700 7fd1363d2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a225bac6-8d07-4ff5-9289-58de1fa8172c -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T01:42:39.085-0700 7f9dd6aaef00 -1 Falling back to public interface
2021-05-18T01:42:39.101-0700 7f9dd6aaef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD/faNgMsw/BRAA8X1ICH+h5g7xoChbRerhqA== --osd-uuid a225bac6-8d07-4ff5-9289-58de1fa8172c 
2021-05-18T01:42:39.409-0700 7f9f18193f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:42:39.409-0700 7f9f18193f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:42:39.409-0700 7f9f18193f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:42:39.453-0700 7f9f18193f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8bee2e0a-4199-4550-a3dc-d5723182123e -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T01:42:39.817-0700 7f2af5feff00 -1 Falling back to public interface
2021-05-18T01:42:39.829-0700 7f2af5feff00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD/faNglGDKMBAAZKaeB7yRbLoTnYJg2o38cQ== --osd-uuid 8bee2e0a-4199-4550-a3dc-d5723182123e 
2021-05-18T01:42:40.145-0700 7fbc03262f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:42:40.145-0700 7fbc03262f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:42:40.145-0700 7fbc03262f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:42:40.181-0700 7fbc03262f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T01:42:40.493-0700 7f5084052f00 -1 Falling back to public interface
2021-05-18T01:42:40.509-0700 7f5084052f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T01:42:44,426354202-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:42:44,435263280-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T01:42:44,517973156-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:42:44,524286656-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '182d6ece-caac-4f2e-aabe-3096f8253deb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 182d6ece-caac-4f2e-aabe-3096f8253deb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T01:42:47,351811924-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:42:47,358401426-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '182d6ece-caac-4f2e-aabe-3096f8253deb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 182d6ece-caac-4f2e-aabe-3096f8253deb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T01:42:50,336589020-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:42:50,342895079-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '182d6ece-caac-4f2e-aabe-3096f8253deb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 182d6ece-caac-4f2e-aabe-3096f8253deb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T01:42:53,153505549-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:42:53,159714450-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '182d6ece-caac-4f2e-aabe-3096f8253deb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 182d6ece-caac-4f2e-aabe-3096f8253deb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T01:42:58,749738461-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:42:58,757902842-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '182d6ece-caac-4f2e-aabe-3096f8253deb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 182d6ece-caac-4f2e-aabe-3096f8253deb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T01:43:02,489946520-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:43:02,496617141-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '182d6ece-caac-4f2e-aabe-3096f8253deb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 182d6ece-caac-4f2e-aabe-3096f8253deb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T01:43:05,947224623-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:43:05,953505297-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '182d6ece-caac-4f2e-aabe-3096f8253deb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 182d6ece-caac-4f2e-aabe-3096f8253deb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T01:43:09,060456656-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:43:09,066767425-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '182d6ece-caac-4f2e-aabe-3096f8253deb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 182d6ece-caac-4f2e-aabe-3096f8253deb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:43:12,424492248-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:43:12,430905905-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '182d6ece-caac-4f2e-aabe-3096f8253deb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 182d6ece-caac-4f2e-aabe-3096f8253deb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:43:15,815895230-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:43:15,822329785-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '182d6ece-caac-4f2e-aabe-3096f8253deb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 182d6ece-caac-4f2e-aabe-3096f8253deb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T01:43:19,079934772-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:43:19,087991855-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '182d6ece-caac-4f2e-aabe-3096f8253deb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 182d6ece-caac-4f2e-aabe-3096f8253deb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T01:43:21,899116174-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:43:21,905672730-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '182d6ece-caac-4f2e-aabe-3096f8253deb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 182d6ece-caac-4f2e-aabe-3096f8253deb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T01:43:24,548067370-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:43:47,668146889-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   198 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:43:55,623582277-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:44:03,713218461-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:44:11,884647478-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 45s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:44:19,863462566-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [=========...................] (remaining: 52s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:44:27,876195293-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 71s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:44:36,032047661-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (45s)
      [=========...................] (remaining: 92s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:44:43,991994016-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 40s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:44:52,135826884-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:44:52,154441165-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:45:00,161903087-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:45:00,180915042-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:45:08,118243705-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:45:08,136486973-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:45:16,119151504-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:45:16,137570812-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:45:24,085659064-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:45:24,104364366-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:45:24,119445498-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:45:24,128239926-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:45:24,148478220-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2676627
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-18T01:45:24,165108923-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-18T01:45:24,206372635-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:45:24,212747420-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '182d6ece-caac-4f2e-aabe-3096f8253deb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 182d6ece-caac-4f2e-aabe-3096f8253deb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:45:25.701+0000 ffffa1256010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:45:25.709+0000 ffffa1256010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:45:25.709+0000 ffffa1256010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:45:25.727913+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-18T08:45:25.727986+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T08:45:25.940698+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:45:25.940698+0000     0       0         0         0         0         0           -           0
2021-05-18T08:45:26.940924+0000     1     255       622       367   366.969       367    0.380146    0.400067
2021-05-18T08:45:27.941168+0000     2     255      1345      1090   544.911       723    0.339014    0.379208
2021-05-18T08:45:28.941406+0000     3     255      2091      1836   611.885       746    0.337324     0.36385
2021-05-18T08:45:29.941614+0000     4     255      2829      2574   643.375       738    0.326181    0.360176
2021-05-18T08:45:30.941851+0000     5     255      3488      3233   646.469       659    0.419165    0.361683
2021-05-18T08:45:31.942130+0000     6     255      4119      3864   643.862       631    0.400301    0.369191
2021-05-18T08:45:32.942495+0000     7     255      4735      4480   639.849       616    0.385649    0.377046
2021-05-18T08:45:33.942972+0000     8     255      5375      5120   639.829       640    0.407945    0.378956
2021-05-18T08:45:34.943278+0000     9     255      6046      5791    643.27       671    0.358346    0.380031
2021-05-18T08:45:35.943577+0000    10     255      6706      6451   644.923       660    0.335459    0.381473
2021-05-18T08:45:36.943769+0000    11     255      7440      7185   653.008       734    0.335451    0.377833
2021-05-18T08:45:37.944060+0000    12     255      8154      7899   658.073       714    0.363641    0.376464
2021-05-18T08:45:38.944398+0000    13     255      8877      8622   663.049       723    0.345035     0.37404
2021-05-18T08:45:39.944829+0000    14     255      9513      9258   661.097       636    0.404523    0.375888
2021-05-18T08:45:40.945175+0000    15     255     10125      9870    657.81       612    0.412553    0.377893
2021-05-18T08:45:41.945613+0000    16     255     10754     10499   655.992       629     0.39566    0.380448
2021-05-18T08:45:42.945995+0000    17     255     11381     11126   654.272       627    0.412853    0.381528
2021-05-18T08:45:43.946751+0000    18     255     12001     11746   652.341       620    0.417551    0.383053
2021-05-18T08:45:44.947325+0000    19     255     12622     12367   650.673       621    0.417194    0.384459
2021-05-18T08:45:45.947739+0000 min lat: 0.310748 max lat: 0.488892 avg lat: 0.385693
2021-05-18T08:45:45.947739+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:45:45.947739+0000    20     255     13257     13002   649.876       635    0.375382    0.385693
2021-05-18T08:45:46.949100+0000 Total time run:         20.0297
Total writes made:      13258
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     661.919
Stddev Bandwidth:       81.329
Max bandwidth (MB/sec): 746
Min bandwidth (MB/sec): 367
Average IOPS:           661
Stddev IOPS:            81.329
Max IOPS:               746
Min IOPS:               367
Average Latency(s):     0.382137
Stddev Latency(s):      0.0437622
Max latency(s):         0.488892
Min latency(s):         0.0217207

[1;32mlocalhost.localdomain	[2021-05-18T01:45:47,327509541-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2676627


[1;33mlocalhost.localdomain	[2021-05-18T01:45:47,337109697-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:46:10,425435788-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.26k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:46:10,444228315-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:46:18,405746505-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.26k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:46:18,426661789-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:46:26,508267084-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.26k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:46:26,527201849-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:46:34,479123058-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.26k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:46:34,497880629-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:46:42,479534006-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.26k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:46:42,498244288-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:46:42,513487718-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:46:42,522485386-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:46:42,543334389-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2679965
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-18T01:46:42,559932550-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-18T01:46:42,599980229-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:46:42,606736663-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '182d6ece-caac-4f2e-aabe-3096f8253deb', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 182d6ece-caac-4f2e-aabe-3096f8253deb --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olAUDc:/tmp/ceph-asok.olAUDc -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:46:44.099+0000 ffffb1e1c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:46:44.107+0000 ffffb1e1c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:46:44.107+0000 ffffb1e1c010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:46:44.128959+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:46:44.128959+0000     0       0         0         0         0         0           -           0
2021-05-18T08:46:45.129137+0000     1     255       954       699   698.776       699    0.309718     0.28976
2021-05-18T08:46:46.129355+0000     2     255      1731      1476   737.801       777    0.311107    0.310854
2021-05-18T08:46:47.129937+0000     3     255      2522      2267   755.385       791    0.232545    0.311568
2021-05-18T08:46:48.130951+0000     4     255      3294      3039   759.345       772    0.237916     0.31695
2021-05-18T08:46:49.131180+0000     5     255      4046      3791   757.842       752    0.521482    0.320647
2021-05-18T08:46:50.131540+0000     6     255      4813      4558   759.322       767    0.249785    0.323184
2021-05-18T08:46:51.133144+0000     7     255      5574      5319   759.388       761    0.334147    0.326123
2021-05-18T08:46:52.133431+0000     8     255      6337      6082   759.812       763    0.229924    0.325608
2021-05-18T08:46:53.133662+0000     9     255      7060      6805   755.704       723    0.515865    0.328922
2021-05-18T08:46:54.134139+0000    10     255      7804      7549   754.498       744     0.50936    0.330402
2021-05-18T08:46:55.134362+0000    11     255      8542      8287   752.984       738    0.494098    0.331997
2021-05-18T08:46:56.134898+0000    12     255      9284      9029   752.036       742    0.248057    0.332317
2021-05-18T08:46:57.135117+0000    13     255     10028      9773   751.405       744    0.504708    0.333723
2021-05-18T08:46:58.135523+0000    14     255     10792     10537   752.283       764    0.238882    0.334095
2021-05-18T08:46:59.135745+0000    15     255     11550     11295   752.652       758    0.520119    0.334006
2021-05-18T08:47:00.135963+0000    16     255     12278     12023   751.102       728    0.524777    0.335191
2021-05-18T08:47:01.136434+0000    17     255     13041     12786   751.781       763    0.232771    0.334937
2021-05-18T08:47:02.136687+0000 Total time run:       17.5174
Total reads made:     13258
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   756.849
Average IOPS:         756
Stddev IOPS:          22.2539
Max IOPS:             791
Min IOPS:             699
Average Latency(s):   0.333759
Max latency(s):       0.604049
Min latency(s):       0.0782121

[1;32mlocalhost.localdomain	[2021-05-18T01:47:02,502625307-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2679965


[1;33mlocalhost.localdomain	[2021-05-18T01:47:02,511979000-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:47:25,813419554-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.26k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:47:25,832268535-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:47:33,813399685-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.26k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:47:33,832238585-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:47:42,077176538-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.26k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:47:42,096025954-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:47:49,910912826-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.26k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:47:49,929695090-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:47:57,956342652-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.26k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:47:57,975526940-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:47:57,990875492-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T01:47:57,996452875-07:00][RUNNING][ROUND 2/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:47:58,004879890-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T01:47:58,023109250-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40876\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.710168\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8aadffd7-d963-449c-a8cd-bc32310373c8\nsetting min_mon_release = octopus\nepoch 0\nfsid 8aadffd7-d963-449c-a8cd-bc32310373c8\nlast_changed 2021-05-18T01:48:26.024394-0700\ncreated 2021-05-18T01:48:26.024394-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40876/0,v1:10.10.1.2:40877/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.710168 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 fa29a4b3-9276-4e78-8a00-3c33a7ccfc01\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 63b92b9e-4987-46a7-b0b4-4171c937cdd9\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 99dbc4ce-63d3-4cbc-a8e6-2964a5b35a54\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42876\n  w/ user/pass: admin / 8f10a513-1ed2-4ce0-9e56-0da173b41ab8\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:48:41 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40876
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.710168
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8aadffd7-d963-449c-a8cd-bc32310373c8
setting min_mon_release = octopus
epoch 0
fsid 8aadffd7-d963-449c-a8cd-bc32310373c8
last_changed 2021-05-18T01:48:26.024394-0700
created 2021-05-18T01:48:26.024394-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40876/0,v1:10.10.1.2:40877/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.710168 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 fa29a4b3-9276-4e78-8a00-3c33a7ccfc01
0
start osd.0
add osd1 63b92b9e-4987-46a7-b0b4-4171c937cdd9
1
start osd.1
add osd2 99dbc4ce-63d3-4cbc-a8e6-2964a5b35a54
2
start osd.2


restful urls: https://10.10.1.2:42876
  w/ user/pass: admin / 8f10a513-1ed2-4ce0-9e56-0da173b41ab8


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T01:47:59.035-0700 7fd02e5361c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:47:59.035-0700 7fd02e5361c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:47:59.051-0700 7ff86935d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:47:59.051-0700 7ff86935d1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40876,v1:10.10.1.2:40877] --print /tmp/ceph_monmap.710168 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.710168 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.710168 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42876 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.azhPYUxJmn 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new fa29a4b3-9276-4e78-8a00-3c33a7ccfc01 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBif6Ng6CpsOhAAXN+Spytsm/yPFFmeopxXIg== --osd-uuid fa29a4b3-9276-4e78-8a00-3c33a7ccfc01 
2021-05-18T01:48:35.304-0700 7fbf0003df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:48:35.304-0700 7fbf0003df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:48:35.304-0700 7fbf0003df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:48:35.380-0700 7fbf0003df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 63b92b9e-4987-46a7-b0b4-4171c937cdd9 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T01:48:35.668-0700 7f30b7834f00 -1 Falling back to public interface
2021-05-18T01:48:35.680-0700 7f30b7834f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBjf6NgRPoDKBAAOJXbeEq9nLg4PPQdH2ZwBQ== --osd-uuid 63b92b9e-4987-46a7-b0b4-4171c937cdd9 
2021-05-18T01:48:36.040-0700 7fb0ec2a8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:48:36.040-0700 7fb0ec2a8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:48:36.040-0700 7fb0ec2a8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:48:36.084-0700 7fb0ec2a8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 99dbc4ce-63d3-4cbc-a8e6-2964a5b35a54 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T01:48:36.384-0700 7fc9a0319f00 -1 Falling back to public interface
2021-05-18T01:48:36.396-0700 7fc9a0319f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBkf6NgJViuFRAAiwWdJSmsDhuk6BdWFyEqfQ== --osd-uuid 99dbc4ce-63d3-4cbc-a8e6-2964a5b35a54 
2021-05-18T01:48:36.752-0700 7f91de474f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:48:36.752-0700 7f91de474f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:48:36.752-0700 7f91de474f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:48:36.804-0700 7f91de474f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T01:48:37.184-0700 7f653c0e7f00 -1 Falling back to public interface
2021-05-18T01:48:37.196-0700 7f653c0e7f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T01:48:41,050665393-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:48:41,059667426-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T01:48:41,142676759-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:48:41,149203040-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '572e2dec-7c37-4aaf-a35a-43a2b97aa169', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 572e2dec-7c37-4aaf-a35a-43a2b97aa169 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T01:48:43,949632870-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:48:43,957083233-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '572e2dec-7c37-4aaf-a35a-43a2b97aa169', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 572e2dec-7c37-4aaf-a35a-43a2b97aa169 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T01:48:46,749797783-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:48:46,756096384-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '572e2dec-7c37-4aaf-a35a-43a2b97aa169', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 572e2dec-7c37-4aaf-a35a-43a2b97aa169 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T01:48:49,627857458-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:48:49,634112117-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '572e2dec-7c37-4aaf-a35a-43a2b97aa169', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 572e2dec-7c37-4aaf-a35a-43a2b97aa169 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T01:48:55,461344138-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:48:55,468053119-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '572e2dec-7c37-4aaf-a35a-43a2b97aa169', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 572e2dec-7c37-4aaf-a35a-43a2b97aa169 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T01:48:58,506754941-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:48:58,513117093-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '572e2dec-7c37-4aaf-a35a-43a2b97aa169', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 572e2dec-7c37-4aaf-a35a-43a2b97aa169 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T01:49:01,763041967-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:49:01,769382876-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '572e2dec-7c37-4aaf-a35a-43a2b97aa169', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 572e2dec-7c37-4aaf-a35a-43a2b97aa169 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T01:49:05,097569999-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:49:05,103867130-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '572e2dec-7c37-4aaf-a35a-43a2b97aa169', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 572e2dec-7c37-4aaf-a35a-43a2b97aa169 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:49:08,673280436-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:49:08,679704174-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '572e2dec-7c37-4aaf-a35a-43a2b97aa169', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 572e2dec-7c37-4aaf-a35a-43a2b97aa169 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:49:11,820610740-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:49:11,827028236-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '572e2dec-7c37-4aaf-a35a-43a2b97aa169', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 572e2dec-7c37-4aaf-a35a-43a2b97aa169 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T01:49:15,266452308-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:49:15,272979849-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '572e2dec-7c37-4aaf-a35a-43a2b97aa169', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 572e2dec-7c37-4aaf-a35a-43a2b97aa169 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T01:49:18,225821749-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:49:18,232498565-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '572e2dec-7c37-4aaf-a35a-43a2b97aa169', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 572e2dec-7c37-4aaf-a35a-43a2b97aa169 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T01:49:20,902408233-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:49:44,053851836-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   188 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=...........................] (remaining: 101s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:49:52,174505804-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:50:00,118578751-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:50:08,126932418-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:50:16,145662507-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:50:24,353119501-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:50:32,314399426-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:50:40,281708196-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:50:40,300576514-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:50:48,382405072-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:50:48,401219426-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:50:56,382910185-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:50:56,401977543-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:51:04,436838215-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:51:04,455878823-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:51:12,377154300-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:51:12,396433944-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:51:12,412324133-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:51:12,421400864-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:51:12,442014971-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2693323
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-18T01:51:12,459095598-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-18T01:51:12,500455774-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:51:12,506979219-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '572e2dec-7c37-4aaf-a35a-43a2b97aa169', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 572e2dec-7c37-4aaf-a35a-43a2b97aa169 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:51:14.141+0000 ffffa661a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:51:14.149+0000 ffffa661a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:51:14.149+0000 ffffa661a010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:51:14.165058+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-18T08:51:14.165130+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T08:51:14.376770+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:51:14.376770+0000     0       0         0         0         0         0           -           0
2021-05-18T08:51:15.377101+0000     1     255       746       491   490.911       491    0.318661    0.337315
2021-05-18T08:51:16.377372+0000     2     255      1521      1266   632.857       775    0.348585     0.33062
2021-05-18T08:51:17.377635+0000     3     255      2267      2012   670.507       746    0.317216    0.337104
2021-05-18T08:51:18.377842+0000     4     255      3106      2851   712.586       839    0.320109    0.328421
2021-05-18T08:51:19.378080+0000     5     255      3818      3563   712.435       712    0.327007    0.333867
2021-05-18T08:51:20.378390+0000     6     255      4488      4233   705.327       670     0.36459    0.340365
2021-05-18T08:51:21.378717+0000     7     255      5235      4980   711.246       747    0.332227    0.341626
2021-05-18T08:51:22.379425+0000     8     255      6001      5746   718.025       766    0.332479     0.34119
2021-05-18T08:51:23.379751+0000     9     255      6717      6462   717.774       716    0.352505    0.342255
2021-05-18T08:51:24.380070+0000    10     255      7446      7191   718.874       729    0.339527     0.34312
2021-05-18T08:51:25.380618+0000    11     255      8165      7910   718.849       719    0.322113    0.344512
2021-05-18T08:51:26.381082+0000    12     255      8852      8597   716.168       687     0.35828    0.346486
2021-05-18T08:51:27.381464+0000    13     255      9527      9272   712.982       675    0.363219     0.34838
2021-05-18T08:51:28.382095+0000    14     255     10267     10012   714.879       740    0.343628    0.348935
2021-05-18T08:51:29.382452+0000    15     255     10976     10721    714.47       709    0.339421    0.349742
2021-05-18T08:51:30.382872+0000    16     255     11634     11379   710.923       658    0.408143    0.351163
2021-05-18T08:51:31.383153+0000    17     255     12306     12051   708.623       672    0.361939    0.353295
2021-05-18T08:51:32.383463+0000    18     255     12991     12736   707.298       685    0.384204    0.354444
2021-05-18T08:51:33.383773+0000    19     255     13715     13460   708.166       724    0.354915     0.35438
2021-05-18T08:51:34.384420+0000 min lat: 0.0296091 max lat: 0.435438 avg lat: 0.353651
2021-05-18T08:51:34.384420+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:51:34.384420+0000    20      19     14360     14341   716.781       881   0.0386997    0.353651
2021-05-18T08:51:35.384808+0000 Total time run:         20.0118
Total writes made:      14360
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     717.577
Stddev Bandwidth:       76.9761
Max bandwidth (MB/sec): 881
Min bandwidth (MB/sec): 491
Average IOPS:           717
Stddev IOPS:            76.9761
Max IOPS:               881
Min IOPS:               491
Average Latency(s):     0.353261
Stddev Latency(s):      0.0352353
Max latency(s):         0.435438
Min latency(s):         0.0253987

[1;32mlocalhost.localdomain	[2021-05-18T01:51:35,747364971-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2693323


[1;33mlocalhost.localdomain	[2021-05-18T01:51:35,757038534-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:51:58,713510754-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.36k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:51:58,732546396-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:52:06,857991174-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.36k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:52:06,877136980-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:52:15,011614297-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.36k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:52:15,030859899-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:52:22,966628203-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.36k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:52:22,985418178-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:52:30,859766505-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.36k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:52:30,878885306-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:52:30,894420175-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:52:30,903614601-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:52:30,924731608-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2696622
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-18T01:52:30,942070955-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-18T01:52:30,983249584-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:52:30,989930630-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '572e2dec-7c37-4aaf-a35a-43a2b97aa169', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 572e2dec-7c37-4aaf-a35a-43a2b97aa169 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bpJiDZ:/tmp/ceph-asok.bpJiDZ -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:52:32.730+0000 ffffa5113010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:52:32.766+0000 ffffa5113010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:52:32.770+0000 ffffa5113010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:52:32.790514+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:52:32.790514+0000     0       0         0         0         0         0           -           0
2021-05-18T08:52:33.790713+0000     1     255       981       726    725.74       726    0.305568    0.280772
2021-05-18T08:52:34.791314+0000     2     255      1819      1564   781.625       838    0.311413    0.292999
2021-05-18T08:52:35.791737+0000     3     255      2591      2336   778.308       772    0.328007    0.304693
2021-05-18T08:52:36.791950+0000     4     255      3395      3140   784.687       804    0.316815     0.30877
2021-05-18T08:52:37.792168+0000     5     255      4195      3940   787.714       800    0.242728    0.309302
2021-05-18T08:52:38.793025+0000     6     255      4955      4700   782.985       760      0.3608    0.313847
2021-05-18T08:52:39.795062+0000     7     255      5743      5488   783.473       788    0.323653    0.316315
2021-05-18T08:52:40.795266+0000     8     255      6489      6234   778.772       746    0.245477    0.318336
2021-05-18T08:52:41.795952+0000     9     255      7249      6994   776.628       760    0.239285    0.320574
2021-05-18T08:52:42.796736+0000    10     255      7995      7740   773.506       746      0.2562    0.322275
2021-05-18T08:52:43.797221+0000    11     255      8772      8517    773.79       777    0.481851     0.32344
2021-05-18T08:52:44.797633+0000    12     255      9553      9298   774.364       781    0.238441    0.323423
2021-05-18T08:52:45.798805+0000    13     255     10330     10075   774.497       777    0.267449    0.323812
2021-05-18T08:52:46.799075+0000    14     255     11074     10819   772.305       744    0.266317    0.325316
2021-05-18T08:52:47.799661+0000    15     255     11834     11579   771.455       760    0.491291     0.32619
2021-05-18T08:52:48.799869+0000    16     255     12592     12337   770.604       758    0.329026    0.326994
2021-05-18T08:52:49.800431+0000    17     255     13329     13074   768.603       737    0.256856     0.32772
2021-05-18T08:52:50.801221+0000    18     255     14089     13834   768.092       760    0.252786    0.328325
2021-05-18T08:52:51.801483+0000 Total time run:       18.5899
Total reads made:     14360
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   772.463
Average IOPS:         772
Stddev IOPS:          27.2099
Max IOPS:             838
Min IOPS:             726
Average Latency(s):   0.327099
Max latency(s):       0.600329
Min latency(s):       0.0679212

[1;32mlocalhost.localdomain	[2021-05-18T01:52:52,167418571-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2696622


[1;33mlocalhost.localdomain	[2021-05-18T01:52:52,177062938-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:53:15,169677617-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.36k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:53:15,189304509-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:53:23,115940209-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.36k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:53:23,135610264-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:53:31,112972606-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.36k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:53:31,132473582-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:53:39,322913728-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.36k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:53:39,342197797-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:53:47,312116436-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.36k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:53:47,332239929-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:53:47,347926638-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T01:53:47,353975416-07:00][RUNNING][ROUND 3/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:53:47,363023387-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T01:53:47,381430871-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40780\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.711283\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c87eca1a-0303-477e-a870-0eaf6f0831b6\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid c87eca1a-0303-477e-a870-0eaf6f0831b6\nlast_changed 2021-05-18T01:54:16.826111-0700\ncreated 2021-05-18T01:54:16.826111-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40780/0,v1:10.10.1.2:40781/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.711283 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 9b5ac90c-80ca-4556-8234-c87522983311\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 c2529b3f-a794-4ee5-8b6b-3cfa706ae1ab\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 a30d34cf-383e-46cd-b886-aeff1875e49b\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42780\n  w/ user/pass: admin / 689b7823-f177-4785-aeaa-eaa147c32633\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:54:31 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40780
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.711283
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c87eca1a-0303-477e-a870-0eaf6f0831b6
setting min_mon_release = octopus
epoch 0
fsid c87eca1a-0303-477e-a870-0eaf6f0831b6
last_changed 2021-05-18T01:54:16.826111-0700
created 2021-05-18T01:54:16.826111-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40780/0,v1:10.10.1.2:40781/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.711283 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 9b5ac90c-80ca-4556-8234-c87522983311
0
start osd.0
add osd1 c2529b3f-a794-4ee5-8b6b-3cfa706ae1ab
1
start osd.1
add osd2 a30d34cf-383e-46cd-b886-aeff1875e49b
2
start osd.2


restful urls: https://10.10.1.2:42780
  w/ user/pass: admin / 689b7823-f177-4785-aeaa-eaa147c32633


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T01:53:48.391-0700 7fa0f5fb41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:53:48.391-0700 7fa0f5fb41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:53:48.407-0700 7f409a6071c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:53:48.407-0700 7f409a6071c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40780,v1:10.10.1.2:40781] --print /tmp/ceph_monmap.711283 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.711283 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.711283 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42780 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.YVhEniSF9r 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9b5ac90c-80ca-4556-8234-c87522983311 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDBgKNgJKG1JxAA30A1rUsv6ULvdQekxwo9CQ== --osd-uuid 9b5ac90c-80ca-4556-8234-c87522983311 
2021-05-18T01:54:25.991-0700 7f340140af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:54:25.991-0700 7f340140af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:54:25.991-0700 7f340140af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T01:54:26.055-0700 7f340140af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c2529b3f-a794-4ee5-8b6b-3cfa706ae1ab -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T01:54:26.335-0700 7f431e7c3f00 -1 Falling back to public interface
2021-05-18T01:54:26.347-0700 7f431e7c3f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDCgKNgKjDCExAA+ASOpc2qzCWSx7nAJoTyIQ== --osd-uuid c2529b3f-a794-4ee5-8b6b-3cfa706ae1ab 
2021-05-18T01:54:26.711-0700 7f9cab442f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:54:26.711-0700 7f9cab442f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:54:26.711-0700 7f9cab442f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T01:54:26.759-0700 7f9cab442f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a30d34cf-383e-46cd-b886-aeff1875e49b -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T01:54:27.063-0700 7f22f853bf00 -1 Falling back to public interface
2021-05-18T01:54:27.079-0700 7f22f853bf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDDgKNgIGaJAhAAs6pJ7oeaiHtlkZUsBC1idw== --osd-uuid a30d34cf-383e-46cd-b886-aeff1875e49b 
2021-05-18T01:54:27.395-0700 7f82aebe3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:54:27.399-0700 7f82aebe3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:54:27.399-0700 7f82aebe3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T01:54:27.451-0700 7f82aebe3f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T01:54:27.855-0700 7f4d21dcef00 -1 Falling back to public interface
2021-05-18T01:54:27.871-0700 7f4d21dcef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T01:54:31,735462902-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:54:31,744322065-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T01:54:31,828733549-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:54:31,835442814-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03cb9d73-5c17-4c7e-8203-a10399cf4165', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03cb9d73-5c17-4c7e-8203-a10399cf4165 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T01:54:34,595075363-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:54:34,601408124-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03cb9d73-5c17-4c7e-8203-a10399cf4165', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03cb9d73-5c17-4c7e-8203-a10399cf4165 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T01:54:37,401521831-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:54:37,408579772-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03cb9d73-5c17-4c7e-8203-a10399cf4165', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03cb9d73-5c17-4c7e-8203-a10399cf4165 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T01:54:40,139023723-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:54:40,145533560-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03cb9d73-5c17-4c7e-8203-a10399cf4165', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03cb9d73-5c17-4c7e-8203-a10399cf4165 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T01:54:45,947758217-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:54:45,954144163-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03cb9d73-5c17-4c7e-8203-a10399cf4165', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03cb9d73-5c17-4c7e-8203-a10399cf4165 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T01:54:49,572623302-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:54:49,579142560-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03cb9d73-5c17-4c7e-8203-a10399cf4165', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03cb9d73-5c17-4c7e-8203-a10399cf4165 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T01:54:52,850379753-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:54:52,856558258-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03cb9d73-5c17-4c7e-8203-a10399cf4165', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03cb9d73-5c17-4c7e-8203-a10399cf4165 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T01:54:56,534217958-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:54:56,540563412-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03cb9d73-5c17-4c7e-8203-a10399cf4165', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03cb9d73-5c17-4c7e-8203-a10399cf4165 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:54:59,737838794-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:54:59,744383917-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03cb9d73-5c17-4c7e-8203-a10399cf4165', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03cb9d73-5c17-4c7e-8203-a10399cf4165 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T01:55:03,055731162-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:55:03,062265708-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03cb9d73-5c17-4c7e-8203-a10399cf4165', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03cb9d73-5c17-4c7e-8203-a10399cf4165 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T01:55:06,404001377-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:55:06,410243733-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03cb9d73-5c17-4c7e-8203-a10399cf4165', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03cb9d73-5c17-4c7e-8203-a10399cf4165 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T01:55:09,024087296-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:55:09,030814310-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03cb9d73-5c17-4c7e-8203-a10399cf4165', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03cb9d73-5c17-4c7e-8203-a10399cf4165 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T01:55:11,826495919-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:55:34,851952209-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:55:42,697954130-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:55:50,884495819-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:55:59,104423634-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:56:07,307928549-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:56:15,277624761-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:56:23,361732606-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:56:31,350980620-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:56:31,370020094-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:56:39,331353421-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:56:39,352577851-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:56:47,327967037-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:56:47,347379505-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:56:55,329745688-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:56:55,349016487-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:57:03,320989777-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:57:03,340228607-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:57:03,355897655-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:57:03,364849044-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:57:03,386024656-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2710147
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-18T01:57:03,403456129-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-18T01:57:03,443998572-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:57:03,450387241-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03cb9d73-5c17-4c7e-8203-a10399cf4165', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03cb9d73-5c17-4c7e-8203-a10399cf4165 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:57:04.957+0000 ffff96827010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:57:04.965+0000 ffff96827010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:57:04.965+0000 ffff96827010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:57:04.980814+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-18T08:57:04.980886+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T08:57:05.202846+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:57:05.202846+0000     0       0         0         0         0         0           -           0
2021-05-18T08:57:06.203079+0000     1     255       608       353    352.97       353      0.3937    0.427293
2021-05-18T08:57:07.203316+0000     2     255      1396      1141   570.408       788    0.303306    0.367922
2021-05-18T08:57:08.203567+0000     3     255      2157      1902   633.879       761    0.341832     0.35396
2021-05-18T08:57:09.203890+0000     4     255      2898      2643   660.602       741    0.320683    0.351895
2021-05-18T08:57:10.204621+0000     5     255      3556      3301   659.985       658    0.404707    0.357111
2021-05-18T08:57:11.204883+0000     6     255      4176      3921   653.294       620    0.407291    0.364551
2021-05-18T08:57:12.205247+0000     7     255      4846      4591   655.646       670    0.385917    0.368173
2021-05-18T08:57:13.205888+0000     8     255      5448      5193    648.89       602    0.390578    0.373939
2021-05-18T08:57:14.206289+0000     9     255      6077      5822   646.652       629    0.395034    0.377957
2021-05-18T08:57:15.206636+0000    10     255      6708      6453   645.065       631    0.380703    0.380977
2021-05-18T08:57:16.206917+0000    11     255      7331      7076   643.043       623    0.420665    0.382669
2021-05-18T08:57:17.207257+0000    12     255      7956      7701   641.522       625    0.402178    0.385263
2021-05-18T08:57:18.207540+0000    13     255      8658      8403   646.159       702    0.350861     0.38468
2021-05-18T08:57:19.208167+0000    14     255      9332      9077   648.118       674    0.372556    0.384276
2021-05-18T08:57:20.208390+0000    15     255     10016      9761   650.499       684    0.349575    0.383382
2021-05-18T08:57:21.208609+0000    16     255     10640     10385   648.835       624    0.386196    0.384383
2021-05-18T08:57:22.208848+0000    17     255     11311     11056   650.129       671    0.363263    0.384903
2021-05-18T08:57:23.209098+0000    18     255     11995     11740   652.001       684    0.372681    0.383947
2021-05-18T08:57:24.209354+0000    19     255     12586     12331   648.783       591    0.419395    0.385971
2021-05-18T08:57:25.209599+0000 min lat: 0.293468 max lat: 0.491144 avg lat: 0.387837
2021-05-18T08:57:25.209599+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:57:25.209599+0000    20     255     13196     12941   646.836       610    0.368082    0.387837
2021-05-18T08:57:26.209914+0000 Total time run:         20.0307
Total writes made:      13197
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     658.839
Stddev Bandwidth:       87.4477
Max bandwidth (MB/sec): 788
Min bandwidth (MB/sec): 353
Average IOPS:           658
Stddev IOPS:            87.4477
Max IOPS:               788
Min IOPS:               353
Average Latency(s):     0.384338
Stddev Latency(s):      0.0445655
Max latency(s):         0.491144
Min latency(s):         0.0258202

[1;32mlocalhost.localdomain	[2021-05-18T01:57:26,558778056-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2710147


[1;33mlocalhost.localdomain	[2021-05-18T01:57:26,568263716-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:57:49,654713661-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.20k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:57:49,674539376-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:57:57,732223277-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.20k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:57:57,755060423-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:58:05,710051669-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.20k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:58:05,729815799-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:58:13,777179859-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.20k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:58:13,797083678-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:58:21,753405125-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.20k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:58:21,773300948-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:58:21,789638797-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:58:21,798895924-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:58:21,820100915-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2713463
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-18T01:58:21,837544061-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-18T01:58:21,879432419-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T01:58:21,885887850-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03cb9d73-5c17-4c7e-8203-a10399cf4165', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03cb9d73-5c17-4c7e-8203-a10399cf4165 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wD1IPr:/tmp/ceph-asok.wD1IPr -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T08:58:23.530+0000 ffffa8cc7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:58:23.534+0000 ffffa8cc7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T08:58:23.534+0000 ffffa8cc7010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T08:58:23.554173+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T08:58:23.554173+0000     0       0         0         0         0         0           -           0
2021-05-18T08:58:24.562361+0000     1     255       879       624   618.847       624    0.356569    0.311995
2021-05-18T08:58:25.562616+0000     2     255      1571      1316   655.189       692    0.361363    0.344414
2021-05-18T08:58:26.562823+0000     3     255      2320      2065   686.323       749    0.250409    0.342185
2021-05-18T08:58:27.563381+0000     4     255      3023      2768   690.387       703    0.260882    0.346479
2021-05-18T08:58:28.563971+0000     5     255      3724      3469   692.424       701    0.259474    0.350538
2021-05-18T08:58:29.564326+0000     6     255      4471      4216   701.464       747    0.506277    0.349773
2021-05-18T08:58:30.564594+0000     7     255      5246      4991   711.926       775    0.324616     0.34783
2021-05-18T08:58:31.564821+0000     8     255      6010      5755   718.406       764    0.341339    0.345587
2021-05-18T08:58:32.566395+0000     9     256      6741      6485   719.567       730    0.340866    0.346282
2021-05-18T08:58:33.567254+0000    10     255      7500      7245   723.544       760    0.499796    0.344699
2021-05-18T08:58:34.567487+0000    11     255      8260      8005   726.838       760    0.234188    0.343827
2021-05-18T08:58:35.567695+0000    12     255      9001      8746   728.005       741     0.57983    0.343491
2021-05-18T08:58:36.569478+0000    13     255      9726      9471   727.674       725    0.357817    0.344958
2021-05-18T08:58:37.569758+0000    14     255     10475     10220   729.181       749    0.339798    0.344645
2021-05-18T08:58:38.570019+0000    15     255     11192     10937   728.357       717    0.360066    0.345534
2021-05-18T08:58:39.570863+0000    16     256     11927     11671   728.671       734     0.35875    0.345502
2021-05-18T08:58:40.571071+0000    17     255     12678     12423   730.033       752    0.333715    0.345545
2021-05-18T08:58:41.571330+0000 Total time run:       17.8774
Total reads made:     13197
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   738.194
Average IOPS:         738
Stddev IOPS:          36.269
Max IOPS:             775
Min IOPS:             624
Average Latency(s):   0.342853
Max latency(s):       0.677227
Min latency(s):       0.0811113

[1;32mlocalhost.localdomain	[2021-05-18T01:58:41,934801453-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2713463


[1;33mlocalhost.localdomain	[2021-05-18T01:58:41,944484082-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T01:59:04,949917710-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.20k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:59:04,969783244-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:59:12,903368550-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.20k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:59:12,922988118-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:59:20,743891128-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.20k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:59:20,763518272-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:59:29,224435671-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.20k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:59:29,244061900-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:59:37,183565972-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.20k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T01:59:37,203264019-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T01:59:37,219272954-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T01:59:37,225174162-07:00][RUNNING][ROUND 4/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T01:59:37,234201315-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T01:59:37,252034470-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40585\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.712401\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 98240e83-b711-4f15-97ca-3e77076284c5\nsetting min_mon_release = octopus\nepoch 0\nfsid 98240e83-b711-4f15-97ca-3e77076284c5\nlast_changed 2021-05-18T02:00:05.907777-0700\ncreated 2021-05-18T02:00:05.907777-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40585/0,v1:10.10.1.2:40586/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.712401 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 fee1298b-2854-40d2-b05b-cab6cc35ff9e\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 b2bf715b-a463-4c4a-b613-c88fb93f0426\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 f16c5022-1144-49f4-8465-51c1c6cd3e64\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42585\n'
10.10.1.2: b'  w/ user/pass: admin / a4427f3d-e971-4865-8d18-1f941cf4a35e\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:00:21 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40585
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.712401
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 98240e83-b711-4f15-97ca-3e77076284c5
setting min_mon_release = octopus
epoch 0
fsid 98240e83-b711-4f15-97ca-3e77076284c5
last_changed 2021-05-18T02:00:05.907777-0700
created 2021-05-18T02:00:05.907777-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40585/0,v1:10.10.1.2:40586/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.712401 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 fee1298b-2854-40d2-b05b-cab6cc35ff9e
0
start osd.0
add osd1 b2bf715b-a463-4c4a-b613-c88fb93f0426
1
start osd.1
add osd2 f16c5022-1144-49f4-8465-51c1c6cd3e64
2
start osd.2


restful urls: https://10.10.1.2:42585
  w/ user/pass: admin / a4427f3d-e971-4865-8d18-1f941cf4a35e


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T01:59:38.238-0700 7f0ac04fc1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:59:38.238-0700 7f0ac04fc1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:59:38.254-0700 7fccb54611c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T01:59:38.254-0700 7fccb54611c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40585,v1:10.10.1.2:40586] --print /tmp/ceph_monmap.712401 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.712401 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.712401 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42585 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.hpKdOa8M85 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new fee1298b-2854-40d2-b05b-cab6cc35ff9e -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAfgqNgn2nSIRAANRvv+YvvJWLdDjtH1fGcIw== --osd-uuid fee1298b-2854-40d2-b05b-cab6cc35ff9e 
2021-05-18T02:00:15.898-0700 7f906ff92f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:00:15.898-0700 7f906ff92f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:00:15.898-0700 7f906ff92f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:00:15.970-0700 7f906ff92f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b2bf715b-a463-4c4a-b613-c88fb93f0426 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T02:00:16.258-0700 7fb3dd0f7f00 -1 Falling back to public interface
2021-05-18T02:00:16.270-0700 7fb3dd0f7f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAggqNg2mxDDxAAqf65c9sCEZ80UK6xHWNBEA== --osd-uuid b2bf715b-a463-4c4a-b613-c88fb93f0426 
2021-05-18T02:00:16.594-0700 7f1d582fef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:00:16.594-0700 7f1d582fef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:00:16.594-0700 7f1d582fef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:00:16.658-0700 7f1d582fef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f16c5022-1144-49f4-8465-51c1c6cd3e64 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T02:00:16.934-0700 7f34b2aa1f00 -1 Falling back to public interface
2021-05-18T02:00:16.946-0700 7f34b2aa1f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAggqNg83qxNxAAU9ukCv49Xrk0l40098swAA== --osd-uuid f16c5022-1144-49f4-8465-51c1c6cd3e64 
2021-05-18T02:00:17.282-0700 7f9f3d033f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:00:17.282-0700 7f9f3d033f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:00:17.282-0700 7f9f3d033f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:00:17.354-0700 7f9f3d033f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T02:00:17.690-0700 7f7f77ccff00 -1 Falling back to public interface
2021-05-18T02:00:17.706-0700 7f7f77ccff00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T02:00:21,590658327-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:00:21,600128792-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T02:00:21,684157399-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:00:21,690525786-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d3d75dd-99e3-4174-9efb-e1fad2b499fe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d3d75dd-99e3-4174-9efb-e1fad2b499fe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T02:00:24,735420659-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:00:24,741895970-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d3d75dd-99e3-4174-9efb-e1fad2b499fe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d3d75dd-99e3-4174-9efb-e1fad2b499fe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T02:00:27,684960581-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:00:27,691490065-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d3d75dd-99e3-4174-9efb-e1fad2b499fe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d3d75dd-99e3-4174-9efb-e1fad2b499fe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T02:00:30,478351721-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:00:30,485307744-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d3d75dd-99e3-4174-9efb-e1fad2b499fe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d3d75dd-99e3-4174-9efb-e1fad2b499fe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T02:00:36,115894905-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:00:36,122479194-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d3d75dd-99e3-4174-9efb-e1fad2b499fe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d3d75dd-99e3-4174-9efb-e1fad2b499fe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T02:00:39,768019601-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:00:39,774439537-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d3d75dd-99e3-4174-9efb-e1fad2b499fe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d3d75dd-99e3-4174-9efb-e1fad2b499fe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T02:00:43,143916470-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:00:43,150273531-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d3d75dd-99e3-4174-9efb-e1fad2b499fe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d3d75dd-99e3-4174-9efb-e1fad2b499fe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T02:00:46,321629153-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:00:46,327934185-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d3d75dd-99e3-4174-9efb-e1fad2b499fe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d3d75dd-99e3-4174-9efb-e1fad2b499fe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:00:49,671538211-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:00:49,677895212-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d3d75dd-99e3-4174-9efb-e1fad2b499fe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d3d75dd-99e3-4174-9efb-e1fad2b499fe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:00:53,017666100-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:00:53,023924924-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d3d75dd-99e3-4174-9efb-e1fad2b499fe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d3d75dd-99e3-4174-9efb-e1fad2b499fe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T02:00:56,232446537-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:00:56,238750984-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d3d75dd-99e3-4174-9efb-e1fad2b499fe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d3d75dd-99e3-4174-9efb-e1fad2b499fe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T02:00:58,970146732-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:00:58,976586382-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d3d75dd-99e3-4174-9efb-e1fad2b499fe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d3d75dd-99e3-4174-9efb-e1fad2b499fe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T02:01:01,667041708-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:01:24,737039923-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   189 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:01:32,627704110-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:01:40,775395751-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 28s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:01:48,723223309-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 51s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:01:56,904990237-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 57s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:02:05,045028193-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 77s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:02:12,998148330-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 88s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:02:21,029227852-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 43s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:02:29,084659266-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:02:29,108573919-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:02:37,073011949-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:02:37,092625190-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:02:45,170652061-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:02:45,190137436-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:02:53,151623162-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:02:53,170968008-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:03:01,296449501-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:03:01,316291577-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:03:01,332362551-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:03:01,341434880-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:03:01,362891861-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2727405
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-18T02:03:01,380574799-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-18T02:03:01,422897465-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:03:01,429340026-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d3d75dd-99e3-4174-9efb-e1fad2b499fe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d3d75dd-99e3-4174-9efb-e1fad2b499fe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:03:03.221+0000 ffff8646b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:03:03.229+0000 ffff8646b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:03:03.229+0000 ffff8646b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:03:03.245411+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-18T09:03:03.245487+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T09:03:03.468237+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:03:03.468237+0000     0       0         0         0         0         0           -           0
2021-05-18T09:03:04.468445+0000     1     255       690       435   434.966       435    0.376971    0.376574
2021-05-18T09:03:05.468706+0000     2     255      1326      1071   535.409       636    0.403394    0.386297
2021-05-18T09:03:06.468972+0000     3     255      1965      1710   569.885       639    0.407643    0.392699
2021-05-18T09:03:07.469259+0000     4     255      2594      2339    584.62       629    0.400979    0.397495
2021-05-18T09:03:08.469560+0000     5     255      3235      2980   595.858       641      0.3964    0.397823
2021-05-18T09:03:09.469833+0000     6     255      3926      3671   611.684       691    0.354199    0.393795
2021-05-18T09:03:10.470099+0000     7     255      4575      4320    616.99       649    0.404328    0.392539
2021-05-18T09:03:11.470454+0000     8     255      5119      4864   607.841       544    0.468869    0.400523
2021-05-18T09:03:12.470942+0000     9     255      5747      5492   610.048       628    0.409108    0.402081
2021-05-18T09:03:13.471273+0000    10     255      6358      6103   610.123       611    0.443901     0.40287
2021-05-18T09:03:14.471530+0000    11     255      6981      6726   611.279       623    0.408302    0.404806
2021-05-18T09:03:15.471841+0000    12     255      7592      7337    611.24       611    0.436867    0.405435
2021-05-18T09:03:16.472110+0000    13     255      8188      7933   610.055       596    0.423339    0.407192
2021-05-18T09:03:17.472331+0000    14     255      8779      8524   608.685       591    0.447777    0.408415
2021-05-18T09:03:18.472583+0000    15     255      9388      9133   608.696       609    0.401033    0.410209
2021-05-18T09:03:19.472877+0000    16     255      9990      9735   608.266       602    0.404722    0.411046
2021-05-18T09:03:20.473193+0000    17     255     10554     10299   605.652       564     0.46335    0.412397
2021-05-18T09:03:21.473407+0000    18     255     11110     10855   602.887       556    0.443947    0.415415
2021-05-18T09:03:22.473671+0000    19     255     11698     11443   602.095       588    0.449972    0.416104
2021-05-18T09:03:23.474173+0000 min lat: 0.0270158 max lat: 0.552054 avg lat: 0.413697
2021-05-18T09:03:23.474173+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:03:23.474173+0000    20      32     12300     12268   613.222       825   0.0270158    0.413697
2021-05-18T09:03:24.474461+0000 Total time run:         20.0783
Total writes made:      12300
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     612.602
Stddev Bandwidth:       72.012
Max bandwidth (MB/sec): 825
Min bandwidth (MB/sec): 435
Average IOPS:           612
Stddev IOPS:            72.012
Max IOPS:               825
Min IOPS:               435
Average Latency(s):     0.412778
Stddev Latency(s):      0.0453891
Max latency(s):         0.552054
Min latency(s):         0.0148409

[1;32mlocalhost.localdomain	[2021-05-18T02:03:24,843093799-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2727405


[1;33mlocalhost.localdomain	[2021-05-18T02:03:24,852474070-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:03:47,708757007-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.30k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:03:47,728235437-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:03:55,759828075-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.30k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:03:55,779533884-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:04:03,771409672-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.30k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:04:03,791409245-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:04:13,796190003-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.30k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:04:13,815762455-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:04:21,817069340-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.30k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:04:21,837111551-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:04:21,853226409-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:04:21,862161315-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:04:21,883329603-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2730770
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-18T02:04:21,901083685-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-18T02:04:21,942111168-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:04:21,948666354-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d3d75dd-99e3-4174-9efb-e1fad2b499fe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d3d75dd-99e3-4174-9efb-e1fad2b499fe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JbVEhV:/tmp/ceph-asok.JbVEhV -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:04:23.483+0000 ffffb0e06010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:04:23.491+0000 ffffb0e06010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:04:23.491+0000 ffffb0e06010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:04:23.510888+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:04:23.510888+0000     0       0         0         0         0         0           -           0
2021-05-18T09:04:24.511187+0000     1     255       882       627   626.724       627    0.342693    0.316246
2021-05-18T09:04:25.511508+0000     2     255      1646      1391   695.236       764    0.335028    0.326463
2021-05-18T09:04:26.511730+0000     3     255      2396      2141   713.433       750    0.489619    0.329808
2021-05-18T09:04:27.511949+0000     4     255      3157      2902   725.282       761    0.330362    0.333174
2021-05-18T09:04:28.512194+0000     5     255      3921      3666   732.988       764    0.334191    0.333153
2021-05-18T09:04:29.513183+0000     6     256      4647      4391   731.536       725    0.338402    0.335931
2021-05-18T09:04:30.513426+0000     7     255      5372      5117    730.72       726    0.494018      0.3378
2021-05-18T09:04:31.513653+0000     8     255      6133      5878   734.483       761    0.255659    0.337455
2021-05-18T09:04:32.516738+0000     9     255      6884      6629   736.066       751    0.342522     0.33841
2021-05-18T09:04:33.517116+0000    10     255      7650      7395   739.029       766    0.330221    0.338259
2021-05-18T09:04:34.517343+0000    11     255      8431      8176   742.827       781     0.32727    0.337172
2021-05-18T09:04:35.517588+0000    12     255      9182      8927   743.493       751    0.277388    0.336626
2021-05-18T09:04:36.517822+0000    13     255      9909      9654   742.212       727     0.26685    0.337722
2021-05-18T09:04:37.518140+0000    14     255     10654     10399   742.394       745    0.249296    0.338205
2021-05-18T09:04:38.518374+0000    15     255     11408     11153   743.156       754    0.499372     0.33826
2021-05-18T09:04:39.518613+0000    16     255     12124     11869   741.448       716    0.263355    0.339067
2021-05-18T09:04:40.518880+0000 Total time run:       16.4591
Total reads made:     12300
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   747.307
Average IOPS:         747
Stddev IOPS:          35.4367
Max IOPS:             781
Min IOPS:             627
Average Latency(s):   0.338086
Max latency(s):       0.608825
Min latency(s):       0.0746365

[1;32mlocalhost.localdomain	[2021-05-18T02:04:40,883671627-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2730770


[1;33mlocalhost.localdomain	[2021-05-18T02:04:40,893481257-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:05:03,916240642-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.30k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:05:03,936053195-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:05:12,240714041-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.30k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:05:12,260592411-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:05:20,404709534-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.30k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:05:20,424202592-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:05:28,488468697-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.30k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:05:28,508084281-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:05:36,442741624-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.30k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:05:36,462496807-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:05:36,478231921-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T02:05:36,484324249-07:00][RUNNING][ROUND 5/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:05:36,493591777-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T02:05:36,511867228-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40003\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.713639\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a7cd8aa2-f44e-4fa2-8db5-0e393898a2d0\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid a7cd8aa2-f44e-4fa2-8db5-0e393898a2d0\nlast_changed 2021-05-18T02:06:05.649279-0700\ncreated 2021-05-18T02:06:05.649279-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40003/0,v1:10.10.1.2:40004/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.713639 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 924097f9-4d29-422e-8679-3dd82067c722\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 21e65baf-7fda-40ab-aee8-e7f5826ddf41\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 8cdb8f9b-1f4c-48e0-8404-bbaee834f6f0\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42003\n  w/ user/pass: admin / 6ca9349a-099d-4940-91ab-85a0788048fb\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:06:20 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40003
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.713639
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a7cd8aa2-f44e-4fa2-8db5-0e393898a2d0
setting min_mon_release = octopus
epoch 0
fsid a7cd8aa2-f44e-4fa2-8db5-0e393898a2d0
last_changed 2021-05-18T02:06:05.649279-0700
created 2021-05-18T02:06:05.649279-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40003/0,v1:10.10.1.2:40004/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.713639 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 924097f9-4d29-422e-8679-3dd82067c722
0
start osd.0
add osd1 21e65baf-7fda-40ab-aee8-e7f5826ddf41
1
start osd.1
add osd2 8cdb8f9b-1f4c-48e0-8404-bbaee834f6f0
2
start osd.2


restful urls: https://10.10.1.2:42003
  w/ user/pass: admin / 6ca9349a-099d-4940-91ab-85a0788048fb


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T02:05:37.513-0700 7f273fa441c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:05:37.513-0700 7f273fa441c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:05:37.529-0700 7fde3793c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:05:37.529-0700 7fde3793c1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40003,v1:10.10.1.2:40004] --print /tmp/ceph_monmap.713639 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.713639 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.713639 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42003 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.Wt1OlRsQ4I 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 924097f9-4d29-422e-8679-3dd82067c722 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCGg6NgWVsnGBAAyDeCRJzZkvaJdDed5TTnig== --osd-uuid 924097f9-4d29-422e-8679-3dd82067c722 
2021-05-18T02:06:14.778-0700 7f244bb46f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:06:14.778-0700 7f244bb46f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:06:14.778-0700 7f244bb46f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:06:14.826-0700 7f244bb46f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 21e65baf-7fda-40ab-aee8-e7f5826ddf41 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T02:06:15.102-0700 7fe36464ef00 -1 Falling back to public interface
2021-05-18T02:06:15.114-0700 7fe36464ef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCHg6Ngy1s6BhAAZB45HHBd9O9Z21xyfSBz8A== --osd-uuid 21e65baf-7fda-40ab-aee8-e7f5826ddf41 
2021-05-18T02:06:15.458-0700 7fa1a7736f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:06:15.458-0700 7fa1a7736f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:06:15.458-0700 7fa1a7736f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:06:15.510-0700 7fa1a7736f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8cdb8f9b-1f4c-48e0-8404-bbaee834f6f0 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T02:06:15.774-0700 7fb1bc0d8f00 -1 Falling back to public interface
2021-05-18T02:06:15.786-0700 7fb1bc0d8f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCHg6NgXQMQLhAAgS2imXB+n2A+mpO/RwaTVw== --osd-uuid 8cdb8f9b-1f4c-48e0-8404-bbaee834f6f0 
2021-05-18T02:06:16.102-0700 7f20955a4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:06:16.102-0700 7f20955a4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:06:16.102-0700 7f20955a4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:06:16.158-0700 7f20955a4f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T02:06:16.534-0700 7fc25c964f00 -1 Falling back to public interface
2021-05-18T02:06:16.550-0700 7fc25c964f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T02:06:20,466632251-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:06:20,477320101-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T02:06:20,574142225-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:06:20,582653624-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cd72210f-9653-44ba-8ff7-0344b394f966', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cd72210f-9653-44ba-8ff7-0344b394f966 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T02:06:23,357390609-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:06:23,363895537-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cd72210f-9653-44ba-8ff7-0344b394f966', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cd72210f-9653-44ba-8ff7-0344b394f966 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T02:06:26,401183494-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:06:26,407670568-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cd72210f-9653-44ba-8ff7-0344b394f966', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cd72210f-9653-44ba-8ff7-0344b394f966 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T02:06:29,200802241-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:06:29,207497956-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cd72210f-9653-44ba-8ff7-0344b394f966', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cd72210f-9653-44ba-8ff7-0344b394f966 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T02:06:34,884006575-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:06:34,890528816-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cd72210f-9653-44ba-8ff7-0344b394f966', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cd72210f-9653-44ba-8ff7-0344b394f966 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T02:06:38,425450089-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:06:38,432061325-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cd72210f-9653-44ba-8ff7-0344b394f966', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cd72210f-9653-44ba-8ff7-0344b394f966 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T02:06:41,775805257-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:06:41,782307049-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cd72210f-9653-44ba-8ff7-0344b394f966', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cd72210f-9653-44ba-8ff7-0344b394f966 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T02:06:45,039039259-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:06:45,045654950-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cd72210f-9653-44ba-8ff7-0344b394f966', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cd72210f-9653-44ba-8ff7-0344b394f966 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:06:48,442264164-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:06:48,448975077-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cd72210f-9653-44ba-8ff7-0344b394f966', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cd72210f-9653-44ba-8ff7-0344b394f966 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:06:51,648732021-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:06:51,655200297-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cd72210f-9653-44ba-8ff7-0344b394f966', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cd72210f-9653-44ba-8ff7-0344b394f966 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T02:06:55,069546220-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:06:55,076084874-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cd72210f-9653-44ba-8ff7-0344b394f966', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cd72210f-9653-44ba-8ff7-0344b394f966 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T02:06:57,724100044-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:06:57,731692582-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cd72210f-9653-44ba-8ff7-0344b394f966', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cd72210f-9653-44ba-8ff7-0344b394f966 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T02:07:00,418087548-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:07:23,368249196-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:07:31,341325436-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:07:39,319955253-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:07:47,464308329-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 48s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:07:55,707973153-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 56s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:08:03,845189748-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 75s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:08:12,011139588-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 85s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:08:19,980862286-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [==============..............] (remaining: 44s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:08:28,011807387-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:08:28,031388138-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:08:35,945260915-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:08:35,964846241-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:08:44,189409772-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:08:44,209176015-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:08:52,162236294-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:08:52,186198212-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:09:00,187738772-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:09:00,207559894-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:09:00,223698368-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:09:00,232918579-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:09:00,254597558-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2744687
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-18T02:09:00,272375392-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-18T02:09:00,313144838-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:09:00,319820436-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cd72210f-9653-44ba-8ff7-0344b394f966', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cd72210f-9653-44ba-8ff7-0344b394f966 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:09:01.811+0000 ffff97057010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:09:01.819+0000 ffff97057010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:09:01.819+0000 ffff97057010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:09:01.840567+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-18T09:09:01.840639+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T09:09:02.059179+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:09:02.059179+0000     0       0         0         0         0         0           -           0
2021-05-18T09:09:03.061260+0000     1     255       771       516   515.007       516    0.307615    0.344883
2021-05-18T09:09:04.062339+0000     2     255      1582      1327   662.504       811    0.313233    0.325858
2021-05-18T09:09:05.062637+0000     3     255      2362      2107    701.56       780    0.310271    0.324913
2021-05-18T09:09:06.063006+0000     4     255      3151      2896   723.336       789    0.349643    0.324983
2021-05-18T09:09:07.063249+0000     5     255      3927      3672   733.825       776     0.34487    0.325981
2021-05-18T09:09:08.063476+0000     6     255      4666      4411   734.659       739    0.334885    0.329185
2021-05-18T09:09:09.063728+0000     7     255      5410      5155   735.966       744    0.337715    0.331482
2021-05-18T09:09:10.064025+0000     8     255      6111      5856   731.571       701    0.354326    0.335835
2021-05-18T09:09:11.064268+0000     9     255      6779      6524   724.491       668    0.378951    0.339511
2021-05-18T09:09:12.064538+0000    10     255      7492      7237   723.323       713    0.354107    0.342029
2021-05-18T09:09:13.064838+0000    11     255      8164      7909    718.64       672    0.404001    0.344097
2021-05-18T09:09:14.065552+0000    12     255      8851      8596   715.962       687    0.369534    0.347417
2021-05-18T09:09:15.065961+0000    13     255      9529      9274   713.021       678    0.374805      0.3493
2021-05-18T09:09:16.066257+0000    14     255     10197      9942   709.792       668    0.364409    0.351651
2021-05-18T09:09:17.066631+0000    15     255     10834     10579   704.924       637     0.40479    0.354664
2021-05-18T09:09:18.066990+0000    16     255     11478     11223   701.102       644    0.404526    0.356576
2021-05-18T09:09:19.067267+0000    17     255     12139     11884   698.733       661    0.372373    0.358791
2021-05-18T09:09:20.067670+0000    18     255     12770     12515   694.956       631     0.40189    0.360874
2021-05-18T09:09:21.068801+0000    19     255     13397     13142    691.34       627     0.40035    0.363242
2021-05-18T09:09:22.069207+0000 min lat: 0.013097 max lat: 0.453914 avg lat: 0.362094
2021-05-18T09:09:22.069207+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:09:22.069207+0000    20      17     14044     14027   701.004       885    0.013097    0.362094
2021-05-18T09:09:23.069532+0000 Total time run:         20.0188
Total writes made:      14044
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     701.54
Stddev Bandwidth:       81.5283
Max bandwidth (MB/sec): 885
Min bandwidth (MB/sec): 516
Average IOPS:           701
Stddev IOPS:            81.5283
Max IOPS:               885
Min IOPS:               516
Average Latency(s):     0.361725
Stddev Latency(s):      0.0414354
Max latency(s):         0.453914
Min latency(s):         0.013097

[1;32mlocalhost.localdomain	[2021-05-18T02:09:23,409066554-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2744687


[1;33mlocalhost.localdomain	[2021-05-18T02:09:23,419045189-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:09:46,421656264-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.04k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:09:46,441234330-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:09:54,320592326-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.04k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:09:54,340204433-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:10:02,628011032-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.04k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:10:02,647715314-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:10:10,406165568-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.04k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:10:10,425841271-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:10:18,429796912-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.04k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:10:18,449648295-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:10:18,465658243-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:10:18,475173855-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:10:18,497001209-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2747991
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-18T02:10:18,514861108-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-18T02:10:18,556041431-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:10:18,562534012-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cd72210f-9653-44ba-8ff7-0344b394f966', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cd72210f-9653-44ba-8ff7-0344b394f966 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N7hx8i:/tmp/ceph-asok.N7hx8i -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:10:20.044+0000 ffff95afa010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:10:20.052+0000 ffff95afa010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:10:20.056+0000 ffff95afa010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:10:20.075009+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:10:20.075009+0000     0       0         0         0         0         0           -           0
2021-05-18T09:10:21.075433+0000     1     255       832       577    576.67       577    0.359923    0.335779
2021-05-18T09:10:22.075658+0000     2     255      1595      1340   669.733       763    0.322884    0.338356
2021-05-18T09:10:23.075884+0000     3     255      2394      2139   712.757       799    0.323491    0.331407
2021-05-18T09:10:24.076228+0000     4     255      3121      2866   716.255       727    0.347712    0.335776
2021-05-18T09:10:25.076484+0000     5     255      3884      3629   725.564       763    0.256731    0.335375
2021-05-18T09:10:26.076777+0000     6     255      4662      4407   734.265       778    0.338108    0.334476
2021-05-18T09:10:27.077018+0000     7     255      5407      5152   735.773       745    0.359148    0.335342
2021-05-18T09:10:28.077579+0000     8     255      6166      5911   738.624       759    0.356426    0.335639
2021-05-18T09:10:29.077811+0000     9     255      6953      6698   743.978       787    0.503492    0.333639
2021-05-18T09:10:30.078315+0000    10     255      7743      7488   748.541       790    0.254476    0.333206
2021-05-18T09:10:31.078571+0000    11     255      8511      8256   750.292       768    0.335762     0.33352
2021-05-18T09:10:32.078959+0000    12     255      9240      8985   748.494       729    0.342045    0.334824
2021-05-18T09:10:33.079295+0000    13     255     10013      9758   750.359       773     0.32774     0.33466
2021-05-18T09:10:34.079554+0000    14     255     10777     10522    751.32       764     0.33355    0.334552
2021-05-18T09:10:35.080675+0000    15     255     11532     11277   751.509       755    0.357608    0.334592
2021-05-18T09:10:36.080937+0000    16     255     12304     12049   752.777       772     0.32812    0.334683
2021-05-18T09:10:37.081184+0000    17     255     13077     12822   753.955       773    0.329402    0.334436
2021-05-18T09:10:38.081417+0000    18     256     13842     13586   754.503       764    0.336143    0.334372
2021-05-18T09:10:39.081675+0000 Total time run:       18.4793
Total reads made:     14044
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   759.986
Average IOPS:         759
Stddev IOPS:          48.1231
Max IOPS:             799
Min IOPS:             577
Average Latency(s):   0.332803
Max latency(s):       0.674235
Min latency(s):       0.107615

[1;32mlocalhost.localdomain	[2021-05-18T02:10:39,442157334-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2747991


[1;33mlocalhost.localdomain	[2021-05-18T02:10:39,452151392-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:11:02,502507769-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.04k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:11:02,522184508-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:11:10,690382644-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.04k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:11:10,709914563-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:11:18,645920159-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.04k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:11:18,665982748-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:11:26,622408751-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.04k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:11:26,642217663-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:11:34,737015392-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.04k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:11:34,756936988-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:11:34,773033171-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T02:11:34,782916001-07:00][RUNNING][ROUND 1/6/40] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:11:34,792138823-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T02:11:34,810289801-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40894\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.714746\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 02920795-6a17-4777-ad6e-f0552dc96382\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 02920795-6a17-4777-ad6e-f0552dc96382\nlast_changed 2021-05-18T02:12:03.184336-0700\ncreated 2021-05-18T02:12:03.184336-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40894/0,v1:10.10.1.2:40895/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.714746 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 687d6eca-f620-4ba8-b1f8-67cc6337fed1\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 cac9e0f6-cf21-4de4-8b34-3d4bed59536f\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 95858268-610f-4728-9f93-fa25b83a485f\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42894\n  w/ user/pass: admin / 939775a9-4fde-40ef-a531-21439a3567be\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:12:18 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40894
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.714746
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 02920795-6a17-4777-ad6e-f0552dc96382
setting min_mon_release = octopus
epoch 0
fsid 02920795-6a17-4777-ad6e-f0552dc96382
last_changed 2021-05-18T02:12:03.184336-0700
created 2021-05-18T02:12:03.184336-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40894/0,v1:10.10.1.2:40895/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.714746 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 687d6eca-f620-4ba8-b1f8-67cc6337fed1
0
start osd.0
add osd1 cac9e0f6-cf21-4de4-8b34-3d4bed59536f
1
start osd.1
add osd2 95858268-610f-4728-9f93-fa25b83a485f
2
start osd.2


restful urls: https://10.10.1.2:42894
  w/ user/pass: admin / 939775a9-4fde-40ef-a531-21439a3567be


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T02:11:35.801-0700 7f9c2afa71c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:11:35.801-0700 7f9c2afa71c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:11:35.817-0700 7fa4c0a331c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:11:35.817-0700 7fa4c0a331c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40894,v1:10.10.1.2:40895] --print /tmp/ceph_monmap.714746 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.714746 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.714746 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42894 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.NuzEqOu4IG 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 687d6eca-f620-4ba8-b1f8-67cc6337fed1 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDshKNgM/4QBRAAFuXNmI7YKFd2AiR1XG2SOw== --osd-uuid 687d6eca-f620-4ba8-b1f8-67cc6337fed1 
2021-05-18T02:12:12.449-0700 7fb0ebed1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:12:12.453-0700 7fb0ebed1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:12:12.453-0700 7fb0ebed1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:12:12.497-0700 7fb0ebed1f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new cac9e0f6-cf21-4de4-8b34-3d4bed59536f -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T02:12:12.781-0700 7f2d1e989f00 -1 Falling back to public interface
2021-05-18T02:12:12.793-0700 7f2d1e989f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDshKNgfASsLhAAnRRVj7ChCaQ1ZPZjdn8RUw== --osd-uuid cac9e0f6-cf21-4de4-8b34-3d4bed59536f 
2021-05-18T02:12:13.169-0700 7faf724cbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:12:13.169-0700 7faf724cbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:12:13.169-0700 7faf724cbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:12:13.217-0700 7faf724cbf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 95858268-610f-4728-9f93-fa25b83a485f -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T02:12:13.561-0700 7f32f6ec1f00 -1 Falling back to public interface
2021-05-18T02:12:13.573-0700 7f32f6ec1f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDthKNgPg1wIRAA0pqVhPyS8IdJPADUzcaLGQ== --osd-uuid 95858268-610f-4728-9f93-fa25b83a485f 
2021-05-18T02:12:13.905-0700 7fcb0b704f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:12:13.905-0700 7fcb0b704f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:12:13.905-0700 7fcb0b704f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:12:13.985-0700 7fcb0b704f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T02:12:14.237-0700 7f3c0f413f00 -1 Falling back to public interface
2021-05-18T02:12:14.253-0700 7f3c0f413f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T02:12:18,182358219-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:12:18,191900236-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T02:12:18,276133924-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:12:18,282638628-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a1353d-f4b2-42b9-80bc-ca3d4f4184db', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a1353d-f4b2-42b9-80bc-ca3d4f4184db --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T02:12:21,350434485-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:12:21,356921426-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a1353d-f4b2-42b9-80bc-ca3d4f4184db', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a1353d-f4b2-42b9-80bc-ca3d4f4184db --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T02:12:24,097489142-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:12:24,104068049-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a1353d-f4b2-42b9-80bc-ca3d4f4184db', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a1353d-f4b2-42b9-80bc-ca3d4f4184db --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T02:12:26,905453104-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:12:26,911851335-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a1353d-f4b2-42b9-80bc-ca3d4f4184db', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a1353d-f4b2-42b9-80bc-ca3d4f4184db --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T02:12:32,670280457-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:12:32,676889189-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a1353d-f4b2-42b9-80bc-ca3d4f4184db', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a1353d-f4b2-42b9-80bc-ca3d4f4184db --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T02:12:36,157468394-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:12:36,164393334-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a1353d-f4b2-42b9-80bc-ca3d4f4184db', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a1353d-f4b2-42b9-80bc-ca3d4f4184db --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T02:12:39,430500138-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:12:39,437982873-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a1353d-f4b2-42b9-80bc-ca3d4f4184db', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a1353d-f4b2-42b9-80bc-ca3d4f4184db --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T02:12:42,706424708-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:12:42,712941399-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a1353d-f4b2-42b9-80bc-ca3d4f4184db', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a1353d-f4b2-42b9-80bc-ca3d4f4184db --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:12:46,201646090-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:12:46,208655029-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a1353d-f4b2-42b9-80bc-ca3d4f4184db', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a1353d-f4b2-42b9-80bc-ca3d4f4184db --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:12:49,202027861-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:12:49,208717291-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a1353d-f4b2-42b9-80bc-ca3d4f4184db', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a1353d-f4b2-42b9-80bc-ca3d4f4184db --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T02:12:52,737786174-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:12:52,744239194-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a1353d-f4b2-42b9-80bc-ca3d4f4184db', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a1353d-f4b2-42b9-80bc-ca3d4f4184db --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T02:12:55,605105919-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:12:55,611507540-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a1353d-f4b2-42b9-80bc-ca3d4f4184db', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a1353d-f4b2-42b9-80bc-ca3d4f4184db --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T02:12:58,193565058-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:13:21,016138688-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:13:29,058866531-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:13:38,994851438-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 30s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:13:47,036613755-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 58s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:13:54,915159259-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (30s)
      [=======.....................] (remaining: 85s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:14:03,187939320-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=======.....................] (remaining: 94s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:14:11,154632324-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (45s)
      [========....................] (remaining: 109s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:14:19,125917792-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [==============..............] (remaining: 48s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:14:27,108891799-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:14:27,128863174-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:14:35,199603659-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:14:35,219174436-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:14:43,209512674-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:14:43,229061157-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:14:51,161349653-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:14:51,183185351-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:14:59,037644161-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:14:59,057767542-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:14:59,074110294-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:14:59,083687252-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:14:59,105610279-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2761847
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-18T02:14:59,123532365-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-18T02:14:59,165345124-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:14:59,171766489-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a1353d-f4b2-42b9-80bc-ca3d4f4184db', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a1353d-f4b2-42b9-80bc-ca3d4f4184db --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:15:00.768+0000 ffffbaa55010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:15:00.776+0000 ffffbaa55010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:15:00.776+0000 ffffbaa55010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:15:00.797181+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-18T09:15:00.797278+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T09:15:01.794334+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:15:01.794334+0000     0       0         0         0         0         0           -           0
2021-05-18T09:15:02.794600+0000     1     132       132         0         0         0           -           0
2021-05-18T09:15:03.794861+0000     2     255       274        19   37.9926        38     1.87524     1.88729
2021-05-18T09:15:04.795093+0000     3     255       407       152   202.625       532      1.8612     1.85692
2021-05-18T09:15:05.795346+0000     4     255       543       288   287.937       544      1.9132     1.87566
2021-05-18T09:15:06.795585+0000     5     255       692       437   349.522       596     1.78981     1.86944
2021-05-18T09:15:07.795859+0000     6     255       830       575   383.245       552     1.76269     1.84692
2021-05-18T09:15:08.796135+0000     7     255       977       722   412.473       588     1.78175     1.83142
2021-05-18T09:15:09.796402+0000     8     255      1117       862   430.896       560     1.80999     1.82484
2021-05-18T09:15:10.796721+0000     9     255      1261      1006   446.999       576      1.7985     1.82265
2021-05-18T09:15:11.796987+0000    10     255      1404      1149   459.484       572     1.77498     1.81672
2021-05-18T09:15:12.797274+0000    11     255      1549      1294   470.426       580     1.78324     1.81165
2021-05-18T09:15:13.797698+0000    12     255      1692      1437   478.871       572     1.76441     1.80902
2021-05-18T09:15:14.798056+0000    13     255      1831      1576   484.789       556      1.8262     1.80916
2021-05-18T09:15:15.798856+0000    14     255      1974      1719   490.989       572     1.80027     1.80999
2021-05-18T09:15:16.799106+0000    15     255      2112      1857   495.047       552     1.81513     1.81048
2021-05-18T09:15:17.799488+0000    16     255      2261      2006   501.343       596     1.76255     1.80976
2021-05-18T09:15:18.799757+0000    17     255      2399      2144   504.314       552     1.80575      1.8078
2021-05-18T09:15:19.800228+0000    18     255      2542      2287    508.06       572     1.80138     1.80933
2021-05-18T09:15:20.800488+0000    19     255      2681      2426   510.575       556     1.76401     1.80927
2021-05-18T09:15:21.800925+0000 min lat: 0.074072 max lat: 1.93079 avg lat: 1.72873
2021-05-18T09:15:21.800925+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:15:21.800925+0000    20       8      2829      2821   564.018      1580    0.074072     1.72873
2021-05-18T09:15:22.801583+0000 Total time run:         20.0294
Total writes made:      2829
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     564.97
Stddev Bandwidth:       293.101
Max bandwidth (MB/sec): 1580
Min bandwidth (MB/sec): 0
Average IOPS:           141
Stddev IOPS:            73.3223
Max IOPS:               395
Min IOPS:               0
Average Latency(s):     1.72399
Stddev Latency(s):      0.309707
Max latency(s):         1.93079
Min latency(s):         0.0348607

[1;32mlocalhost.localdomain	[2021-05-18T02:15:23,327160627-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2761847


[1;33mlocalhost.localdomain	[2021-05-18T02:15:23,337111014-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:15:46,283693274-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:15:46,303830472-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:15:54,403025823-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:15:54,423535025-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:16:02,375505980-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:16:02,395887390-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:16:10,388648559-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:16:10,408920465-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:16:18,511547585-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:16:18,532115448-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:16:18,548586081-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:16:18,558053731-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:16:18,580141142-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2765181
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-18T02:16:18,598135450-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-18T02:16:18,639044786-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:16:18,645444217-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07a1353d-f4b2-42b9-80bc-ca3d4f4184db', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07a1353d-f4b2-42b9-80bc-ca3d4f4184db --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lytO9l:/tmp/ceph-asok.lytO9l -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:16:20.409+0000 ffff85ad1010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:16:20.417+0000 ffff85ad1010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:16:20.417+0000 ffff85ad1010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:16:20.467835+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:16:20.467835+0000     0       0         0         0         0         0           -           0
2021-05-18T09:16:21.468128+0000     1     250       250         0         0         0           -           0
2021-05-18T09:16:22.469586+0000     2     255       432       177    353.66       354      1.3363     1.21331
2021-05-18T09:16:23.471576+0000     3     255       597       342   455.406       660     1.51714      1.3279
2021-05-18T09:16:24.471801+0000     4     255       774       519   518.463       708     1.47974     1.38656
2021-05-18T09:16:25.472405+0000     5     255       932       677   541.087       632     1.56431     1.41362
2021-05-18T09:16:26.472635+0000     6     255      1108       853   568.196       704     1.50095     1.44253
2021-05-18T09:16:27.472962+0000     7     255      1286      1031   588.697       712     1.42849     1.44454
2021-05-18T09:16:28.473231+0000     8     255      1454      1199   599.083       672     1.48425      1.4465
2021-05-18T09:16:29.474990+0000     9     255      1634      1379    612.39       720     1.46243     1.45201
2021-05-18T09:16:30.475756+0000    10     255      1803      1548   618.699       676     1.47625     1.45072
2021-05-18T09:16:31.475985+0000    11     255      1985      1730   628.615       728     1.46395     1.45507
2021-05-18T09:16:32.476266+0000    12     255      2166      1911   636.544       724     1.39006     1.44948
2021-05-18T09:16:33.476539+0000    13     255      2349      2094   643.868       732      1.4124     1.44648
2021-05-18T09:16:34.477415+0000    14     255      2519      2264   646.407       680     1.46722     1.44578
2021-05-18T09:16:35.478850+0000    15     255      2687      2432    648.05       672     1.51098     1.44936
2021-05-18T09:16:36.487124+0000    16     145      2829      2684   670.185      1008     1.03067     1.44506
2021-05-18T09:16:37.487379+0000 Total time run:       16.2264
Total reads made:     2829
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   697.38
Average IOPS:         174
Stddev IOPS:          53.0612
Max IOPS:             252
Min IOPS:             0
Average Latency(s):   1.40764
Max latency(s):       1.57388
Min latency(s):       0.404427

[1;32mlocalhost.localdomain	[2021-05-18T02:16:37,972415166-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2765181


[1;33mlocalhost.localdomain	[2021-05-18T02:16:37,982308543-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:17:01,112931546-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:17:01,133361205-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:17:09,115636979-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:17:09,135949348-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:17:17,123949374-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:17:17,144760728-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:17:25,122239022-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:17:25,143194400-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:17:33,141991295-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:17:33,162539864-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:17:33,179651813-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T02:17:33,185966554-07:00][RUNNING][ROUND 2/6/40] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:17:33,195526260-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T02:17:33,213871883-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40419\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.715853\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 621c0059-0a00-4020-b5f6-eb63bbc65624\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 621c0059-0a00-4020-b5f6-eb63bbc65624\nlast_changed 2021-05-18T02:18:01.912521-0700\ncreated 2021-05-18T02:18:01.912521-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40419/0,v1:10.10.1.2:40420/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.715853 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 edcb26ba-a057-4e68-b3ac-7152bb49c1a9\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 a7021e29-d820-49d3-98dc-2d0f9f3695de\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 00311876-c57a-473a-95d8-1c8ec4f97026\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42419\n  w/ user/pass: admin / 7a8cdc1a-a504-4138-8e97-02825b7da334\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 02:18:16 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40419
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.715853
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 621c0059-0a00-4020-b5f6-eb63bbc65624
setting min_mon_release = octopus
epoch 0
fsid 621c0059-0a00-4020-b5f6-eb63bbc65624
last_changed 2021-05-18T02:18:01.912521-0700
created 2021-05-18T02:18:01.912521-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40419/0,v1:10.10.1.2:40420/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.715853 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 edcb26ba-a057-4e68-b3ac-7152bb49c1a9
0
start osd.0
add osd1 a7021e29-d820-49d3-98dc-2d0f9f3695de
1
start osd.1
add osd2 00311876-c57a-473a-95d8-1c8ec4f97026
2
start osd.2


restful urls: https://10.10.1.2:42419
  w/ user/pass: admin / 7a8cdc1a-a504-4138-8e97-02825b7da334


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T02:17:34.196-0700 7f6d2e6ff1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:17:34.196-0700 7f6d2e6ff1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:17:34.212-0700 7fba536c31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:17:34.212-0700 7fba536c31c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40419,v1:10.10.1.2:40420] --print /tmp/ceph_monmap.715853 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.715853 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.715853 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42419 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.7FO7oUVgNs 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new edcb26ba-a057-4e68-b3ac-7152bb49c1a9 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBShqNgVXE6KxAA4Q72jdyOQDSavygT2AIXXw== --osd-uuid edcb26ba-a057-4e68-b3ac-7152bb49c1a9 
2021-05-18T02:18:11.048-0700 7f8108a70f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:18:11.048-0700 7f8108a70f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:18:11.048-0700 7f8108a70f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:18:11.120-0700 7f8108a70f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a7021e29-d820-49d3-98dc-2d0f9f3695de -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T02:18:11.404-0700 7f813b9fcf00 -1 Falling back to public interface
2021-05-18T02:18:11.416-0700 7f813b9fcf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBThqNgcvA5GBAAj48h1l/2lTnyFM00ql7Kaw== --osd-uuid a7021e29-d820-49d3-98dc-2d0f9f3695de 
2021-05-18T02:18:11.736-0700 7f6073390f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:18:11.736-0700 7f6073390f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:18:11.736-0700 7f6073390f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:18:11.812-0700 7f6073390f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 00311876-c57a-473a-95d8-1c8ec4f97026 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T02:18:12.092-0700 7f93e733ff00 -1 Falling back to public interface
2021-05-18T02:18:12.104-0700 7f93e733ff00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBUhqNgatSdBRAA5yWIiwI7vxnO2uCov678/w== --osd-uuid 00311876-c57a-473a-95d8-1c8ec4f97026 
2021-05-18T02:18:12.448-0700 7f555c4b6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:18:12.448-0700 7f555c4b6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:18:12.448-0700 7f555c4b6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:18:12.500-0700 7f555c4b6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T02:18:12.828-0700 7f2a2d1c7f00 -1 Falling back to public interface
2021-05-18T02:18:12.844-0700 7f2a2d1c7f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T02:18:16,775488254-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:18:16,785126393-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T02:18:16,870039959-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:18:16,876919727-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1dd16172-9acb-4deb-a3c2-3425077151a3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1dd16172-9acb-4deb-a3c2-3425077151a3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T02:18:19,922896560-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:18:19,929574544-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1dd16172-9acb-4deb-a3c2-3425077151a3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1dd16172-9acb-4deb-a3c2-3425077151a3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T02:18:22,713526173-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:18:22,719777904-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1dd16172-9acb-4deb-a3c2-3425077151a3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1dd16172-9acb-4deb-a3c2-3425077151a3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T02:18:25,543984174-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:18:25,550515883-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1dd16172-9acb-4deb-a3c2-3425077151a3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1dd16172-9acb-4deb-a3c2-3425077151a3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T02:18:31,271862436-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:18:31,278389059-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1dd16172-9acb-4deb-a3c2-3425077151a3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1dd16172-9acb-4deb-a3c2-3425077151a3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T02:18:34,860165008-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:18:34,866838371-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1dd16172-9acb-4deb-a3c2-3425077151a3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1dd16172-9acb-4deb-a3c2-3425077151a3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T02:18:38,174845504-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:18:38,181122605-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1dd16172-9acb-4deb-a3c2-3425077151a3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1dd16172-9acb-4deb-a3c2-3425077151a3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T02:18:41,400479561-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:18:41,407051477-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1dd16172-9acb-4deb-a3c2-3425077151a3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1dd16172-9acb-4deb-a3c2-3425077151a3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:18:45,160788453-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:18:45,168332115-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1dd16172-9acb-4deb-a3c2-3425077151a3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1dd16172-9acb-4deb-a3c2-3425077151a3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:18:48,063871975-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:18:48,070364602-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1dd16172-9acb-4deb-a3c2-3425077151a3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1dd16172-9acb-4deb-a3c2-3425077151a3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T02:18:51,423456395-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:18:51,430002777-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1dd16172-9acb-4deb-a3c2-3425077151a3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1dd16172-9acb-4deb-a3c2-3425077151a3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T02:18:54,249655258-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:18:54,256073082-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1dd16172-9acb-4deb-a3c2-3425077151a3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1dd16172-9acb-4deb-a3c2-3425077151a3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T02:18:57,035047597-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:19:20,015796383-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:19:28,317653925-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:19:36,352066638-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 29s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:19:44,312822858-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:19:52,345435521-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 57s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:20:00,302961346-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 77s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:20:08,481501694-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 88s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:20:16,517001054-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   227 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [=========...................] (remaining: 102s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:20:24,490844377-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:20:24,511135338-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:20:32,659746235-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:20:32,680176180-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:20:40,706494627-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:20:40,727133333-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:20:48,765888504-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:20:48,786229723-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:20:56,705762283-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:20:56,726335067-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:20:56,743232479-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:20:56,752673964-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:20:56,774928700-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2778993
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-18T02:20:56,792916108-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-18T02:20:56,835061506-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:20:56,841724502-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1dd16172-9acb-4deb-a3c2-3425077151a3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1dd16172-9acb-4deb-a3c2-3425077151a3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8 -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:20:58.413+0000 ffffbb75a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:20:58.421+0000 ffffbb75a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:20:58.421+0000 ffffbb75a010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:20:58.439957+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-18T09:20:58.440052+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T09:20:59.394911+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:20:59.394911+0000     0       0         0         0         0         0           -           0
2021-05-18T09:21:00.395148+0000     1     143       143         0         0         0           -           0
2021-05-18T09:21:01.395376+0000     2     255       282        27   53.9918        54     1.84322     1.85733
2021-05-18T09:21:02.395814+0000     3     255       421       166   221.279       556     1.84381     1.87029
2021-05-18T09:21:03.396133+0000     4     255       584       329   328.913       652     1.63773      1.8029
2021-05-18T09:21:04.396769+0000     5     255       743       488   390.268       636     1.58648     1.73081
2021-05-18T09:21:05.396999+0000     6     255       900       645   429.862       628     1.62704     1.70197
2021-05-18T09:21:06.397247+0000     7     255      1050       795   454.145       600     1.65802     1.69266
2021-05-18T09:21:07.397858+0000     8     255      1185       930   464.838       540     1.80902     1.69962
2021-05-18T09:21:08.398350+0000     9     255      1316      1061   471.384       524     1.94279     1.72436
2021-05-18T09:21:09.398768+0000    10     255      1455      1200   479.823       556     1.87156     1.74798
2021-05-18T09:21:10.399144+0000    11     255      1597      1342   487.819       568     1.80238     1.75668
2021-05-18T09:21:11.399587+0000    12     255      1736      1481   493.481       556     1.82821     1.76182
2021-05-18T09:21:12.400001+0000    13     255      1869      1614   496.427       532     1.89708     1.77117
2021-05-18T09:21:13.400264+0000    14     255      2007      1752   500.386       552     1.88602     1.78016
2021-05-18T09:21:14.400535+0000    15     255      2146      1891   504.083       556     1.83348     1.78641
2021-05-18T09:21:15.400851+0000    16     255      2278      2023   505.567       528     1.87865     1.79238
2021-05-18T09:21:16.401122+0000    17     255      2415      2160   508.055       548     1.90044     1.79924
2021-05-18T09:21:17.401366+0000    18     255      2550      2295   509.822       540     1.85787     1.80454
2021-05-18T09:21:18.401728+0000    19     255      2684      2429   511.189       536     1.89174     1.80844
2021-05-18T09:21:19.402043+0000 min lat: 0.0445179 max lat: 1.97813 avg lat: 1.7366
2021-05-18T09:21:19.402043+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:21:19.402043+0000    20       7      2824      2817   563.204      1552   0.0445179      1.7366
2021-05-18T09:21:20.402333+0000 Total time run:         20.0558
Total writes made:      2824
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     563.229
Stddev Bandwidth:       288.277
Max bandwidth (MB/sec): 1552
Min bandwidth (MB/sec): 0
Average IOPS:           140
Stddev IOPS:            72.1156
Max IOPS:               388
Min IOPS:               0
Average Latency(s):     1.73243
Stddev Latency(s):      0.321143
Max latency(s):         1.97813
Min latency(s):         0.0354127

[1;32mlocalhost.localdomain	[2021-05-18T02:21:20,993082883-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2778993


[1;33mlocalhost.localdomain	[2021-05-18T02:21:21,003008951-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:21:43,920404001-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:21:43,940883559-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:21:51,918162365-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:21:51,939182826-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:22:00,001905067-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:22:00,022913316-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:22:08,088804959-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:22:08,109702053-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:22:16,314699470-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:22:16,335369156-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:22:16,352207578-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:22:16,361881003-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:22:16,384274153-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2782403
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-18T02:22:16,402624157-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-18T02:22:16,444214431-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:22:16,450495418-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1dd16172-9acb-4deb-a3c2-3425077151a3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1dd16172-9acb-4deb-a3c2-3425077151a3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.I9HDx8:/tmp/ceph-asok.I9HDx8 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:22:17.950+0000 ffff9155f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:22:17.958+0000 ffff9155f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:22:17.958+0000 ffff9155f010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:22:17.982636+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:22:17.982636+0000     0       4         4         0         0         0           -           0
2021-05-18T09:22:18.982849+0000     1     255       258         3   11.9906        12    0.997767    0.993442
2021-05-18T09:22:19.983458+0000     2     255       435       180   359.749       708     1.34672     1.20622
2021-05-18T09:22:20.984944+0000     3     255       610       355   472.879       700      1.4493     1.31233
2021-05-18T09:22:21.985197+0000     4     255       798       543   542.575       752      1.3976     1.35485
2021-05-18T09:22:22.986902+0000     5     255       980       725   579.439       728     1.38397     1.35867
2021-05-18T09:22:23.987481+0000     6     255      1151       896   596.794       684     1.47718     1.37557
2021-05-18T09:22:24.987707+0000     7     255      1337      1082   617.787       744     1.38854     1.38777
2021-05-18T09:22:25.989264+0000     8     255      1504      1249   623.938       668     1.46382     1.39222
2021-05-18T09:22:26.990427+0000     9     255      1670      1415   628.305       664     1.54387     1.40905
2021-05-18T09:22:27.991909+0000    10     255      1860      1605   641.368       760     1.38287     1.41487
2021-05-18T09:22:28.992193+0000    11     255      2038      1783   647.767       712     1.41331     1.41078
2021-05-18T09:22:29.992522+0000    12     255      2214      1959   652.431       704     1.45786     1.41488
2021-05-18T09:22:30.992812+0000    13     255      2393      2138   657.303       716     1.40811     1.41736
2021-05-18T09:22:31.993971+0000    14     255      2576      2321   662.579       732     1.39085     1.41734
2021-05-18T09:22:32.994245+0000    15     255      2757      2502   666.658       724     1.39774     1.41579
2021-05-18T09:22:33.994903+0000 Total time run:       15.8071
Total reads made:     2824
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   714.616
Average IOPS:         178
Stddev IOPS:          45.8494
Max IOPS:             190
Min IOPS:             3
Average Latency(s):   1.37225
Max latency(s):       1.57004
Min latency(s):       0.435342

[1;32mlocalhost.localdomain	[2021-05-18T02:22:34,650926278-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2782403


[1;33mlocalhost.localdomain	[2021-05-18T02:22:34,661368255-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:22:57,587032656-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:22:57,607783866-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:23:05,741754556-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:23:05,762284478-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:23:13,748958233-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:23:13,769677713-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:23:21,714445156-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:23:21,735513340-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:23:29,903131411-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:23:29,924240581-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:23:29,941410785-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T02:23:29,948000674-07:00][RUNNING][ROUND 3/6/40] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:23:29,957488187-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T02:23:29,976333427-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40433\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.717027\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 5a077659-2c34-4cf1-add4-dba06210ab42\nsetting min_mon_release = octopus\nepoch 0\nfsid 5a077659-2c34-4cf1-add4-dba06210ab42\nlast_changed 2021-05-18T02:23:57.833135-0700\ncreated 2021-05-18T02:23:57.833135-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40433/0,v1:10.10.1.2:40434/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.717027 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 1122a3a6-22c6-4e02-b2fb-a54d11180ae7\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 2224a3b9-c4bd-4dfd-8608-de36d0a0c5e9\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 d580b118-e3b1-45ac-81a0-7aaa4dcec755\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42433\n  w/ user/pass: admin / 6a84ab18-e75e-4d99-a059-183c52503b09\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:24:12 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40433
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.717027
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 5a077659-2c34-4cf1-add4-dba06210ab42
setting min_mon_release = octopus
epoch 0
fsid 5a077659-2c34-4cf1-add4-dba06210ab42
last_changed 2021-05-18T02:23:57.833135-0700
created 2021-05-18T02:23:57.833135-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40433/0,v1:10.10.1.2:40434/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.717027 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 1122a3a6-22c6-4e02-b2fb-a54d11180ae7
0
start osd.0
add osd1 2224a3b9-c4bd-4dfd-8608-de36d0a0c5e9
1
start osd.1
add osd2 d580b118-e3b1-45ac-81a0-7aaa4dcec755
2
start osd.2


restful urls: https://10.10.1.2:42433
  w/ user/pass: admin / 6a84ab18-e75e-4d99-a059-183c52503b09


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T02:23:30.987-0700 7f126f1361c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:23:30.987-0700 7f126f1361c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:23:31.003-0700 7fda9a3d91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:23:31.003-0700 7fda9a3d91c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40433,v1:10.10.1.2:40434] --print /tmp/ceph_monmap.717027 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.717027 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.717027 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42433 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.stcv9zu6u1 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 1122a3a6-22c6-4e02-b2fb-a54d11180ae7 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC2h6Ngav88MRAAOaOP+QayOX8g3MQnTOxyPA== --osd-uuid 1122a3a6-22c6-4e02-b2fb-a54d11180ae7 
2021-05-18T02:24:07.224-0700 7faf76c08f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:24:07.224-0700 7faf76c08f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:24:07.224-0700 7faf76c08f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:24:07.296-0700 7faf76c08f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 2224a3b9-c4bd-4dfd-8608-de36d0a0c5e9 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T02:24:07.580-0700 7fd170ec7f00 -1 Falling back to public interface
2021-05-18T02:24:07.592-0700 7fd170ec7f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC3h6NgHxLKIhAAY8GKkK6Q615gFv82fYKrBA== --osd-uuid 2224a3b9-c4bd-4dfd-8608-de36d0a0c5e9 
2021-05-18T02:24:07.900-0700 7fdde0751f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:24:07.900-0700 7fdde0751f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:24:07.900-0700 7fdde0751f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:24:07.956-0700 7fdde0751f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d580b118-e3b1-45ac-81a0-7aaa4dcec755 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T02:24:08.376-0700 7f5d5ceaef00 -1 Falling back to public interface
2021-05-18T02:24:08.388-0700 7f5d5ceaef00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC4h6NgFbd8FhAASzKPXyu1FBWq0BPN0Fh6wA== --osd-uuid d580b118-e3b1-45ac-81a0-7aaa4dcec755 
2021-05-18T02:24:08.712-0700 7f51c35b6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:24:08.712-0700 7f51c35b6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:24:08.712-0700 7f51c35b6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:24:08.800-0700 7f51c35b6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T02:24:09.068-0700 7fbc85d3cf00 -1 Falling back to public interface
2021-05-18T02:24:09.084-0700 7fbc85d3cf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T02:24:12,983696708-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:24:12,993352235-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T02:24:13,076860995-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:24:13,083264957-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '31c489a3-71c1-42fe-a49c-f1011ba6151b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 31c489a3-71c1-42fe-a49c-f1011ba6151b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T02:24:17,880263180-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:24:17,886500614-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '31c489a3-71c1-42fe-a49c-f1011ba6151b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 31c489a3-71c1-42fe-a49c-f1011ba6151b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T02:24:20,645736673-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:24:20,652091757-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '31c489a3-71c1-42fe-a49c-f1011ba6151b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 31c489a3-71c1-42fe-a49c-f1011ba6151b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T02:24:23,420637504-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:24:23,427353159-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '31c489a3-71c1-42fe-a49c-f1011ba6151b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 31c489a3-71c1-42fe-a49c-f1011ba6151b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T02:24:29,127313442-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:24:29,133817231-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '31c489a3-71c1-42fe-a49c-f1011ba6151b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 31c489a3-71c1-42fe-a49c-f1011ba6151b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T02:24:32,694001456-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:24:32,700631747-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '31c489a3-71c1-42fe-a49c-f1011ba6151b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 31c489a3-71c1-42fe-a49c-f1011ba6151b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T02:24:35,960438891-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:24:35,967001595-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '31c489a3-71c1-42fe-a49c-f1011ba6151b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 31c489a3-71c1-42fe-a49c-f1011ba6151b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T02:24:40,000704289-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:24:40,008054373-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '31c489a3-71c1-42fe-a49c-f1011ba6151b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 31c489a3-71c1-42fe-a49c-f1011ba6151b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:24:42,951728822-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:24:42,958122718-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '31c489a3-71c1-42fe-a49c-f1011ba6151b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 31c489a3-71c1-42fe-a49c-f1011ba6151b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:24:46,796830912-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:24:46,804722799-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '31c489a3-71c1-42fe-a49c-f1011ba6151b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 31c489a3-71c1-42fe-a49c-f1011ba6151b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T02:24:50,753981012-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:24:50,760637692-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '31c489a3-71c1-42fe-a49c-f1011ba6151b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 31c489a3-71c1-42fe-a49c-f1011ba6151b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T02:24:53,449940033-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:24:53,457043445-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '31c489a3-71c1-42fe-a49c-f1011ba6151b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 31c489a3-71c1-42fe-a49c-f1011ba6151b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  146 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  146 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.99   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.05   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.97   70      up          osd.2  
                       TOTAL  300 GiB  148 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.97/1.05  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T02:24:56,104390199-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:25:19,063142180-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (5s)
      [============................] (remaining: 5s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:25:27,131272608-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:25:35,201598919-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:25:43,097531932-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:25:51,113189527-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:25:59,225842017-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:26:07,176033717-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   213 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:26:07,196672618-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:26:15,071399499-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:26:15,092106646-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:26:23,047099413-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:26:23,067561824-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:26:31,173453756-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:26:31,195883708-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:26:39,081518166-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:26:39,101839972-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:26:39,118597426-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:26:39,128280483-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:26:39,150993757-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2795354
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-18T02:26:39,169371862-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-18T02:26:39,211732068-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:26:39,218320427-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '31c489a3-71c1-42fe-a49c-f1011ba6151b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 31c489a3-71c1-42fe-a49c-f1011ba6151b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:26:40.683+0000 ffff8e1a7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:26:40.691+0000 ffff8e1a7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:26:40.691+0000 ffff8e1a7010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:26:40.714285+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-18T09:26:40.714382+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T09:26:41.690817+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:26:41.690817+0000     0       0         0         0         0         0           -           0
2021-05-18T09:26:42.691067+0000     1     165       165         0         0         0           -           0
2021-05-18T09:26:43.691277+0000     2     255       334        79   157.977       158     1.53997     1.53939
2021-05-18T09:26:44.691510+0000     3     255       496       241   321.277       648     1.54476     1.53512
2021-05-18T09:26:45.691771+0000     4     255       652       397   396.922       624     1.62809     1.55627
2021-05-18T09:26:46.692024+0000     5     255       819       564   451.106       668     1.59456     1.57239
2021-05-18T09:26:47.692374+0000     6     255       971       716   477.222       608     1.60821     1.57404
2021-05-18T09:26:48.692769+0000     7     255      1116       861   491.874       580     1.74328     1.59518
2021-05-18T09:26:49.693239+0000     8     255      1286      1031   515.354       680     1.57819     1.60837
2021-05-18T09:26:50.693560+0000     9     255      1436      1181   524.738       600     1.63338     1.60754
2021-05-18T09:26:51.693856+0000    10     255      1604      1349   539.445       672     1.54833     1.60558
2021-05-18T09:26:52.694433+0000    11     255      1741      1486   540.194       548      1.7244      1.6091
2021-05-18T09:26:53.695005+0000    12     255      1890      1635   544.817       596     1.76488     1.62535
2021-05-18T09:26:54.695347+0000    13     255      2053      1798   553.045       652     1.58772     1.62982
2021-05-18T09:26:55.695616+0000    14     255      2212      1957   558.958       636     1.59553     1.62665
2021-05-18T09:26:56.695871+0000    15     255      2364      2109   562.217       608     1.64813     1.62895
2021-05-18T09:26:57.696187+0000    16     255      2535      2280   569.815       684     1.53706     1.62781
2021-05-18T09:26:58.696536+0000    17     255      2705      2450   576.282       680     1.49344     1.61927
2021-05-18T09:26:59.696897+0000    18     255      2863      2608   579.365       632     1.58669     1.61342
2021-05-18T09:27:00.697593+0000    19     255      3021      2766   582.113       632     1.64282     1.61307
2021-05-18T09:27:01.697969+0000 min lat: 1.4718 max lat: 1.83597 avg lat: 1.61391
2021-05-18T09:27:01.697969+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:27:01.697969+0000    20     255      3174      2919   583.596       612     1.64046     1.61391
2021-05-18T09:27:02.698249+0000 Total time run:         20.0373
Total writes made:      3175
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     633.819
Stddev Bandwidth:       175.579
Max bandwidth (MB/sec): 684
Min bandwidth (MB/sec): 0
Average IOPS:           158
Stddev IOPS:            43.9575
Max IOPS:               171
Min IOPS:               0
Average Latency(s):     1.55177
Stddev Latency(s):      0.258414
Max latency(s):         1.83597
Min latency(s):         0.0469608

[1;32mlocalhost.localdomain	[2021-05-18T02:27:03,148527276-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2795354


[1;33mlocalhost.localdomain	[2021-05-18T02:27:03,159292692-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:27:26,202905599-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.18k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:27:26,223191714-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:27:34,355020640-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.18k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:27:34,375629370-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:27:42,294711552-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.18k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:27:42,317059784-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:27:50,294988788-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.18k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:27:50,315169255-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:27:58,279204392-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.18k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:27:58,299446745-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:27:58,316620280-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:27:58,326428788-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:27:58,349083328-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2798672
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-18T02:27:58,367565971-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-18T02:27:58,408983416-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:27:58,415364905-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '31c489a3-71c1-42fe-a49c-f1011ba6151b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 31c489a3-71c1-42fe-a49c-f1011ba6151b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xzrpxA:/tmp/ceph-asok.xzrpxA -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:27:59.949+0000 ffff98733010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:27:59.957+0000 ffff98733010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:27:59.957+0000 ffff98733010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:27:59.978481+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:27:59.978481+0000     0       0         0         0         0         0           -           0
2021-05-18T09:28:00.978702+0000     1     244       244         0         0         0           -           0
2021-05-18T09:28:01.980325+0000     2     255       421       166   331.664       332     1.36215     1.25064
2021-05-18T09:28:02.984588+0000     3     255       588       333   443.071       668     1.52034     1.35307
2021-05-18T09:28:03.984868+0000     4     255       763       508   507.167       700     1.50239     1.41048
2021-05-18T09:28:04.988169+0000     5     256       940       684   546.122       704     1.44676      1.4249
2021-05-18T09:28:05.988393+0000     6     255      1117       862   573.702       712     1.43749     1.42964
2021-05-18T09:28:06.988601+0000     7     255      1293      1038   592.271       704     1.44659     1.43179
2021-05-18T09:28:07.990154+0000     8     255      1467      1212   605.103       696     1.45236     1.43338
2021-05-18T09:28:08.990531+0000     9     255      1662      1407   624.485       780     1.36365     1.42806
2021-05-18T09:28:09.990791+0000    10     255      1838      1583    632.41       704     1.42899     1.42316
2021-05-18T09:28:10.991117+0000    11     255      2023      1768   642.161       740      1.3954     1.42382
2021-05-18T09:28:11.991630+0000    12     255      2200      1945   647.614       708     1.41375     1.42146
2021-05-18T09:28:12.992451+0000    13     255      2386      2131   654.979       744     1.40452     1.42142
2021-05-18T09:28:13.992819+0000    14     255      2560      2305   657.889       696      1.4371     1.42101
2021-05-18T09:28:14.993151+0000    15     255      2741      2486   662.278       724     1.41548     1.42264
2021-05-18T09:28:15.993590+0000    16     255      2915      2660   664.365       696     1.45286     1.42306
2021-05-18T09:28:16.996186+0000    17     255      3088      2833   665.888       692     1.48397     1.42501
2021-05-18T09:28:17.996479+0000 Total time run:       17.8934
Total reads made:     3175
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   709.758
Average IOPS:         177
Stddev IOPS:          47.9942
Max IOPS:             195
Min IOPS:             0
Average Latency(s):   1.38723
Max latency(s):       1.54178
Min latency(s):       0.401458

[1;32mlocalhost.localdomain	[2021-05-18T02:28:18,465458871-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2798672


[1;33mlocalhost.localdomain	[2021-05-18T02:28:18,475929157-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:28:41,300692677-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.18k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:28:41,321430805-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:28:49,396404006-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.18k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:28:49,416601922-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:28:57,392393401-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.18k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:28:57,414930561-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:29:05,247127117-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.18k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:29:05,267581051-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:29:13,429529243-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.18k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:29:13,450517059-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:29:13,467780369-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T02:29:13,474362472-07:00][RUNNING][ROUND 4/6/40] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:29:13,484142385-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T02:29:13,502732671-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40398\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.718227\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 40dad8d9-0a92-456b-8f56-a5aa7fede007\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 40dad8d9-0a92-456b-8f56-a5aa7fede007\nlast_changed 2021-05-18T02:29:42.807175-0700\ncreated 2021-05-18T02:29:42.807175-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40398/0,v1:10.10.1.2:40399/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.718227 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 12936ff2-2b6f-48d9-a253-96670a037a36\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 b9a19ecc-32e7-41da-9578-3d808d485dd4\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 fb7594dc-6841-4673-89ec-598e92920630\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42398\n  w/ user/pass: admin / 746331c4-06fc-47f9-8425-a7d98496ec15\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:29:58 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40398
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.718227
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 40dad8d9-0a92-456b-8f56-a5aa7fede007
setting min_mon_release = octopus
epoch 0
fsid 40dad8d9-0a92-456b-8f56-a5aa7fede007
last_changed 2021-05-18T02:29:42.807175-0700
created 2021-05-18T02:29:42.807175-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40398/0,v1:10.10.1.2:40399/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.718227 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 12936ff2-2b6f-48d9-a253-96670a037a36
0
start osd.0
add osd1 b9a19ecc-32e7-41da-9578-3d808d485dd4
1
start osd.1
add osd2 fb7594dc-6841-4673-89ec-598e92920630
2
start osd.2


restful urls: https://10.10.1.2:42398
  w/ user/pass: admin / 746331c4-06fc-47f9-8425-a7d98496ec15


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T02:29:14.514-0700 7f4106f621c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:29:14.514-0700 7f4106f621c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:29:14.534-0700 7fc1170ea1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:29:14.534-0700 7fc1170ea1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40398,v1:10.10.1.2:40399] --print /tmp/ceph_monmap.718227 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.718227 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.718227 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42398 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.LSfA6w6epf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 12936ff2-2b6f-48d9-a253-96670a037a36 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAQiaNgVkHlGxAA90o1x6LLaPTGFytM/8Apww== --osd-uuid 12936ff2-2b6f-48d9-a253-96670a037a36 
2021-05-18T02:29:52.811-0700 7f81111d4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:29:52.811-0700 7f81111d4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:29:52.811-0700 7f81111d4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:29:52.859-0700 7f81111d4f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b9a19ecc-32e7-41da-9578-3d808d485dd4 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T02:29:53.203-0700 7f835fdcaf00 -1 Falling back to public interface
2021-05-18T02:29:53.215-0700 7f835fdcaf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQARiaNgkSkPDBAAjgF2HJ5Ptx+NdeKS0JquKg== --osd-uuid b9a19ecc-32e7-41da-9578-3d808d485dd4 
2021-05-18T02:29:53.547-0700 7f25b9fcbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:29:53.551-0700 7f25b9fcbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:29:53.551-0700 7f25b9fcbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:29:53.627-0700 7f25b9fcbf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new fb7594dc-6841-4673-89ec-598e92920630 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T02:29:54.023-0700 7f75ff6d4f00 -1 Falling back to public interface
2021-05-18T02:29:54.035-0700 7f75ff6d4f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQASiaNgzvxEARAAtFnK6EmVHhlscsimzHUJ+g== --osd-uuid fb7594dc-6841-4673-89ec-598e92920630 
2021-05-18T02:29:54.351-0700 7fddd3669f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:29:54.351-0700 7fddd3669f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:29:54.351-0700 7fddd3669f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:29:54.415-0700 7fddd3669f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T02:29:54.703-0700 7f79300b8f00 -1 Falling back to public interface
2021-05-18T02:29:54.715-0700 7f79300b8f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T02:29:58,662207528-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:29:58,672230318-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T02:29:58,757559925-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:29:58,764285722-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c48149b5-d1c5-41e1-920e-ceae8cf027b0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c48149b5-d1c5-41e1-920e-ceae8cf027b0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T02:30:01,476956739-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:30:01,483757250-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c48149b5-d1c5-41e1-920e-ceae8cf027b0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c48149b5-d1c5-41e1-920e-ceae8cf027b0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T02:30:04,231496644-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:30:04,237801380-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c48149b5-d1c5-41e1-920e-ceae8cf027b0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c48149b5-d1c5-41e1-920e-ceae8cf027b0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T02:30:06,986989887-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:30:06,993426751-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c48149b5-d1c5-41e1-920e-ceae8cf027b0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c48149b5-d1c5-41e1-920e-ceae8cf027b0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T02:30:12,802546082-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:30:12,808983984-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c48149b5-d1c5-41e1-920e-ceae8cf027b0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c48149b5-d1c5-41e1-920e-ceae8cf027b0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T02:30:16,530242172-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:30:16,536889600-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c48149b5-d1c5-41e1-920e-ceae8cf027b0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c48149b5-d1c5-41e1-920e-ceae8cf027b0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T02:30:20,197086230-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:30:20,203656679-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c48149b5-d1c5-41e1-920e-ceae8cf027b0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c48149b5-d1c5-41e1-920e-ceae8cf027b0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T02:30:23,375124691-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:30:23,381821463-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c48149b5-d1c5-41e1-920e-ceae8cf027b0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c48149b5-d1c5-41e1-920e-ceae8cf027b0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:30:27,072211711-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:30:27,078795017-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c48149b5-d1c5-41e1-920e-ceae8cf027b0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c48149b5-d1c5-41e1-920e-ceae8cf027b0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:30:30,423250613-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:30:30,429391568-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c48149b5-d1c5-41e1-920e-ceae8cf027b0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c48149b5-d1c5-41e1-920e-ceae8cf027b0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T02:30:33,749490147-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:30:33,755932414-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c48149b5-d1c5-41e1-920e-ceae8cf027b0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c48149b5-d1c5-41e1-920e-ceae8cf027b0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T02:30:36,476051848-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:30:36,482629694-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c48149b5-d1c5-41e1-920e-ceae8cf027b0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c48149b5-d1c5-41e1-920e-ceae8cf027b0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T02:30:39,143935148-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:31:02,179267007-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:31:10,247952410-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:31:18,254316478-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 28s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:31:26,108937392-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:31:34,103260695-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 55s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:31:42,040663185-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 73s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:31:49,985794194-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 83s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:31:57,985204020-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   237 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 43s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:32:05,920389757-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:32:05,944884224-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:32:13,883594300-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:32:13,904088084-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:32:21,871683261-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:32:21,892159687-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:32:29,868433274-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:32:29,889522660-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:32:38,021401356-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:32:38,041815115-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:32:38,058994569-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:32:38,068815042-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:32:38,091627460-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2812586
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-18T02:32:38,110393033-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-18T02:32:38,153047251-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:32:38,159670416-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c48149b5-d1c5-41e1-920e-ceae8cf027b0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c48149b5-d1c5-41e1-920e-ceae8cf027b0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:32:39.657+0000 ffff87019010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:32:39.665+0000 ffff87019010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:32:39.665+0000 ffff87019010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:32:39.683725+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-18T09:32:39.683839+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-18T09:32:40.698376+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:32:40.698376+0000     0       0         0         0         0         0           -           0
2021-05-18T09:32:41.698631+0000     1     145       145         0         0         0           -           0
2021-05-18T09:32:42.698880+0000     2     255       290        35   69.9884        70     1.77588     1.76454
2021-05-18T09:32:43.699110+0000     3     255       422       167   222.625       528     1.84448      1.8236
2021-05-18T09:32:44.699327+0000     4     255       558       303   302.941       544     1.91126     1.86717
2021-05-18T09:32:45.699544+0000     5     255       686       431   344.731       512     1.93278      1.8829
2021-05-18T09:32:46.699817+0000     6     255       811       556   370.588       500      2.0309      1.8991
2021-05-18T09:32:47.700093+0000     7     255       944       689   393.627       532     1.97194     1.91809
2021-05-18T09:32:48.700392+0000     8     255      1080       825   412.405       544      1.9071     1.92326
2021-05-18T09:32:49.700653+0000     9     255      1222       967   429.677       568     1.83261     1.91637
2021-05-18T09:32:50.700893+0000    10     255      1358      1103   441.097       544      1.8444     1.90969
2021-05-18T09:32:51.701187+0000    11     255      1499      1244   452.255       564     1.81861     1.90127
2021-05-18T09:32:52.701472+0000    12     255      1641      1386   461.887       568     1.79162     1.89094
2021-05-18T09:32:53.701761+0000    13     255      1777      1522   468.192       544     1.84227     1.88419
2021-05-18T09:32:54.702091+0000    14     255      1912      1657   473.309       540     1.89519     1.88427
2021-05-18T09:32:55.702342+0000    15     255      2052      1797   479.079       560     1.87397     1.88592
2021-05-18T09:32:56.702701+0000    16     255      2191      1936   483.874       556     1.82033     1.88226
2021-05-18T09:32:57.703017+0000    17     255      2322      2067   486.225       524     1.89774     1.88018
2021-05-18T09:32:58.703462+0000    18     255      2452      2197   488.089       520     1.97414     1.88267
2021-05-18T09:32:59.703939+0000    19     255      2596      2341   492.702       576      1.8745     1.88579
2021-05-18T09:33:00.704388+0000 min lat: 0.041217 max lat: 2.0367 avg lat: 1.80898
2021-05-18T09:33:00.704388+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:33:00.704388+0000    20       7      2716      2709   541.642      1472    0.041217     1.80898
2021-05-18T09:33:01.704721+0000 Total time run:         20.0611
Total writes made:      2716
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     541.545
Stddev Bandwidth:       270.329
Max bandwidth (MB/sec): 1472
Min bandwidth (MB/sec): 0
Average IOPS:           135
Stddev IOPS:            67.6278
Max IOPS:               368
Min IOPS:               0
Average Latency(s):     1.8045
Stddev Latency(s):      0.316148
Max latency(s):         2.0367
Min latency(s):         0.041217

[1;32mlocalhost.localdomain	[2021-05-18T02:33:02,190397626-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2812586


[1;33mlocalhost.localdomain	[2021-05-18T02:33:02,201047891-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:33:25,134884553-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.72k objects, 11 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:33:25,155805876-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:33:33,286837285-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.72k objects, 11 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:33:33,307879603-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:33:41,449440537-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.72k objects, 11 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:33:41,470731927-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:33:49,777119086-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.72k objects, 11 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:33:49,798038861-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:33:59,908454922-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.72k objects, 11 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:33:59,929024882-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:33:59,946290521-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:33:59,956093873-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:33:59,979347331-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2815986
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-18T02:33:59,997775476-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-18T02:34:00,038685120-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:34:00,045134298-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c48149b5-d1c5-41e1-920e-ceae8cf027b0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c48149b5-d1c5-41e1-920e-ceae8cf027b0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.evknjC:/tmp/ceph-asok.evknjC -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:34:01.519+0000 ffff9d9ac010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:34:01.531+0000 ffff9d9ac010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:34:01.531+0000 ffff9d9ac010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:34:01.555637+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:34:01.555637+0000     0       0         0         0         0         0           -           0
2021-05-18T09:34:02.555868+0000     1     255       283        28   111.953       112    0.973419    0.927368
2021-05-18T09:34:03.556170+0000     2     255       482       227   453.836       796     1.23697     1.07271
2021-05-18T09:34:04.556425+0000     3     255       678       423   563.816       784     1.30801     1.17539
2021-05-18T09:34:05.558465+0000     4     255       854       599   598.549       704      1.4064     1.22756
2021-05-18T09:34:06.558989+0000     5     255      1034       779   622.759       720     1.42531     1.27513
2021-05-18T09:34:07.559200+0000     6     255      1207       952    634.27       692     1.46598     1.30515
2021-05-18T09:34:08.561317+0000     7     255      1381      1126   642.889       696     1.48963     1.33166
2021-05-18T09:34:09.562737+0000     8     255      1567      1312   655.403       744     1.40358     1.34756
2021-05-18T09:34:10.564243+0000     9     255      1739      1484   658.912       688     1.44219     1.35401
2021-05-18T09:34:11.564474+0000    10     255      1922      1667   666.199       732     1.42936     1.36411
2021-05-18T09:34:12.564791+0000    11     255      2094      1839    668.16       688     1.45985     1.37133
2021-05-18T09:34:13.565100+0000    12     255      2271      2016    671.46       708      1.4524     1.37966
2021-05-18T09:34:14.565341+0000    13     255      2446      2191   673.641       700       1.463     1.38627
2021-05-18T09:34:15.565916+0000    14     255      2621      2366   675.495       700     1.44256     1.39183
2021-05-18T09:34:16.566243+0000 Total time run:       14.9532
Total reads made:     2716
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   726.533
Average IOPS:         181
Stddev IOPS:          41.4729
Max IOPS:             199
Min IOPS:             28
Average Latency(s):   1.35194
Max latency(s):       1.49209
Min latency(s):       0.419429

[1;32mlocalhost.localdomain	[2021-05-18T02:34:17,026897560-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2815986


[1;33mlocalhost.localdomain	[2021-05-18T02:34:17,037405702-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:34:39,833480579-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.72k objects, 11 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:34:39,854155074-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:34:48,153087625-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.72k objects, 11 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:34:48,173868743-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:34:56,149370341-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.72k objects, 11 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:34:56,172184039-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:35:04,203175595-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.72k objects, 11 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:35:04,223949451-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:35:12,138652753-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.72k objects, 11 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:35:12,159170919-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:35:12,176405365-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T02:35:12,183047159-07:00][RUNNING][ROUND 5/6/40] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:35:12,192858055-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T02:35:12,211288867-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40183\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.719438\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 2311a68c-f426-4194-bc71-3e8956a791f2\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 2311a68c-f426-4194-bc71-3e8956a791f2\nlast_changed 2021-05-18T02:35:40.996221-0700\ncreated 2021-05-18T02:35:40.996221-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40183/0,v1:10.10.1.2:40184/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.719438 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 005d686a-6f48-4d67-aed0-34c8d46c60d7\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 175f1608-9609-4921-9b90-e7156be9fedd\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 287b2f81-ae12-4f68-aa37-94fac1101036\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42183\n  w/ user/pass: admin / 156e5c95-c23a-453a-96ee-b03586e80733\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:35:56 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40183
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.719438
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 2311a68c-f426-4194-bc71-3e8956a791f2
setting min_mon_release = octopus
epoch 0
fsid 2311a68c-f426-4194-bc71-3e8956a791f2
last_changed 2021-05-18T02:35:40.996221-0700
created 2021-05-18T02:35:40.996221-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40183/0,v1:10.10.1.2:40184/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.719438 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 005d686a-6f48-4d67-aed0-34c8d46c60d7
0
start osd.0
add osd1 175f1608-9609-4921-9b90-e7156be9fedd
1
start osd.1
add osd2 287b2f81-ae12-4f68-aa37-94fac1101036
2
start osd.2


restful urls: https://10.10.1.2:42183
  w/ user/pass: admin / 156e5c95-c23a-453a-96ee-b03586e80733


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T02:35:13.218-0700 7fd43bca51c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:35:13.218-0700 7fd43bca51c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:35:13.234-0700 7f2ebd72a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:35:13.234-0700 7f2ebd72a1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40183,v1:10.10.1.2:40184] --print /tmp/ceph_monmap.719438 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.719438 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.719438 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42183 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.h0eaLssu7Q 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 005d686a-6f48-4d67-aed0-34c8d46c60d7 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB1iqNgI/rsOhAAOchgjkWlfvY5NCHsjIFi3A== --osd-uuid 005d686a-6f48-4d67-aed0-34c8d46c60d7 
2021-05-18T02:35:50.374-0700 7f9aca16bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:35:50.374-0700 7f9aca16bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:35:50.374-0700 7f9aca16bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:35:50.418-0700 7f9aca16bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 175f1608-9609-4921-9b90-e7156be9fedd -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T02:35:50.698-0700 7fad75325f00 -1 Falling back to public interface
2021-05-18T02:35:50.710-0700 7fad75325f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB2iqNgQO2IKRAAWtThGXTeDUro3buu4JZ6Sg== --osd-uuid 175f1608-9609-4921-9b90-e7156be9fedd 
2021-05-18T02:35:51.034-0700 7f6a1b7d0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:35:51.034-0700 7f6a1b7d0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:35:51.034-0700 7f6a1b7d0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:35:51.090-0700 7f6a1b7d0f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 287b2f81-ae12-4f68-aa37-94fac1101036 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T02:35:51.442-0700 7f01ec89bf00 -1 Falling back to public interface
2021-05-18T02:35:51.454-0700 7f01ec89bf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB3iqNgLsI9GhAAeo1UeEYn1kyLvmY8cr340A== --osd-uuid 287b2f81-ae12-4f68-aa37-94fac1101036 
2021-05-18T02:35:51.778-0700 7f52d1b39f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:35:51.778-0700 7f52d1b39f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:35:51.778-0700 7f52d1b39f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:35:51.834-0700 7f52d1b39f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T02:35:52.154-0700 7f8604a71f00 -1 Falling back to public interface
2021-05-18T02:35:52.170-0700 7f8604a71f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T02:35:56,119879367-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:35:56,129619104-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T02:35:56,214321518-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:35:56,220799048-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '043dae7f-0e1c-42ef-b307-270694320718', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 043dae7f-0e1c-42ef-b307-270694320718 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T02:35:58,991691273-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:35:58,998172735-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '043dae7f-0e1c-42ef-b307-270694320718', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 043dae7f-0e1c-42ef-b307-270694320718 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T02:36:01,929748058-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:36:01,936054934-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '043dae7f-0e1c-42ef-b307-270694320718', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 043dae7f-0e1c-42ef-b307-270694320718 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T02:36:04,744678767-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:36:04,751422983-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '043dae7f-0e1c-42ef-b307-270694320718', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 043dae7f-0e1c-42ef-b307-270694320718 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T02:36:10,394087262-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:36:10,402180706-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '043dae7f-0e1c-42ef-b307-270694320718', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 043dae7f-0e1c-42ef-b307-270694320718 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T02:36:14,196191553-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:36:14,202776082-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '043dae7f-0e1c-42ef-b307-270694320718', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 043dae7f-0e1c-42ef-b307-270694320718 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T02:36:17,502323732-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:36:17,508755685-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '043dae7f-0e1c-42ef-b307-270694320718', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 043dae7f-0e1c-42ef-b307-270694320718 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T02:36:20,824311140-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:36:20,830627752-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '043dae7f-0e1c-42ef-b307-270694320718', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 043dae7f-0e1c-42ef-b307-270694320718 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:36:24,110026056-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:36:24,116504996-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '043dae7f-0e1c-42ef-b307-270694320718', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 043dae7f-0e1c-42ef-b307-270694320718 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:36:27,522282915-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:36:27,528756395-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '043dae7f-0e1c-42ef-b307-270694320718', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 043dae7f-0e1c-42ef-b307-270694320718 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T02:36:30,930857349-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:36:30,937606666-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '043dae7f-0e1c-42ef-b307-270694320718', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 043dae7f-0e1c-42ef-b307-270694320718 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T02:36:33,526203070-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:36:33,532737985-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '043dae7f-0e1c-42ef-b307-270694320718', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 043dae7f-0e1c-42ef-b307-270694320718 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T02:36:36,125272596-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:36:59,147572155-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:37:07,235167480-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:37:15,095458702-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:37:23,213841470-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 48s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:37:31,049726104-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 57s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:37:39,113896937-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 71s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:37:47,214155187-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 81s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:37:55,185330820-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 40s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:38:03,314629074-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:38:03,335768474-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:38:11,327046631-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:38:11,348558080-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:38:19,313055819-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:38:19,334314774-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:38:27,347107178-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:38:27,368353575-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:38:35,681948071-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:38:35,702847518-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:38:35,720080575-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:38:35,729789554-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:38:35,753037985-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2829782
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-18T02:38:35,771986707-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-18T02:38:35,814224457-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:38:35,820743108-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '043dae7f-0e1c-42ef-b307-270694320718', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 043dae7f-0e1c-42ef-b307-270694320718 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:38:37.338+0000 ffff9185a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:38:37.374+0000 ffff9185a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:38:37.374+0000 ffff9185a010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:38:37.394416+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-18T09:38:37.394521+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T09:38:38.359411+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:38:38.359411+0000     0       0         0         0         0         0           -           0
2021-05-18T09:38:39.359664+0000     1     149       149         0         0         0           -           0
2021-05-18T09:38:40.359946+0000     2     255       301        46   91.9816        92     1.69958     1.70635
2021-05-18T09:38:41.360207+0000     3     255       458       203   270.607       628     1.65673     1.68392
2021-05-18T09:38:42.360519+0000     4     255       613       358   357.913       620     1.65147     1.67028
2021-05-18T09:38:43.360800+0000     5     255       760       505   403.899       588      1.7103     1.67532
2021-05-18T09:38:44.361064+0000     6     255       908       653   435.223       592     1.72461      1.6835
2021-05-18T09:38:45.361414+0000     7     255      1053       798   455.878       580     1.75303     1.69539
2021-05-18T09:38:46.361815+0000     8     255      1200       945   472.366       588     1.74793     1.70668
2021-05-18T09:38:47.362628+0000     9     255      1349      1094   486.056       596     1.72064     1.70838
2021-05-18T09:38:48.362855+0000    10     255      1503      1248   499.035       616      1.7018     1.70859
2021-05-18T09:38:49.363108+0000    11     255      1642      1387     504.2       556     1.74879     1.70861
2021-05-18T09:38:50.363401+0000    12     255      1789      1534   511.169       588     1.78631     1.71574
2021-05-18T09:38:51.363811+0000    13     255      1939      1684   517.984       600     1.69854     1.71985
2021-05-18T09:38:52.364071+0000    14     255      2089      1834   523.831       600     1.69719     1.71887
2021-05-18T09:38:53.364341+0000    15     255      2230      1975   526.498       564     1.75731     1.71896
2021-05-18T09:38:54.364613+0000    16     255      2375      2120   529.832       580     1.77985     1.72244
2021-05-18T09:38:55.364973+0000    17     255      2527      2272   534.418       608     1.74404      1.7252
2021-05-18T09:38:56.365495+0000    18     255      2675      2420     537.6       592     1.70187     1.72454
2021-05-18T09:38:57.366122+0000    19     255      2822      2567   540.234       588     1.74677     1.72428
2021-05-18T09:38:58.366509+0000 min lat: 0.062134 max lat: 1.80948 avg lat: 1.65664
2021-05-18T09:38:58.366509+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:38:58.366509+0000    20       7      2968      2961   591.994      1576    0.062134     1.65664
2021-05-18T09:38:59.366840+0000 Total time run:         20.0365
Total writes made:      2968
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     592.518
Stddev Bandwidth:       287.807
Max bandwidth (MB/sec): 1576
Min bandwidth (MB/sec): 0
Average IOPS:           148
Stddev IOPS:            71.9517
Max IOPS:               394
Min IOPS:               0
Average Latency(s):     1.65286
Stddev Latency(s):      0.283154
Max latency(s):         1.80948
Min latency(s):         0.0403712

[1;32mlocalhost.localdomain	[2021-05-18T02:38:59,844480626-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2829782


[1;33mlocalhost.localdomain	[2021-05-18T02:38:59,855185115-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:39:22,888661332-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.97k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:39:22,909968803-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:39:30,850727835-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.97k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:39:30,871627236-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:39:38,900611082-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.97k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:39:38,922151498-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:39:46,797805632-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.97k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:39:46,819267778-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:39:54,872496871-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.97k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:39:54,893511445-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:39:54,910833840-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:39:54,920676235-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:39:54,943969746-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2833114
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-18T02:39:54,963044607-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-18T02:39:55,004339455-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:39:55,010730537-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '043dae7f-0e1c-42ef-b307-270694320718', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 043dae7f-0e1c-42ef-b307-270694320718 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.0hrTFw:/tmp/ceph-asok.0hrTFw -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:39:56.456+0000 ffff8036e010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:39:56.464+0000 ffff8036e010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:39:56.464+0000 ffff8036e010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:39:56.485471+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:39:56.485471+0000     0       0         0         0         0         0           -           0
2021-05-18T09:39:57.485771+0000     1     255       279        24   95.9564        96    0.995848     0.94658
2021-05-18T09:39:58.486077+0000     2     255       473       218   435.834       776     1.25597     1.10788
2021-05-18T09:39:59.486346+0000     3     255       666       411   547.812       772     1.32604     1.20584
2021-05-18T09:40:00.486795+0000     4     255       859       604   603.777       772     1.31512     1.24275
2021-05-18T09:40:01.487111+0000     5     255      1033       778   622.177       696     1.41515     1.27062
2021-05-18T09:40:02.487433+0000     6     255      1216       961   640.441       732     1.42588      1.3029
2021-05-18T09:40:03.487706+0000     7     255      1398      1143    652.92       728     1.38981     1.31928
2021-05-18T09:40:04.489005+0000     8     255      1580      1325   662.195       728     1.40336     1.32924
2021-05-18T09:40:05.489251+0000     9     255      1766      1511   671.262       744     1.40187     1.33814
2021-05-18T09:40:06.489521+0000    10     255      1954      1699   679.314       752     1.35969     1.34199
2021-05-18T09:40:07.489865+0000    11     255      2128      1873   680.809       696     1.42618     1.34676
2021-05-18T09:40:08.490373+0000    12     255      2308      2053   684.045       720     1.43498     1.35581
2021-05-18T09:40:09.490729+0000    13     255      2484      2229   685.561       704     1.43643     1.36235
2021-05-18T09:40:10.490933+0000    14     255      2673      2418    690.58       756     1.38002     1.36664
2021-05-18T09:40:11.493036+0000    15     255      2859      2604   694.043       744     1.36229     1.36578
2021-05-18T09:40:12.493511+0000    16       7      2968      2961   739.871      1428    0.438621     1.33128
2021-05-18T09:40:13.493776+0000 Total time run:       16.0181
Total reads made:     2968
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   741.159
Average IOPS:         185
Stddev IOPS:          61.1615
Max IOPS:             357
Min IOPS:             24
Average Latency(s):   1.32914
Max latency(s):       1.46762
Min latency(s):       0.410739

[1;32mlocalhost.localdomain	[2021-05-18T02:40:13,986423769-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2833114


[1;33mlocalhost.localdomain	[2021-05-18T02:40:13,997089686-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:40:37,085203982-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.97k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:40:37,106530038-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:40:45,084679607-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.97k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:40:45,105968367-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:40:53,065713780-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.97k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:40:53,087321104-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:41:01,267861006-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.97k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:41:01,289262885-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:41:09,377724749-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.97k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:41:09,399165199-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:41:09,416490937-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T02:41:09,426729235-07:00][RUNNING][ROUND 1/7/40] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:41:09,436756056-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T02:41:09,455251245-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40714\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.720645\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f349460d-48d0-4d3b-900e-54bd1961a48a\nsetting min_mon_release = octopus\nepoch 0\nfsid f349460d-48d0-4d3b-900e-54bd1961a48a\nlast_changed 2021-05-18T02:41:38.486577-0700\ncreated 2021-05-18T02:41:38.486577-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40714/0,v1:10.10.1.2:40715/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.720645 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 7f9ca433-c1be-49ed-bc8f-490646c77968\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 ff5db962-c42c-49d4-b5a7-1db5bed0d179\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 0198d015-07d7-4f0f-b56a-83f6506ebabb\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42714\n'
10.10.1.2: b'  w/ user/pass: admin / fa91b095-d9db-4cef-9eb1-a8bfd7db5b3b\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:41:53 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40714
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.720645
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f349460d-48d0-4d3b-900e-54bd1961a48a
setting min_mon_release = octopus
epoch 0
fsid f349460d-48d0-4d3b-900e-54bd1961a48a
last_changed 2021-05-18T02:41:38.486577-0700
created 2021-05-18T02:41:38.486577-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40714/0,v1:10.10.1.2:40715/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.720645 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 7f9ca433-c1be-49ed-bc8f-490646c77968
0
start osd.0
add osd1 ff5db962-c42c-49d4-b5a7-1db5bed0d179
1
start osd.1
add osd2 0198d015-07d7-4f0f-b56a-83f6506ebabb
2
start osd.2


restful urls: https://10.10.1.2:42714
  w/ user/pass: admin / fa91b095-d9db-4cef-9eb1-a8bfd7db5b3b


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T02:41:10.441-0700 7f27259621c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:41:10.441-0700 7f27259621c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:41:10.457-0700 7f9f698711c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:41:10.457-0700 7f9f698711c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40714,v1:10.10.1.2:40715] --print /tmp/ceph_monmap.720645 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.720645 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.720645 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42714 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.pAhWFASv8i 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7f9ca433-c1be-49ed-bc8f-490646c77968 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDbi6NgdNlEERAABggDTzc0yDE8rlQlMvL0zg== --osd-uuid 7f9ca433-c1be-49ed-bc8f-490646c77968 
2021-05-18T02:41:47.645-0700 7f9931e00f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:41:47.649-0700 7f9931e00f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:41:47.649-0700 7f9931e00f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:41:47.717-0700 7f9931e00f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ff5db962-c42c-49d4-b5a7-1db5bed0d179 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T02:41:48.005-0700 7f39e8fe3f00 -1 Falling back to public interface
2021-05-18T02:41:48.017-0700 7f39e8fe3f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDci6NgCRxVABAAURk+OntnjPacfifjws/WSg== --osd-uuid ff5db962-c42c-49d4-b5a7-1db5bed0d179 
2021-05-18T02:41:48.341-0700 7fbf03fc5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:41:48.341-0700 7fbf03fc5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:41:48.341-0700 7fbf03fc5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:41:48.413-0700 7fbf03fc5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0198d015-07d7-4f0f-b56a-83f6506ebabb -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T02:41:48.689-0700 7f8e54e42f00 -1 Falling back to public interface
2021-05-18T02:41:48.701-0700 7f8e54e42f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDci6Ngb5wvKRAAORgN+QpQ3Vi/XbD2Jz5KXw== --osd-uuid 0198d015-07d7-4f0f-b56a-83f6506ebabb 
2021-05-18T02:41:49.009-0700 7ff83f06af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:41:49.013-0700 7ff83f06af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:41:49.013-0700 7ff83f06af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:41:49.065-0700 7ff83f06af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T02:41:49.469-0700 7fa7c134af00 -1 Falling back to public interface
2021-05-18T02:41:49.481-0700 7fa7c134af00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T02:41:53,332149241-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:41:53,342444368-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T02:41:53,428199757-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:41:53,434923775-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd922d818-764d-4f12-9102-c9890078ed31', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d922d818-764d-4f12-9102-c9890078ed31 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T02:41:56,100326763-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:41:56,109163504-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd922d818-764d-4f12-9102-c9890078ed31', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d922d818-764d-4f12-9102-c9890078ed31 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T02:41:58,909635721-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:41:58,916402542-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd922d818-764d-4f12-9102-c9890078ed31', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d922d818-764d-4f12-9102-c9890078ed31 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T02:42:01,953624399-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:42:01,960012420-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd922d818-764d-4f12-9102-c9890078ed31', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d922d818-764d-4f12-9102-c9890078ed31 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T02:42:07,577703803-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:42:07,584626726-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd922d818-764d-4f12-9102-c9890078ed31', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d922d818-764d-4f12-9102-c9890078ed31 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T02:42:11,367555863-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:42:11,374062479-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd922d818-764d-4f12-9102-c9890078ed31', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d922d818-764d-4f12-9102-c9890078ed31 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T02:42:14,703221158-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:42:14,709631803-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd922d818-764d-4f12-9102-c9890078ed31', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d922d818-764d-4f12-9102-c9890078ed31 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T02:42:17,943915212-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:42:17,950545899-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd922d818-764d-4f12-9102-c9890078ed31', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d922d818-764d-4f12-9102-c9890078ed31 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:42:21,280150503-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:42:21,286390598-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd922d818-764d-4f12-9102-c9890078ed31', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d922d818-764d-4f12-9102-c9890078ed31 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:42:24,793631226-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:42:24,799857699-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd922d818-764d-4f12-9102-c9890078ed31', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d922d818-764d-4f12-9102-c9890078ed31 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T02:42:27,964261227-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:42:27,970895170-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd922d818-764d-4f12-9102-c9890078ed31', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d922d818-764d-4f12-9102-c9890078ed31 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T02:42:30,624503502-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:42:30,630940611-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd922d818-764d-4f12-9102-c9890078ed31', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d922d818-764d-4f12-9102-c9890078ed31 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T02:42:33,247157661-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:42:58,467513452-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:43:06,847824277-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (5s)
      [=======.....................] (remaining: 13s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:43:14,864832664-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 27s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:43:22,815141854-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 51s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:43:30,818873412-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (30s)
      [========....................] (remaining: 72s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:43:38,730357612-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 80s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:43:46,690309493-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (45s)
      [========....................] (remaining: 99s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:43:54,896028419-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   238 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [========....................] (remaining: 110s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:44:02,879948999-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:44:02,901280196-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:44:10,976086039-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:44:10,997501460-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:44:19,062608393-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:44:19,084040586-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:44:27,304931145-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:44:27,326285581-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:44:35,348801034-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:44:35,370008864-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:44:35,387906857-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:44:35,398023541-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:44:35,421430088-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2846903
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-18T02:44:35,440247393-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-18T02:44:35,482702390-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:44:35,489329716-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd922d818-764d-4f12-9102-c9890078ed31', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d922d818-764d-4f12-9102-c9890078ed31 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:44:37.208+0000 ffffba9fd010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:44:37.216+0000 ffffba9fd010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:44:37.216+0000 ffffba9fd010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:44:37.244159+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-18T09:44:37.244271+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T09:44:41.125354+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:44:41.125354+0000     0       0         0         0         0         0           -           0
2021-05-18T09:44:42.125577+0000     1      21        21         0         0         0           -           0
2021-05-18T09:44:43.125986+0000     2      42        42         0         0         0           -           0
2021-05-18T09:44:44.126332+0000     3      63        63         0         0         0           -           0
2021-05-18T09:44:45.126841+0000     4      84        84         0         0         0           -           0
2021-05-18T09:44:46.127188+0000     5     106       106         0         0         0           -           0
2021-05-18T09:44:47.127488+0000     6     127       127         0         0         0           -           0
2021-05-18T09:44:48.127711+0000     7     148       148         0         0         0           -           0
2021-05-18T09:44:49.127958+0000     8     169       169         0         0         0           -           0
2021-05-18T09:44:50.128166+0000     9     190       190         0         0         0           -           0
2021-05-18T09:44:51.128420+0000    10     212       212         0         0         0           -           0
2021-05-18T09:44:52.128716+0000    11     233       233         0         0         0           -           0
2021-05-18T09:44:53.128925+0000    12     255       255         0         0         0           -           0
2021-05-18T09:44:54.129268+0000    13     255       276        21   25.8387   25.8462     12.0278     12.0422
2021-05-18T09:44:55.129602+0000    14     255       298        43   49.1285       352     11.9927      12.032
2021-05-18T09:44:56.129804+0000    15     255       320        65   69.3134       352     11.9714     12.0184
2021-05-18T09:44:57.130066+0000    16     255       342        87   86.9752       352     11.9358     12.0033
2021-05-18T09:44:58.130577+0000    17     255       364       109   102.558       352     11.9098     11.9878
2021-05-18T09:44:59.130859+0000    18     255       385       130   115.521       336     11.9122     11.9746
2021-05-18T09:45:00.131073+0000    19     255       406       151   127.121       336     11.8862      11.961
2021-05-18T09:45:01.131312+0000 min lat: 11.8538 max lat: 12.1142 avg lat: 11.9524
2021-05-18T09:45:01.131312+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:45:01.131312+0000    20     255       428       173    138.36       352     11.8893     11.9524
2021-05-18T09:45:02.131664+0000 Total time run:         20.1271
Total writes made:      429
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     341.033
Stddev Bandwidth:       169.198
Max bandwidth (MB/sec): 352
Min bandwidth (MB/sec): 0
Average IOPS:           21
Stddev IOPS:            10.5943
Max IOPS:               22
Min IOPS:               0
Average Latency(s):     8.38136
Stddev Latency(s):      3.95194
Max latency(s):         12.1142
Min latency(s):         0.128083

[1;32mlocalhost.localdomain	[2021-05-18T02:45:03,045888119-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2846903


[1;33mlocalhost.localdomain	[2021-05-18T02:45:03,056702618-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:45:25,957138239-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:45:25,978648325-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:45:33,979596390-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:45:34,003134989-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:45:41,949879434-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:45:41,971456101-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:45:50,065496992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:45:50,086880036-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:45:58,050759564-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:45:58,072593855-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:45:58,090446554-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:45:58,100592702-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:45:58,124099720-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2850397
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-18T02:45:58,143486056-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-18T02:45:58,185058704-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:45:58,191683539-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd922d818-764d-4f12-9102-c9890078ed31', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d922d818-764d-4f12-9102-c9890078ed31 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.PAjzrL:/tmp/ceph-asok.PAjzrL -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:45:59.674+0000 ffffa993b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:45:59.682+0000 ffffa993b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:45:59.682+0000 ffffa993b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:45:59.717366+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:45:59.717366+0000     0       0         0         0         0         0           -           0
2021-05-18T09:46:00.717611+0000     1      62        62         0         0         0           -           0
2021-05-18T09:46:01.717848+0000     2     117       117         0         0         0           -           0
2021-05-18T09:46:02.718058+0000     3     176       176         0         0         0           -           0
2021-05-18T09:46:03.718268+0000     4     229       229         0         0         0           -           0
2021-05-18T09:46:04.718493+0000     5     255       276        21   67.1824      67.2     4.71803     4.63823
2021-05-18T09:46:05.718761+0000     6     255       317        62    165.29       656     5.03037     4.82338
2021-05-18T09:46:06.724884+0000     7     255       362       107   244.303       720     5.17594     4.93832
2021-05-18T09:46:07.725115+0000     8     255       403       148   295.707       656     5.46331     5.05078
2021-05-18T09:46:08.915243+0000     9     152       429       277   481.841      2064     4.21883     5.06717
2021-05-18T09:46:09.915508+0000 Total time run:       10.0618
Total reads made:     429
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   682.185
Average IOPS:         42
Stddev IOPS:          42.6217
Max IOPS:             129
Min IOPS:             0
Average Latency(s):   4.26984
Max latency(s):       5.61586
Min latency(s):       1.49689

[1;32mlocalhost.localdomain	[2021-05-18T02:46:10,885105270-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2850397


[1;33mlocalhost.localdomain	[2021-05-18T02:46:10,895769652-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:46:33,868699108-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:46:33,890181918-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:46:41,981027587-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:46:42,002041507-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:46:50,015250090-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:46:50,036344468-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:46:57,803535660-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:46:57,825037373-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:47:05,772739546-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:47:05,794184835-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:47:05,811930354-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T02:47:05,818846149-07:00][RUNNING][ROUND 2/7/40] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:47:05,828783506-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T02:47:05,847510149-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40955\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.721853\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 7288d1b7-cdf1-4917-a9fb-bbb1b2a6c340\nsetting min_mon_release = octopus\nepoch 0\nfsid 7288d1b7-cdf1-4917-a9fb-bbb1b2a6c340\nlast_changed 2021-05-18T02:47:33.591767-0700\ncreated 2021-05-18T02:47:33.591767-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40955/0,v1:10.10.1.2:40956/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.721853 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 f3fad63f-5ac5-4daa-b5a8-db044921002b\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 5a9e0688-bf42-4c8b-b391-2cdc9bf64e28\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 d00e9dfc-f897-4880-acbb-761df1f64a70\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42955\n  w/ user/pass: admin / f1311014-28d7-47a6-9aad-0aae3b830b4c\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:47:48 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40955
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.721853
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 7288d1b7-cdf1-4917-a9fb-bbb1b2a6c340
setting min_mon_release = octopus
epoch 0
fsid 7288d1b7-cdf1-4917-a9fb-bbb1b2a6c340
last_changed 2021-05-18T02:47:33.591767-0700
created 2021-05-18T02:47:33.591767-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40955/0,v1:10.10.1.2:40956/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.721853 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 f3fad63f-5ac5-4daa-b5a8-db044921002b
0
start osd.0
add osd1 5a9e0688-bf42-4c8b-b391-2cdc9bf64e28
1
start osd.1
add osd2 d00e9dfc-f897-4880-acbb-761df1f64a70
2
start osd.2


restful urls: https://10.10.1.2:42955
  w/ user/pass: admin / f1311014-28d7-47a6-9aad-0aae3b830b4c


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T02:47:06.844-0700 7f05cf5c21c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:47:06.844-0700 7f05cf5c21c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:47:06.860-0700 7f68fc5be1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:47:06.860-0700 7f68fc5be1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40955,v1:10.10.1.2:40956] --print /tmp/ceph_monmap.721853 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.721853 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.721853 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42955 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.CBvJIDEfBw 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f3fad63f-5ac5-4daa-b5a8-db044921002b -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA+jaNgvasMGhAAgK6bwxn5iDDRzrGUrPnuVg== --osd-uuid f3fad63f-5ac5-4daa-b5a8-db044921002b 
2021-05-18T02:47:42.793-0700 7f3cf48a5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:47:42.797-0700 7f3cf48a5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:47:42.797-0700 7f3cf48a5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:47:42.873-0700 7f3cf48a5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5a9e0688-bf42-4c8b-b391-2cdc9bf64e28 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T02:47:43.209-0700 7f850984bf00 -1 Falling back to public interface
2021-05-18T02:47:43.221-0700 7f850984bf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA/jaNg0sKcDBAAQ3sKTKGLIJoVcIUAicMoMA== --osd-uuid 5a9e0688-bf42-4c8b-b391-2cdc9bf64e28 
2021-05-18T02:47:43.549-0700 7fb1d67cbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:47:43.549-0700 7fb1d67cbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:47:43.549-0700 7fb1d67cbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:47:43.593-0700 7fb1d67cbf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d00e9dfc-f897-4880-acbb-761df1f64a70 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T02:47:43.893-0700 7f6f58b8ef00 -1 Falling back to public interface
2021-05-18T02:47:43.905-0700 7f6f58b8ef00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA/jaNgfSMENRAAgSRNmTGN4rSZqv2Ohlpl4w== --osd-uuid d00e9dfc-f897-4880-acbb-761df1f64a70 
2021-05-18T02:47:44.217-0700 7fcda6e43f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:47:44.221-0700 7fcda6e43f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:47:44.221-0700 7fcda6e43f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:47:44.281-0700 7fcda6e43f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T02:47:44.609-0700 7f60a69c6f00 -1 Falling back to public interface
2021-05-18T02:47:44.625-0700 7f60a69c6f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T02:47:48,542545058-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:47:48,555572119-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T02:47:48,651492846-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:47:48,659091632-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5e4db634-3a12-4df3-9f34-5de3b541e075', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5e4db634-3a12-4df3-9f34-5de3b541e075 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T02:47:51,411475266-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:47:51,418043076-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5e4db634-3a12-4df3-9f34-5de3b541e075', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5e4db634-3a12-4df3-9f34-5de3b541e075 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T02:47:54,692210602-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:47:54,698719737-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5e4db634-3a12-4df3-9f34-5de3b541e075', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5e4db634-3a12-4df3-9f34-5de3b541e075 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T02:47:57,372577842-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:47:57,379120748-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5e4db634-3a12-4df3-9f34-5de3b541e075', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5e4db634-3a12-4df3-9f34-5de3b541e075 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T02:48:03,006644897-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:48:03,013197930-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5e4db634-3a12-4df3-9f34-5de3b541e075', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5e4db634-3a12-4df3-9f34-5de3b541e075 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T02:48:06,102090425-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:48:06,108770784-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5e4db634-3a12-4df3-9f34-5de3b541e075', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5e4db634-3a12-4df3-9f34-5de3b541e075 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T02:48:09,336533879-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:48:09,343344775-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5e4db634-3a12-4df3-9f34-5de3b541e075', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5e4db634-3a12-4df3-9f34-5de3b541e075 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T02:48:12,800499561-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:48:12,807045707-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5e4db634-3a12-4df3-9f34-5de3b541e075', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5e4db634-3a12-4df3-9f34-5de3b541e075 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:48:16,211846005-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:48:16,218440294-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5e4db634-3a12-4df3-9f34-5de3b541e075', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5e4db634-3a12-4df3-9f34-5de3b541e075 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:48:19,463708667-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:48:19,471006591-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5e4db634-3a12-4df3-9f34-5de3b541e075', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5e4db634-3a12-4df3-9f34-5de3b541e075 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T02:48:23,190697687-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:48:23,197348836-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5e4db634-3a12-4df3-9f34-5de3b541e075', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5e4db634-3a12-4df3-9f34-5de3b541e075 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T02:48:25,839015283-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:48:25,845626075-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5e4db634-3a12-4df3-9f34-5de3b541e075', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5e4db634-3a12-4df3-9f34-5de3b541e075 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T02:48:28,458109401-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:48:51,448834951-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   198 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:48:59,603826651-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:49:07,528320246-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:49:15,831273731-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:49:23,851086586-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 60s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:49:31,679626802-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 82s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:49:39,679223945-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 94s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:49:47,819206493-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [=========...................] (remaining: 102s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:49:55,973516626-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:49:55,994997948-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:50:03,848104427-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:50:03,872350273-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:50:11,813051617-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:50:11,834055647-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:50:19,786478992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:50:19,808018689-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:50:27,842580344-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:50:27,864002151-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:50:27,881279093-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:50:27,891343746-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:50:27,914926433-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2864003
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-18T02:50:27,934063033-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-18T02:50:27,977592009-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:50:27,984205186-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5e4db634-3a12-4df3-9f34-5de3b541e075', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5e4db634-3a12-4df3-9f34-5de3b541e075 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:50:29.508+0000 ffffa4606010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:50:29.516+0000 ffffa4606010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:50:29.516+0000 ffffa4606010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:50:29.545021+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-18T09:50:29.545134+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-18T09:50:33.491020+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:50:33.491020+0000     0       0         0         0         0         0           -           0
2021-05-18T09:50:34.491237+0000     1      22        22         0         0         0           -           0
2021-05-18T09:50:35.491444+0000     2      43        43         0         0         0           -           0
2021-05-18T09:50:36.491675+0000     3      64        64         0         0         0           -           0
2021-05-18T09:50:37.491870+0000     4      87        87         0         0         0           -           0
2021-05-18T09:50:38.492099+0000     5     107       107         0         0         0           -           0
2021-05-18T09:50:39.492325+0000     6     129       129         0         0         0           -           0
2021-05-18T09:50:40.492532+0000     7     150       150         0         0         0           -           0
2021-05-18T09:50:41.492758+0000     8     171       171         0         0         0           -           0
2021-05-18T09:50:42.492984+0000     9     193       193         0         0         0           -           0
2021-05-18T09:50:43.493462+0000    10     214       214         0         0         0           -           0
2021-05-18T09:50:44.493705+0000    11     235       235         0         0         0           -           0
2021-05-18T09:50:45.494031+0000    12     255       256         1   1.33302   1.33333     11.9823     11.9823
2021-05-18T09:50:46.494435+0000    13     255       278        23   28.3006       352     12.0236     12.0101
2021-05-18T09:50:47.494777+0000    14     255       299        44   50.2728       336     12.0015     12.0062
2021-05-18T09:50:48.495052+0000    15     255       320        65   69.3154       336     12.0248     12.0044
2021-05-18T09:50:49.495300+0000    16     255       341        86   85.9778       336     12.0696     12.0151
2021-05-18T09:50:50.495581+0000    17     255       363       108   101.621       352     12.0247     12.0226
2021-05-18T09:50:51.495824+0000    18     255       383       128   113.748       320     12.0802     12.0286
2021-05-18T09:50:52.496039+0000    19     255       405       150   126.283       352     12.0629     12.0348
2021-05-18T09:50:53.496268+0000 min lat: 0.14819 max lat: 12.0953 avg lat: 8.49704
2021-05-18T09:50:53.496268+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:50:53.496268+0000    20       2       426       424   339.114      4384     0.14819     8.49704
2021-05-18T09:50:54.496586+0000 Total time run:         20.0835
Total writes made:      426
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     339.382
Stddev Bandwidth:       966.302
Max bandwidth (MB/sec): 4384
Min bandwidth (MB/sec): 0
Average IOPS:           21
Stddev IOPS:            60.3954
Max IOPS:               274
Min IOPS:               0
Average Latency(s):     8.45783
Stddev Latency(s):      3.98053
Max latency(s):         12.0953
Min latency(s):         0.136444

[1;32mlocalhost.localdomain	[2021-05-18T02:50:55,373804843-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2864003


[1;33mlocalhost.localdomain	[2021-05-18T02:50:55,384942449-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:51:18,255512140-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 427 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:51:18,278893987-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:51:26,224680652-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 427 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:51:26,246444891-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:51:34,397671905-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 427 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:51:34,418991364-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:51:42,392121551-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 427 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:51:42,413822975-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:51:50,278080164-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 427 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:51:50,299272836-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:51:50,317086450-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:51:50,327285615-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:51:50,351052967-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2867455
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-18T02:51:50,369830931-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-18T02:51:50,411417778-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:51:50,417499714-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5e4db634-3a12-4df3-9f34-5de3b541e075', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5e4db634-3a12-4df3-9f34-5de3b541e075 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gNcAez:/tmp/ceph-asok.gNcAez -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:51:51.923+0000 ffffad5c3010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:51:51.931+0000 ffffad5c3010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:51:51.931+0000 ffffad5c3010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:51:51.963259+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:51:51.963259+0000     0       0         0         0         0         0           -           0
2021-05-18T09:51:52.963471+0000     1      68        68         0         0         0           -           0
2021-05-18T09:51:53.963720+0000     2     124       124         0         0         0           -           0
2021-05-18T09:51:54.963953+0000     3     189       189         0         0         0           -           0
2021-05-18T09:51:55.964191+0000     4     251       251         0         0         0           -           0
2021-05-18T09:51:56.964474+0000     5     255       296        41   131.164     131.2     4.44293     4.27588
2021-05-18T09:51:57.978756+0000     6     256       342        86   228.736       720     4.66058     4.42665
2021-05-18T09:51:58.979007+0000     7     255       384       129   294.189       688     4.90114     4.54095
2021-05-18T09:52:00.107708+0000     8     236       426       190   373.253       976     5.16523      4.7302
2021-05-18T09:52:01.122927+0000     9      59       426       367    641.06      2832     2.61054     4.38453
2021-05-18T09:52:02.123173+0000 Total time run:       9.49323
Total reads made:     426
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   717.986
Average IOPS:         44
Stddev IOPS:          57.6551
Max IOPS:             177
Min IOPS:             0
Average Latency(s):   4.06112
Max latency(s):       5.2931
Min latency(s):       1.51685

[1;32mlocalhost.localdomain	[2021-05-18T02:52:03,216969171-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2867455


[1;33mlocalhost.localdomain	[2021-05-18T02:52:03,227901857-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:52:26,282620972-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 427 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:52:26,307871583-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:52:34,498704615-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 427 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:52:34,520119430-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:52:42,474803328-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 427 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:52:42,496405945-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:52:50,542411603-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 427 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:52:50,563971567-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:52:58,693602396-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 427 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:52:58,715001022-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:52:58,732932841-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T02:52:58,739899988-07:00][RUNNING][ROUND 3/7/40] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:52:58,749965735-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T02:52:58,768835559-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40966\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.723054\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8147a7ae-18e1-4c6f-a8f6-e60ea729f917\nsetting min_mon_release = octopus\nepoch 0\nfsid 8147a7ae-18e1-4c6f-a8f6-e60ea729f917\nlast_changed 2021-05-18T02:53:26.199751-0700\ncreated 2021-05-18T02:53:26.199751-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40966/0,v1:10.10.1.2:40967/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.723054 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 0ef6362b-229f-4a75-b67b-c910f72362cd\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 f287c848-ad0a-4d75-a711-125fd6e85247\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 d1208c2d-846c-4773-9774-906a7f8d168d\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42966\n  w/ user/pass: admin / 29b7adb4-8e1b-417f-8092-84d0f7d3d115\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:53:40 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40966
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.723054
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8147a7ae-18e1-4c6f-a8f6-e60ea729f917
setting min_mon_release = octopus
epoch 0
fsid 8147a7ae-18e1-4c6f-a8f6-e60ea729f917
last_changed 2021-05-18T02:53:26.199751-0700
created 2021-05-18T02:53:26.199751-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40966/0,v1:10.10.1.2:40967/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.723054 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 0ef6362b-229f-4a75-b67b-c910f72362cd
0
start osd.0
add osd1 f287c848-ad0a-4d75-a711-125fd6e85247
1
start osd.1
add osd2 d1208c2d-846c-4773-9774-906a7f8d168d
2
start osd.2


restful urls: https://10.10.1.2:42966
  w/ user/pass: admin / 29b7adb4-8e1b-417f-8092-84d0f7d3d115


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T02:52:59.768-0700 7facf19561c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:52:59.768-0700 7facf19561c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:52:59.784-0700 7effefcb11c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:52:59.784-0700 7effefcb11c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40966,v1:10.10.1.2:40967] --print /tmp/ceph_monmap.723054 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.723054 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.723054 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42966 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.ETHyp7P7I3 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0ef6362b-229f-4a75-b67b-c910f72362cd -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCfjqNgALM0ABAA6dM0iGFWks6FxBHe2Mai4Q== --osd-uuid 0ef6362b-229f-4a75-b67b-c910f72362cd 
2021-05-18T02:53:35.328-0700 7fb4e6da8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:53:35.328-0700 7fb4e6da8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:53:35.328-0700 7fb4e6da8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:53:35.392-0700 7fb4e6da8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f287c848-ad0a-4d75-a711-125fd6e85247 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T02:53:35.668-0700 7f9f44abdf00 -1 Falling back to public interface
2021-05-18T02:53:35.680-0700 7f9f44abdf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCfjqNg6qoDKBAAKXC3kR/ONuLJp823Eraleg== --osd-uuid f287c848-ad0a-4d75-a711-125fd6e85247 
2021-05-18T02:53:36.000-0700 7f0ff7a88f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:53:36.000-0700 7f0ff7a88f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:53:36.000-0700 7f0ff7a88f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:53:36.044-0700 7f0ff7a88f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d1208c2d-846c-4773-9774-906a7f8d168d -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T02:53:36.320-0700 7fb04373cf00 -1 Falling back to public interface
2021-05-18T02:53:36.336-0700 7fb04373cf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCgjqNgvtNPExAA1CPknvC514VfduufunwLCw== --osd-uuid d1208c2d-846c-4773-9774-906a7f8d168d 
2021-05-18T02:53:36.644-0700 7f88e8627f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:53:36.644-0700 7f88e8627f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:53:36.644-0700 7f88e8627f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:53:36.708-0700 7f88e8627f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T02:53:37.084-0700 7fd95c169f00 -1 Falling back to public interface
2021-05-18T02:53:37.096-0700 7fd95c169f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T02:53:40,993806364-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:53:41,004487246-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T02:53:41,088490429-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:53:41,095063984-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43d1a474-eab9-4d1d-8601-00da6eeaf55a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43d1a474-eab9-4d1d-8601-00da6eeaf55a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T02:53:43,895167596-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:53:43,901616210-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43d1a474-eab9-4d1d-8601-00da6eeaf55a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43d1a474-eab9-4d1d-8601-00da6eeaf55a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T02:53:46,621340807-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:53:46,627671770-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43d1a474-eab9-4d1d-8601-00da6eeaf55a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43d1a474-eab9-4d1d-8601-00da6eeaf55a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T02:53:51,548797997-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:53:51,555162731-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43d1a474-eab9-4d1d-8601-00da6eeaf55a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43d1a474-eab9-4d1d-8601-00da6eeaf55a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T02:53:57,100001222-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:53:57,107486783-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43d1a474-eab9-4d1d-8601-00da6eeaf55a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43d1a474-eab9-4d1d-8601-00da6eeaf55a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T02:54:00,996340217-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:54:01,002961254-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43d1a474-eab9-4d1d-8601-00da6eeaf55a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43d1a474-eab9-4d1d-8601-00da6eeaf55a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T02:54:04,354784108-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:54:04,361367250-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43d1a474-eab9-4d1d-8601-00da6eeaf55a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43d1a474-eab9-4d1d-8601-00da6eeaf55a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T02:54:07,665098187-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:54:07,671717244-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43d1a474-eab9-4d1d-8601-00da6eeaf55a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43d1a474-eab9-4d1d-8601-00da6eeaf55a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:54:10,987741642-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:54:10,994136771-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43d1a474-eab9-4d1d-8601-00da6eeaf55a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43d1a474-eab9-4d1d-8601-00da6eeaf55a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:54:14,778250173-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:54:14,784544440-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43d1a474-eab9-4d1d-8601-00da6eeaf55a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43d1a474-eab9-4d1d-8601-00da6eeaf55a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T02:54:17,986939960-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:54:17,993261322-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43d1a474-eab9-4d1d-8601-00da6eeaf55a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43d1a474-eab9-4d1d-8601-00da6eeaf55a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T02:54:20,822763408-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:54:20,829218502-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43d1a474-eab9-4d1d-8601-00da6eeaf55a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43d1a474-eab9-4d1d-8601-00da6eeaf55a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T02:54:23,466919641-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:54:46,417933745-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   206 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=============...............] (remaining: 5s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:54:54,404829279-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:55:02,684024239-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:55:10,725541576-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:55:18,559505081-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:55:26,652151926-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:55:34,809739673-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:55:42,946299507-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:55:42,967672268-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:55:50,959984105-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:55:50,981252433-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:55:58,972637726-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:55:58,994596037-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:56:07,154317114-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:56:07,176106481-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:56:15,432505760-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:56:15,453955349-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:56:15,471923937-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:56:15,482182452-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:56:15,506325782-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2880797
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-18T02:56:15,526100004-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-18T02:56:15,569087488-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:56:15,575659587-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43d1a474-eab9-4d1d-8601-00da6eeaf55a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43d1a474-eab9-4d1d-8601-00da6eeaf55a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6 -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:56:17.096+0000 ffffa1a38010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:56:17.100+0000 ffffa1a38010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:56:17.104+0000 ffffa1a38010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:56:17.132167+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-18T09:56:17.132284+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-18T09:56:21.266613+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:56:21.266613+0000     0       0         0         0         0         0           -           0
2021-05-18T09:56:22.266870+0000     1      22        22         0         0         0           -           0
2021-05-18T09:56:23.267111+0000     2      44        44         0         0         0           -           0
2021-05-18T09:56:24.267610+0000     3      68        68         0         0         0           -           0
2021-05-18T09:56:25.267827+0000     4      89        89         0         0         0           -           0
2021-05-18T09:56:26.268035+0000     5     111       111         0         0         0           -           0
2021-05-18T09:56:27.268663+0000     6     135       135         0         0         0           -           0
2021-05-18T09:56:28.268916+0000     7     156       156         0         0         0           -           0
2021-05-18T09:56:29.269235+0000     8     179       179         0         0         0           -           0
2021-05-18T09:56:30.269492+0000     9     201       201         0         0         0           -           0
2021-05-18T09:56:31.269788+0000    10     224       224         0         0         0           -           0
2021-05-18T09:56:32.270070+0000    11     247       247         0         0         0           -           0
2021-05-18T09:56:33.270331+0000    12     255       270        15   19.9941        20      11.335     11.3744
2021-05-18T09:56:34.270588+0000    13     255       292        37   45.5251       352     11.3727     11.3688
2021-05-18T09:56:35.270813+0000    14     255       313        58   66.2666       336     11.4379     11.3881
2021-05-18T09:56:36.271035+0000    15     255       334        79   84.2428       336      11.514      11.412
2021-05-18T09:56:37.271278+0000    16     255       356       101   100.972       352      11.481     11.4287
2021-05-18T09:56:38.271522+0000    17     255       379       124   116.673       368      11.509     11.4396
2021-05-18T09:56:39.271836+0000    18     255       401       146   129.741       352     11.5886     11.4541
2021-05-18T09:56:40.272135+0000    19     255       422       167   140.592       336     11.5781     11.4674
2021-05-18T09:56:41.272371+0000 min lat: 11.319 max lat: 11.5886 avg lat: 11.4767
2021-05-18T09:56:41.272371+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:56:41.272371+0000    20     255       445       190   151.958       368     11.5274     11.4767
2021-05-18T09:56:42.272671+0000 Total time run:         20.1257
Total writes made:      446
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     354.571
Stddev Bandwidth:       175.323
Max bandwidth (MB/sec): 368
Min bandwidth (MB/sec): 0
Average IOPS:           22
Stddev IOPS:            10.9669
Max IOPS:               23
Min IOPS:               0
Average Latency(s):     8.2402
Stddev Latency(s):      3.77606
Max latency(s):         11.5886
Min latency(s):         0.132228

[1;32mlocalhost.localdomain	[2021-05-18T02:56:43,198419735-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2880797


[1;33mlocalhost.localdomain	[2021-05-18T02:56:43,209286094-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:57:06,162880050-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 447 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:57:06,186028379-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:57:14,012295129-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 447 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:57:14,034297637-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:57:22,075732331-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 447 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:57:22,097494123-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:57:30,224671049-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 447 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:57:30,246532788-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:57:38,289082566-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 447 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:57:38,311116445-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:57:38,329228863-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:57:38,339834522-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:57:38,363915590-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2884179
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-18T02:57:38,383604913-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-18T02:57:38,425223396-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:57:38,431622456-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43d1a474-eab9-4d1d-8601-00da6eeaf55a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43d1a474-eab9-4d1d-8601-00da6eeaf55a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.MuPSl6:/tmp/ceph-asok.MuPSl6 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T09:57:39.890+0000 ffffa2340010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:57:39.898+0000 ffffa2340010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T09:57:39.902+0000 ffffa2340010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T09:57:39.936105+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T09:57:39.936105+0000     0       0         0         0         0         0           -           0
2021-05-18T09:57:40.936345+0000     1      72        72         0         0         0           -           0
2021-05-18T09:57:41.936575+0000     2     136       136         0         0         0           -           0
2021-05-18T09:57:42.936803+0000     3     196       196         0         0         0           -           0
2021-05-18T09:57:43.940401+0000     4     255       259         4    15.982        16     3.99381     3.96846
2021-05-18T09:57:44.942457+0000     5     255       306        51   162.986       752     4.31888     4.14469
2021-05-18T09:57:45.942715+0000     6     255       350        95   253.046       704     4.64727     4.30315
2021-05-18T09:57:46.942983+0000     7     255       391       136   310.543       656     5.01313     4.46604
2021-05-18T09:57:47.943271+0000     8     255       433       178   355.672       672      5.3733     4.63817
2021-05-18T09:57:49.709160+0000 Total time run:       9.77315
Total reads made:     446
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   730.164
Average IOPS:         45
Stddev IOPS:          23.1852
Max IOPS:             47
Min IOPS:             0
Average Latency(s):   4.10066
Max latency(s):       5.42401
Min latency(s):       1.48221

[1;32mlocalhost.localdomain	[2021-05-18T02:57:50,815664836-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2884179


[1;33mlocalhost.localdomain	[2021-05-18T02:57:50,826521413-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T02:58:13,945938850-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 447 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:58:13,971850157-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:58:21,985603612-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 447 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:58:22,007508573-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:58:29,888422307-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 447 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:58:29,910642593-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:58:37,926193617-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 447 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:58:37,948403491-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:58:46,202671580-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 447 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T02:58:46,224868326-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T02:58:46,243112497-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T02:58:46,249838012-07:00][RUNNING][ROUND 4/7/40] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:58:46,260223658-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T02:58:46,278927867-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40698\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.724258\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fe1bd46c-c51e-4f4a-8d0b-a0d64661a7f4\nsetting min_mon_release = octopus\nepoch 0\nfsid fe1bd46c-c51e-4f4a-8d0b-a0d64661a7f4\nlast_changed 2021-05-18T02:59:13.711596-0700\ncreated 2021-05-18T02:59:13.711596-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40698/0,v1:10.10.1.2:40699/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.724258 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 d582ed81-45bf-4b0e-b543-e07ca7aed09d\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 2c171fec-ff94-4591-8256-c5268f5075e5\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 ee9f67da-5db6-42ff-b3dc-c8edf7c9aac6\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42698\n  w/ user/pass: admin / 71a9a15b-9d5e-4c88-892e-33a3deaf50ac\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:59:28 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40698
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.724258
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fe1bd46c-c51e-4f4a-8d0b-a0d64661a7f4
setting min_mon_release = octopus
epoch 0
fsid fe1bd46c-c51e-4f4a-8d0b-a0d64661a7f4
last_changed 2021-05-18T02:59:13.711596-0700
created 2021-05-18T02:59:13.711596-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40698/0,v1:10.10.1.2:40699/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.724258 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 d582ed81-45bf-4b0e-b543-e07ca7aed09d
0
start osd.0
add osd1 2c171fec-ff94-4591-8256-c5268f5075e5
1
start osd.1
add osd2 ee9f67da-5db6-42ff-b3dc-c8edf7c9aac6
2
start osd.2


restful urls: https://10.10.1.2:42698
  w/ user/pass: admin / 71a9a15b-9d5e-4c88-892e-33a3deaf50ac


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T02:58:47.275-0700 7f636e0a01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:58:47.275-0700 7f636e0a01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:58:47.295-0700 7f2e214e81c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T02:58:47.295-0700 7f2e214e81c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40698,v1:10.10.1.2:40699] --print /tmp/ceph_monmap.724258 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.724258 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.724258 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42698 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.lTFM0tKvT5 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d582ed81-45bf-4b0e-b543-e07ca7aed09d -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD6j6NgYdepGhAASWRKOrecQs0M4eKC+v0nRw== --osd-uuid d582ed81-45bf-4b0e-b543-e07ca7aed09d 
2021-05-18T02:59:22.771-0700 7f6fde5d2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:59:22.771-0700 7f6fde5d2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:59:22.771-0700 7f6fde5d2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T02:59:22.851-0700 7f6fde5d2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 2c171fec-ff94-4591-8256-c5268f5075e5 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T02:59:23.207-0700 7fba83bd9f00 -1 Falling back to public interface
2021-05-18T02:59:23.219-0700 7fba83bd9f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD7j6Ngk/AhDBAARUtMSwiiuqhVDsbe4pyq0w== --osd-uuid 2c171fec-ff94-4591-8256-c5268f5075e5 
2021-05-18T02:59:23.559-0700 7fc3b4a5df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:59:23.559-0700 7fc3b4a5df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:59:23.559-0700 7fc3b4a5df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T02:59:23.607-0700 7fc3b4a5df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ee9f67da-5db6-42ff-b3dc-c8edf7c9aac6 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T02:59:23.919-0700 7f51bbe94f00 -1 Falling back to public interface
2021-05-18T02:59:23.931-0700 7f51bbe94f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD7j6Ng/RbCNhAA4qon5ePoa/cPfr8gS2uLEQ== --osd-uuid ee9f67da-5db6-42ff-b3dc-c8edf7c9aac6 
2021-05-18T02:59:24.247-0700 7f9a55693f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:59:24.247-0700 7f9a55693f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:59:24.247-0700 7f9a55693f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T02:59:24.291-0700 7f9a55693f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T02:59:24.747-0700 7f550d2c3f00 -1 Falling back to public interface
2021-05-18T02:59:24.759-0700 7f550d2c3f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T02:59:28,629890648-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T02:59:28,640492391-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T02:59:28,727471731-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:59:28,733900661-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '20b0a77f-68e1-41cc-8deb-6eccf1abf1a1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 20b0a77f-68e1-41cc-8deb-6eccf1abf1a1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T02:59:31,709454075-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:59:31,715677245-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '20b0a77f-68e1-41cc-8deb-6eccf1abf1a1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 20b0a77f-68e1-41cc-8deb-6eccf1abf1a1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T02:59:34,528031308-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:59:34,534417360-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '20b0a77f-68e1-41cc-8deb-6eccf1abf1a1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 20b0a77f-68e1-41cc-8deb-6eccf1abf1a1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T02:59:37,259948319-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:59:37,266232519-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '20b0a77f-68e1-41cc-8deb-6eccf1abf1a1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 20b0a77f-68e1-41cc-8deb-6eccf1abf1a1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T02:59:42,899078001-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:59:42,905515752-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '20b0a77f-68e1-41cc-8deb-6eccf1abf1a1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 20b0a77f-68e1-41cc-8deb-6eccf1abf1a1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T02:59:46,035163707-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:59:46,041618515-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '20b0a77f-68e1-41cc-8deb-6eccf1abf1a1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 20b0a77f-68e1-41cc-8deb-6eccf1abf1a1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T02:59:49,366551415-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:59:49,373041285-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '20b0a77f-68e1-41cc-8deb-6eccf1abf1a1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 20b0a77f-68e1-41cc-8deb-6eccf1abf1a1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T02:59:52,647069863-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:59:52,653595019-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '20b0a77f-68e1-41cc-8deb-6eccf1abf1a1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 20b0a77f-68e1-41cc-8deb-6eccf1abf1a1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:59:56,121051524-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:59:56,127307129-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '20b0a77f-68e1-41cc-8deb-6eccf1abf1a1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 20b0a77f-68e1-41cc-8deb-6eccf1abf1a1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T02:59:59,259174197-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T02:59:59,265645794-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '20b0a77f-68e1-41cc-8deb-6eccf1abf1a1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 20b0a77f-68e1-41cc-8deb-6eccf1abf1a1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T03:00:02,586618505-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:00:02,592983418-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '20b0a77f-68e1-41cc-8deb-6eccf1abf1a1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 20b0a77f-68e1-41cc-8deb-6eccf1abf1a1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T03:00:05,396847021-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:00:05,403670593-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '20b0a77f-68e1-41cc-8deb-6eccf1abf1a1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 20b0a77f-68e1-41cc-8deb-6eccf1abf1a1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T03:00:08,047832064-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T03:00:31,289854649-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (5s)
      [=====.......................] (remaining: 19s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:00:39,581887379-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:00:47,761944341-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:00:55,684606809-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:01:03,665829546-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:01:11,761522616-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:01:19,727188291-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:01:27,870552010-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:01:27,892818743-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:01:35,843116494-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:01:35,865048774-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:01:43,936164067-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:01:43,958038573-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:01:52,102965534-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:01:52,125377431-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:02:00,104422250-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:02:00,126989289-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:02:00,145236099-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T03:02:00,155579722-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T03:02:00,179684569-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2897420
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-18T03:02:00,199486665-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-18T03:02:00,241379484-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:02:00,247900649-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '20b0a77f-68e1-41cc-8deb-6eccf1abf1a1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 20b0a77f-68e1-41cc-8deb-6eccf1abf1a1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7 -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T10:02:01.755+0000 ffffaff93010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T10:02:01.763+0000 ffffaff93010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T10:02:01.763+0000 ffffaff93010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T10:02:01.791215+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-18T10:02:01.791329+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T10:02:05.869108+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T10:02:05.869108+0000     0       0         0         0         0         0           -           0
2021-05-18T10:02:06.869347+0000     1      20        20         0         0         0           -           0
2021-05-18T10:02:07.869677+0000     2      40        40         0         0         0           -           0
2021-05-18T10:02:08.869880+0000     3      60        60         0         0         0           -           0
2021-05-18T10:02:09.870121+0000     4      79        79         0         0         0           -           0
2021-05-18T10:02:10.870358+0000     5      98        98         0         0         0           -           0
2021-05-18T10:02:11.870592+0000     6     120       120         0         0         0           -           0
2021-05-18T10:02:12.870994+0000     7     140       140         0         0         0           -           0
2021-05-18T10:02:13.871238+0000     8     160       160         0         0         0           -           0
2021-05-18T10:02:14.871468+0000     9     182       182         0         0         0           -           0
2021-05-18T10:02:15.871686+0000    10     201       201         0         0         0           -           0
2021-05-18T10:02:16.871903+0000    11     221       221         0         0         0           -           0
2021-05-18T10:02:17.872110+0000    12     242       242         0         0         0           -           0
2021-05-18T10:02:18.872334+0000    13     255       263         8   9.84382   9.84615     12.6796     12.7067
2021-05-18T10:02:19.872577+0000    14     255       283        28   31.9924       320     12.7156     12.6831
2021-05-18T10:02:20.872818+0000    15     255       304        49   52.2543       336     12.6493     12.6809
2021-05-18T10:02:21.873061+0000    16     255       325        70   69.9834       336     12.5104     12.6488
2021-05-18T10:02:22.873273+0000    17     255       345        90   84.6859       320     12.4929      12.618
2021-05-18T10:02:23.873513+0000    18     255       365       110   97.7547       320     12.4623     12.5869
2021-05-18T10:02:24.873718+0000    19     255       386       131    110.29       336     12.4358     12.5699
2021-05-18T10:02:25.873927+0000 min lat: 12.406 max lat: 12.7806 avg lat: 12.5585
2021-05-18T10:02:25.873927+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T10:02:25.873927+0000    20     255       407       152   121.572       336     12.4539     12.5585
2021-05-18T10:02:26.874202+0000 Total time run:         20.0995
Total writes made:      408
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     324.784
Stddev Bandwidth:       160.785
Max bandwidth (MB/sec): 336
Min bandwidth (MB/sec): 0
Average IOPS:           20
Stddev IOPS:            10.0713
Max IOPS:               21
Min IOPS:               0
Average Latency(s):     8.58188
Stddev Latency(s):      4.18923
Max latency(s):         12.7806
Min latency(s):         0.133292

[1;32mlocalhost.localdomain	[2021-05-18T03:02:27,994227953-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2897420


[1;33mlocalhost.localdomain	[2021-05-18T03:02:28,005331697-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T03:02:51,083132847-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 409 objects, 6.4 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:02:51,104871070-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:02:59,286544381-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 409 objects, 6.4 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:02:59,309017369-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:03:07,236972661-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 409 objects, 6.4 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:03:07,259225606-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:03:15,249844491-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 409 objects, 6.4 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:03:15,272106033-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:03:23,348414193-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 409 objects, 6.4 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:03:23,370933478-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:03:23,389736058-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T03:03:23,400125785-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T03:03:23,424480559-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2900812
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-18T03:03:23,443902805-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-18T03:03:23,485166611-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:03:23,491521832-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '20b0a77f-68e1-41cc-8deb-6eccf1abf1a1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 20b0a77f-68e1-41cc-8deb-6eccf1abf1a1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JDlIg7:/tmp/ceph-asok.JDlIg7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T10:03:25.285+0000 ffffa8022010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T10:03:25.293+0000 ffffa8022010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T10:03:25.293+0000 ffffa8022010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T10:03:25.324191+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T10:03:25.324191+0000     0       0         0         0         0         0           -           0
2021-05-18T10:03:26.324406+0000     1      62        62         0         0         0           -           0
2021-05-18T10:03:27.324630+0000     2     120       120         0         0         0           -           0
2021-05-18T10:03:28.324859+0000     3     179       179         0         0         0           -           0
2021-05-18T10:03:29.325127+0000     4     248       248         0         0         0           -           0
2021-05-18T10:03:30.325410+0000     5     255       298        43   137.561     137.6     4.35203     4.23082
2021-05-18T10:03:31.330358+0000     6     255       347        92   245.073       784     4.48122     4.32937
2021-05-18T10:03:32.330707+0000     7     255       386       131   299.141       624     4.78701     4.42812
2021-05-18T10:03:34.240599+0000 Total time run:       8.91653
Total reads made:     408
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   732.124
Average IOPS:         45
Stddev IOPS:          21.0928
Max IOPS:             49
Min IOPS:             0
Average Latency(s):   3.82268
Max latency(s):       4.89248
Min latency(s):       1.49785

[1;32mlocalhost.localdomain	[2021-05-18T03:03:35,371691497-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2900812


[1;33mlocalhost.localdomain	[2021-05-18T03:03:35,383063126-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T03:03:58,309869044-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 409 objects, 6.4 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:03:58,335161509-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:04:06,224924289-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 409 objects, 6.4 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:04:06,247448795-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:04:14,404702656-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 409 objects, 6.4 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:04:14,427178584-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:04:22,366960783-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 409 objects, 6.4 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:04:22,392413931-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:04:30,427374061-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 409 objects, 6.4 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:04:30,449920455-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:04:30,468278235-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T03:04:30,475308345-07:00][RUNNING][ROUND 5/7/40] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T03:04:30,485577966-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T03:04:30,504784706-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40249\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.725362\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid cad2decf-99b1-49a5-a189-1a3a02c35b1f\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid cad2decf-99b1-49a5-a189-1a3a02c35b1f\nlast_changed 2021-05-18T03:04:56.663416-0700\ncreated 2021-05-18T03:04:56.663416-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40249/0,v1:10.10.1.2:40250/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.725362 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 c2330f8a-258d-49d2-a19f-8c6acac0e6f1\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 55ce7b1c-2fbd-4e8d-9268-ee59c10ca6bd\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 e8c79abf-73aa-4698-927a-9bc652729367\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42249\n  w/ user/pass: admin / ab0504b8-0a75-4d8e-a15b-c8e6f4f1b940\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 03:05:11 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40249
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.725362
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid cad2decf-99b1-49a5-a189-1a3a02c35b1f
setting min_mon_release = octopus
epoch 0
fsid cad2decf-99b1-49a5-a189-1a3a02c35b1f
last_changed 2021-05-18T03:04:56.663416-0700
created 2021-05-18T03:04:56.663416-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40249/0,v1:10.10.1.2:40250/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.725362 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 c2330f8a-258d-49d2-a19f-8c6acac0e6f1
0
start osd.0
add osd1 55ce7b1c-2fbd-4e8d-9268-ee59c10ca6bd
1
start osd.1
add osd2 e8c79abf-73aa-4698-927a-9bc652729367
2
start osd.2


restful urls: https://10.10.1.2:42249
  w/ user/pass: admin / ab0504b8-0a75-4d8e-a15b-c8e6f4f1b940


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T03:04:31.486-0700 7fe6364e41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T03:04:31.486-0700 7fe6364e41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T03:04:31.502-0700 7f21fcc891c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T03:04:31.502-0700 7f21fcc891c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40249,v1:10.10.1.2:40250] --print /tmp/ceph_monmap.725362 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.725362 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.725362 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42249 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.k3JN7BtbdC 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c2330f8a-258d-49d2-a19f-8c6acac0e6f1 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBRkaNgNRMVHhAA3ZJTsmCAnzPMS1hKHw8obA== --osd-uuid c2330f8a-258d-49d2-a19f-8c6acac0e6f1 
2021-05-18T03:05:05.862-0700 7f8c035d4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T03:05:05.862-0700 7f8c035d4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T03:05:05.862-0700 7f8c035d4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T03:05:05.910-0700 7f8c035d4f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 55ce7b1c-2fbd-4e8d-9268-ee59c10ca6bd -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T03:05:06.194-0700 7f1bd593bf00 -1 Falling back to public interface
2021-05-18T03:05:06.206-0700 7f1bd593bf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBSkaNgZlmvCxAAaoVCChiPIfTdo1LZoWouKQ== --osd-uuid 55ce7b1c-2fbd-4e8d-9268-ee59c10ca6bd 
2021-05-18T03:05:06.526-0700 7ff83ce06f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T03:05:06.526-0700 7ff83ce06f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T03:05:06.526-0700 7ff83ce06f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T03:05:06.586-0700 7ff83ce06f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e8c79abf-73aa-4698-927a-9bc652729367 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T03:05:06.874-0700 7fe96fcf6f00 -1 Falling back to public interface
2021-05-18T03:05:06.886-0700 7fe96fcf6f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBSkaNg05EQNBAALKW/nN6Fzkfw+GZWc7htZw== --osd-uuid e8c79abf-73aa-4698-927a-9bc652729367 
2021-05-18T03:05:07.210-0700 7f39d4826f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T03:05:07.210-0700 7f39d4826f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T03:05:07.210-0700 7f39d4826f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T03:05:07.274-0700 7f39d4826f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T03:05:07.618-0700 7fdd6f76df00 -1 Falling back to public interface
2021-05-18T03:05:07.634-0700 7fdd6f76df00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T03:05:11,535915413-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T03:05:11,546484938-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T03:05:11,632855887-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:05:11,639419033-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T03:05:14,572045747-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:05:14,578749556-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T03:05:17,332793088-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:05:17,339534703-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T03:05:20,118355538-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:05:20,125056917-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T03:05:25,899679161-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:05:25,906348406-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T03:05:29,425796139-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:05:29,433471284-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T03:05:32,752978784-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:05:32,759879381-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T03:05:36,096948419-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:05:36,104444556-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T03:05:39,797798324-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:05:39,806108720-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T03:05:42,893280974-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:05:42,900401644-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T03:05:46,133160384-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:05:46,139829359-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T03:05:48,986311553-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:05:48,992832335-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T03:05:51,672823498-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T03:06:14,609136258-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:06:22,663936765-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:06:30,922393354-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:06:39,580878795-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:06:47,550628365-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:06:55,552995265-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:07:03,447264323-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:07:11,434151965-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:07:11,456249383-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:07:19,581340967-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:07:19,603471197-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:07:27,511686739-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:07:27,533591243-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:07:35,478745128-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:07:35,501346753-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:07:43,637352381-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:07:43,659707637-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:07:43,678237663-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T03:07:43,688646706-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T03:07:43,713565933-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2914121
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-18T03:07:43,733902439-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-18T03:07:43,776619376-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:07:43,783060815-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T10:07:45.496+0000 ffffb476d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T10:07:45.504+0000 ffffb476d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T10:07:45.504+0000 ffffb476d010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T10:07:45.530935+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-18T10:07:45.531051+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T10:07:49.461760+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T10:07:49.461760+0000     0       0         0         0         0         0           -           0
2021-05-18T10:07:50.462030+0000     1      22        22         0         0         0           -           0
2021-05-18T10:07:51.462315+0000     2      45        45         0         0         0           -           0
2021-05-18T10:07:52.462615+0000     3      66        66         0         0         0           -           0
2021-05-18T10:07:53.462849+0000     4      87        87         0         0         0           -           0
2021-05-18T10:07:54.463080+0000     5     109       109         0         0         0           -           0
2021-05-18T10:07:55.463322+0000     6     131       131         0         0         0           -           0
2021-05-18T10:07:56.463550+0000     7     155       155         0         0         0           -           0
2021-05-18T10:07:57.463785+0000     8     176       176         0         0         0           -           0
2021-05-18T10:07:58.464200+0000     9     197       197         0         0         0           -           0
2021-05-18T10:07:59.464443+0000    10     220       220         0         0         0           -           0
2021-05-18T10:08:00.464698+0000    11     243       243         0         0         0           -           0
2021-05-18T10:08:01.464941+0000    12     255       265        10     13.33   13.3333     11.6084     11.6113
2021-05-18T10:08:02.465210+0000    13     255       287        32   39.3747       352     11.6162     11.6125
2021-05-18T10:08:03.465486+0000    14     255       310        55   62.8412       368     11.5355     11.6045
2021-05-18T10:08:04.465758+0000    15     255       333        78   83.1788       368     11.5249     11.5806
2021-05-18T10:08:05.466004+0000    16     255       355       100   99.9746       352     11.4716     11.5649
2021-05-18T10:08:06.466264+0000    17     255       376       121   113.853       336     11.5126     11.5532
2021-05-18T10:08:07.466519+0000    18     255       399       144   127.967       368     11.5123     11.5467
2021-05-18T10:08:08.466784+0000    19     255       420       165   138.912       336     11.5111     11.5457
2021-05-18T10:08:09.467038+0000 min lat: 11.4594 max lat: 11.6524 avg lat: 11.5464
2021-05-18T10:08:09.467038+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T10:08:09.467038+0000    20     255       442       187   149.562       352     11.5399     11.5464
2021-05-18T10:08:10.467385+0000 Total time run:         20.0829
Total writes made:      443
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     352.937
Stddev Bandwidth:       177.58
Max bandwidth (MB/sec): 368
Min bandwidth (MB/sec): 0
Average IOPS:           22
Stddev IOPS:            11.1321
Max IOPS:               23
Min IOPS:               0
Average Latency(s):     8.22439
Stddev Latency(s):      3.80113
Max latency(s):         11.6524
Min latency(s):         0.11739

[1;32mlocalhost.localdomain	[2021-05-18T03:08:11,377320257-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2914121


[1;33mlocalhost.localdomain	[2021-05-18T03:08:11,388328726-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T03:08:34,507953833-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 444 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:08:34,529943097-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:08:42,515809516-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 444 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:08:42,540143281-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:08:50,601236987-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 444 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:08:50,623484761-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:08:58,591148379-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 444 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:08:58,613270238-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:09:06,671900918-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 444 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:09:06,693955087-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:09:06,712575259-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T03:09:06,722838799-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T03:09:06,747259792-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2917611
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-18T03:09:06,767479546-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-18T03:09:06,809393558-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:09:06,815860265-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e78782ff-aa9e-4dc1-ba9a-e0fbde5d1198 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.olGOHr:/tmp/ceph-asok.olGOHr -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T10:09:08.294+0000 ffffa3a71010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T10:09:08.302+0000 ffffa3a71010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T10:09:08.302+0000 ffffa3a71010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T10:09:08.337057+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T10:09:08.337057+0000     0       0         0         0         0         0           -           0
2021-05-18T10:09:09.337300+0000     1      62        62         0         0         0           -           0
2021-05-18T10:09:10.337537+0000     2     118       118         0         0         0           -           0
2021-05-18T10:09:11.337767+0000     3     177       177         0         0         0           -           0
2021-05-18T10:09:12.338003+0000     4     235       235         0         0         0           -           0
2021-05-18T10:09:13.338263+0000     5     255       284        29   92.7742      92.8     4.60427     4.48346
2021-05-18T10:09:14.338620+0000     6     255       328        73    194.61       704     4.78858     4.63119
2021-05-18T10:09:15.345865+0000     7     255       377       122   278.499       784     4.95894     4.72699
2021-05-18T10:09:16.346157+0000     8     255       421       166   331.615       704     5.19599     4.82246
2021-05-18T10:09:17.462778+0000     9     158       443       285   499.676      1904     4.13599     4.85865
2021-05-18T10:09:18.463077+0000 Total time run:       10.0194
Total reads made:     443
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   707.43
Average IOPS:         44
Stddev IOPS:          40.1404
Max IOPS:             119
Min IOPS:             0
Average Latency(s):   4.13132
Max latency(s):       5.37205
Min latency(s):       1.51426

[1;32mlocalhost.localdomain	[2021-05-18T03:09:19,425205536-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 2917611


[1;33mlocalhost.localdomain	[2021-05-18T03:09:19,436221317-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T03:09:42,442815126-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 444 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:09:42,464976853-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:09:50,680322866-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 444 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:09:50,702754924-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:09:58,758219769-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 444 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:09:58,780152919-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:10:06,730807372-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 444 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:10:06,752831289-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:10:14,764172466-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 444 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:10:14,786143680-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:10:14,804340099-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-18T03:10:14,815141618-07:00][RUNNING][ROUND 1/8/40] object_size=64MB[0m


[1;33mlocalhost.localdomain	[2021-05-18T03:10:14,825594298-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-18T03:10:14,844792164-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40805\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.726488\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 3d79ca3d-f2da-4c65-8ce5-3a18b961db47\nsetting min_mon_release = octopus\nepoch 0\nfsid 3d79ca3d-f2da-4c65-8ce5-3a18b961db47\nlast_changed 2021-05-18T03:10:42.719703-0700\ncreated 2021-05-18T03:10:42.719703-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40805/0,v1:10.10.1.2:40806/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.726488 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 d86db6c3-deb8-4ea9-bd8c-847af0f5edbc\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 f37a7100-4cf6-4244-965b-ce63d009ef64\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 3022ebe0-4a89-4053-aa23-ed6815d5d8c7\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42805\n  w/ user/pass: admin / 1a78c42c-3d68-4a46-ad49-812c02a718bd\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 03:10:57 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40805
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.726488
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 3d79ca3d-f2da-4c65-8ce5-3a18b961db47
setting min_mon_release = octopus
epoch 0
fsid 3d79ca3d-f2da-4c65-8ce5-3a18b961db47
last_changed 2021-05-18T03:10:42.719703-0700
created 2021-05-18T03:10:42.719703-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40805/0,v1:10.10.1.2:40806/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.726488 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 d86db6c3-deb8-4ea9-bd8c-847af0f5edbc
0
start osd.0
add osd1 f37a7100-4cf6-4244-965b-ce63d009ef64
1
start osd.1
add osd2 3022ebe0-4a89-4053-aa23-ed6815d5d8c7
2
start osd.2


restful urls: https://10.10.1.2:42805
  w/ user/pass: admin / 1a78c42c-3d68-4a46-ad49-812c02a718bd


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-18T03:10:15.849-0700 7f7a678a01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T03:10:15.849-0700 7f7a678a01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T03:10:15.865-0700 7f7c3fc751c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T03:10:15.865-0700 7f7c3fc751c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40805,v1:10.10.1.2:40806] --print /tmp/ceph_monmap.726488 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.726488 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.726488 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42805 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.TnWDzrcQGn 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d86db6c3-deb8-4ea9-bd8c-847af0f5edbc -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCrkqNg85vaJhAA0BVw5NcuHiEcuoxsziWFoA== --osd-uuid d86db6c3-deb8-4ea9-bd8c-847af0f5edbc 
2021-05-18T03:10:51.985-0700 7f20f5575f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T03:10:51.985-0700 7f20f5575f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T03:10:51.985-0700 7f20f5575f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-18T03:10:52.053-0700 7f20f5575f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f37a7100-4cf6-4244-965b-ce63d009ef64 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-18T03:10:52.357-0700 7f891d9a1f00 -1 Falling back to public interface
2021-05-18T03:10:52.369-0700 7f891d9a1f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCskqNgtYdnFRAAobGqnxXKraH11G7DAiulKw== --osd-uuid f37a7100-4cf6-4244-965b-ce63d009ef64 
2021-05-18T03:10:52.713-0700 7f2a6bfb8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T03:10:52.713-0700 7f2a6bfb8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T03:10:52.713-0700 7f2a6bfb8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-18T03:10:52.757-0700 7f2a6bfb8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3022ebe0-4a89-4053-aa23-ed6815d5d8c7 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-18T03:10:53.017-0700 7f3121849f00 -1 Falling back to public interface
2021-05-18T03:10:53.033-0700 7f3121849f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCtkqNgveMuARAAjldsxRyiyD7P4fNKt5WMsA== --osd-uuid 3022ebe0-4a89-4053-aa23-ed6815d5d8c7 
2021-05-18T03:10:53.389-0700 7f4d36ab4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T03:10:53.393-0700 7f4d36ab4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T03:10:53.393-0700 7f4d36ab4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-18T03:10:53.453-0700 7f4d36ab4f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-18T03:10:53.805-0700 7fde696e9f00 -1 Falling back to public interface
2021-05-18T03:10:53.821-0700 7fde696e9f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-18T03:10:57,650915368-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-18T03:10:57,662462199-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-18T03:10:57,753373682-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:10:57,759916171-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '248ccc36-b03a-46e9-97da-2cbdec29b4e4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 248ccc36-b03a-46e9-97da-2cbdec29b4e4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-18T03:11:00,673325745-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:11:00,679627264-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '248ccc36-b03a-46e9-97da-2cbdec29b4e4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 248ccc36-b03a-46e9-97da-2cbdec29b4e4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-18T03:11:03,362651191-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:11:03,369283681-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '248ccc36-b03a-46e9-97da-2cbdec29b4e4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 248ccc36-b03a-46e9-97da-2cbdec29b4e4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-18T03:11:06,055268975-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:11:06,062003662-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '248ccc36-b03a-46e9-97da-2cbdec29b4e4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 248ccc36-b03a-46e9-97da-2cbdec29b4e4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-18T03:11:11,682252188-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:11:11,689124223-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '248ccc36-b03a-46e9-97da-2cbdec29b4e4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 248ccc36-b03a-46e9-97da-2cbdec29b4e4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-18T03:11:15,148836561-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:11:15,155492320-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '248ccc36-b03a-46e9-97da-2cbdec29b4e4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 248ccc36-b03a-46e9-97da-2cbdec29b4e4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-18T03:11:18,825018952-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:11:18,831618286-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '248ccc36-b03a-46e9-97da-2cbdec29b4e4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 248ccc36-b03a-46e9-97da-2cbdec29b4e4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-18T03:11:22,358521622-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:11:22,365088431-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '248ccc36-b03a-46e9-97da-2cbdec29b4e4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 248ccc36-b03a-46e9-97da-2cbdec29b4e4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-18T03:11:25,627254384-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:11:25,633567156-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '248ccc36-b03a-46e9-97da-2cbdec29b4e4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 248ccc36-b03a-46e9-97da-2cbdec29b4e4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-18T03:11:28,894394591-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:11:28,900989425-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '248ccc36-b03a-46e9-97da-2cbdec29b4e4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 248ccc36-b03a-46e9-97da-2cbdec29b4e4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-18T03:11:32,311727273-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:11:32,318199641-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '248ccc36-b03a-46e9-97da-2cbdec29b4e4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 248ccc36-b03a-46e9-97da-2cbdec29b4e4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-18T03:11:35,032136514-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:11:35,038602642-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '248ccc36-b03a-46e9-97da-2cbdec29b4e4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 248ccc36-b03a-46e9-97da-2cbdec29b4e4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-18T03:11:37,797522197-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-18T03:12:00,703174615-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   203 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:12:08,555256273-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:12:16,776289960-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 29s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:12:24,795261987-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 53s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:12:32,688048705-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 60s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:12:40,970901302-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 78s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:12:49,026761011-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 90s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:12:57,023663992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 43s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:13:05,115973149-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:13:05,138131013-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:13:13,304828574-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:13:13,326877564-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:13:21,517509852-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:13:21,540188064-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:13:29,526939666-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:13:29,549113989-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:13:37,536757736-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-18T03:13:37,559036042-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-18T03:13:37,577506349-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-18T03:13:37,588281788-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-18T03:13:37,613438688-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=2931242
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-18T03:13:37,633678482-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 67108864 -O 67108864 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-18T03:13:37,676800389-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-18T03:13:37,683374493-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '248ccc36-b03a-46e9-97da-2cbdec29b4e4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '67108864', '-O', '67108864', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 248ccc36-b03a-46e9-97da-2cbdec29b4e4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QfQLnz:/tmp/ceph-asok.QfQLnz -- rados bench 20 write --pool bench_rados -b 67108864 -O 67108864 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T10:13:39.172+0000 ffff7f9a9010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T10:13:39.180+0000 ffff7f9a9010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T10:13:39.180+0000 ffff7f9a9010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T10:13:39.242950+0000 Maintaining 256 concurrent writes of 67108864 bytes to objects of size 67108864 for up to 20 seconds or 0 objects
2021-05-18T10:13:39.243071+0000 Object prefix: benchmark_data_localhost.localdomain_7
