

[1;7;39;49m[2021-05-17T13:09:50,847253125-07:00][RUNNING][ROUND 1/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:09:50,853731048-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T13:09:50,870254247-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40638\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.407869\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 753e57e3-cb7b-4ac6-9616-f868835cbe45\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 753e57e3-cb7b-4ac6-9616-f868835cbe45\nlast_changed 2021-05-17T13:09:59.810638-0700\ncreated 2021-05-17T13:09:59.810638-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40638/0,v1:10.10.1.2:40639/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.407869 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 b54ba925-c55f-46f6-94e1-c9a5b75d6dfc\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 d191bb6a-adaf-4b0a-8d83-b1590da8a496\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 0388d001-11d7-494a-8014-fd348bf34a99\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42638\n  w/ user/pass: admin / 3e800ed2-bf00-4ce4-ba53-45d071a902c6\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:10:15 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40638
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.407869
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 753e57e3-cb7b-4ac6-9616-f868835cbe45
setting min_mon_release = octopus
epoch 0
fsid 753e57e3-cb7b-4ac6-9616-f868835cbe45
last_changed 2021-05-17T13:09:59.810638-0700
created 2021-05-17T13:09:59.810638-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40638/0,v1:10.10.1.2:40639/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.407869 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 b54ba925-c55f-46f6-94e1-c9a5b75d6dfc
0
start osd.0
add osd1 d191bb6a-adaf-4b0a-8d83-b1590da8a496
1
start osd.1
add osd2 0388d001-11d7-494a-8014-fd348bf34a99
2
start osd.2


restful urls: https://10.10.1.2:42638
  w/ user/pass: admin / 3e800ed2-bf00-4ce4-ba53-45d071a902c6


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T13:09:51.902-0700 7f5723b9a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:09:51.902-0700 7f5723b9a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:09:51.922-0700 7f92bc91c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:09:51.922-0700 7f92bc91c1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40638,v1:10.10.1.2:40639] --print /tmp/ceph_monmap.407869 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.407869 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.407869 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42638 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.YEYTtqkRWN 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b54ba925-c55f-46f6-94e1-c9a5b75d6dfc -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCgzaJgylxIIhAAvwNTnSJLR4zZjAwQrHPLtw== --osd-uuid b54ba925-c55f-46f6-94e1-c9a5b75d6dfc 
2021-05-17T13:10:08.918-0700 7f49c324ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:10:08.918-0700 7f49c324ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:10:08.918-0700 7f49c324ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:10:08.982-0700 7f49c324ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d191bb6a-adaf-4b0a-8d83-b1590da8a496 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T13:10:09.266-0700 7efdc1791f00 -1 Falling back to public interface
2021-05-17T13:10:09.278-0700 7efdc1791f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQChzaJgCSrfDxAAQKY+k5gKELf8X7YtalpzFQ== --osd-uuid d191bb6a-adaf-4b0a-8d83-b1590da8a496 
2021-05-17T13:10:09.606-0700 7fa80bba8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:10:09.606-0700 7fa80bba8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:10:09.606-0700 7fa80bba8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:10:09.650-0700 7fa80bba8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0388d001-11d7-494a-8014-fd348bf34a99 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T13:10:09.930-0700 7fbcaee21f00 -1 Falling back to public interface
2021-05-17T13:10:09.942-0700 7fbcaee21f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQChzaJg3CVSNxAAHXJBONpK3qMnxoIUX5BDKw== --osd-uuid 0388d001-11d7-494a-8014-fd348bf34a99 
2021-05-17T13:10:10.274-0700 7ff72f2c7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:10:10.274-0700 7ff72f2c7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:10:10.274-0700 7ff72f2c7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:10:10.322-0700 7ff72f2c7f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T13:10:10.658-0700 7f59c036ef00 -1 Falling back to public interface
2021-05-17T13:10:10.670-0700 7f59c036ef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T13:10:15,207793039-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:10:15,212963189-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T13:10:15,302790984-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:10:15,309021965-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86f42411-c729-4792-9411-23606081fc3f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86f42411-c729-4792-9411-23606081fc3f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T13:10:20,448063462-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:10:20,454421904-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86f42411-c729-4792-9411-23606081fc3f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86f42411-c729-4792-9411-23606081fc3f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T13:10:23,214589937-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:10:23,221219160-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86f42411-c729-4792-9411-23606081fc3f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86f42411-c729-4792-9411-23606081fc3f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T13:10:26,178320593-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:10:26,184818739-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86f42411-c729-4792-9411-23606081fc3f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86f42411-c729-4792-9411-23606081fc3f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T13:10:31,878313320-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:10:31,885187746-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86f42411-c729-4792-9411-23606081fc3f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86f42411-c729-4792-9411-23606081fc3f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T13:10:34,987837431-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:10:34,994426703-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86f42411-c729-4792-9411-23606081fc3f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86f42411-c729-4792-9411-23606081fc3f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T13:10:38,238628247-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:10:38,245332799-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86f42411-c729-4792-9411-23606081fc3f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86f42411-c729-4792-9411-23606081fc3f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T13:10:41,308401226-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:10:41,315075427-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86f42411-c729-4792-9411-23606081fc3f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86f42411-c729-4792-9411-23606081fc3f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:10:44,652506247-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:10:44,658890869-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86f42411-c729-4792-9411-23606081fc3f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86f42411-c729-4792-9411-23606081fc3f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:10:48,300956865-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:10:48,307491093-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86f42411-c729-4792-9411-23606081fc3f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86f42411-c729-4792-9411-23606081fc3f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T13:10:51,456660884-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:10:51,464012376-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86f42411-c729-4792-9411-23606081fc3f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86f42411-c729-4792-9411-23606081fc3f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T13:10:54,159681788-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:10:54,166188830-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86f42411-c729-4792-9411-23606081fc3f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86f42411-c729-4792-9411-23606081fc3f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T13:10:56,839363621-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:11:19,678494216-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   196 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [============................] (remaining: 5s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:11:29,712496939-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:11:37,822698230-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:11:45,826405230-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:11:53,767000780-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:12:01,789396353-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:12:09,722838487-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:12:09,734386868-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:12:17,910342086-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:12:17,921838904-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:12:26,009342459-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:12:26,023881584-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:12:33,979609896-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:12:33,991005311-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:12:41,850517134-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:12:41,862114153-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:12:41,870272482-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:12:41,875418553-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:12:41,887032165-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1001613
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T13:12:41,896746229-07:00] INFO: > Run rados bench[0m
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.1
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mlocalhost.localdomain	[2021-05-17T13:12:41,936606050-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:12:41,943412904-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86f42411-c729-4792-9411-23606081fc3f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86f42411-c729-4792-9411-23606081fc3f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:12:43.499+0000 ffff97e44010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:12:43.507+0000 ffff97e44010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:12:43.507+0000 ffff97e44010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:12:43.566293+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-17T20:12:43.566335+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T20:12:43.568553+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:12:43.568553+0000     0       0         0         0         0         0           -           0
2021-05-17T20:12:44.568703+0000     1     255     18245     17990   70.2691   70.2734  0.00807184   0.0136061
2021-05-17T20:12:45.568865+0000     2     255     36978     36723   71.7166   73.1758  0.00202212    0.013495
2021-05-17T20:12:46.569031+0000     3     255     54047     53792   70.0326   66.6758   0.0110844   0.0141728
2021-05-17T20:12:47.569189+0000     4     255     72472     72217   70.5148   71.9727  0.00127979   0.0139595
2021-05-17T20:12:48.569347+0000     5     255     90840     90585   70.7595     71.75  0.00161278   0.0139519
2021-05-17T20:12:49.569514+0000     6     255    109025    108770   70.8035   71.0352  0.00465006   0.0140048
2021-05-17T20:12:50.569736+0000     7     255    127360    127105    70.918   71.6211  0.00156642   0.0139637
2021-05-17T20:12:51.569921+0000     8     255    144828    144573    70.581   68.2344  0.00137606   0.0140468
2021-05-17T20:12:52.570097+0000     9     255    162753    162498   70.5172   70.0195    0.110341   0.0140902
2021-05-17T20:12:53.570267+0000    10     255    178442    178187    69.593   61.2852  0.00296219   0.0143238
2021-05-17T20:12:54.570465+0000    11     255    193896    193641   68.7532   60.3672  0.00493989   0.0144637
2021-05-17T20:12:55.570736+0000    12     255    210795    210540   68.5232   66.0117   0.0106206   0.0145163
2021-05-17T20:12:56.570939+0000    13     256    228055    227799   68.4371    67.418    0.092055   0.0145271
2021-05-17T20:12:57.571195+0000    14     255    244145    243890   68.0373   62.8555  0.00619082   0.0146571
2021-05-17T20:12:58.571417+0000    15     255    259254    258999   67.4352   59.0195  0.00172595   0.0147609
2021-05-17T20:12:59.571594+0000    16     255    276258    276003   67.3711   66.4219  0.00330796   0.0147815
2021-05-17T20:13:00.571814+0000    17     255    293688    293433   67.4123   68.0859  0.00176099   0.0147897
2021-05-17T20:13:01.572034+0000    18     255    309046    308791   66.9993   59.9922  0.00295421   0.0148739
2021-05-17T20:13:02.572194+0000    19     255    325015    324760   66.7556   62.3789   0.0449566   0.0149568
2021-05-17T20:13:03.572397+0000 min lat: 0.00067555 max lat: 0.211599 avg lat: 0.0149923
2021-05-17T20:13:03.572397+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:13:03.572397+0000    20      20    341223    341203   66.6287   64.2305   0.0142027   0.0149923
2021-05-17T20:13:04.572664+0000 Total time run:         20.011
Total writes made:      341223
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     66.6085
Stddev Bandwidth:       4.47374
Max bandwidth (MB/sec): 73.1758
Min bandwidth (MB/sec): 59.0195
Average IOPS:           17051
Stddev IOPS:            1145.28
Max IOPS:               18733
Min IOPS:               15109
Average Latency(s):     0.0149922
Stddev Latency(s):      0.0321155
Max latency(s):         0.211599
Min latency(s):         0.00067555

[1;32mlocalhost.localdomain	[2021-05-17T13:13:04,883921924-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1001613


[1;33mlocalhost.localdomain	[2021-05-17T13:13:04,889501494-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:13:28,104511654-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.22k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:13:28,116516068-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:13:35,966739966-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.22k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:13:35,978581137-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:13:44,286211910-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.22k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:13:44,298380752-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:13:52,543091271-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.22k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:13:52,554838361-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:14:00,916984245-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.22k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:14:00,929309158-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:14:00,938339900-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:14:00,943525653-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:14:00,955762921-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1005037
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T13:14:00,967785998-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-17T13:14:01,006081892-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:14:01,012398568-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86f42411-c729-4792-9411-23606081fc3f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86f42411-c729-4792-9411-23606081fc3f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.V5d81z:/tmp/ceph-asok.V5d81z -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:14:02.528+0000 ffff98f76010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:14:02.536+0000 ffff98f76010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:14:02.536+0000 ffff98f76010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:14:02.555992+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:14:02.555992+0000     0       0         0         0         0         0           -           0
2021-05-17T20:14:03.556146+0000     1     255     31420     31165   121.705   121.738  0.00552225   0.0081513
2021-05-17T20:14:04.556642+0000     2     255     61634     61379   119.835   118.023  0.00896034  0.00830737
2021-05-17T20:14:05.556837+0000     3     255     88999     88744   115.515   106.895     0.00567   0.0086278
2021-05-17T20:14:06.557005+0000     4     255    123452    123197   120.275   134.582  0.00628712  0.00829307
2021-05-17T20:14:07.557182+0000     5     255    152101    151846   118.599    111.91  0.00506681  0.00839889
2021-05-17T20:14:08.557370+0000     6     256    181546    181290   117.998   115.016  0.00396261  0.00845627
2021-05-17T20:14:09.557562+0000     7     255    211546    211291   117.879   117.191   0.0147965  0.00846288
2021-05-17T20:14:10.557876+0000     8     255    240518    240263   117.287   113.172  0.00347393  0.00850733
2021-05-17T20:14:11.558096+0000     9     255    265767    265512   115.211   98.6289  0.00937925  0.00866501
2021-05-17T20:14:12.558325+0000    10     255    291779    291524   113.849   101.609  0.00619265  0.00877049
2021-05-17T20:14:13.558596+0000    11     255    320805    320550   113.803   113.383   0.0076498   0.0087744
2021-05-17T20:14:14.558836+0000 Total time run:       11.744
Total reads made:     341223
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   113.497
Average IOPS:         29055
Stddev IOPS:          2503.46
Max IOPS:             34453
Min IOPS:             25249
Average Latency(s):   0.00879832
Max latency(s):       0.138891
Min latency(s):       0.000303187

[1;32mlocalhost.localdomain	[2021-05-17T13:14:14,886816548-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1005037


[1;33mlocalhost.localdomain	[2021-05-17T13:14:14,893786164-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:14:37,982878501-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.22k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:14:37,998268001-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:14:46,044777481-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.22k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:14:46,058832339-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:14:54,036693682-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.22k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:14:54,050675567-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:15:02,030654559-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.22k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:15:02,044566892-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:15:10,285025109-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.22k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:15:10,298593329-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:15:10,309170969-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:15:10,313295216-07:00][RUNNING][ROUND 2/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:15:10,319810135-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T13:15:10,336077870-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40540\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.411614\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid efb1589e-f375-48fe-adf5-2d9a3b61e441\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid efb1589e-f375-48fe-adf5-2d9a3b61e441\nlast_changed 2021-05-17T13:15:35.454413-0700\ncreated 2021-05-17T13:15:35.454413-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40540/0,v1:10.10.1.2:40541/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.411614 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 cf4f650b-f674-435e-8ca6-8a8b446e272e\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 aa0b928f-62a0-4caa-ae3e-c27b1226c64f\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 42dcc3be-3082-4cce-a812-3cf5012b50fa\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42540\n  w/ user/pass: admin / 9c0022e2-acd7-4b6f-9e17-68351ac3a133\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 13:15:50 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40540
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.411614
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid efb1589e-f375-48fe-adf5-2d9a3b61e441
setting min_mon_release = octopus
epoch 0
fsid efb1589e-f375-48fe-adf5-2d9a3b61e441
last_changed 2021-05-17T13:15:35.454413-0700
created 2021-05-17T13:15:35.454413-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40540/0,v1:10.10.1.2:40541/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.411614 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 cf4f650b-f674-435e-8ca6-8a8b446e272e
0
start osd.0
add osd1 aa0b928f-62a0-4caa-ae3e-c27b1226c64f
1
start osd.1
add osd2 42dcc3be-3082-4cce-a812-3cf5012b50fa
2
start osd.2


restful urls: https://10.10.1.2:42540
  w/ user/pass: admin / 9c0022e2-acd7-4b6f-9e17-68351ac3a133


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T13:15:11.321-0700 7ff69c2141c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:15:11.321-0700 7ff69c2141c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:15:11.337-0700 7f7720b791c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:15:11.337-0700 7f7720b791c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40540,v1:10.10.1.2:40541] --print /tmp/ceph_monmap.411614 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.411614 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.411614 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42540 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.NEOGgxrbhY 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new cf4f650b-f674-435e-8ca6-8a8b446e272e -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDwzqJgOhvQHhAAlcVPb1IaVMsW8VSpSBbNjg== --osd-uuid cf4f650b-f674-435e-8ca6-8a8b446e272e 
2021-05-17T13:15:44.841-0700 7fa05bb1bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:15:44.841-0700 7fa05bb1bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:15:44.841-0700 7fa05bb1bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:15:44.885-0700 7fa05bb1bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new aa0b928f-62a0-4caa-ae3e-c27b1226c64f -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T13:15:45.197-0700 7f469b4f1f00 -1 Falling back to public interface
2021-05-17T13:15:45.209-0700 7f469b4f1f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDxzqJgPybBCxAA0JRwtfqR2oDkdY2jZVftDQ== --osd-uuid aa0b928f-62a0-4caa-ae3e-c27b1226c64f 
2021-05-17T13:15:45.521-0700 7f4d6c909f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:15:45.521-0700 7f4d6c909f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:15:45.521-0700 7f4d6c909f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:15:45.605-0700 7f4d6c909f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 42dcc3be-3082-4cce-a812-3cf5012b50fa -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T13:15:45.933-0700 7fa76185df00 -1 Falling back to public interface
2021-05-17T13:15:45.949-0700 7fa76185df00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDxzqJg1S65NxAArJdstdmrbDBUPRLnp+Bx/A== --osd-uuid 42dcc3be-3082-4cce-a812-3cf5012b50fa 
2021-05-17T13:15:46.261-0700 7f789dff1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:15:46.261-0700 7f789dff1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:15:46.261-0700 7f789dff1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:15:46.309-0700 7f789dff1f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T13:15:46.601-0700 7f2f69238f00 -1 Falling back to public interface
2021-05-17T13:15:46.617-0700 7f2f69238f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T13:15:50,587021390-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:15:50,593594669-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T13:15:50,677110241-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:15:50,684030634-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dcfcae6f-e98a-43bd-8eea-080a3576be21', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dcfcae6f-e98a-43bd-8eea-080a3576be21 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T13:15:53,563299714-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:15:53,569731369-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dcfcae6f-e98a-43bd-8eea-080a3576be21', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dcfcae6f-e98a-43bd-8eea-080a3576be21 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T13:15:56,217289620-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:15:56,223684428-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dcfcae6f-e98a-43bd-8eea-080a3576be21', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dcfcae6f-e98a-43bd-8eea-080a3576be21 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T13:15:59,123992458-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:15:59,130264606-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dcfcae6f-e98a-43bd-8eea-080a3576be21', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dcfcae6f-e98a-43bd-8eea-080a3576be21 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T13:16:04,697740362-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:16:04,704163900-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dcfcae6f-e98a-43bd-8eea-080a3576be21', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dcfcae6f-e98a-43bd-8eea-080a3576be21 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T13:16:08,411962707-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:16:08,418610654-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dcfcae6f-e98a-43bd-8eea-080a3576be21', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dcfcae6f-e98a-43bd-8eea-080a3576be21 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T13:16:12,078229343-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:16:12,085670880-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dcfcae6f-e98a-43bd-8eea-080a3576be21', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dcfcae6f-e98a-43bd-8eea-080a3576be21 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T13:16:15,344368116-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:16:15,351106754-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dcfcae6f-e98a-43bd-8eea-080a3576be21', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dcfcae6f-e98a-43bd-8eea-080a3576be21 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:16:19,256201036-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:16:19,263581377-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dcfcae6f-e98a-43bd-8eea-080a3576be21', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dcfcae6f-e98a-43bd-8eea-080a3576be21 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:16:22,952481613-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:16:22,958879542-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dcfcae6f-e98a-43bd-8eea-080a3576be21', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dcfcae6f-e98a-43bd-8eea-080a3576be21 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T13:16:26,192680488-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:16:26,198967788-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dcfcae6f-e98a-43bd-8eea-080a3576be21', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dcfcae6f-e98a-43bd-8eea-080a3576be21 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T13:16:28,899846923-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:16:28,906268586-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dcfcae6f-e98a-43bd-8eea-080a3576be21', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dcfcae6f-e98a-43bd-8eea-080a3576be21 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  145 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  145 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.97   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.06   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.97   70      up          osd.2  
                       TOTAL  300 GiB  147 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.97/1.06  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T13:16:31,558003285-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:16:54,567694263-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [============................] (remaining: 5s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:17:02,595193532-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:17:10,551828083-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:17:18,668350710-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:17:26,506355169-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:17:34,512708123-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:17:42,508470200-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:17:50,533346603-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:17:50,547404026-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:17:58,581050122-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:17:58,595335329-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:18:06,553467867-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:18:06,567512448-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:18:14,475775248-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:18:14,490049892-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:18:22,664749279-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:18:22,679002365-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:18:22,690335069-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:18:22,696795034-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:18:22,710761301-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1018155
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T13:18:22,723392311-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T13:18:22,764524963-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:18:22,771105053-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dcfcae6f-e98a-43bd-8eea-080a3576be21', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dcfcae6f-e98a-43bd-8eea-080a3576be21 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7 -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:18:24.293+0000 ffffa2d89010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:18:24.301+0000 ffffa2d89010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:18:24.301+0000 ffffa2d89010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:18:24.317484+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-17T20:18:24.317529+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T20:18:24.319825+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:18:24.319825+0000     0       0         0         0         0         0           -           0
2021-05-17T20:18:25.319987+0000     1     255     19215     18960   74.0592   74.0625  0.00288861   0.0128742
2021-05-17T20:18:26.320157+0000     2     255     37119     36864   71.9923   69.9375  0.00217605   0.0135067
2021-05-17T20:18:27.320322+0000     3     255     54478     54223   70.5939   67.8086  0.00124122    0.013981
2021-05-17T20:18:28.320482+0000     4     255     72140     71885   70.1907   68.9922  0.00166216   0.0140418
2021-05-17T20:18:29.320690+0000     5     255     89251     88996   69.5177   66.8398  0.00152834   0.0142132
2021-05-17T20:18:30.320872+0000     6     255    106128    105873   68.9171   65.9258    0.114637   0.0143754
2021-05-17T20:18:31.321237+0000     7     255    122471    122216   68.1883   63.8398  0.00581754   0.0145773
2021-05-17T20:18:32.321469+0000     8     255    139852    139597   68.1496   67.8945  0.00480451   0.0145703
2021-05-17T20:18:33.321647+0000     9     255    154139    153884   66.7773   55.8086   0.0105399   0.0149482
2021-05-17T20:18:34.321819+0000    10     255    171893    171638   67.0335   69.3516  0.00602939   0.0148632
2021-05-17T20:18:35.321989+0000    11     255    188940    188685   66.9922   66.5898   0.0937545    0.014864
2021-05-17T20:18:36.322141+0000    12     255    206216    205961   67.0323   67.4844  0.00179863   0.0148406
2021-05-17T20:18:37.322297+0000    13     255    222965    222710    66.908   65.4258  0.00475735   0.0148959
2021-05-17T20:18:38.322458+0000    14     256    238244    237988    66.391   59.6797 0.000747577    0.014994
2021-05-17T20:18:39.322585+0000    15     255    253240    252985   65.8699    58.582  0.00184735   0.0151031
2021-05-17T20:18:40.322737+0000    16     255    269486    269231   65.7187   63.4609  0.00169062   0.0151563
2021-05-17T20:18:41.322893+0000    17     255    283475    283220   65.0668   54.6445  0.00423184   0.0153096
2021-05-17T20:18:42.323037+0000    18     255    299004    298749   64.8215   60.6602  0.00523621   0.0153537
2021-05-17T20:18:43.323215+0000    19     255    314709    314454   64.6381   61.3477  0.00247215   0.0154225
2021-05-17T20:18:44.323373+0000 min lat: 0.000669234 max lat: 0.272033 avg lat: 0.0155232
2021-05-17T20:18:44.323373+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:18:44.323373+0000    20     253    329141    328888   64.2249   56.3828    0.101437   0.0155232
2021-05-17T20:18:45.323648+0000 Total time run:         20.0719
Total writes made:      329141
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     64.0551
Stddev Bandwidth:       5.28381
Max bandwidth (MB/sec): 74.0625
Min bandwidth (MB/sec): 54.6445
Average IOPS:           16398
Stddev IOPS:            1352.65
Max IOPS:               18960
Min IOPS:               13989
Average Latency(s):     0.015594
Stddev Latency(s):      0.0351625
Max latency(s):         0.272033
Min latency(s):         0.000669234

[1;32mlocalhost.localdomain	[2021-05-17T13:18:45,635213236-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1018155


[1;33mlocalhost.localdomain	[2021-05-17T13:18:45,642199730-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:19:08,439357574-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.14k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:19:08,453668450-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:19:16,731122983-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.14k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:19:16,745421602-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:19:24,726600797-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.14k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:19:24,740902012-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:19:32,951053311-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.14k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:19:32,965199384-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:19:41,011757765-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.14k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:19:41,026265849-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:19:41,037509213-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:19:41,044219375-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:19:41,058886592-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1021507
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T13:19:41,071507310-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T13:19:41,110769787-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:19:41,117390534-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dcfcae6f-e98a-43bd-8eea-080a3576be21', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dcfcae6f-e98a-43bd-8eea-080a3576be21 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tUnHu7:/tmp/ceph-asok.tUnHu7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:19:42.758+0000 ffff99129010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:19:43.062+0000 ffff99129010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:19:43.062+0000 ffff99129010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:19:43.085501+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:19:43.085501+0000     0       0         0         0         0         0           -           0
2021-05-17T20:19:44.085659+0000     1     256     27122     26866   104.914   104.945 0.000377494  0.00938759
2021-05-17T20:19:45.085857+0000     2     255     49917     49662   96.9722   89.0469   0.0205209   0.0102472
2021-05-17T20:19:46.086023+0000     3     255     76910     76655   99.7893   105.441  0.00801171   0.0099855
2021-05-17T20:19:47.086211+0000     4     255    104998    104743   102.266   109.719   0.0173782  0.00974706
2021-05-17T20:19:48.086379+0000     5     255    130057    129802   101.387   97.8867    0.019877  0.00983699
2021-05-17T20:19:49.086590+0000     6     255    156462    156207   101.677   103.145   0.0066646   0.0098163
2021-05-17T20:19:50.086835+0000     7     256    182761    182505   101.823   102.727   0.0169896  0.00979972
2021-05-17T20:19:51.087011+0000     8     255    206305    206050    100.59   91.9727   0.0115768   0.0099231
2021-05-17T20:19:52.087216+0000     9     255    233073    232818   101.029   104.562   0.0157047  0.00987971
2021-05-17T20:19:53.087410+0000    10     255    258044    257789   100.678    97.543   0.0127389  0.00991764
2021-05-17T20:19:54.087969+0000    11     255    288065    287810   102.181    117.27  0.00213411    0.009768
2021-05-17T20:19:55.088262+0000    12     255    311746    311491   101.372   92.5039  0.00940785  0.00985106
2021-05-17T20:19:56.088490+0000 Total time run:       12.7195
Total reads made:     329141
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   101.081
Average IOPS:         25876
Stddev IOPS:          2060.45
Max IOPS:             30021
Min IOPS:             22796
Average Latency(s):   0.00987846
Max latency(s):       0.141091
Min latency(s):       0.00029654

[1;32mlocalhost.localdomain	[2021-05-17T13:19:56,401037666-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1021507


[1;33mlocalhost.localdomain	[2021-05-17T13:19:56,408134039-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:20:19,389218608-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.14k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:20:19,403811802-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:20:27,637223110-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.14k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:20:27,651733744-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:20:35,769435875-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.14k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:20:35,783897421-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:20:43,767243445-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.14k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:20:43,781543954-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:20:51,776668869-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.14k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:20:51,791381378-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:20:51,802674790-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:20:51,806501041-07:00][RUNNING][ROUND 3/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:20:51,812901956-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T13:20:51,828892923-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40307\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.414915\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 23159ffb-83ae-45b2-90dd-348c146f7961\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 23159ffb-83ae-45b2-90dd-348c146f7961\nlast_changed 2021-05-17T13:21:17.744293-0700\ncreated 2021-05-17T13:21:17.744293-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40307/0,v1:10.10.1.2:40308/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.414915 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 e2f59241-9b56-4e93-8eff-0aef16705bb0\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 46ce9cfe-2a58-4ff9-ac0d-9631235fad8c\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 3b58e78e-acbc-4a46-a066-6b233e903ca6\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42307\n  w/ user/pass: admin / 842e6bab-212f-40ae-903f-66e88fa08cea\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:21:32 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40307
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.414915
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 23159ffb-83ae-45b2-90dd-348c146f7961
setting min_mon_release = octopus
epoch 0
fsid 23159ffb-83ae-45b2-90dd-348c146f7961
last_changed 2021-05-17T13:21:17.744293-0700
created 2021-05-17T13:21:17.744293-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40307/0,v1:10.10.1.2:40308/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.414915 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 e2f59241-9b56-4e93-8eff-0aef16705bb0
0
start osd.0
add osd1 46ce9cfe-2a58-4ff9-ac0d-9631235fad8c
1
start osd.1
add osd2 3b58e78e-acbc-4a46-a066-6b233e903ca6
2
start osd.2


restful urls: https://10.10.1.2:42307
  w/ user/pass: admin / 842e6bab-212f-40ae-903f-66e88fa08cea


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T13:20:52.816-0700 7fd1cee081c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:20:52.816-0700 7fd1cee081c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:20:52.832-0700 7f8f48bdb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:20:52.832-0700 7f8f48bdb1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40307,v1:10.10.1.2:40308] --print /tmp/ceph_monmap.414915 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.414915 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.414915 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42307 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.5YHuN1xyrZ 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e2f59241-9b56-4e93-8eff-0aef16705bb0 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBG0KJgdN9nIRAAzt32QRxUuNTXEliMf5PtPQ== --osd-uuid e2f59241-9b56-4e93-8eff-0aef16705bb0 
2021-05-17T13:21:26.916-0700 7f7504351f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:21:26.916-0700 7f7504351f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:21:26.916-0700 7f7504351f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:21:26.972-0700 7f7504351f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 46ce9cfe-2a58-4ff9-ac0d-9631235fad8c -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T13:21:27.300-0700 7fbf3fd75f00 -1 Falling back to public interface
2021-05-17T13:21:27.312-0700 7fbf3fd75f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBH0KJgGXrtERAAOTQaqyhCWNAKzOVlWzIWeQ== --osd-uuid 46ce9cfe-2a58-4ff9-ac0d-9631235fad8c 
2021-05-17T13:21:27.656-0700 7f3b1f09df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:21:27.656-0700 7f3b1f09df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:21:27.656-0700 7f3b1f09df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:21:27.716-0700 7f3b1f09df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3b58e78e-acbc-4a46-a066-6b233e903ca6 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T13:21:28.068-0700 7f7b16c12f00 -1 Falling back to public interface
2021-05-17T13:21:28.080-0700 7f7b16c12f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBI0KJgJCIhBBAAzNCkiQKFwf2Id/DW5XA6xw== --osd-uuid 3b58e78e-acbc-4a46-a066-6b233e903ca6 
2021-05-17T13:21:28.404-0700 7fe5a5f3df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:21:28.408-0700 7fe5a5f3df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:21:28.408-0700 7fe5a5f3df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:21:28.496-0700 7fe5a5f3df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T13:21:28.788-0700 7f33be466f00 -1 Falling back to public interface
2021-05-17T13:21:28.804-0700 7f33be466f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T13:21:32,731470456-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:21:32,738145767-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T13:21:32,820520302-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:21:32,826973276-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afe325b9-8248-419f-8415-ba57eff201c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afe325b9-8248-419f-8415-ba57eff201c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T13:21:35,762702435-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:21:35,770688504-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afe325b9-8248-419f-8415-ba57eff201c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afe325b9-8248-419f-8415-ba57eff201c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T13:21:38,621914970-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:21:38,628440796-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afe325b9-8248-419f-8415-ba57eff201c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afe325b9-8248-419f-8415-ba57eff201c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T13:21:41,458849703-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:21:41,465188237-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afe325b9-8248-419f-8415-ba57eff201c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afe325b9-8248-419f-8415-ba57eff201c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T13:21:48,975191851-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:21:48,981716732-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afe325b9-8248-419f-8415-ba57eff201c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afe325b9-8248-419f-8415-ba57eff201c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T13:21:51,836766402-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:21:51,843455365-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afe325b9-8248-419f-8415-ba57eff201c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afe325b9-8248-419f-8415-ba57eff201c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T13:21:55,563940818-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:21:55,570323850-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afe325b9-8248-419f-8415-ba57eff201c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afe325b9-8248-419f-8415-ba57eff201c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T13:21:59,159414900-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:21:59,165870603-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afe325b9-8248-419f-8415-ba57eff201c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afe325b9-8248-419f-8415-ba57eff201c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:22:02,457764294-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:22:02,464336522-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afe325b9-8248-419f-8415-ba57eff201c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afe325b9-8248-419f-8415-ba57eff201c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:22:05,674819771-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:22:05,681110011-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afe325b9-8248-419f-8415-ba57eff201c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afe325b9-8248-419f-8415-ba57eff201c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T13:22:09,517827460-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:22:09,524245283-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afe325b9-8248-419f-8415-ba57eff201c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afe325b9-8248-419f-8415-ba57eff201c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T13:22:12,343682369-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:22:12,350100821-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afe325b9-8248-419f-8415-ba57eff201c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afe325b9-8248-419f-8415-ba57eff201c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T13:22:15,071709542-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:22:37,961755327-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   206 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (5s)
      [=========...................] (remaining: 10s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:22:45,941089262-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:22:53,874293185-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:23:01,834336721-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:23:09,869089702-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:23:18,089808620-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:23:26,143805702-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:23:34,099420615-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   238 KiB used, 300 GiB / 300 GiB avail
    pgs:     1.042% pgs not active
             190 active+clean
             2   activating
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:23:34,113592785-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:23:42,093569394-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:23:42,107749020-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:23:50,080401273-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:23:50,094633963-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:23:58,209670283-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:23:58,225194572-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:24:06,211405612-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:24:06,225751871-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:24:06,237309727-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:24:06,243987286-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:24:06,259018426-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1034816
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T13:24:06,271194508-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-17T13:24:06,311014310-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:24:06,317670460-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afe325b9-8248-419f-8415-ba57eff201c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afe325b9-8248-419f-8415-ba57eff201c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:24:07.959+0000 ffff86cfb010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:24:07.967+0000 ffff86cfb010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:24:07.967+0000 ffff86cfb010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:24:07.984422+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-17T20:24:07.984460+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T20:24:07.986853+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:24:07.986853+0000     0       0         0         0         0         0           -           0
2021-05-17T20:24:08.987020+0000     1     255     18317     18062   70.5505   70.5547  0.00196301   0.0133352
2021-05-17T20:24:09.987172+0000     2     256     36012     35756   69.8285   69.1172  0.00166717   0.0139086
2021-05-17T20:24:10.987326+0000     3     255     54834     54579   71.0577   73.5273  0.00511161   0.0138534
2021-05-17T20:24:11.987501+0000     4     255     73010     72755   71.0402        71  0.00286481   0.0139369
2021-05-17T20:24:12.987742+0000     5     255     91166     90911   71.0131   70.9219    0.121567   0.0139079
2021-05-17T20:24:13.987933+0000     6     255    109007    108752   70.7906   69.6914  0.00178593   0.0139885
2021-05-17T20:24:14.988158+0000     7     255    127182    126927   70.8177   70.9961  0.00506743   0.0139973
2021-05-17T20:24:15.988357+0000     8     256    145344    145088   70.8314   70.9414  0.00269519   0.0140153
2021-05-17T20:24:16.988538+0000     9     255    163916    163661    71.021   72.5508  0.00212137   0.0140011
2021-05-17T20:24:17.988719+0000    10     255    182062    181807   71.0059   70.8828   0.0028219   0.0139953
2021-05-17T20:24:18.989102+0000    11     255    199124    198869   70.6073   66.6484  0.00538135   0.0141283
2021-05-17T20:24:19.989293+0000    12     255    215131    214876    69.933   62.5273  0.00293384   0.0142408
2021-05-17T20:24:20.989500+0000    13     255    233702    233447   70.1326    72.543   0.0021144   0.0142122
2021-05-17T20:24:21.989781+0000    14     255    252108    251853   70.2573   71.8984   0.0054383   0.0141856
2021-05-17T20:24:22.989976+0000    15     256    268035    267779   69.7201   62.2109 0.000850509   0.0142837
2021-05-17T20:24:23.990133+0000    16     255    283759    283504   69.2011   61.4258  0.00274568   0.0143969
2021-05-17T20:24:24.990335+0000    17     256    301007    300751   69.0927   67.3711  0.00161199   0.0144176
2021-05-17T20:24:25.990543+0000    18     255    315884    315629   68.4822   58.1172  0.00508846   0.0145493
2021-05-17T20:24:26.990760+0000    19     255    332797    332542   68.3543   66.0664  0.00174153   0.0145841
2021-05-17T20:24:27.990941+0000 min lat: 0.000690327 max lat: 0.241711 avg lat: 0.0146659
2021-05-17T20:24:27.990941+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:24:27.990941+0000    20     200    348397    348197   67.9937   61.1523    0.112961   0.0146659
2021-05-17T20:24:28.991224+0000 Total time run:         20.0779
Total writes made:      348397
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     67.7821
Stddev Bandwidth:       4.5792
Max bandwidth (MB/sec): 73.5273
Min bandwidth (MB/sec): 58.1172
Average IOPS:           17352
Stddev IOPS:            1172.28
Max IOPS:               18823
Min IOPS:               14878
Average Latency(s):     0.0147126
Stddev Latency(s):      0.0334216
Max latency(s):         0.241711
Min latency(s):         0.000690327

[1;32mlocalhost.localdomain	[2021-05-17T13:24:29,317184267-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1034816


[1;33mlocalhost.localdomain	[2021-05-17T13:24:29,324193291-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:24:52,268230246-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.40k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:24:52,284355673-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:25:00,244659327-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.40k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:25:00,259056341-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:25:08,268031496-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.40k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:25:08,281739953-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:25:16,337959147-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.40k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:25:16,352413366-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:25:24,436527753-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.40k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:25:24,450893202-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:25:24,462073130-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:25:24,468710826-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:25:24,483855207-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1038159
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T13:25:24,496355074-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-17T13:25:24,535677454-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:25:24,542173692-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'afe325b9-8248-419f-8415-ba57eff201c1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid afe325b9-8248-419f-8415-ba57eff201c1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tbsz8b:/tmp/ceph-asok.tbsz8b -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:25:26.041+0000 ffffa12d3010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:25:26.049+0000 ffffa12d3010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:25:26.053+0000 ffffa12d3010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:25:26.070267+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:25:26.070267+0000     0       0         0         0         0         0           -           0
2021-05-17T20:25:27.070442+0000     1     255     24031     23776   92.8467    92.875  0.00317467   0.0106593
2021-05-17T20:25:28.070611+0000     2     255     54372     54117   105.672    118.52  0.00886635  0.00942279
2021-05-17T20:25:29.070803+0000     3     255     80961     80706   105.063   103.863   0.0140392  0.00947846
2021-05-17T20:25:30.070969+0000     4     256    105144    104888   102.408   94.4609 0.000399008  0.00956811
2021-05-17T20:25:31.071136+0000     5     255    137822    137567   107.453   127.652  0.00271243  0.00928424
2021-05-17T20:25:32.071313+0000     6     255    166548    166293   108.242   112.211   0.0125426  0.00921835
2021-05-17T20:25:33.071483+0000     7     255    194941    194686   108.621    110.91  0.00794614  0.00918839
2021-05-17T20:25:34.071651+0000     8     255    224854    224599   109.647   116.848   0.0072355   0.0091051
2021-05-17T20:25:35.071826+0000     9     255    252184    251929   109.324   106.758   0.0158277  0.00913125
2021-05-17T20:25:36.072006+0000    10     256    275315    275059   107.425   90.3516  0.00674883  0.00921902
2021-05-17T20:25:37.072164+0000    11     255    297795    297540   105.641   87.8164  0.00522689  0.00945233
2021-05-17T20:25:38.072459+0000    12     255    329436    329181   107.135   123.598   0.0140401  0.00931979
2021-05-17T20:25:39.072815+0000 Total time run:       12.6795
Total reads made:     348397
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   107.333
Average IOPS:         27477
Stddev IOPS:          3439.5
Max IOPS:             32679
Min IOPS:             22481
Average Latency(s):   0.00930491
Max latency(s):       0.176489
Min latency(s):       0.000307972

[1;32mlocalhost.localdomain	[2021-05-17T13:25:39,390336595-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1038159


[1;33mlocalhost.localdomain	[2021-05-17T13:25:39,397670140-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:26:02,455525883-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.40k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:26:02,470368248-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:26:10,571088637-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.40k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:26:10,585130031-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:26:18,534979534-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.40k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:26:18,549560285-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:26:26,945699108-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.40k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:26:26,960390502-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:26:35,146488676-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 348.40k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:26:35,160874274-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:26:35,172109622-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:26:35,176208273-07:00][RUNNING][ROUND 4/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:26:35,182825416-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T13:26:35,199162875-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40000\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.418489\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 40d1bbce-f934-4e99-b44a-aceb7d0c7440\nsetting min_mon_release = octopus\nepoch 0\nfsid 40d1bbce-f934-4e99-b44a-aceb7d0c7440\nlast_changed 2021-05-17T13:27:00.563880-0700\ncreated 2021-05-17T13:27:00.563880-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40000/0,v1:10.10.1.2:40001/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.418489 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 63562ff7-564e-4a58-b035-b13bdca97629\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 e30efb44-7fda-4a99-b91b-cc358a03f273\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 567ff261-df9b-4f72-9a33-cf90e0a03fb5\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42000\n  w/ user/pass: admin / 4c43d4b8-1adf-4954-88aa-5e348a64cb2b\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:27:15 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40000
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.418489
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 40d1bbce-f934-4e99-b44a-aceb7d0c7440
setting min_mon_release = octopus
epoch 0
fsid 40d1bbce-f934-4e99-b44a-aceb7d0c7440
last_changed 2021-05-17T13:27:00.563880-0700
created 2021-05-17T13:27:00.563880-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40000/0,v1:10.10.1.2:40001/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.418489 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 63562ff7-564e-4a58-b035-b13bdca97629
0
start osd.0
add osd1 e30efb44-7fda-4a99-b91b-cc358a03f273
1
start osd.1
add osd2 567ff261-df9b-4f72-9a33-cf90e0a03fb5
2
start osd.2


restful urls: https://10.10.1.2:42000
  w/ user/pass: admin / 4c43d4b8-1adf-4954-88aa-5e348a64cb2b


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T13:26:36.187-0700 7f49974861c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:26:36.187-0700 7f49974861c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:26:36.203-0700 7f2efdfc31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:26:36.203-0700 7f2efdfc31c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40000,v1:10.10.1.2:40001] --print /tmp/ceph_monmap.418489 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.418489 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.418489 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42000 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.Irid5R7iyv 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 63562ff7-564e-4a58-b035-b13bdca97629 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCd0aJgYHVtFxAAzi5NzniAhCZgYOQh1cmMlA== --osd-uuid 63562ff7-564e-4a58-b035-b13bdca97629 
2021-05-17T13:27:09.767-0700 7fe0ff42cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:27:09.767-0700 7fe0ff42cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:27:09.767-0700 7fe0ff42cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:27:09.819-0700 7fe0ff42cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e30efb44-7fda-4a99-b91b-cc358a03f273 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T13:27:10.055-0700 7fb07cab8f00 -1 Falling back to public interface
2021-05-17T13:27:10.067-0700 7fb07cab8f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCe0aJgF3dsAxAAvDPk1RXSixEnelv+EVy/DQ== --osd-uuid e30efb44-7fda-4a99-b91b-cc358a03f273 
2021-05-17T13:27:10.359-0700 7ff250c02f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:27:10.363-0700 7ff250c02f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:27:10.363-0700 7ff250c02f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:27:10.415-0700 7ff250c02f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 567ff261-df9b-4f72-9a33-cf90e0a03fb5 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T13:27:10.687-0700 7fade3c77f00 -1 Falling back to public interface
2021-05-17T13:27:10.699-0700 7fade3c77f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCe0aJg/BJpKBAAPHkcXDDnMeGZ0BqF8F8weg== --osd-uuid 567ff261-df9b-4f72-9a33-cf90e0a03fb5 
2021-05-17T13:27:11.003-0700 7f393679cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:27:11.007-0700 7f393679cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:27:11.007-0700 7f393679cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:27:11.059-0700 7f393679cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T13:27:11.367-0700 7fb501701f00 -1 Falling back to public interface
2021-05-17T13:27:11.379-0700 7fb501701f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T13:27:15,270776217-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:27:15,277399586-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T13:27:15,357326425-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:27:15,364048121-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd306eeb0-1135-4e3e-a4c1-8320348c56ce', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d306eeb0-1135-4e3e-a4c1-8320348c56ce --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T13:27:18,076322097-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:27:18,083031912-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd306eeb0-1135-4e3e-a4c1-8320348c56ce', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d306eeb0-1135-4e3e-a4c1-8320348c56ce --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T13:27:20,879115810-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:27:20,885240825-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd306eeb0-1135-4e3e-a4c1-8320348c56ce', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d306eeb0-1135-4e3e-a4c1-8320348c56ce --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T13:27:23,745642750-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:27:23,752966769-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd306eeb0-1135-4e3e-a4c1-8320348c56ce', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d306eeb0-1135-4e3e-a4c1-8320348c56ce --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T13:27:29,521359869-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:27:29,527841284-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd306eeb0-1135-4e3e-a4c1-8320348c56ce', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d306eeb0-1135-4e3e-a4c1-8320348c56ce --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T13:27:33,188330002-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:27:33,194706910-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd306eeb0-1135-4e3e-a4c1-8320348c56ce', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d306eeb0-1135-4e3e-a4c1-8320348c56ce --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T13:27:36,411823865-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:27:36,418290743-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd306eeb0-1135-4e3e-a4c1-8320348c56ce', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d306eeb0-1135-4e3e-a4c1-8320348c56ce --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T13:27:40,026458523-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:27:40,032834726-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd306eeb0-1135-4e3e-a4c1-8320348c56ce', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d306eeb0-1135-4e3e-a4c1-8320348c56ce --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:27:43,375456003-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:27:43,382035970-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd306eeb0-1135-4e3e-a4c1-8320348c56ce', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d306eeb0-1135-4e3e-a4c1-8320348c56ce --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:27:46,574908884-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:27:46,581309961-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd306eeb0-1135-4e3e-a4c1-8320348c56ce', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d306eeb0-1135-4e3e-a4c1-8320348c56ce --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T13:27:50,078543443-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:27:50,085083579-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd306eeb0-1135-4e3e-a4c1-8320348c56ce', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d306eeb0-1135-4e3e-a4c1-8320348c56ce --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T13:27:52,765326248-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:27:52,771838914-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd306eeb0-1135-4e3e-a4c1-8320348c56ce', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d306eeb0-1135-4e3e-a4c1-8320348c56ce --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T13:27:55,549124549-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:28:18,356308312-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:28:26,209041727-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:28:34,317397230-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 28s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:28:42,255823368-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 48s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:28:50,274579795-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 55s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:28:58,087637148-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 70s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:29:06,180234030-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 80s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:29:14,220616773-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   235 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 41s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:29:22,160667975-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:29:22,174852240-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:29:30,249952676-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:29:30,264460275-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:29:38,107602431-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:29:38,122207397-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:29:46,206354302-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:29:46,220715447-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:29:54,170137179-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:29:54,184682870-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:29:54,195754181-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:29:54,202550006-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:29:54,217384706-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1051761
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T13:29:54,229852619-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-17T13:29:54,270925232-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:29:54,277373058-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd306eeb0-1135-4e3e-a4c1-8320348c56ce', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d306eeb0-1135-4e3e-a4c1-8320348c56ce --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:29:55.862+0000 ffff95a7a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:29:55.870+0000 ffff95a7a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:29:55.870+0000 ffff95a7a010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:29:55.890845+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-17T20:29:55.890902+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T20:29:55.893176+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:29:55.893176+0000     0       0         0         0         0         0           -           0
2021-05-17T20:29:56.893377+0000     1     255     19399     19144   74.7778   74.7812  0.00867825   0.0131573
2021-05-17T20:29:57.893565+0000     2     255     39079     38824   75.8192    76.875  0.00252902    0.013036
2021-05-17T20:29:58.893799+0000     3     255     58743     58488   76.1443   76.8125  0.00278506   0.0129293
2021-05-17T20:29:59.893967+0000     4     255     77163     76908   75.0935   71.9531  0.00393545   0.0131909
2021-05-17T20:30:00.894154+0000     5     255     95624     95369   74.4947   72.1133   0.0456538   0.0133419
2021-05-17T20:30:01.894322+0000     6     255    114150    113895   74.1381   72.3672  0.00347381   0.0134103
2021-05-17T20:30:02.894493+0000     7     255    132846    132591   73.9782   73.0312  0.00513438   0.0134389
2021-05-17T20:30:03.894723+0000     8     255    151816    151561   73.9915   74.1016  0.00433272   0.0134367
2021-05-17T20:30:04.894909+0000     9     255    170992    170737   74.0916   74.9062  0.00263496   0.0134254
2021-05-17T20:30:05.895067+0000    10     255    189415    189160   73.8778   71.9648    0.132668   0.0134423
2021-05-17T20:30:06.895218+0000    11     255    206067    205812   73.0741   65.0469  0.00248235   0.0136259
2021-05-17T20:30:07.895377+0000    12     256    223737    223481   72.7353   69.0195  0.00339433   0.0137027
2021-05-17T20:30:08.895542+0000    13     255    239461    239206   71.8646   61.4258   0.0544769    0.013872
2021-05-17T20:30:09.895812+0000    14     256    252950    252694   70.4936   52.6875 0.000900153   0.0141216
2021-05-17T20:30:10.896001+0000    15     255    268963    268708   69.9636   62.5547   0.0153245   0.0142614
2021-05-17T20:30:11.896220+0000    16     255    284171    283916   69.3029   59.4062   0.0113349   0.0143974
2021-05-17T20:30:12.896451+0000    17     255    300292    300037   68.9297   62.9727  0.00396427   0.0144782
2021-05-17T20:30:13.896728+0000    18     255    314176    313921   68.1124   54.2344    0.149499   0.0146285
2021-05-17T20:30:14.896955+0000    19     256    330985    330729   67.9823   65.6562  0.00117984   0.0146603
2021-05-17T20:30:15.897174+0000 min lat: 0.00070788 max lat: 0.199856 avg lat: 0.0147187
2021-05-17T20:30:15.897174+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:30:15.897174+0000    20     256    345881    345625   67.4919   58.1875  0.00133727   0.0147187
2021-05-17T20:30:16.897435+0000 Total time run:         20.1055
Total writes made:      345881
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     67.2003
Stddev Bandwidth:       7.5484
Max bandwidth (MB/sec): 76.875
Min bandwidth (MB/sec): 52.6875
Average IOPS:           17203
Stddev IOPS:            1932.39
Max IOPS:               19680
Min IOPS:               13488
Average Latency(s):     0.0148482
Stddev Latency(s):      0.026489
Max latency(s):         0.221514
Min latency(s):         0.00070788

[1;32mlocalhost.localdomain	[2021-05-17T13:30:17,195285510-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1051761


[1;33mlocalhost.localdomain	[2021-05-17T13:30:17,203049209-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:30:40,460395266-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 345.88k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:30:40,474799828-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:30:48,836834535-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 345.88k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:30:48,851562597-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:30:57,061500074-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 345.88k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:30:57,077945433-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:31:05,101665456-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 345.88k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:31:05,115758131-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:31:13,107640156-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 345.88k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:31:13,122336833-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:31:13,133952242-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:31:13,140739620-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:31:13,155867246-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1055064
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T13:31:13,168343036-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-17T13:31:13,206929742-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:31:13,213421585-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd306eeb0-1135-4e3e-a4c1-8320348c56ce', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d306eeb0-1135-4e3e-a4c1-8320348c56ce --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FsiuJQ:/tmp/ceph-asok.FsiuJQ -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:31:15.084+0000 ffff89ad7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:31:15.100+0000 ffff89ad7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:31:15.100+0000 ffff89ad7010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:31:15.120399+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:31:15.120399+0000     0       0         0         0         0         0           -           0
2021-05-17T20:31:16.120577+0000     1     255     30832     30577   119.404   119.441   0.0072059  0.00830069
2021-05-17T20:31:17.120804+0000     2     255     61849     61594   120.268    121.16  0.00470501  0.00827503
2021-05-17T20:31:18.121009+0000     3     255     85908     85653     111.5   93.9805   0.0157667  0.00893409
2021-05-17T20:31:19.121253+0000     4     255    118553    118298   115.497    127.52   0.0032843  0.00863224
2021-05-17T20:31:20.121587+0000     5     255    150613    150358   117.436   125.234  0.00589282   0.0084959
2021-05-17T20:31:21.122043+0000     6     255    180508    180253   117.317   116.777  0.00816268  0.00850676
2021-05-17T20:31:22.122277+0000     7     255    212458    212203   118.383   124.805  0.00344133  0.00842899
2021-05-17T20:31:23.122463+0000     8     255    245766    245511   119.846   130.109   0.0010011   0.0083211
2021-05-17T20:31:24.122689+0000     9     256    265234    264978   114.977    76.043 0.000366543  0.00867054
2021-05-17T20:31:25.122920+0000    10     255    290656    290401   113.408   99.3086  0.00256541  0.00880082
2021-05-17T20:31:26.123110+0000    11     256    319452    319196   113.322    112.48  0.00318861  0.00881001
2021-05-17T20:31:27.123447+0000 Total time run:       11.9364
Total reads made:     345881
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   113.192
Average IOPS:         28977
Stddev IOPS:          4305.72
Max IOPS:             33308
Min IOPS:             19467
Average Latency(s):   0.00882121
Max latency(s):       0.165926
Min latency(s):       0.000311228

[1;32mlocalhost.localdomain	[2021-05-17T13:31:27,416804588-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1055064


[1;33mlocalhost.localdomain	[2021-05-17T13:31:27,424314474-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:31:50,290669293-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 345.88k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:31:50,305042861-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:31:58,271600428-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 345.88k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:31:58,286110039-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:32:06,233914844-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 345.88k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:32:06,248651699-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:32:14,253442042-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 345.88k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:32:14,267981148-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:32:22,106551962-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 345.88k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:32:22,121421379-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:32:22,132708578-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:32:22,136872955-07:00][RUNNING][ROUND 5/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:32:22,143364784-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T13:32:22,159752624-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40496\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.421844\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 75901773-ef6b-4d58-839c-0cf3aa5ff88c\nsetting min_mon_release = octopus\nepoch 0\nfsid 75901773-ef6b-4d58-839c-0cf3aa5ff88c\nlast_changed 2021-05-17T13:32:46.547738-0700\ncreated 2021-05-17T13:32:46.547738-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40496/0,v1:10.10.1.2:40497/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.421844 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 908bf9b0-0d0c-4f93-b00e-b4eda91f9434\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 ab9f4b27-0142-4885-82e7-45b621a8ffe4\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 ce2ea892-da19-4ed2-857c-d459da913ec9\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42496\n  w/ user/pass: admin / 00c7ece6-49eb-40d0-8834-05bb7ff698d4\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:33:01 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40496
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.421844
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 75901773-ef6b-4d58-839c-0cf3aa5ff88c
setting min_mon_release = octopus
epoch 0
fsid 75901773-ef6b-4d58-839c-0cf3aa5ff88c
last_changed 2021-05-17T13:32:46.547738-0700
created 2021-05-17T13:32:46.547738-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40496/0,v1:10.10.1.2:40497/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.421844 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 908bf9b0-0d0c-4f93-b00e-b4eda91f9434
0
start osd.0
add osd1 ab9f4b27-0142-4885-82e7-45b621a8ffe4
1
start osd.1
add osd2 ce2ea892-da19-4ed2-857c-d459da913ec9
2
start osd.2


restful urls: https://10.10.1.2:42496
  w/ user/pass: admin / 00c7ece6-49eb-40d0-8834-05bb7ff698d4


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T13:32:23.146-0700 7fa52eb831c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:32:23.146-0700 7fa52eb831c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:32:23.162-0700 7f5a238be1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:32:23.162-0700 7f5a238be1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40496,v1:10.10.1.2:40497] --print /tmp/ceph_monmap.421844 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.421844 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.421844 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42496 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.xC5wst1z4u 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 908bf9b0-0d0c-4f93-b00e-b4eda91f9434 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD30qJgFrpEEBAAoWXnQF+q0COjYP22Hyv9ag== --osd-uuid 908bf9b0-0d0c-4f93-b00e-b4eda91f9434 
2021-05-17T13:32:55.606-0700 7f0e4cad4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:32:55.606-0700 7f0e4cad4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:32:55.606-0700 7f0e4cad4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:32:55.650-0700 7f0e4cad4f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ab9f4b27-0142-4885-82e7-45b621a8ffe4 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T13:32:55.930-0700 7fa2474c7f00 -1 Falling back to public interface
2021-05-17T13:32:55.942-0700 7fa2474c7f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD30qJgsDNnNxAAHpEtXKyWGP/DNN9w5GjVUA== --osd-uuid ab9f4b27-0142-4885-82e7-45b621a8ffe4 
2021-05-17T13:32:56.258-0700 7f33b3c0df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:32:56.258-0700 7f33b3c0df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:32:56.258-0700 7f33b3c0df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:32:56.306-0700 7f33b3c0df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ce2ea892-da19-4ed2-857c-d459da913ec9 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T13:32:56.642-0700 7f4d7ec43f00 -1 Falling back to public interface
2021-05-17T13:32:56.654-0700 7f4d7ec43f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD40qJgApovJhAAc5KRFkDqgxIjtHin+dfhBQ== --osd-uuid ce2ea892-da19-4ed2-857c-d459da913ec9 
2021-05-17T13:32:56.978-0700 7f43f2192f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:32:56.982-0700 7f43f2192f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:32:56.982-0700 7f43f2192f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:32:57.034-0700 7f43f2192f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T13:32:57.314-0700 7f8ba76a4f00 -1 Falling back to public interface
2021-05-17T13:32:57.330-0700 7f8ba76a4f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T13:33:01,299619304-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:33:01,306781656-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T13:33:01,389679055-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:33:01,396448640-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b59af6e9-9e65-4a45-a33f-8118b6f59519', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b59af6e9-9e65-4a45-a33f-8118b6f59519 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T13:33:04,260014391-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:33:04,266413158-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b59af6e9-9e65-4a45-a33f-8118b6f59519', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b59af6e9-9e65-4a45-a33f-8118b6f59519 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T13:33:07,080702526-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:33:07,087095473-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b59af6e9-9e65-4a45-a33f-8118b6f59519', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b59af6e9-9e65-4a45-a33f-8118b6f59519 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T13:33:09,872031559-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:33:09,878349838-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b59af6e9-9e65-4a45-a33f-8118b6f59519', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b59af6e9-9e65-4a45-a33f-8118b6f59519 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T13:33:15,603275049-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:33:15,610284060-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b59af6e9-9e65-4a45-a33f-8118b6f59519', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b59af6e9-9e65-4a45-a33f-8118b6f59519 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T13:33:19,176526003-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:33:19,183055593-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b59af6e9-9e65-4a45-a33f-8118b6f59519', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b59af6e9-9e65-4a45-a33f-8118b6f59519 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T13:33:22,537611231-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:33:22,544080301-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b59af6e9-9e65-4a45-a33f-8118b6f59519', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b59af6e9-9e65-4a45-a33f-8118b6f59519 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T13:33:26,166751746-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:33:26,173208769-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b59af6e9-9e65-4a45-a33f-8118b6f59519', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b59af6e9-9e65-4a45-a33f-8118b6f59519 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:33:29,290928745-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:33:29,297441712-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b59af6e9-9e65-4a45-a33f-8118b6f59519', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b59af6e9-9e65-4a45-a33f-8118b6f59519 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:33:32,581701771-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:33:32,588179422-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b59af6e9-9e65-4a45-a33f-8118b6f59519', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b59af6e9-9e65-4a45-a33f-8118b6f59519 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T13:33:36,025975265-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:33:36,032414404-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b59af6e9-9e65-4a45-a33f-8118b6f59519', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b59af6e9-9e65-4a45-a33f-8118b6f59519 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T13:33:38,724337342-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:33:38,730903883-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b59af6e9-9e65-4a45-a33f-8118b6f59519', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b59af6e9-9e65-4a45-a33f-8118b6f59519 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T13:33:41,498204404-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:34:04,619459645-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [======================......] (remaining: 1s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:34:12,586526134-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:34:20,731757732-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:34:28,612511429-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:34:36,571083294-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:34:44,732957189-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:34:52,804817074-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:35:00,659846285-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   228 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:35:00,675058871-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:35:08,776822902-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:35:08,791755480-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:35:16,867962126-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:35:16,883014199-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:35:24,885155038-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:35:24,899798637-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:35:32,852914529-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:35:32,867781562-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:35:32,879312012-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:35:32,886303350-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:35:32,902294209-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1068173
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T13:35:32,915274130-07:00] INFO: > Run rados bench[0m
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.5
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mlocalhost.localdomain	[2021-05-17T13:35:32,955816752-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:35:32,962308100-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b59af6e9-9e65-4a45-a33f-8118b6f59519', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b59af6e9-9e65-4a45-a33f-8118b6f59519 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:35:34.764+0000 ffff977ac010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:35:34.792+0000 ffff977ac010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:35:34.792+0000 ffff977ac010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:35:34.808389+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-17T20:35:34.808431+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T20:35:34.810761+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:35:34.810761+0000     0       0         0         0         0         0           -           0
2021-05-17T20:35:35.810920+0000     1     255     19744     19489    76.125   76.1289  0.00256665   0.0126953
2021-05-17T20:35:36.811070+0000     2     255     38825     38570   75.3245   74.5352  0.00222188   0.0129128
2021-05-17T20:35:37.811238+0000     3     255     57345     57090   74.3268   72.3438  0.00429535   0.0132645
2021-05-17T20:35:38.811496+0000     4     255     76641     76386    74.584    75.375     0.11647   0.0132336
2021-05-17T20:35:39.811673+0000     5     255     95671     95416   74.5318   74.3359  0.00322352   0.0133088
2021-05-17T20:35:40.811898+0000     6     255    114228    113973   74.1884   72.4883  0.00813315   0.0134252
2021-05-17T20:35:41.812152+0000     7     255    134224    133969   74.7458   78.1094  0.00244258     0.01329
2021-05-17T20:35:42.812507+0000     8     255    153088    152833   74.6102   73.6875  0.00210442   0.0133353
2021-05-17T20:35:43.812712+0000     9     255    168380    168125    72.956   59.7344   0.0551177   0.0135892
2021-05-17T20:35:44.812880+0000    10     255    187335    187080   73.0634    74.043  0.00626918    0.013647
2021-05-17T20:35:45.813046+0000    11     255    205611    205356   72.9103   71.3906   0.0330852    0.013682
2021-05-17T20:35:46.813216+0000    12     255    223891    223636   72.7839   71.4062  0.00912161   0.0137199
2021-05-17T20:35:47.813405+0000    13     255    242032    241777   72.6352   70.8633  0.00259078   0.0137458
2021-05-17T20:35:48.813616+0000    14     255    261115    260860   72.7703    74.543  0.00194244   0.0137023
2021-05-17T20:35:49.813875+0000    15     255    277571    277316   72.2032   64.2812   0.0255145   0.0138267
2021-05-17T20:35:50.814089+0000    16     256    295752    295496   72.1281   71.0156   0.0106427   0.0138477
2021-05-17T20:35:51.814424+0000    17     255    311009    310754   71.3899   59.6016  0.00354294   0.0139859
2021-05-17T20:35:52.814747+0000    18     256    327791    327535   71.0643   65.5508    0.016723   0.0140216
2021-05-17T20:35:53.814903+0000    19     255    344467    344212   70.7522   65.1445   0.0103858   0.0141197
2021-05-17T20:35:54.815081+0000 min lat: 0.00067366 max lat: 0.236681 avg lat: 0.0141729
2021-05-17T20:35:54.815081+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:35:54.815081+0000    20      47    361094    361047   70.5021   65.7617   0.0269443   0.0141729
2021-05-17T20:35:55.815329+0000 Total time run:         20.0207
Total writes made:      361094
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     70.4534
Stddev Bandwidth:       5.33973
Max bandwidth (MB/sec): 78.1094
Min bandwidth (MB/sec): 59.6016
Average IOPS:           18036
Stddev IOPS:            1366.97
Max IOPS:               19996
Min IOPS:               15258
Average Latency(s):     0.0141741
Stddev Latency(s):      0.0243
Max latency(s):         0.236681
Min latency(s):         0.00067366

[1;32mlocalhost.localdomain	[2021-05-17T13:35:56,112906765-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1068173


[1;33mlocalhost.localdomain	[2021-05-17T13:35:56,120198875-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:36:19,363512773-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.10k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:36:19,378833618-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:36:27,229072665-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.10k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:36:27,244369190-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:36:35,399410250-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.10k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:36:35,414393072-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:36:43,773454473-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.10k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:36:43,788131183-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:36:51,868932671-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.10k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:36:51,884089042-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:36:51,896129444-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:36:51,903113762-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:36:51,918710203-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1071562
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T13:36:51,931788812-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-17T13:36:51,971112687-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:36:51,977323548-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b59af6e9-9e65-4a45-a33f-8118b6f59519', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b59af6e9-9e65-4a45-a33f-8118b6f59519 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.q9mhXT:/tmp/ceph-asok.q9mhXT -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:36:53.513+0000 ffffa8574010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:36:53.517+0000 ffffa8574010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:36:53.521+0000 ffffa8574010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:36:53.537469+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:36:53.537469+0000     0       0         0         0         0         0           -           0
2021-05-17T20:36:54.538068+0000     1     255     24627     24372   95.1327   95.2031  0.00723512   0.0104362
2021-05-17T20:36:55.538378+0000     2     256     55780     55524   108.388   121.688  0.00946781  0.00918638
2021-05-17T20:36:56.538549+0000     3     255     84323     84068   109.419     111.5  0.00766189  0.00911201
2021-05-17T20:36:57.538722+0000     4     255    117163    116908   114.128   128.281  0.00732366  0.00874097
2021-05-17T20:36:58.538895+0000     5     255    140844    140589   109.801   92.5039  0.00481735  0.00908518
2021-05-17T20:36:59.539067+0000     6     255    171267    171012   111.304    118.84  0.00461518  0.00896635
2021-05-17T20:37:00.539226+0000     7     255    201520    201265   112.283   118.176  0.00756873  0.00889026
2021-05-17T20:37:01.542600+0000     8     255    227588    227333   110.929   101.828  0.00707897  0.00899836
2021-05-17T20:37:02.542784+0000     9     256    252384    252128   109.364   96.8555  0.00501729  0.00912572
2021-05-17T20:37:03.542960+0000    10     256    281054    280798   109.625   111.992 0.000582594  0.00906878
2021-05-17T20:37:04.543124+0000    11     255    308199    307944   109.298   106.039  0.00768821  0.00913648
2021-05-17T20:37:05.543297+0000    12     256    336279    336023   109.328   109.684    0.022404   0.0091321
2021-05-17T20:37:06.543469+0000    13     255    360264    360009   108.125   93.6953  0.00534782  0.00923473
2021-05-17T20:37:07.543707+0000 Total time run:       13.0511
Total reads made:     361094
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   108.077
Average IOPS:         27667
Stddev IOPS:          2979.03
Max IOPS:             32840
Min IOPS:             23681
Average Latency(s):   0.00923982
Max latency(s):       0.141789
Min latency(s):       0.000304221

[1;32mlocalhost.localdomain	[2021-05-17T13:37:07,853366870-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1071562


[1;33mlocalhost.localdomain	[2021-05-17T13:37:07,860682473-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:37:31,077969069-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.10k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:37:31,092804566-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:37:39,121049107-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.10k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:37:39,136460417-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:37:47,179891690-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.10k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:37:47,195339546-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:37:55,603739844-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.10k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:37:55,618972444-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:38:03,577165215-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.10k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:38:03,592326193-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:38:03,604018175-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:38:03,611173912-07:00][RUNNING][ROUND 1/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:38:03,618219785-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T13:38:03,634852003-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40584\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.425102\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 13438656-d757-4b9c-9daf-1878d42656c3\nsetting min_mon_release = octopus\nepoch 0\nfsid 13438656-d757-4b9c-9daf-1878d42656c3\nlast_changed 2021-05-17T13:38:29.160113-0700\ncreated 2021-05-17T13:38:29.160113-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40584/0,v1:10.10.1.2:40585/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.425102 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 b9cc9a30-f631-4c22-b23a-66d5bd74c770\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 a2e1d91a-07c1-42f7-ba4c-6422bb3d7ace\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 f60ac805-5cb5-4052-8f1e-4a9ab7818449\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42584\n  w/ user/pass: admin / 2151ecb9-5071-4a3e-8e0f-643e878fb747\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:38:43 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40584
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.425102
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 13438656-d757-4b9c-9daf-1878d42656c3
setting min_mon_release = octopus
epoch 0
fsid 13438656-d757-4b9c-9daf-1878d42656c3
last_changed 2021-05-17T13:38:29.160113-0700
created 2021-05-17T13:38:29.160113-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40584/0,v1:10.10.1.2:40585/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.425102 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 b9cc9a30-f631-4c22-b23a-66d5bd74c770
0
start osd.0
add osd1 a2e1d91a-07c1-42f7-ba4c-6422bb3d7ace
1
start osd.1
add osd2 f60ac805-5cb5-4052-8f1e-4a9ab7818449
2
start osd.2


restful urls: https://10.10.1.2:42584
  w/ user/pass: admin / 2151ecb9-5071-4a3e-8e0f-643e878fb747


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T13:38:04.617-0700 7f94b61321c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:38:04.617-0700 7f94b61321c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:38:04.633-0700 7f59d8be41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:38:04.637-0700 7f59d8be41c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40584,v1:10.10.1.2:40585] --print /tmp/ceph_monmap.425102 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.425102 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.425102 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42584 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.3izcFsaDwx 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b9cc9a30-f631-4c22-b23a-66d5bd74c770 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBN1KJgeJVIMhAA8yKAFMgWxPu4f77qRIHLGQ== --osd-uuid b9cc9a30-f631-4c22-b23a-66d5bd74c770 
2021-05-17T13:38:38.170-0700 7f949fdabf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:38:38.170-0700 7f949fdabf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:38:38.170-0700 7f949fdabf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:38:38.230-0700 7f949fdabf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a2e1d91a-07c1-42f7-ba4c-6422bb3d7ace -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T13:38:38.526-0700 7fa1db388f00 -1 Falling back to public interface
2021-05-17T13:38:38.538-0700 7fa1db388f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBO1KJgboNMHxAAM1/tTJ9waWH7xMezwjrdDg== --osd-uuid a2e1d91a-07c1-42f7-ba4c-6422bb3d7ace 
2021-05-17T13:38:38.922-0700 7f06bfad6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:38:38.922-0700 7f06bfad6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:38:38.922-0700 7f06bfad6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:38:38.966-0700 7f06bfad6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f60ac805-5cb5-4052-8f1e-4a9ab7818449 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T13:38:39.294-0700 7f4b35641f00 -1 Falling back to public interface
2021-05-17T13:38:39.306-0700 7f4b35641f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBP1KJg0zKDERAA/V8VCplP9Bx8Jg0fzMCyEw== --osd-uuid f60ac805-5cb5-4052-8f1e-4a9ab7818449 
2021-05-17T13:38:39.622-0700 7f9df74baf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:38:39.622-0700 7f9df74baf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:38:39.622-0700 7f9df74baf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:38:39.678-0700 7f9df74baf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T13:38:40.022-0700 7fd9aafbdf00 -1 Falling back to public interface
2021-05-17T13:38:40.038-0700 7fd9aafbdf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T13:38:43,951868768-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:38:43,959130588-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T13:38:44,043113457-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:38:44,050016596-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6f162a0e-6843-4d23-ad91-30bf98769830', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6f162a0e-6843-4d23-ad91-30bf98769830 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T13:38:46,867454571-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:38:46,873797274-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6f162a0e-6843-4d23-ad91-30bf98769830', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6f162a0e-6843-4d23-ad91-30bf98769830 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T13:38:49,595809666-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:38:49,603134091-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6f162a0e-6843-4d23-ad91-30bf98769830', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6f162a0e-6843-4d23-ad91-30bf98769830 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T13:38:52,491703408-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:38:52,498170527-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6f162a0e-6843-4d23-ad91-30bf98769830', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6f162a0e-6843-4d23-ad91-30bf98769830 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T13:38:58,352918964-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:38:58,359688205-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6f162a0e-6843-4d23-ad91-30bf98769830', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6f162a0e-6843-4d23-ad91-30bf98769830 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T13:39:02,002922902-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:39:02,009463204-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6f162a0e-6843-4d23-ad91-30bf98769830', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6f162a0e-6843-4d23-ad91-30bf98769830 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T13:39:05,262951061-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:39:05,269251938-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6f162a0e-6843-4d23-ad91-30bf98769830', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6f162a0e-6843-4d23-ad91-30bf98769830 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T13:39:08,574252944-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:39:08,580788866-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6f162a0e-6843-4d23-ad91-30bf98769830', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6f162a0e-6843-4d23-ad91-30bf98769830 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:39:11,959012149-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:39:11,965385713-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6f162a0e-6843-4d23-ad91-30bf98769830', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6f162a0e-6843-4d23-ad91-30bf98769830 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:39:15,380904663-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:39:15,387365797-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6f162a0e-6843-4d23-ad91-30bf98769830', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6f162a0e-6843-4d23-ad91-30bf98769830 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T13:39:18,522892878-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:39:18,529281819-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6f162a0e-6843-4d23-ad91-30bf98769830', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6f162a0e-6843-4d23-ad91-30bf98769830 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T13:39:21,252222760-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:39:21,258819322-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6f162a0e-6843-4d23-ad91-30bf98769830', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6f162a0e-6843-4d23-ad91-30bf98769830 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T13:39:23,941678234-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:39:47,002148433-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:39:54,877289813-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:40:02,994842043-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:40:11,063658479-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 45s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:40:19,150995002-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [=========...................] (remaining: 52s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:40:27,122034338-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 70s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:40:35,023527960-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 80s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:40:43,131923885-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 41s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:40:51,454232297-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:40:51,469614817-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:40:59,668894156-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:40:59,683979731-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:41:07,530547092-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:41:07,546213536-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:41:15,506408184-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:41:15,521106259-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:41:23,574491090-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:41:23,589410362-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:41:23,601297078-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:41:23,608099430-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:41:23,623636987-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1085284
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T13:41:23,636597451-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T13:41:23,677325956-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:41:23,683780189-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6f162a0e-6843-4d23-ad91-30bf98769830', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6f162a0e-6843-4d23-ad91-30bf98769830 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:41:25.376+0000 ffff91a1b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:41:25.384+0000 ffff91a1b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:41:25.384+0000 ffff91a1b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:41:25.402096+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-17T20:41:25.402143+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T20:41:25.406747+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:41:25.406747+0000     0       0         0         0         0         0           -           0
2021-05-17T20:41:26.406917+0000     1     255     18888     18633   291.124   291.141   0.0147997   0.0135301
2021-05-17T20:41:27.407071+0000     2     255     37820     37565   293.445   295.812    0.010717   0.0135114
2021-05-17T20:41:28.407320+0000     3     255     55773     55518   289.112   280.516    0.012203     0.01376
2021-05-17T20:41:29.407562+0000     4     255     73261     73006    285.13    273.25   0.0247882   0.0139656
2021-05-17T20:41:30.407812+0000     5     255     91291     91036   284.433   281.719  0.00786104   0.0140124
2021-05-17T20:41:31.407971+0000     6     255    109935    109680   285.572   291.312   0.0097373   0.0139615
2021-05-17T20:41:32.408134+0000     7     255    127445    127190   283.855   273.594   0.0119066   0.0140518
2021-05-17T20:41:33.408440+0000     8     255    146536    146281   285.649   298.297  0.00923054   0.0139469
2021-05-17T20:41:34.408596+0000     9     255    164655    164400   285.362   283.109   0.0135938   0.0139844
2021-05-17T20:41:35.408765+0000    10     255    181475    181220   283.102   262.812  0.00702997   0.0141053
2021-05-17T20:41:36.409029+0000    11     255    196740    196485   279.043   238.516   0.0132599   0.0143121
2021-05-17T20:41:37.409257+0000    12     255    210852    210597    274.16     220.5   0.0100846     0.01456
2021-05-17T20:41:38.409474+0000    13     255    225656    225401    270.86   231.312   0.0134459   0.0147484
2021-05-17T20:41:39.409763+0000    14     256    240317    240061    267.87   229.062   0.0201781   0.0148388
2021-05-17T20:41:40.409923+0000    15     255    255129    254874    265.44   231.453   0.0138758   0.0150432
2021-05-17T20:41:41.410126+0000    16     255    269660    269405   263.037   227.047   0.0182283   0.0151896
2021-05-17T20:41:42.410498+0000    17     256    283245    282989   260.045    212.25   0.0068806    0.015366
2021-05-17T20:41:43.410908+0000    18     255    300318    300063   260.413   266.781   0.0131706   0.0153439
2021-05-17T20:41:44.411075+0000    19     255    315150    314895   258.902    231.75   0.0132844    0.015429
2021-05-17T20:41:45.411302+0000 min lat: 0.000770876 max lat: 0.191138 avg lat: 0.0154833
2021-05-17T20:41:45.411302+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:41:45.411302+0000    20       7    330496    330489   258.137   243.656  0.00761722   0.0154833
2021-05-17T20:41:46.411538+0000 Total time run:         20.0067
Total writes made:      330496
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     258.114
Stddev Bandwidth:       28.5833
Max bandwidth (MB/sec): 298.297
Min bandwidth (MB/sec): 212.25
Average IOPS:           16519
Stddev IOPS:            1829.33
Max IOPS:               19091
Min IOPS:               13584
Average Latency(s):     0.0154831
Stddev Latency(s):      0.013683
Max latency(s):         0.191138
Min latency(s):         0.000770876

[1;32mlocalhost.localdomain	[2021-05-17T13:41:46,734059493-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1085284


[1;33mlocalhost.localdomain	[2021-05-17T13:41:46,741917590-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:42:09,723812077-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.50k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:42:09,738959088-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:42:18,152263040-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.50k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:42:18,167589106-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:42:26,558157463-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.50k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:42:26,573621672-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:42:34,746281520-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.50k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:42:34,761680919-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:42:42,600116443-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.50k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:42:42,615500645-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:42:42,627819194-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:42:42,634806423-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:42:42,650618571-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1088599
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T13:42:42,664228616-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-17T13:42:42,704168182-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:42:42,710514637-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6f162a0e-6843-4d23-ad91-30bf98769830', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6f162a0e-6843-4d23-ad91-30bf98769830 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RJRZHY:/tmp/ceph-asok.RJRZHY -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:42:44.333+0000 ffff81004010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:42:44.393+0000 ffff81004010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:42:44.393+0000 ffff81004010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:42:44.410320+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:42:44.410320+0000     0       0         0         0         0         0           -           0
2021-05-17T20:42:45.410499+0000     1     256     18424     18168   283.787   283.875  0.00969771   0.0139394
2021-05-17T20:42:46.410717+0000     2     255     37094     36839   287.729   291.734  0.00853096   0.0138268
2021-05-17T20:42:47.410888+0000     3     255     56283     56028   291.745   299.828  0.00273375   0.0136079
2021-05-17T20:42:48.411065+0000     4     256     76389     76133    297.33   314.141  0.00637883   0.0133999
2021-05-17T20:42:49.411290+0000     5     255     95788     95533   298.475   303.125  0.00667797   0.0133521
2021-05-17T20:42:50.411599+0000     6     255    118220    117965   307.128     350.5   0.0110911   0.0129886
2021-05-17T20:42:51.411790+0000     7     256    139719    139463    311.23   335.906   0.0230818    0.012815
2021-05-17T20:42:52.412014+0000     8     255    158152    157897   308.322   288.031   0.0267427   0.0129338
2021-05-17T20:42:53.412222+0000     9     255    176566    176311   306.026   287.719   0.0159655   0.0130403
2021-05-17T20:42:54.412413+0000    10     255    196283    196028   306.226   308.078  0.00305968   0.0130303
2021-05-17T20:42:55.412718+0000    11     255    214108    213853   303.699   278.516  0.00778449   0.0131455
2021-05-17T20:42:56.412904+0000    12     255    233426    233171   303.539   301.844    0.012958   0.0131527
2021-05-17T20:42:57.413092+0000    13     256    254697    254441    305.75   332.344   0.0184553   0.0130565
2021-05-17T20:42:58.413285+0000    14     256    274838    274582   306.385   314.703   0.0102663   0.0130332
2021-05-17T20:42:59.413481+0000    15     255    294734    294479   306.682   310.891   0.0316615   0.0130169
2021-05-17T20:43:00.413672+0000    16     255    316220    315965   308.492   335.719  0.00541363   0.0129437
2021-05-17T20:43:01.413920+0000 Total time run:       16.7455
Total reads made:     330496
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   308.381
Average IOPS:         19736
Stddev IOPS:          1351.23
Max IOPS:             22432
Min IOPS:             17825
Average Latency(s):   0.0129505
Max latency(s):       0.171923
Min latency(s):       0.000345104

[1;32mlocalhost.localdomain	[2021-05-17T13:43:01,711222493-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1088599


[1;33mlocalhost.localdomain	[2021-05-17T13:43:01,719404780-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:43:24,757186022-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.50k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:43:24,772755055-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:43:33,033632270-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.50k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:43:33,048789362-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:43:41,555296453-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.50k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:43:41,570667857-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:43:49,610144422-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.50k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:43:49,625245150-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:43:57,649335674-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.50k objects, 5.0 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:43:57,666320317-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:43:57,679972759-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:43:57,685291253-07:00][RUNNING][ROUND 2/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:43:57,693452491-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T13:43:57,711787604-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40554\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.428491\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 0782daf2-957b-4515-8a81-51423d8c3bbd\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 0782daf2-957b-4515-8a81-51423d8c3bbd\nlast_changed 2021-05-17T13:44:25.452233-0700\ncreated 2021-05-17T13:44:25.452233-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40554/0,v1:10.10.1.2:40555/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.428491 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 8193bcad-4187-43b8-b758-01ed11c6fb09\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 9f346844-dc47-4e68-9f25-b236934f9aa0\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 b30fbd94-8c1a-4a60-92ba-b32927ef92ec\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42554\n  w/ user/pass: admin / 43a0a537-cd7b-463c-8116-2efc3a169d39\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:44:40 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40554
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.428491
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 0782daf2-957b-4515-8a81-51423d8c3bbd
setting min_mon_release = octopus
epoch 0
fsid 0782daf2-957b-4515-8a81-51423d8c3bbd
last_changed 2021-05-17T13:44:25.452233-0700
created 2021-05-17T13:44:25.452233-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40554/0,v1:10.10.1.2:40555/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.428491 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 8193bcad-4187-43b8-b758-01ed11c6fb09
0
start osd.0
add osd1 9f346844-dc47-4e68-9f25-b236934f9aa0
1
start osd.1
add osd2 b30fbd94-8c1a-4a60-92ba-b32927ef92ec
2
start osd.2


restful urls: https://10.10.1.2:42554
  w/ user/pass: admin / 43a0a537-cd7b-463c-8116-2efc3a169d39


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T13:43:58.764-0700 7f5e82c741c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:43:58.764-0700 7f5e82c741c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:43:58.780-0700 7f8df5d511c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:43:58.780-0700 7f8df5d511c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40554,v1:10.10.1.2:40555] --print /tmp/ceph_monmap.428491 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.428491 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.428491 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42554 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.GujVTVk0BL 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8193bcad-4187-43b8-b758-01ed11c6fb09 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCy1aJg/IdsCBAANTR8yvLOHbU18ehdC/loTQ== --osd-uuid 8193bcad-4187-43b8-b758-01ed11c6fb09 
2021-05-17T13:44:34.465-0700 7f6b8d847f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:44:34.465-0700 7f6b8d847f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:44:34.465-0700 7f6b8d847f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:44:34.529-0700 7f6b8d847f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9f346844-dc47-4e68-9f25-b236934f9aa0 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T13:44:34.781-0700 7fd1862fef00 -1 Falling back to public interface
2021-05-17T13:44:34.793-0700 7fd1862fef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCy1aJgGpatLhAAUWLZK+JHHvqViqfu4W3Azg== --osd-uuid 9f346844-dc47-4e68-9f25-b236934f9aa0 
2021-05-17T13:44:35.153-0700 7f74eed82f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:44:35.153-0700 7f74eed82f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:44:35.153-0700 7f74eed82f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:44:35.197-0700 7f74eed82f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b30fbd94-8c1a-4a60-92ba-b32927ef92ec -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T13:44:35.493-0700 7fc4dc821f00 -1 Falling back to public interface
2021-05-17T13:44:35.505-0700 7fc4dc821f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCz1aJgTGODHRAABBFrTGTsR3XhhZxoFUmujw== --osd-uuid b30fbd94-8c1a-4a60-92ba-b32927ef92ec 
2021-05-17T13:44:35.821-0700 7eff598c4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:44:35.825-0700 7eff598c4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:44:35.825-0700 7eff598c4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:44:35.877-0700 7eff598c4f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T13:44:36.237-0700 7f52183d3f00 -1 Falling back to public interface
2021-05-17T13:44:36.253-0700 7f52183d3f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T13:44:40,140363325-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:44:40,147377618-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T13:44:40,231341752-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:44:40,237863751-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5b84bf9f-010e-438a-89b5-8209adfff78c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5b84bf9f-010e-438a-89b5-8209adfff78c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T13:44:43,215402137-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:44:43,221799450-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5b84bf9f-010e-438a-89b5-8209adfff78c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5b84bf9f-010e-438a-89b5-8209adfff78c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T13:44:45,961660248-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:44:45,968141695-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5b84bf9f-010e-438a-89b5-8209adfff78c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5b84bf9f-010e-438a-89b5-8209adfff78c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T13:44:48,744234915-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:44:48,750787835-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5b84bf9f-010e-438a-89b5-8209adfff78c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5b84bf9f-010e-438a-89b5-8209adfff78c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T13:44:54,574048055-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:44:54,581355497-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5b84bf9f-010e-438a-89b5-8209adfff78c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5b84bf9f-010e-438a-89b5-8209adfff78c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T13:44:58,410399064-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:44:58,418020585-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5b84bf9f-010e-438a-89b5-8209adfff78c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5b84bf9f-010e-438a-89b5-8209adfff78c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T13:45:01,439780000-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:45:01,446601375-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5b84bf9f-010e-438a-89b5-8209adfff78c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5b84bf9f-010e-438a-89b5-8209adfff78c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T13:45:04,800337547-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:45:04,806997180-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5b84bf9f-010e-438a-89b5-8209adfff78c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5b84bf9f-010e-438a-89b5-8209adfff78c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:45:08,166134646-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:45:08,173734308-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5b84bf9f-010e-438a-89b5-8209adfff78c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5b84bf9f-010e-438a-89b5-8209adfff78c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:45:11,332654311-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:45:11,339164398-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5b84bf9f-010e-438a-89b5-8209adfff78c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5b84bf9f-010e-438a-89b5-8209adfff78c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T13:45:14,947156896-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:45:14,954170844-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5b84bf9f-010e-438a-89b5-8209adfff78c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5b84bf9f-010e-438a-89b5-8209adfff78c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T13:45:17,691484769-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:45:17,697918343-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5b84bf9f-010e-438a-89b5-8209adfff78c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5b84bf9f-010e-438a-89b5-8209adfff78c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T13:45:20,441710536-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:45:43,454073650-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   198 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:45:51,530179371-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:45:59,496644261-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 27s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:46:07,511896956-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:46:15,429267023-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 56s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:46:23,523198251-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 73s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:46:31,565809370-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 83s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:46:39,545645377-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   226 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [=========...................] (remaining: 104s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:46:47,612863448-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:46:47,627944237-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:46:55,703536046-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:46:55,718635544-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:47:03,737856814-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:47:03,752886100-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:47:11,725174802-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:47:11,740347212-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:47:19,854795174-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:47:19,870005556-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:47:19,882104918-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:47:19,889487059-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:47:19,905736922-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1102468
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T13:47:19,919033519-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T13:47:19,959418587-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:47:19,965817985-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5b84bf9f-010e-438a-89b5-8209adfff78c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5b84bf9f-010e-438a-89b5-8209adfff78c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:47:21.468+0000 ffff93606010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:47:21.476+0000 ffff93606010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:47:21.480+0000 ffff93606010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:47:21.495985+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-17T20:47:21.496026+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T20:47:21.500608+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:47:21.500608+0000     0       0         0         0         0         0           -           0
2021-05-17T20:47:22.500796+0000     1     255     18804     18549   289.811   289.828  0.00854238   0.0134427
2021-05-17T20:47:23.500984+0000     2     255     36607     36352   283.965   278.172   0.0144698    0.013969
2021-05-17T20:47:24.501419+0000     3     255     55719     55464   288.809   298.625  0.00677907   0.0136756
2021-05-17T20:47:25.501896+0000     4     255     73373     73118   285.534   275.844  0.00393924   0.0138232
2021-05-17T20:47:26.502269+0000     5     255     89784     89529   279.692   256.422   0.0281932   0.0142424
2021-05-17T20:47:27.502450+0000     6     255    106015    105760   275.338   253.609   0.0112223   0.0144695
2021-05-17T20:47:28.502671+0000     7     256    120778    120522   268.948   230.656   0.0101194   0.0148293
2021-05-17T20:47:29.502913+0000     8     255    135624    135369   264.321   231.984   0.0245716   0.0150953
2021-05-17T20:47:30.503152+0000     9     256    151306    151050   262.169   245.016   0.0116439   0.0152245
2021-05-17T20:47:31.503329+0000    10     255    168559    168304   262.907   269.594   0.0243423    0.015178
2021-05-17T20:47:32.503546+0000    11     255    184417    184162   261.527   247.781   0.0307412    0.015262
2021-05-17T20:47:33.503784+0000    12     256    198806    198550   258.463   224.812    0.010636   0.0153351
2021-05-17T20:47:34.504017+0000    13     255    215891    215636   259.112   266.969   0.0065492   0.0154033
2021-05-17T20:47:35.504455+0000    14     255    232068    231813   258.651   252.766   0.0045974   0.0154165
2021-05-17T20:47:36.504651+0000    15     256    246854    246598   256.806   231.016 0.000912425   0.0155024
2021-05-17T20:47:37.504860+0000    16     255    260856    260601   254.428   218.797   0.0117629   0.0156851
2021-05-17T20:47:38.505038+0000    17     256    273541    273285   251.118   198.188 0.000873314   0.0158641
2021-05-17T20:47:39.505212+0000    18     255    288211    287956     249.9   229.234   0.0197493   0.0159874
2021-05-17T20:47:40.505393+0000    19     255    302613    302358   248.589   225.031   0.0254691   0.0160723
2021-05-17T20:47:41.505579+0000 min lat: 0.000775692 max lat: 0.233197 avg lat: 0.0160724
2021-05-17T20:47:41.505579+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:47:41.505579+0000    20      75    318419    318344   248.646   249.781   0.0146871   0.0160724
2021-05-17T20:47:42.505813+0000 Total time run:         20.0108
Total writes made:      318419
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     248.631
Stddev Bandwidth:       25.6994
Max bandwidth (MB/sec): 298.625
Min bandwidth (MB/sec): 198.188
Average IOPS:           15912
Stddev IOPS:            1644.76
Max IOPS:               19112
Min IOPS:               12684
Average Latency(s):     0.016072
Stddev Latency(s):      0.0200475
Max latency(s):         0.233197
Min latency(s):         0.000775692

[1;32mlocalhost.localdomain	[2021-05-17T13:47:42,801052113-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1102468


[1;33mlocalhost.localdomain	[2021-05-17T13:47:42,808829615-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:48:05,880690551-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.42k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:48:05,895886260-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:48:13,908824084-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.42k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:48:13,924797060-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:48:22,000794452-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.42k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:48:22,015870801-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:48:30,120807968-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.42k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:48:30,136248954-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:48:38,462083421-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.42k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:48:38,477308701-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:48:38,489445165-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:48:38,496556749-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:48:38,512400899-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1105801
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T13:48:38,525384602-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T13:48:38,564632927-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:48:38,571154161-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5b84bf9f-010e-438a-89b5-8209adfff78c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5b84bf9f-010e-438a-89b5-8209adfff78c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.xSeSEW:/tmp/ceph-asok.xSeSEW -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:48:40.133+0000 ffffb98a7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:48:40.141+0000 ffffb98a7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:48:40.141+0000 ffffb98a7010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:48:40.158531+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:48:40.158531+0000     0       0         0         0         0         0           -           0
2021-05-17T20:48:41.158700+0000     1     255     23645     23390   365.363   365.469    0.021963   0.0108024
2021-05-17T20:48:42.158990+0000     2     256     47977     47721   372.712   380.172    0.017798   0.0106742
2021-05-17T20:48:43.159179+0000     3     255     71323     71068   370.051   364.797  0.00221999   0.0107569
2021-05-17T20:48:44.159375+0000     4     255     92229     91974   359.187   326.656   0.0184767    0.011099
2021-05-17T20:48:45.159562+0000     5     255    113098    112843   352.553   326.078  0.00461809   0.0113098
2021-05-17T20:48:46.159739+0000     6     256    138893    138637   360.954   403.031  0.00462629   0.0110501
2021-05-17T20:48:47.159981+0000     7     255    161310    161055   359.417   350.281  0.00250325   0.0110993
2021-05-17T20:48:48.160174+0000     8     255    185322    185067   361.379   375.188  0.00239174    0.011033
2021-05-17T20:48:49.160444+0000     9     255    206826    206571   358.549       336  0.00142203   0.0111228
2021-05-17T20:48:50.160655+0000    10     255    229543    229288   358.182   354.953  0.00135977    0.011141
2021-05-17T20:48:51.160853+0000    11     255    247824    247569   351.582   285.641  0.00657793   0.0113534
2021-05-17T20:48:52.161048+0000    12     256    268211    267955   348.823   318.531   0.0268017   0.0114447
2021-05-17T20:48:53.161438+0000    13     256    288544    288288   346.419   317.703  0.00158658   0.0115223
2021-05-17T20:48:54.161700+0000    14     255    309355    309100   344.897   325.188  0.00114027   0.0115036
2021-05-17T20:48:55.161953+0000 Total time run:       14.4883
Total reads made:     318419
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   343.401
Average IOPS:         21977
Stddev IOPS:          1991.78
Max IOPS:             25794
Min IOPS:             18281
Average Latency(s):   0.0116284
Max latency(s):       0.16343
Min latency(s):       0.000333822

[1;32mlocalhost.localdomain	[2021-05-17T13:48:55,476481731-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1105801


[1;33mlocalhost.localdomain	[2021-05-17T13:48:55,484236445-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:49:18,589971520-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.42k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:49:18,605314374-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:49:26,905958029-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.42k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:49:26,921147753-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:49:35,069195043-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.42k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:49:35,086736746-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:49:43,589784811-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.42k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:49:43,605196962-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:49:51,505684228-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.42k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:49:51,520681274-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:49:51,532570886-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:49:51,537094380-07:00][RUNNING][ROUND 3/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:49:51,544183011-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T13:49:51,561311229-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40355\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.434162\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 85a1acd7-1463-4819-837e-93a2fe6b2b09\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 85a1acd7-1463-4819-837e-93a2fe6b2b09\nlast_changed 2021-05-17T13:50:19.251052-0700\ncreated 2021-05-17T13:50:19.251052-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40355/0,v1:10.10.1.2:40356/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.434162 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 9243bee0-52d4-490c-a19e-c16a5ba090ca\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 e99aa16a-07f5-448c-a62f-f0d7f727d14a\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 00b245bc-a19f-435e-bba4-d643ff28ed69\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42355\n  w/ user/pass: admin / 04193942-797c-4bb5-83b5-786f69b50e49\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:50:34 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40355
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.434162
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 85a1acd7-1463-4819-837e-93a2fe6b2b09
setting min_mon_release = octopus
epoch 0
fsid 85a1acd7-1463-4819-837e-93a2fe6b2b09
last_changed 2021-05-17T13:50:19.251052-0700
created 2021-05-17T13:50:19.251052-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40355/0,v1:10.10.1.2:40356/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.434162 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 9243bee0-52d4-490c-a19e-c16a5ba090ca
0
start osd.0
add osd1 e99aa16a-07f5-448c-a62f-f0d7f727d14a
1
start osd.1
add osd2 00b245bc-a19f-435e-bba4-d643ff28ed69
2
start osd.2


restful urls: https://10.10.1.2:42355
  w/ user/pass: admin / 04193942-797c-4bb5-83b5-786f69b50e49


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T13:49:52.568-0700 7f95ab09e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:49:52.572-0700 7f95ab09e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:49:52.588-0700 7f68e05a01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:49:52.588-0700 7f68e05a01c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40355,v1:10.10.1.2:40356] --print /tmp/ceph_monmap.434162 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.434162 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.434162 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42355 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.luBBbhNh1k 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9243bee0-52d4-490c-a19e-c16a5ba090ca -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAU16JgVnX/AhAAsOCnroRt7LGcuW3FFEtdjw== --osd-uuid 9243bee0-52d4-490c-a19e-c16a5ba090ca 
2021-05-17T13:50:28.384-0700 7f9f90575f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:50:28.384-0700 7f9f90575f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:50:28.384-0700 7f9f90575f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:50:28.444-0700 7f9f90575f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e99aa16a-07f5-448c-a62f-f0d7f727d14a -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T13:50:28.756-0700 7f8a59095f00 -1 Falling back to public interface
2021-05-17T13:50:28.768-0700 7f8a59095f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAU16JgwmEiLRAAi5+4PH8eu9ezWORje9OSkg== --osd-uuid e99aa16a-07f5-448c-a62f-f0d7f727d14a 
2021-05-17T13:50:29.092-0700 7f84b9356f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:50:29.092-0700 7f84b9356f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:50:29.092-0700 7f84b9356f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:50:29.164-0700 7f84b9356f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 00b245bc-a19f-435e-bba4-d643ff28ed69 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T13:50:29.448-0700 7f1aa6297f00 -1 Falling back to public interface
2021-05-17T13:50:29.460-0700 7f1aa6297f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAV16JgC177GRAAoAbVu9lQH0w9AawrRaV8hg== --osd-uuid 00b245bc-a19f-435e-bba4-d643ff28ed69 
2021-05-17T13:50:29.788-0700 7f49e513ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:50:29.788-0700 7f49e513ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:50:29.788-0700 7f49e513ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:50:29.860-0700 7f49e513ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T13:50:30.184-0700 7f50d137bf00 -1 Falling back to public interface
2021-05-17T13:50:30.200-0700 7f50d137bf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T13:50:34,126437547-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:50:34,133659191-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T13:50:34,218448976-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:50:34,225215142-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2732960d-a166-442b-8d32-225a07bc1257', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2732960d-a166-442b-8d32-225a07bc1257 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T13:50:37,200945023-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:50:37,207400396-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2732960d-a166-442b-8d32-225a07bc1257', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2732960d-a166-442b-8d32-225a07bc1257 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T13:50:40,016394230-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:50:40,023025222-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2732960d-a166-442b-8d32-225a07bc1257', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2732960d-a166-442b-8d32-225a07bc1257 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T13:50:42,770755162-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:50:42,777035725-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2732960d-a166-442b-8d32-225a07bc1257', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2732960d-a166-442b-8d32-225a07bc1257 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T13:50:48,517368871-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:50:48,523937948-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2732960d-a166-442b-8d32-225a07bc1257', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2732960d-a166-442b-8d32-225a07bc1257 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T13:50:52,253420762-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:50:52,261019239-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2732960d-a166-442b-8d32-225a07bc1257', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2732960d-a166-442b-8d32-225a07bc1257 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T13:50:55,833325196-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:50:55,840357207-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2732960d-a166-442b-8d32-225a07bc1257', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2732960d-a166-442b-8d32-225a07bc1257 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T13:50:58,984841221-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:50:58,992932007-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2732960d-a166-442b-8d32-225a07bc1257', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2732960d-a166-442b-8d32-225a07bc1257 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:51:01,950041699-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:51:01,956624954-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2732960d-a166-442b-8d32-225a07bc1257', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2732960d-a166-442b-8d32-225a07bc1257 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:51:05,265244954-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:51:05,271980530-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2732960d-a166-442b-8d32-225a07bc1257', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2732960d-a166-442b-8d32-225a07bc1257 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T13:51:08,739223694-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:51:08,745774859-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2732960d-a166-442b-8d32-225a07bc1257', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2732960d-a166-442b-8d32-225a07bc1257 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T13:51:11,540956971-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:51:11,547378138-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2732960d-a166-442b-8d32-225a07bc1257', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2732960d-a166-442b-8d32-225a07bc1257 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  156 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T13:51:14,227502487-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:51:37,263081965-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:51:45,384741199-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:51:53,439245923-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:52:01,390863885-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:52:09,421039680-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:52:17,499833391-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:52:25,636919308-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:52:33,631478195-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:52:33,647975016-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:52:41,600900700-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:52:41,616195951-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:52:49,550325119-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:52:49,565447541-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:52:57,490471948-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:52:57,505923931-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:53:05,569148290-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:53:05,584594047-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:53:05,596958354-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:53:05,604132590-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:53:05,620230793-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1119079
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T13:53:05,633589635-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-17T13:53:05,673536974-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:53:05,679936958-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2732960d-a166-442b-8d32-225a07bc1257', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2732960d-a166-442b-8d32-225a07bc1257 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:53:07.279+0000 ffff93a60010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:53:07.287+0000 ffff93a60010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:53:07.287+0000 ffff93a60010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:53:07.305035+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-17T20:53:07.305092+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T20:53:07.310527+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:53:07.310527+0000     0       6         6         0         0         0           -           0
2021-05-17T20:53:08.310761+0000     1     255     16445     16190   252.777   252.969    0.110924   0.0149353
2021-05-17T20:53:09.311051+0000     2     255     31520     31265    244.13   235.547  0.00148077   0.0158915
2021-05-17T20:53:10.311348+0000     3     255     48241     47986   249.815   261.266  0.00173602   0.0157929
2021-05-17T20:53:11.311605+0000     4     255     65410     65155    254.41   268.266   0.0390137     0.01562
2021-05-17T20:53:12.312132+0000     5     256     82443     82187   256.725   266.125  0.00122623   0.0153339
2021-05-17T20:53:13.312456+0000     6     255     97581     97326   253.349   236.547  0.00181018   0.0156618
2021-05-17T20:53:14.312774+0000     7     255    114054    113799   253.915   257.391   0.0206633   0.0157091
2021-05-17T20:53:15.313078+0000     8     255    129023    128768   251.403   233.891   0.0102617   0.0158658
2021-05-17T20:53:16.313308+0000     9     255    146467    146212   253.747   272.562  0.00315391   0.0156705
2021-05-17T20:53:17.313530+0000    10     255    163591    163336   255.122   267.562  0.00176757   0.0155825
2021-05-17T20:53:18.313730+0000    11     255    179754    179499   254.884   252.547  0.00295774   0.0156164
2021-05-17T20:53:19.314127+0000    12     255    194098    193843   252.313   224.125  0.00454766   0.0158081
2021-05-17T20:53:20.314367+0000    13     255    209760    209505   251.724   244.719   0.0186129   0.0158682
2021-05-17T20:53:21.314524+0000    14     255    225204    224949   250.978   241.312   0.0168294   0.0159177
2021-05-17T20:53:22.314761+0000    15     255    241251    240996   250.958   250.734  0.00675584    0.015901
2021-05-17T20:53:23.314923+0000    16     255    256219    255964   249.888   233.875  0.00617792   0.0159882
2021-05-17T20:53:24.315094+0000    17     255    271186    270931   248.943   233.859  0.00910551     0.01605
2021-05-17T20:53:25.315256+0000    18     255    287327    287072   249.122   252.203  0.00416553   0.0160273
2021-05-17T20:53:26.315416+0000    19     256    302853    302597   248.775   242.578  0.00118697   0.0159969
2021-05-17T20:53:27.315566+0000 min lat: 0.000773156 max lat: 0.238753 avg lat: 0.0160829
2021-05-17T20:53:27.315566+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:53:27.315566+0000    20     105    318178    318073   248.425   241.812   0.0506249   0.0160829
2021-05-17T20:53:28.315816+0000 Total time run:         20.0404
Total writes made:      318178
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     248.076
Stddev Bandwidth:       13.8279
Max bandwidth (MB/sec): 272.562
Min bandwidth (MB/sec): 224.125
Average IOPS:           15876
Stddev IOPS:            884.983
Max IOPS:               17444
Min IOPS:               14344
Average Latency(s):     0.0160905
Stddev Latency(s):      0.0296309
Max latency(s):         0.238753
Min latency(s):         0.000773156

[1;32mlocalhost.localdomain	[2021-05-17T13:53:28,616867784-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1119079


[1;33mlocalhost.localdomain	[2021-05-17T13:53:28,625032383-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:53:51,761040032-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.18k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:53:51,776164239-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:54:00,033202822-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.18k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:54:00,048526099-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:54:08,184045230-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.18k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:54:08,201718851-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:54:16,121230075-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.18k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:54:16,136835759-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:54:24,292313959-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.18k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:54:24,308180417-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:54:24,320244899-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:54:24,327630836-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:54:24,343949426-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1122407
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T13:54:24,357537747-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-17T13:54:24,396529994-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:54:24,402881324-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2732960d-a166-442b-8d32-225a07bc1257', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2732960d-a166-442b-8d32-225a07bc1257 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.qG3RCC:/tmp/ceph-asok.qG3RCC -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:54:26.068+0000 ffff81503010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:54:26.112+0000 ffff81503010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:54:26.112+0000 ffff81503010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:54:26.131478+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:54:26.131478+0000     0       0         0         0         0         0           -           0
2021-05-17T20:54:27.131670+0000     1     255     16480     16225   253.428   253.516  0.00106206   0.0153419
2021-05-17T20:54:28.131862+0000     2     255     39285     39030    304.84   356.328   0.0176799   0.0130315
2021-05-17T20:54:29.132063+0000     3     255     57094     56839   295.964   278.266  0.00344562   0.0134457
2021-05-17T20:54:30.132364+0000     4     255     74909     74654   291.541   278.359  0.00808552   0.0136601
2021-05-17T20:54:31.132602+0000     5     256     97121     96865   302.626   347.047    0.023817   0.0131638
2021-05-17T20:54:32.132783+0000     6     256    114659    114403   297.852   274.031  0.00318141   0.0131994
2021-05-17T20:54:33.132961+0000     7     256    135053    134797   300.816   318.656   0.0196543   0.0132577
2021-05-17T20:54:34.133241+0000     8     255    159620    159365   311.185   383.875  0.00360696   0.0128251
2021-05-17T20:54:35.133725+0000     9     255    180690    180435   313.172   329.219  0.00775192    0.012746
2021-05-17T20:54:36.134061+0000    10     256    200980    200724   313.545   317.016  0.00644435   0.0127213
2021-05-17T20:54:37.134283+0000    11     256    217925    217669   309.106   264.766   0.0221682    0.012912
2021-05-17T20:54:38.134540+0000    12     255    241402    241147   313.909   366.844  0.00290416   0.0127183
2021-05-17T20:54:39.134740+0000    13     256    260961    260705   313.265   305.594 0.000458748   0.0126887
2021-05-17T20:54:40.134922+0000    14     255    280775    280520       313   309.609  0.00313081   0.0127567
2021-05-17T20:54:41.135099+0000    15     256    301396    301140   313.609   322.188   0.0245354   0.0127164
2021-05-17T20:54:42.135325+0000 Total time run:       15.9683
Total reads made:     318178
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   311.338
Average IOPS:         19925
Stddev IOPS:          2490.7
Max IOPS:             24568
Min IOPS:             16225
Average Latency(s):   0.0128257
Max latency(s):       0.167438
Min latency(s):       0.000335232

[1;32mlocalhost.localdomain	[2021-05-17T13:54:42,456459748-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1122407


[1;33mlocalhost.localdomain	[2021-05-17T13:54:42,467200659-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:55:05,483995056-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.18k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:55:05,499054827-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:55:13,406632086-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.18k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:55:13,422408724-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:55:21,632945233-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.18k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:55:21,647893220-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:55:29,502109766-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.18k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:55:29,520480885-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:55:37,465711252-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 318.18k objects, 4.9 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:55:37,481309795-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:55:37,493263709-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:55:37,497773221-07:00][RUNNING][ROUND 4/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:55:37,504934824-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T13:55:37,521874505-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40284\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.435748\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ab3ee8ab-ea43-4b7e-a997-e5579f7d6216\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid ab3ee8ab-ea43-4b7e-a997-e5579f7d6216\nlast_changed 2021-05-17T13:56:04.719300-0700\ncreated 2021-05-17T13:56:04.719300-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40284/0,v1:10.10.1.2:40285/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.435748 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 3a67dc6a-b90a-4901-8edd-d9ad014542a0\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 4fb2aa31-2354-4109-82b6-98349a61ca95\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 6ace6d11-4634-46ef-971a-2d919fcb6ee9\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42284\n  w/ user/pass: admin / ce0703c2-6062-4233-a151-dd66faf3f405\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:56:19 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40284
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.435748
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ab3ee8ab-ea43-4b7e-a997-e5579f7d6216
setting min_mon_release = octopus
epoch 0
fsid ab3ee8ab-ea43-4b7e-a997-e5579f7d6216
last_changed 2021-05-17T13:56:04.719300-0700
created 2021-05-17T13:56:04.719300-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40284/0,v1:10.10.1.2:40285/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.435748 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 3a67dc6a-b90a-4901-8edd-d9ad014542a0
0
start osd.0
add osd1 4fb2aa31-2354-4109-82b6-98349a61ca95
1
start osd.1
add osd2 6ace6d11-4634-46ef-971a-2d919fcb6ee9
2
start osd.2


restful urls: https://10.10.1.2:42284
  w/ user/pass: admin / ce0703c2-6062-4233-a151-dd66faf3f405


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T13:55:38.507-0700 7f204e3841c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:55:38.511-0700 7f204e3841c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:55:38.527-0700 7f707838a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T13:55:38.527-0700 7f707838a1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40284,v1:10.10.1.2:40285] --print /tmp/ceph_monmap.435748 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.435748 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.435748 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42284 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.Tmn4YNHOtR 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3a67dc6a-b90a-4901-8edd-d9ad014542a0 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBt2KJgXfTVIBAAf3iyDOIe36Wt2yimgORDlg== --osd-uuid 3a67dc6a-b90a-4901-8edd-d9ad014542a0 
2021-05-17T13:56:13.903-0700 7f7be7258f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:56:13.903-0700 7f7be7258f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:56:13.903-0700 7f7be7258f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T13:56:13.955-0700 7f7be7258f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4fb2aa31-2354-4109-82b6-98349a61ca95 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T13:56:14.267-0700 7f0117a03f00 -1 Falling back to public interface
2021-05-17T13:56:14.279-0700 7f0117a03f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBu2KJgjtnbDxAAiPx7vOK00yXLyrP7pza2Vg== --osd-uuid 4fb2aa31-2354-4109-82b6-98349a61ca95 
2021-05-17T13:56:14.591-0700 7fd2b6a5cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:56:14.591-0700 7fd2b6a5cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:56:14.591-0700 7fd2b6a5cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T13:56:14.635-0700 7fd2b6a5cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6ace6d11-4634-46ef-971a-2d919fcb6ee9 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T13:56:14.963-0700 7f772d41af00 -1 Falling back to public interface
2021-05-17T13:56:14.975-0700 7f772d41af00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBu2KJgEoBNORAA2DU4UmxZvARAH5rMNcBPUA== --osd-uuid 6ace6d11-4634-46ef-971a-2d919fcb6ee9 
2021-05-17T13:56:15.291-0700 7f18a228cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:56:15.291-0700 7f18a228cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:56:15.291-0700 7f18a228cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T13:56:15.339-0700 7f18a228cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T13:56:15.731-0700 7f528a820f00 -1 Falling back to public interface
2021-05-17T13:56:15.747-0700 7f528a820f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T13:56:19,575316393-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:56:19,582716012-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T13:56:19,665196935-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:56:19,671739248-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e7482748-9747-403c-93cf-fe4cfbdfb077', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e7482748-9747-403c-93cf-fe4cfbdfb077 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T13:56:22,601676431-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:56:22,608325787-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e7482748-9747-403c-93cf-fe4cfbdfb077', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e7482748-9747-403c-93cf-fe4cfbdfb077 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T13:56:25,719248968-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:56:25,725509953-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e7482748-9747-403c-93cf-fe4cfbdfb077', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e7482748-9747-403c-93cf-fe4cfbdfb077 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T13:56:28,666309239-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:56:28,672714684-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e7482748-9747-403c-93cf-fe4cfbdfb077', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e7482748-9747-403c-93cf-fe4cfbdfb077 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T13:56:34,264406077-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:56:34,270977030-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e7482748-9747-403c-93cf-fe4cfbdfb077', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e7482748-9747-403c-93cf-fe4cfbdfb077 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T13:56:37,636368348-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:56:37,642865833-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e7482748-9747-403c-93cf-fe4cfbdfb077', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e7482748-9747-403c-93cf-fe4cfbdfb077 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T13:56:41,019038810-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:56:41,025654725-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e7482748-9747-403c-93cf-fe4cfbdfb077', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e7482748-9747-403c-93cf-fe4cfbdfb077 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T13:56:44,236320482-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:56:44,242966042-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e7482748-9747-403c-93cf-fe4cfbdfb077', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e7482748-9747-403c-93cf-fe4cfbdfb077 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:56:47,516663145-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:56:47,523094649-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e7482748-9747-403c-93cf-fe4cfbdfb077', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e7482748-9747-403c-93cf-fe4cfbdfb077 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T13:56:50,732242613-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:56:50,738748860-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e7482748-9747-403c-93cf-fe4cfbdfb077', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e7482748-9747-403c-93cf-fe4cfbdfb077 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T13:56:54,289110195-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:56:54,295547640-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e7482748-9747-403c-93cf-fe4cfbdfb077', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e7482748-9747-403c-93cf-fe4cfbdfb077 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T13:56:56,954097140-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:56:56,963643819-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e7482748-9747-403c-93cf-fe4cfbdfb077', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e7482748-9747-403c-93cf-fe4cfbdfb077 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T13:56:59,689192135-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:57:22,517367799-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   202 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:57:30,368158671-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:57:38,471649795-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 29s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:57:46,463206492-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 55s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:57:54,346687887-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [=======.....................] (remaining: 65s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:58:02,272284839-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 85s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:58:10,369719388-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 97s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:58:18,326109412-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   223 KiB used, 300 GiB / 300 GiB avail
    pgs:     0.521% pgs not active
             191 active+clean
             1   activating
 
  progress:
    Global Recovery Event (50s)
      [========....................] (remaining: 112s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:58:26,316256608-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:58:26,332151047-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:58:34,431203189-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:58:34,448020917-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:58:42,463057171-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:58:42,478998687-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:58:50,341277185-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:58:50,357180174-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:58:58,333929252-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:58:58,349758698-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:58:58,362099909-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T13:58:58,369377840-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:58:58,385652848-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1136219
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T13:58:58,399903873-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-17T13:58:58,441055677-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T13:58:58,447720603-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e7482748-9747-403c-93cf-fe4cfbdfb077', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e7482748-9747-403c-93cf-fe4cfbdfb077 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T20:59:00.071+0000 ffffb62d5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:59:00.079+0000 ffffb62d5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T20:59:00.079+0000 ffffb62d5010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T20:59:00.098707+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-17T20:59:00.098747+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-17T20:59:00.103334+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:59:00.103334+0000     0       0         0         0         0         0           -           0
2021-05-17T20:59:01.103525+0000     1     255     19137     18882   295.007   295.031  0.00805621   0.0132823
2021-05-17T20:59:02.103716+0000     2     255     37404     37149   290.187   285.422   0.0051878   0.0136238
2021-05-17T20:59:03.103878+0000     3     255     56661     56406   293.739   300.891  0.00568917   0.0135111
2021-05-17T20:59:04.104044+0000     4     255     75980     75725   295.756   301.859   0.0100386   0.0134391
2021-05-17T20:59:05.104218+0000     5     255     93236     92981   290.521   269.625   0.0099965   0.0137288
2021-05-17T20:59:06.104399+0000     6     255    112907    112652   293.318   307.359  0.00494302   0.0135957
2021-05-17T20:59:07.104566+0000     7     255    131401    131146    292.69   288.969   0.0151433    0.013636
2021-05-17T20:59:08.104752+0000     8     255    150346    150091   293.099   296.016   0.0750326   0.0136001
2021-05-17T20:59:09.105073+0000     9     255    167281    167026   289.923   264.609  0.00944076   0.0137692
2021-05-17T20:59:10.105292+0000    10     255    183988    183733    287.03   261.047   0.0105019   0.0139142
2021-05-17T20:59:11.105596+0000    11     255    200950    200695   285.022   265.031  0.00276883   0.0140064
2021-05-17T20:59:12.105850+0000    12     256    217286    217030   282.534   255.234  0.00903551   0.0141091
2021-05-17T20:59:13.106004+0000    13     255    232080    231825   278.581   231.172   0.0197747   0.0143362
2021-05-17T20:59:14.106247+0000    14     255    248335    248080    276.82   253.984   0.0161374   0.0144324
2021-05-17T20:59:15.106433+0000    15     256    263542    263286   274.202   237.594   0.0112645   0.0145662
2021-05-17T20:59:16.106946+0000    16     255    279506    279251   272.646   249.453  0.00267525   0.0146323
2021-05-17T20:59:17.107165+0000    17     256    296852    296596   272.547   271.016  0.00328885   0.0146397
2021-05-17T20:59:18.107526+0000    18     255    311182    310927   269.841   223.922   0.0125596   0.0148084
2021-05-17T20:59:19.107836+0000    19     255    326319    326064   268.083   236.516   0.0139373    0.014905
2021-05-17T20:59:20.108150+0000 min lat: 0.000807259 max lat: 0.143465 avg lat: 0.0149294
2021-05-17T20:59:20.108150+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T20:59:20.108150+0000    20     154    342833    342679   267.655   259.609  0.00517214   0.0149294
2021-05-17T20:59:21.108442+0000 Total time run:         20.0153
Total writes made:      342833
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     267.634
Stddev Bandwidth:       25.2397
Max bandwidth (MB/sec): 307.359
Min bandwidth (MB/sec): 223.922
Average IOPS:           17128
Stddev IOPS:            1615.34
Max IOPS:               19671
Min IOPS:               14331
Average Latency(s):     0.0149309
Stddev Latency(s):      0.0140341
Max latency(s):         0.143465
Min latency(s):         0.000807259

[1;32mlocalhost.localdomain	[2021-05-17T13:59:21,407074044-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1136219


[1;33mlocalhost.localdomain	[2021-05-17T13:59:21,414889622-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T13:59:44,548043869-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:59:44,563904129-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T13:59:52,454841388-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T13:59:52,473194700-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:00:00,422488524-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:00:00,438270770-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:00:08,451550799-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:00:08,467678568-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:00:16,459026205-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:00:16,475073590-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:00:16,487212730-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:00:16,494395348-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:00:16,510447504-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1139569
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T14:00:16,524183412-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-17T14:00:16,563981930-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:00:16,570270904-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e7482748-9747-403c-93cf-fe4cfbdfb077', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e7482748-9747-403c-93cf-fe4cfbdfb077 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wni5kt:/tmp/ceph-asok.wni5kt -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:00:18.057+0000 ffffa9f06010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:00:18.189+0000 ffffa9f06010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:00:18.189+0000 ffffa9f06010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:00:18.205673+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:00:18.205673+0000     0       0         0         0         0         0           -           0
2021-05-17T21:00:19.205843+0000     1     256     13606     13350   208.529   208.594  0.00040804   0.0183297
2021-05-17T21:00:20.206033+0000     2     255     36114     35859   280.078   351.703  0.00126764   0.0141697
2021-05-17T21:00:21.206209+0000     3     256     56494     56238    292.84   318.422  0.00776172   0.0135897
2021-05-17T21:00:22.206453+0000     4     255     79478     79223   309.394   359.141 0.000582745   0.0128794
2021-05-17T21:00:23.206666+0000     5     255     99736     99481   310.808   316.531  0.00317177   0.0128213
2021-05-17T21:00:24.206849+0000     6     255    120321    120066   312.603   321.641   0.0241948   0.0127565
2021-05-17T21:00:25.207023+0000     7     255    139613    139358   311.001   301.438  0.00533246   0.0128304
2021-05-17T21:00:26.209837+0000     8     255    161233    160978   314.241   337.812   0.0102656   0.0127051
2021-05-17T21:00:27.210022+0000     9     255    180993    180738   313.625    308.75   0.0127608   0.0127279
2021-05-17T21:00:28.210207+0000    10     255    200974    200719   313.477   312.203   0.0019627   0.0127303
2021-05-17T21:00:29.210385+0000    11     255    223112    222857   316.419   345.906  0.00364824   0.0126184
2021-05-17T21:00:30.210606+0000    12     256    243440    243184   316.512   317.609  0.00536186   0.0126154
2021-05-17T21:00:31.210793+0000    13     255    264613    264358   317.609   330.844     0.01485   0.0125744
2021-05-17T21:00:32.210977+0000    14     256    284938    284682   317.602   317.562   0.0193822    0.012573
2021-05-17T21:00:33.211151+0000    15     255    303074    302819   315.318   283.391   0.0142671   0.0126656
2021-05-17T21:00:34.211355+0000    16     255    321771    321516   313.866   292.141  0.00146713   0.0127249
2021-05-17T21:00:35.211545+0000    17     255    342569    342314   314.516   324.969   0.0101228   0.0127006
2021-05-17T21:00:36.211814+0000 Total time run:       17.0249
Total reads made:     342833
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   314.644
Average IOPS:         20137
Stddev IOPS:          2162.4
Max IOPS:             22985
Min IOPS:             13350
Average Latency(s):   0.0126955
Max latency(s):       0.186985
Min latency(s):       0.000326861

[1;32mlocalhost.localdomain	[2021-05-17T14:00:36,528438190-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1139569


[1;33mlocalhost.localdomain	[2021-05-17T14:00:36,537895784-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:00:59,462460614-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:00:59,478046773-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:01:07,631921410-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:01:07,648062530-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:01:15,798273035-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:01:15,814041865-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:01:24,140100706-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:01:24,156479090-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:01:32,208399072-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:01:32,224204537-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:01:32,236965636-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:01:32,241625750-07:00][RUNNING][ROUND 5/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:01:32,248756323-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T14:01:32,265877709-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40448\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.436854\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid d4460775-54d3-46ff-a27a-1b98235f0656\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid d4460775-54d3-46ff-a27a-1b98235f0656\nlast_changed 2021-05-17T14:02:00.430454-0700\ncreated 2021-05-17T14:02:00.430454-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40448/0,v1:10.10.1.2:40449/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.436854 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 827d1d84-bfa0-4da6-863c-9aeb20adea1a\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 70185d7f-fded-4681-98b2-ec1155c78081\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 89b1921c-ef82-4e5a-a86d-c81734cf380c\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42448\n'
10.10.1.2: b'  w/ user/pass: admin / 7267144a-e768-4e11-9093-eb166a8891b5\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:02:15 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40448
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.436854
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid d4460775-54d3-46ff-a27a-1b98235f0656
setting min_mon_release = octopus
epoch 0
fsid d4460775-54d3-46ff-a27a-1b98235f0656
last_changed 2021-05-17T14:02:00.430454-0700
created 2021-05-17T14:02:00.430454-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40448/0,v1:10.10.1.2:40449/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.436854 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 827d1d84-bfa0-4da6-863c-9aeb20adea1a
0
start osd.0
add osd1 70185d7f-fded-4681-98b2-ec1155c78081
1
start osd.1
add osd2 89b1921c-ef82-4e5a-a86d-c81734cf380c
2
start osd.2


restful urls: https://10.10.1.2:42448
  w/ user/pass: admin / 7267144a-e768-4e11-9093-eb166a8891b5


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T14:01:33.290-0700 7fc8b358d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:01:33.290-0700 7fc8b358d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:01:33.306-0700 7f642539d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:01:33.306-0700 7f642539d1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40448,v1:10.10.1.2:40449] --print /tmp/ceph_monmap.436854 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.436854 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.436854 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42448 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.Ly8HKCcqr4 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 827d1d84-bfa0-4da6-863c-9aeb20adea1a -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDR2aJg59Y1IBAA0mx3w7Kkcw+24fmkVTGiPg== --osd-uuid 827d1d84-bfa0-4da6-863c-9aeb20adea1a 
2021-05-17T14:02:09.890-0700 7f01da929f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:02:09.890-0700 7f01da929f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:02:09.890-0700 7f01da929f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:02:09.934-0700 7f01da929f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 70185d7f-fded-4681-98b2-ec1155c78081 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T14:02:10.246-0700 7f1502d98f00 -1 Falling back to public interface
2021-05-17T14:02:10.258-0700 7f1502d98f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDS2aJgz0qvDhAA7SzCQZdEqPRp5l+ulEre+g== --osd-uuid 70185d7f-fded-4681-98b2-ec1155c78081 
2021-05-17T14:02:10.574-0700 7fa89e6dcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:02:10.578-0700 7fa89e6dcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:02:10.578-0700 7fa89e6dcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:02:10.622-0700 7fa89e6dcf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 89b1921c-ef82-4e5a-a86d-c81734cf380c -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T14:02:11.006-0700 7fb61b439f00 -1 Falling back to public interface
2021-05-17T14:02:11.018-0700 7fb61b439f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDT2aJg95NoABAAsi7/bxbeSzDiOn4hVF3dVg== --osd-uuid 89b1921c-ef82-4e5a-a86d-c81734cf380c 
2021-05-17T14:02:11.394-0700 7f7615b09f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:02:11.394-0700 7f7615b09f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:02:11.394-0700 7f7615b09f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:02:11.458-0700 7f7615b09f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T14:02:11.758-0700 7f81fc77ff00 -1 Falling back to public interface
2021-05-17T14:02:11.774-0700 7f81fc77ff00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T14:02:15,747290897-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:02:15,754897216-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T14:02:15,838490194-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:02:15,845348275-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0d7acca-106d-4210-94ea-c202a27e9b86', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0d7acca-106d-4210-94ea-c202a27e9b86 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T14:02:18,830688079-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:02:18,836974382-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0d7acca-106d-4210-94ea-c202a27e9b86', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0d7acca-106d-4210-94ea-c202a27e9b86 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T14:02:21,610328120-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:02:21,616727796-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0d7acca-106d-4210-94ea-c202a27e9b86', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0d7acca-106d-4210-94ea-c202a27e9b86 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T14:02:24,426080834-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:02:24,432373692-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0d7acca-106d-4210-94ea-c202a27e9b86', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0d7acca-106d-4210-94ea-c202a27e9b86 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T14:02:30,083381544-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:02:30,089777605-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0d7acca-106d-4210-94ea-c202a27e9b86', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0d7acca-106d-4210-94ea-c202a27e9b86 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T14:02:34,170123846-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:02:34,177882710-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0d7acca-106d-4210-94ea-c202a27e9b86', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0d7acca-106d-4210-94ea-c202a27e9b86 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T14:02:37,426350333-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:02:37,433622724-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0d7acca-106d-4210-94ea-c202a27e9b86', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0d7acca-106d-4210-94ea-c202a27e9b86 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T14:02:40,607031263-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:02:40,613508293-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0d7acca-106d-4210-94ea-c202a27e9b86', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0d7acca-106d-4210-94ea-c202a27e9b86 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:02:44,187459975-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:02:44,196450595-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0d7acca-106d-4210-94ea-c202a27e9b86', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0d7acca-106d-4210-94ea-c202a27e9b86 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:02:47,645039752-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:02:47,652665844-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0d7acca-106d-4210-94ea-c202a27e9b86', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0d7acca-106d-4210-94ea-c202a27e9b86 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T14:02:50,893699066-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:02:50,900961661-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0d7acca-106d-4210-94ea-c202a27e9b86', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0d7acca-106d-4210-94ea-c202a27e9b86 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T14:02:53,600319798-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:02:53,607106151-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0d7acca-106d-4210-94ea-c202a27e9b86', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0d7acca-106d-4210-94ea-c202a27e9b86 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  146 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  146 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.05   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.99   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.97   70      up          osd.2  
                       TOTAL  300 GiB  147 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.97/1.05  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T14:02:56,474843732-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:03:19,403987734-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   205 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=========...................] (remaining: 9s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:03:27,269485826-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:03:35,403079537-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:03:43,534596250-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:03:51,484763656-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:03:59,410442486-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:04:07,813015305-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:04:16,038658865-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:04:16,054790181-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:04:24,066920968-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:04:24,082701614-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:04:32,037320853-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:04:32,053668581-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:04:40,016505660-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:04:40,032397730-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:04:48,264761969-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:04:48,280654699-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:04:48,292946827-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:04:48,300077218-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:04:48,316418254-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1152984
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T14:04:48,330343367-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-17T14:04:48,371249292-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:04:48,377897095-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0d7acca-106d-4210-94ea-c202a27e9b86', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0d7acca-106d-4210-94ea-c202a27e9b86 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:04:50.123+0000 ffff8a0f5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:04:50.135+0000 ffff8a0f5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:04:50.135+0000 ffff8a0f5010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:04:50.152626+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-17T21:04:50.152677+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T21:04:50.157130+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:04:50.157130+0000     0       0         0         0         0         0           -           0
2021-05-17T21:04:51.157311+0000     1     255     19810     19555   305.531   305.547   0.0109972   0.0129089
2021-05-17T21:04:52.157480+0000     2     255     37121     36866   287.984   270.484  0.00761182    0.013576
2021-05-17T21:04:53.157647+0000     3     255     53578     53323   277.688   257.141    0.108116    0.014246
2021-05-17T21:04:54.157847+0000     4     255     70675     70420   275.038   267.141  0.00376987   0.0143941
2021-05-17T21:04:55.158016+0000     5     255     87635     87380   273.021       265  0.00274149   0.0145214
2021-05-17T21:04:56.158194+0000     6     255    104937    104682   272.567   270.344  0.00255425   0.0145497
2021-05-17T21:04:57.158374+0000     7     255    121352    121097   270.263   256.484  0.00795789   0.0147652
2021-05-17T21:04:58.158638+0000     8     255    137984    137729   268.956   259.875  0.00399569   0.0147787
2021-05-17T21:04:59.158966+0000     9     255    151914    151659   263.247   217.656    0.013553   0.0151684
2021-05-17T21:05:00.159301+0000    10     255    168292    168037   262.504   255.906  0.00985411   0.0152128
2021-05-17T21:05:01.159476+0000    11     255    186992    186737   265.198   292.188    0.015934   0.0150613
2021-05-17T21:05:02.159643+0000    12     255    203855    203600   265.051   263.484  0.00407299   0.0150484
2021-05-17T21:05:03.159803+0000    13     255    219772    219517   263.791   248.703  0.00233506   0.0150874
2021-05-17T21:05:04.159989+0000    14     255    236722    236467   263.863   264.844   0.0272502   0.0151419
2021-05-17T21:05:05.160364+0000    15     255    255159    254904    265.47   288.078   0.0182114   0.0150501
2021-05-17T21:05:06.160581+0000    16     255    272249    271994   265.564   267.031   0.0136604   0.0150462
2021-05-17T21:05:07.160761+0000    17     255    288252    287997   264.649   250.047  0.00687515   0.0150997
2021-05-17T21:05:08.161036+0000    18     256    303123    302867    262.85   232.344  0.00811532   0.0151452
2021-05-17T21:05:09.161223+0000    19     255    318682    318427    261.81   243.125   0.0144915   0.0152641
2021-05-17T21:05:10.161579+0000 min lat: 0.000742192 max lat: 0.307699 avg lat: 0.015315
2021-05-17T21:05:10.161579+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:05:10.161579+0000    20      36    334173    334137   260.988   245.469   0.0140361    0.015315
2021-05-17T21:05:11.161936+0000 Total time run:         20.01
Total writes made:      334173
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     260.942
Stddev Bandwidth:       19.9642
Max bandwidth (MB/sec): 305.547
Min bandwidth (MB/sec): 217.656
Average IOPS:           16700
Stddev IOPS:            1277.71
Max IOPS:               19555
Min IOPS:               13930
Average Latency(s):     0.0153147
Stddev Latency(s):      0.0222773
Max latency(s):         0.307699
Min latency(s):         0.000742192

[1;32mlocalhost.localdomain	[2021-05-17T14:05:11,474470152-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1152984


[1;33mlocalhost.localdomain	[2021-05-17T14:05:11,482242607-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:05:34,496080705-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.17k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:05:34,511674898-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:05:42,501380562-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.17k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:05:42,517314908-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:05:50,929831756-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.17k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:05:50,945676821-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:05:58,823171199-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.17k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:05:58,839088306-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:06:07,108115899-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.17k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:06:07,123592306-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:06:07,136017971-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:06:07,143325571-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:06:07,159637515-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1156291
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T14:06:07,173089836-07:00] INFO: > Run rados bench[0m
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.5
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
[1;30mlocalhost.localdomain	[2021-05-17T14:06:07,212835833-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:06:07,219262662-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0d7acca-106d-4210-94ea-c202a27e9b86', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0d7acca-106d-4210-94ea-c202a27e9b86 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BQ81Vb:/tmp/ceph-asok.BQ81Vb -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:06:08.733+0000 ffffba29a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:06:08.741+0000 ffffba29a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:06:08.741+0000 ffffba29a010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:06:08.761052+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:06:08.761052+0000     0       0         0         0         0         0           -           0
2021-05-17T21:06:09.761225+0000     1     255     21538     21283   332.445   332.547   0.0288581   0.0118183
2021-05-17T21:06:10.761407+0000     2     256     41035     40779   318.508   304.625   0.0238921   0.0123631
2021-05-17T21:06:11.761981+0000     3     255     58969     58714   305.694   280.234  0.00169958   0.0129635
2021-05-17T21:06:12.762191+0000     4     255     77627     77372   302.138   291.531  0.00412862   0.0131679
2021-05-17T21:06:13.762399+0000     5     255     98998     98743   308.481   333.922    0.026778   0.0129108
2021-05-17T21:06:14.762576+0000     6     256    119431    119175   310.266    319.25 0.000377299   0.0127448
2021-05-17T21:06:15.762741+0000     7     256    140764    140508   313.552   333.328   0.0080796   0.0127223
2021-05-17T21:06:16.762941+0000     8     255    164611    164356   320.927   372.625  0.00189736   0.0124274
2021-05-17T21:06:17.763132+0000     9     256    184012    183756   318.942   303.125  0.00157726   0.0124326
2021-05-17T21:06:18.763326+0000    10     255    204286    204031   318.722   316.797  0.00301853   0.0125169
2021-05-17T21:06:19.763581+0000    11     255    226024    225769   320.617   339.656  0.00485297   0.0124476
2021-05-17T21:06:20.763846+0000    12     256    244179    243923   317.531   283.656  0.00200284   0.0125697
2021-05-17T21:06:21.764044+0000    13     255    261915    261660    314.42   277.141   0.0339373   0.0126921
2021-05-17T21:06:22.764256+0000    14     256    277265    277009   309.088   239.828   0.0374615   0.0129047
2021-05-17T21:06:23.764518+0000    15     255    296170    295915   308.171   295.406  0.00198521   0.0129555
2021-05-17T21:06:24.764854+0000    16     255    315586    315331   307.865   303.375  0.00353822   0.0129679
2021-05-17T21:06:25.765219+0000 Total time run:       16.9749
Total reads made:     334173
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   307.599
Average IOPS:         19686
Stddev IOPS:          2006.79
Max IOPS:             23848
Min IOPS:             15349
Average Latency(s):   0.0129816
Max latency(s):       0.177411
Min latency(s):       0.000326486

[1;32mlocalhost.localdomain	[2021-05-17T14:06:26,121846935-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1156291


[1;33mlocalhost.localdomain	[2021-05-17T14:06:26,129979841-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:06:49,159932602-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.17k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:06:49,175900432-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:06:57,908617060-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.17k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:06:57,924436005-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:07:06,330842740-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.17k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:07:06,346624733-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:07:14,342797979-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.17k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:07:14,359140107-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:07:22,413521832-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.17k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:07:22,429670742-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:07:22,442311303-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:07:22,450122553-07:00][RUNNING][ROUND 1/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:07:22,457447570-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T14:07:22,474334171-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40611\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.437966\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid dcba3ea0-ab10-4bfc-9b84-fe67e8454105\n'
10.10.1.2: b'setting min_mon_release = octopus\nepoch 0\nfsid dcba3ea0-ab10-4bfc-9b84-fe67e8454105\nlast_changed 2021-05-17T14:07:50.947611-0700\ncreated 2021-05-17T14:07:50.947611-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40611/0,v1:10.10.1.2:40612/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.437966 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 d7489fff-edb5-4d43-80ca-ee6edb6a4b93\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 7ea3ed7c-82f9-4d6a-a44f-d4f97f3f2e72\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 f4af5579-87a1-4cbc-a61f-e9543b90623b\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42611\n  w/ user/pass: admin / 5fc56dd0-941e-4c07-90fb-b0cb02d7885f\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:08:05 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40611
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.437966
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid dcba3ea0-ab10-4bfc-9b84-fe67e8454105
setting min_mon_release = octopus
epoch 0
fsid dcba3ea0-ab10-4bfc-9b84-fe67e8454105
last_changed 2021-05-17T14:07:50.947611-0700
created 2021-05-17T14:07:50.947611-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40611/0,v1:10.10.1.2:40612/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.437966 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 d7489fff-edb5-4d43-80ca-ee6edb6a4b93
0
start osd.0
add osd1 7ea3ed7c-82f9-4d6a-a44f-d4f97f3f2e72
1
start osd.1
add osd2 f4af5579-87a1-4cbc-a61f-e9543b90623b
2
start osd.2


restful urls: https://10.10.1.2:42611
  w/ user/pass: admin / 5fc56dd0-941e-4c07-90fb-b0cb02d7885f


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T14:07:23.489-0700 7f98ebb941c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:07:23.489-0700 7f98ebb941c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:07:23.505-0700 7f45f68ae1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:07:23.505-0700 7f45f68ae1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40611,v1:10.10.1.2:40612] --print /tmp/ceph_monmap.437966 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.437966 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.437966 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42611 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.WxOioxzADe 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d7489fff-edb5-4d43-80ca-ee6edb6a4b93 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAv26JgZz+/LhAAmW9/fOWgDxaXzp8iXqCMgA== --osd-uuid d7489fff-edb5-4d43-80ca-ee6edb6a4b93 
2021-05-17T14:08:00.109-0700 7f567ef86f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:08:00.109-0700 7f567ef86f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:08:00.109-0700 7f567ef86f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:08:00.189-0700 7f567ef86f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7ea3ed7c-82f9-4d6a-a44f-d4f97f3f2e72 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T14:08:00.465-0700 7f72eb624f00 -1 Falling back to public interface
2021-05-17T14:08:00.481-0700 7f72eb624f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAw26JgCsjZGxAAaJTjasgq+dxDkYzLfY0ilA== --osd-uuid 7ea3ed7c-82f9-4d6a-a44f-d4f97f3f2e72 
2021-05-17T14:08:00.797-0700 7f65143fef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:08:00.797-0700 7f65143fef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:08:00.797-0700 7f65143fef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:08:00.865-0700 7f65143fef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f4af5579-87a1-4cbc-a61f-e9543b90623b -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T14:08:01.145-0700 7f5a56992f00 -1 Falling back to public interface
2021-05-17T14:08:01.157-0700 7f5a56992f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAx26JgU6KbCBAARM+qYgisT2FklWmjWmLMaQ== --osd-uuid f4af5579-87a1-4cbc-a61f-e9543b90623b 
2021-05-17T14:08:01.473-0700 7f5ddede7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:08:01.473-0700 7f5ddede7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:08:01.473-0700 7f5ddede7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:08:01.521-0700 7f5ddede7f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T14:08:01.901-0700 7fa2528a9f00 -1 Falling back to public interface
2021-05-17T14:08:01.917-0700 7fa2528a9f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T14:08:05,804099762-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:08:05,811596888-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T14:08:05,895562531-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:08:05,901956834-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b55a1bc-c742-4ea3-ae7d-1239f99d39de', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b55a1bc-c742-4ea3-ae7d-1239f99d39de --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T14:08:08,688454811-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:08:08,697490240-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b55a1bc-c742-4ea3-ae7d-1239f99d39de', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b55a1bc-c742-4ea3-ae7d-1239f99d39de --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T14:08:11,501701047-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:08:11,508122924-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b55a1bc-c742-4ea3-ae7d-1239f99d39de', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b55a1bc-c742-4ea3-ae7d-1239f99d39de --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T14:08:14,410358611-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:08:14,416745668-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b55a1bc-c742-4ea3-ae7d-1239f99d39de', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b55a1bc-c742-4ea3-ae7d-1239f99d39de --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T14:08:20,181701975-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:08:20,188465671-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b55a1bc-c742-4ea3-ae7d-1239f99d39de', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b55a1bc-c742-4ea3-ae7d-1239f99d39de --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T14:08:23,880578534-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:08:23,887003397-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b55a1bc-c742-4ea3-ae7d-1239f99d39de', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b55a1bc-c742-4ea3-ae7d-1239f99d39de --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T14:08:27,460338628-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:08:27,466694329-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b55a1bc-c742-4ea3-ae7d-1239f99d39de', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b55a1bc-c742-4ea3-ae7d-1239f99d39de --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T14:08:30,684315218-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:08:30,690769126-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b55a1bc-c742-4ea3-ae7d-1239f99d39de', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b55a1bc-c742-4ea3-ae7d-1239f99d39de --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:08:33,961055438-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:08:33,967365081-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b55a1bc-c742-4ea3-ae7d-1239f99d39de', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b55a1bc-c742-4ea3-ae7d-1239f99d39de --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:08:37,211829960-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:08:37,218229513-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b55a1bc-c742-4ea3-ae7d-1239f99d39de', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b55a1bc-c742-4ea3-ae7d-1239f99d39de --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T14:08:40,473988417-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:08:40,480770341-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b55a1bc-c742-4ea3-ae7d-1239f99d39de', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b55a1bc-c742-4ea3-ae7d-1239f99d39de --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T14:08:43,311571375-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:08:43,318014165-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b55a1bc-c742-4ea3-ae7d-1239f99d39de', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b55a1bc-c742-4ea3-ae7d-1239f99d39de --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T14:08:45,975722536-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:09:08,936410367-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   192 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:09:17,013033414-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:09:25,356838701-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 28s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:09:33,326075747-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:09:41,642934121-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (30s)
      [========....................] (remaining: 69s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:09:49,669126060-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 75s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:09:57,652838385-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 85s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:10:05,775367170-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 41s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:10:13,695891553-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:10:13,711918880-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:10:21,693599503-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:10:21,709696757-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:10:29,687635990-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:10:29,703574982-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:10:37,790943058-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:10:37,806986422-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:10:45,695486584-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:10:45,711672007-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:10:45,724527792-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:10:45,731804905-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:10:45,748650516-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1170200
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T14:10:45,762879740-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T14:10:45,803092591-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:10:45,809781198-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b55a1bc-c742-4ea3-ae7d-1239f99d39de', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b55a1bc-c742-4ea3-ae7d-1239f99d39de --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:10:47.568+0000 ffff971dc010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:10:47.576+0000 ffff971dc010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:10:47.576+0000 ffff971dc010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:10:47.595367+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-17T21:10:47.595408+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T21:10:47.609543+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:10:47.609543+0000     0       6         6         0         0         0           -           0
2021-05-17T21:10:48.609713+0000     1     255      8495      8240   514.672       515   0.0106434    0.030271
2021-05-17T21:10:49.609913+0000     2     256     16751     16495   515.253   515.938   0.0143791   0.0304653
2021-05-17T21:10:50.610117+0000     3     256     25140     24884   518.237   524.312  0.00896334    0.030666
2021-05-17T21:10:51.610308+0000     4     256     33642     33386   521.496   531.375   0.0126814   0.0304699
2021-05-17T21:10:52.610500+0000     5     256     42108     41852   523.001   529.125   0.0085773   0.0304339
2021-05-17T21:10:53.610722+0000     6     256     50451     50195   522.721   521.438  0.00245685   0.0304302
2021-05-17T21:10:54.610949+0000     7     256     58973     58717   524.119   532.625   0.0069259   0.0304085
2021-05-17T21:10:55.611379+0000     8     255     67343     67088   523.974   523.188   0.0387341   0.0303892
2021-05-17T21:10:56.611621+0000     9     255     75905     75650   525.199   535.125    0.010171   0.0303817
2021-05-17T21:10:57.611831+0000    10     256     84366     84110   525.543    528.75   0.0124593   0.0303607
2021-05-17T21:10:58.612050+0000    11     255     92794     92539   525.648   526.812   0.0129182   0.0303244
2021-05-17T21:10:59.612226+0000    12     256    100873    100617   523.909   504.875  0.00147736   0.0302866
2021-05-17T21:11:00.612389+0000    13     255    108787    108532   521.656   494.688   0.0114863   0.0306225
2021-05-17T21:11:01.612581+0000    14     255    116969    116714   520.914   511.375   0.0355944    0.030625
2021-05-17T21:11:02.612843+0000    15     255    124263    124008    516.57   455.875  0.00509221   0.0309213
2021-05-17T21:11:03.613027+0000    16     256    131373    131117   512.049   444.312  0.00110251   0.0311364
2021-05-17T21:11:04.613197+0000    17     255    138786    138531   509.182   463.375  0.00930666   0.0313587
2021-05-17T21:11:05.613453+0000    18     256    146949    146693   509.227   510.125  0.00961619   0.0313616
2021-05-17T21:11:06.613783+0000    19     256    155177    154921   509.482    514.25  0.00679805   0.0313624
2021-05-17T21:11:07.614006+0000 min lat: 0.00102377 max lat: 0.196998 avg lat: 0.0314302
2021-05-17T21:11:07.614006+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:11:07.614006+0000    20     242    162841    162599   507.997   479.875  0.00169353   0.0314302
2021-05-17T21:11:08.614288+0000 Total time run:         20.021
Total writes made:      162841
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     508.345
Stddev Bandwidth:       26.8581
Max bandwidth (MB/sec): 535.125
Min bandwidth (MB/sec): 444.312
Average IOPS:           8133
Stddev IOPS:            429.73
Max IOPS:               8562
Min IOPS:               7109
Average Latency(s):     0.0314508
Stddev Latency(s):      0.0273367
Max latency(s):         0.196998
Min latency(s):         0.00102377

[1;32mlocalhost.localdomain	[2021-05-17T14:11:08,937287365-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1170200


[1;33mlocalhost.localdomain	[2021-05-17T14:11:08,945311994-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:11:34,021326494-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 162.84k objects, 9.9 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:11:34,037251548-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:11:42,457567756-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 162.84k objects, 9.9 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:11:42,473278738-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:11:50,512606354-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 162.84k objects, 9.9 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:11:50,528778829-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:11:58,599623795-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 162.84k objects, 9.9 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:11:58,616478407-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:12:06,872277761-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 162.84k objects, 9.9 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:12:06,888371128-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:12:06,901160091-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:12:06,908614533-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:12:06,925253378-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1173619
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T14:12:06,939820774-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-17T14:12:06,978760490-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:12:06,985100212-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b55a1bc-c742-4ea3-ae7d-1239f99d39de', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b55a1bc-c742-4ea3-ae7d-1239f99d39de --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.elrpKQ:/tmp/ceph-asok.elrpKQ -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:12:08.462+0000 ffffbd443010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:12:08.470+0000 ffffbd443010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:12:08.470+0000 ffffbd443010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:12:08.491040+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:12:08.491040+0000     0       0         0         0         0         0           -           0
2021-05-17T21:12:09.491259+0000     1     255     11216     10961   684.805   685.062   0.0522129   0.0228598
2021-05-17T21:12:10.491666+0000     2     256     23794     23538   735.275   786.062  0.00254527   0.0214514
2021-05-17T21:12:11.492001+0000     3     256     34829     34573   720.002   689.688   0.0543677    0.022004
2021-05-17T21:12:12.492287+0000     4     255     44834     44579   696.302   625.375   0.0132741   0.0228657
2021-05-17T21:12:13.492523+0000     5     255     57334     57079   713.253    781.25   0.0449079   0.0223156
2021-05-17T21:12:14.493043+0000     6     256     69145     68889   717.336   738.125  0.00865316   0.0221728
2021-05-17T21:12:15.493272+0000     7     256     81720     81464   727.109   785.938   0.0420048   0.0219122
2021-05-17T21:12:16.493576+0000     8     255     94816     94561   738.509   818.562  0.00363439   0.0215871
2021-05-17T21:12:17.494323+0000     9     256    107756    107500   746.243   808.688   0.0482681   0.0213546
2021-05-17T21:12:18.494871+0000    10     255    119284    119029   743.635   720.562   0.0054663   0.0214361
2021-05-17T21:12:19.495103+0000    11     256    130648    130392   740.579   710.188   0.0339338   0.0215038
2021-05-17T21:12:20.495316+0000    12     256    141168    140912   733.646     657.5   0.0169524   0.0217516
2021-05-17T21:12:21.495512+0000    13     256    154200    153944   739.852     814.5  0.00393001   0.0215543
2021-05-17T21:12:22.495753+0000 Total time run:       13.8169
Total reads made:     162841
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   736.603
Average IOPS:         11785
Stddev IOPS:          1023.86
Max IOPS:             13097
Min IOPS:             10006
Average Latency(s):   0.0216591
Max latency(s):       0.179755
Min latency(s):       0.000457278

[1;32mlocalhost.localdomain	[2021-05-17T14:12:22,906097837-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1173619


[1;33mlocalhost.localdomain	[2021-05-17T14:12:22,915429430-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:12:45,993194259-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 162.84k objects, 9.9 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:12:46,009215763-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:12:54,022316910-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 162.84k objects, 9.9 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:12:54,038698716-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:13:02,134716304-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 162.84k objects, 9.9 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:13:02,150817216-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:13:10,227780959-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 162.84k objects, 9.9 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:13:10,243861933-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:13:18,260093536-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 162.84k objects, 9.9 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:13:18,276227108-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:13:18,289141957-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:13:18,294030226-07:00][RUNNING][ROUND 2/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:13:18,301945967-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T14:13:18,319346326-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40005\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.439077\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid eb4430d2-1a61-4cd6-8512-9429fdc32660\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid eb4430d2-1a61-4cd6-8512-9429fdc32660\nlast_changed 2021-05-17T14:13:46.546770-0700\ncreated 2021-05-17T14:13:46.546770-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40005/0,v1:10.10.1.2:40006/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.439077 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 942b78ba-78ed-4bf2-a0ab-529511e6e128\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 acc7180a-59f2-4ee4-83cf-f2108dddf3f6\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 d2361373-ae6f-477f-8967-ab2abb44a505\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42005\n'
10.10.1.2: b'  w/ user/pass: admin / cdacf6d5-fc7c-460f-a0a8-698e85431157\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:14:01 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40005
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.439077
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid eb4430d2-1a61-4cd6-8512-9429fdc32660
setting min_mon_release = octopus
epoch 0
fsid eb4430d2-1a61-4cd6-8512-9429fdc32660
last_changed 2021-05-17T14:13:46.546770-0700
created 2021-05-17T14:13:46.546770-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40005/0,v1:10.10.1.2:40006/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.439077 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 942b78ba-78ed-4bf2-a0ab-529511e6e128
0
start osd.0
add osd1 acc7180a-59f2-4ee4-83cf-f2108dddf3f6
1
start osd.1
add osd2 d2361373-ae6f-477f-8967-ab2abb44a505
2
start osd.2


restful urls: https://10.10.1.2:42005
  w/ user/pass: admin / cdacf6d5-fc7c-460f-a0a8-698e85431157


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T14:13:19.304-0700 7efdc02191c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:13:19.304-0700 7efdc02191c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:13:19.320-0700 7fc2c25ac1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:13:19.320-0700 7fc2c25ac1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40005,v1:10.10.1.2:40006] --print /tmp/ceph_monmap.439077 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.439077 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.439077 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42005 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.iTBVmYOKWN 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 942b78ba-78ed-4bf2-a0ab-529511e6e128 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCT3KJgEEPlExAAeKL/eTG2nQEFmFtRj5X7XA== --osd-uuid 942b78ba-78ed-4bf2-a0ab-529511e6e128 
2021-05-17T14:13:55.717-0700 7f4cf0650f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:13:55.717-0700 7f4cf0650f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:13:55.717-0700 7f4cf0650f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:13:55.769-0700 7f4cf0650f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new acc7180a-59f2-4ee4-83cf-f2108dddf3f6 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T14:13:56.089-0700 7f499ce6ef00 -1 Falling back to public interface
2021-05-17T14:13:56.101-0700 7f499ce6ef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCU3KJgaMKBBRAAg/Kf7RgRfMNBOKT5YBv8Gw== --osd-uuid acc7180a-59f2-4ee4-83cf-f2108dddf3f6 
2021-05-17T14:13:56.437-0700 7fdc98d5ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:13:56.437-0700 7fdc98d5ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:13:56.437-0700 7fdc98d5ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:13:56.489-0700 7fdc98d5ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d2361373-ae6f-477f-8967-ab2abb44a505 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T14:13:56.861-0700 7fb17a75cf00 -1 Falling back to public interface
2021-05-17T14:13:56.873-0700 7fb17a75cf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCU3KJgcvZJMxAAcowRF8uJYiF7WkgqQ2LUfQ== --osd-uuid d2361373-ae6f-477f-8967-ab2abb44a505 
2021-05-17T14:13:57.197-0700 7fd913742f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:13:57.197-0700 7fd913742f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:13:57.197-0700 7fd913742f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:13:57.285-0700 7fd913742f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T14:13:57.605-0700 7f82d337ef00 -1 Falling back to public interface
2021-05-17T14:13:57.621-0700 7f82d337ef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T14:14:01,564703808-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:14:01,572307180-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T14:14:01,655553443-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:14:01,662107056-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8811455e-f506-4130-86c9-eee01318287c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8811455e-f506-4130-86c9-eee01318287c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T14:14:04,478783089-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:14:04,485209136-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8811455e-f506-4130-86c9-eee01318287c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8811455e-f506-4130-86c9-eee01318287c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T14:14:07,341210865-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:14:07,347597079-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8811455e-f506-4130-86c9-eee01318287c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8811455e-f506-4130-86c9-eee01318287c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T14:14:10,247267429-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:14:10,255720981-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8811455e-f506-4130-86c9-eee01318287c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8811455e-f506-4130-86c9-eee01318287c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T14:14:15,917218737-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:14:15,923719556-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8811455e-f506-4130-86c9-eee01318287c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8811455e-f506-4130-86c9-eee01318287c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T14:14:19,591671845-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:14:19,598166978-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8811455e-f506-4130-86c9-eee01318287c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8811455e-f506-4130-86c9-eee01318287c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T14:14:23,228311281-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:14:23,234929900-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8811455e-f506-4130-86c9-eee01318287c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8811455e-f506-4130-86c9-eee01318287c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T14:14:26,569983951-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:14:26,576348666-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8811455e-f506-4130-86c9-eee01318287c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8811455e-f506-4130-86c9-eee01318287c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:14:30,096294283-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:14:30,102689964-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8811455e-f506-4130-86c9-eee01318287c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8811455e-f506-4130-86c9-eee01318287c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:14:34,012333503-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:14:34,018951732-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8811455e-f506-4130-86c9-eee01318287c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8811455e-f506-4130-86c9-eee01318287c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T14:14:37,223402050-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:14:37,229733430-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8811455e-f506-4130-86c9-eee01318287c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8811455e-f506-4130-86c9-eee01318287c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T14:14:39,953972210-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:14:39,960545431-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8811455e-f506-4130-86c9-eee01318287c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8811455e-f506-4130-86c9-eee01318287c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T14:14:42,694262869-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:15:05,692877394-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   196 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [==========..................] (remaining: 7s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:15:13,796524277-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:15:21,747960327-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:15:29,773145778-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:15:38,052892444-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:15:46,235332004-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:15:54,278053635-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:16:02,126741987-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:16:02,143501030-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:16:10,168954033-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:16:10,185376734-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:16:18,331700345-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:16:18,347886005-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:16:26,314922774-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:16:26,337577642-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:16:34,267127492-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:16:34,283453006-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:16:34,296358522-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:16:34,303823690-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:16:34,320505455-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1186945
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T14:16:34,334486880-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T14:16:34,375432252-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:16:34,381880172-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8811455e-f506-4130-86c9-eee01318287c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8811455e-f506-4130-86c9-eee01318287c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:16:35.888+0000 ffff9c86b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:16:35.896+0000 ffff9c86b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:16:35.896+0000 ffff9c86b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:16:35.911972+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-17T21:16:35.912018+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T21:16:35.925695+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:16:35.925695+0000     0       0         0         0         0         0           -           0
2021-05-17T21:16:36.925895+0000     1     255      9060      8805   550.279   550.312  0.00170263   0.0282809
2021-05-17T21:16:37.926083+0000     2     255     17957     17702   553.119   556.062   0.0585741   0.0285485
2021-05-17T21:16:38.926315+0000     3     255     26778     26523   552.474   551.312   0.0114531   0.0286823
2021-05-17T21:16:39.926646+0000     4     255     35647     35392   552.888   554.312   0.0615791   0.0287577
2021-05-17T21:16:40.926885+0000     5     256     44094     43838    547.86   527.875   0.0158628   0.0291019
2021-05-17T21:16:41.927132+0000     6     255     52723     52468   546.423   539.375  0.00174268   0.0291413
2021-05-17T21:16:42.927364+0000     7     255     60920     60665   541.533   512.312   0.0275939   0.0294187
2021-05-17T21:16:43.927612+0000     8     256     68457     68201   532.702       471   0.0081804   0.0299304
2021-05-17T21:16:44.927962+0000     9     256     77325     77069   535.075    554.25  0.00493673   0.0297805
2021-05-17T21:16:45.928381+0000    10     255     87099     86844   542.637   610.938   0.0114243   0.0294094
2021-05-17T21:16:46.930655+0000    11     255     96414     96159   546.119   582.188  0.00542003   0.0292071
2021-05-17T21:16:47.930849+0000    12     255    105276    105021   546.756   553.875   0.0238308   0.0291941
2021-05-17T21:16:48.931042+0000    13     255    113658    113403   544.988   523.875  0.00800889   0.0292687
2021-05-17T21:16:49.931263+0000    14     255    121676    121421   541.848   501.125   0.0496529    0.029469
2021-05-17T21:16:50.931542+0000    15     255    130635    130380   543.043   559.938  0.00269484   0.0293764
2021-05-17T21:16:51.931748+0000    16     255    139564    139309   543.975   558.062   0.0190799    0.029363
2021-05-17T21:16:52.932032+0000    17     256    147394    147138   540.751   489.312   0.0618879   0.0294797
2021-05-17T21:16:53.932328+0000    18     255    155410    155155   538.538   501.062  0.00863759    0.029665
2021-05-17T21:16:54.932510+0000    19     255    163717    163462   537.515   519.188   0.0475839    0.029733
2021-05-17T21:16:55.932820+0000 min lat: 0.000953233 max lat: 0.170968 avg lat: 0.0298645
2021-05-17T21:16:55.932820+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:16:55.932820+0000    20     256    171524    171268   535.026   487.875   0.0245153   0.0298645
2021-05-17T21:16:56.933324+0000 Total time run:         20.0288
Total writes made:      171524
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     535.242
Stddev Bandwidth:       34.6804
Max bandwidth (MB/sec): 610.938
Min bandwidth (MB/sec): 471
Average IOPS:           8563
Stddev IOPS:            554.886
Max IOPS:               9775
Min IOPS:               7536
Average Latency(s):     0.0298712
Stddev Latency(s):      0.0225118
Max latency(s):         0.170968
Min latency(s):         0.000953233

[1;32mlocalhost.localdomain	[2021-05-17T14:16:57,450865507-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1186945


[1;33mlocalhost.localdomain	[2021-05-17T14:16:57,459819829-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:17:20,420921585-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.53k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:17:20,437393674-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:17:28,891571048-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.53k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:17:28,907898543-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:17:36,925291504-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.53k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:17:36,941517145-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:17:44,914705602-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.53k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:17:44,930906924-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:17:53,148879752-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.53k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:17:53,165394913-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:17:53,178652014-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:17:53,186370755-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:17:53,203740417-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1190260
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T14:17:53,218096649-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T14:17:53,258193206-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:17:53,264704406-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8811455e-f506-4130-86c9-eee01318287c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8811455e-f506-4130-86c9-eee01318287c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZZ0grW:/tmp/ceph-asok.ZZ0grW -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:17:55.069+0000 ffff8ecbd010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:17:55.077+0000 ffff8ecbd010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:17:55.077+0000 ffff8ecbd010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:17:55.096448+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:17:55.096448+0000     0       0         0         0         0         0           -           0
2021-05-17T21:17:56.096654+0000     1     255     13849     13594   849.343   849.625   0.0169319   0.0185625
2021-05-17T21:17:57.097002+0000     2     255     26377     26122   816.035       783   0.0326169   0.0193528
2021-05-17T21:17:58.097214+0000     3     255     37093     36838    767.23    669.75   0.0419931   0.0207062
2021-05-17T21:17:59.097441+0000     4     255     49098     48843   762.959   750.312   0.0034242   0.0207972
2021-05-17T21:18:00.097635+0000     5     255     60363     60108   751.153   704.062  0.00131121   0.0211422
2021-05-17T21:18:01.097831+0000     6     255     72452     72197   751.863   755.562  0.00116594   0.0211498
2021-05-17T21:18:02.098011+0000     7     256     83455     83199   742.669   687.625 0.000574508   0.0213929
2021-05-17T21:18:03.103711+0000     8     256     94543     94287   735.938       693  0.00399174   0.0216165
2021-05-17T21:18:04.103950+0000     9     255    105203    104948   728.188   666.312  0.00288505   0.0218567
2021-05-17T21:18:05.104152+0000    10     256    117383    117127   731.471   761.188   0.0469756   0.0217917
2021-05-17T21:18:06.104350+0000    11     255    128276    128021   726.862   680.875  0.00362615   0.0219363
2021-05-17T21:18:07.104563+0000    12     255    139701    139446   725.783   714.062   0.0293442   0.0219815
2021-05-17T21:18:08.104803+0000    13     255    151572    151317   727.011   741.938   0.0191187   0.0219537
2021-05-17T21:18:09.105295+0000    14     255    161951    161696   721.395   648.688  0.00273974   0.0221059
2021-05-17T21:18:10.105531+0000 Total time run:       14.9477
Total reads made:     171524
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   717.185
Average IOPS:         11474
Stddev IOPS:          878.316
Max IOPS:             13594
Min IOPS:             10379
Average Latency(s):   0.0222487
Max latency(s):       0.162247
Min latency(s):       0.000453962

[1;32mlocalhost.localdomain	[2021-05-17T14:18:10,417297766-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1190260


[1;33mlocalhost.localdomain	[2021-05-17T14:18:10,425643629-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:18:33,325106375-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.53k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:18:33,341467894-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:18:41,351930795-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.53k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:18:41,368484918-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:18:49,333971136-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.53k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:18:49,350909924-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:18:57,616002304-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.53k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:18:57,632343615-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:19:05,519927064-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 171.53k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:19:05,536414725-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:19:05,549719653-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:19:05,554484300-07:00][RUNNING][ROUND 3/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:19:05,561892413-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T14:19:05,579102012-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40028\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.440194\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fa744ec3-26b9-44c3-93c5-71f5b364ccc6\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid fa744ec3-26b9-44c3-93c5-71f5b364ccc6\nlast_changed 2021-05-17T14:19:34.372174-0700\ncreated 2021-05-17T14:19:34.372174-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40028/0,v1:10.10.1.2:40029/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.440194 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 de556ea0-aa46-4d81-9c94-932a13245b42\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 3fbd72b7-a485-4395-a833-b3645555f052\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 ae7c4c10-2c98-4a49-ae1f-9dd3fbbb80e7\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42028\n  w/ user/pass: admin / fe6f4179-2adc-4e11-9bf3-bab1aabc5bef\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:19:49 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40028
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.440194
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fa744ec3-26b9-44c3-93c5-71f5b364ccc6
setting min_mon_release = octopus
epoch 0
fsid fa744ec3-26b9-44c3-93c5-71f5b364ccc6
last_changed 2021-05-17T14:19:34.372174-0700
created 2021-05-17T14:19:34.372174-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40028/0,v1:10.10.1.2:40029/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.440194 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 de556ea0-aa46-4d81-9c94-932a13245b42
0
start osd.0
add osd1 3fbd72b7-a485-4395-a833-b3645555f052
1
start osd.1
add osd2 ae7c4c10-2c98-4a49-ae1f-9dd3fbbb80e7
2
start osd.2


restful urls: https://10.10.1.2:42028
  w/ user/pass: admin / fe6f4179-2adc-4e11-9bf3-bab1aabc5bef


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T14:19:06.575-0700 7fe246b221c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:19:06.575-0700 7fe246b221c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:19:06.595-0700 7f6a83c611c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:19:06.595-0700 7f6a83c611c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40028,v1:10.10.1.2:40029] --print /tmp/ceph_monmap.440194 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.440194 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.440194 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42028 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.SfboTDyR8Q 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new de556ea0-aa46-4d81-9c94-932a13245b42 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDv3aJgtb2dDRAANWQZbHr9HETE1n0IGA253Q== --osd-uuid de556ea0-aa46-4d81-9c94-932a13245b42 
2021-05-17T14:19:43.576-0700 7feaf0757f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:19:43.576-0700 7feaf0757f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:19:43.576-0700 7feaf0757f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:19:43.640-0700 7feaf0757f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3fbd72b7-a485-4395-a833-b3645555f052 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T14:19:43.932-0700 7fa0e5999f00 -1 Falling back to public interface
2021-05-17T14:19:43.944-0700 7fa0e5999f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDv3aJgN+uwNxAAslEn7BjLtJEPetPtyoO0xw== --osd-uuid 3fbd72b7-a485-4395-a833-b3645555f052 
2021-05-17T14:19:44.272-0700 7fadbecb6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:19:44.272-0700 7fadbecb6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:19:44.272-0700 7fadbecb6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:19:44.352-0700 7fadbecb6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ae7c4c10-2c98-4a49-ae1f-9dd3fbbb80e7 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T14:19:44.604-0700 7fc816dadf00 -1 Falling back to public interface
2021-05-17T14:19:44.616-0700 7fc816dadf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDw3aJg26MNJBAAvL1ryu6N8JWwJnuxXxgjeQ== --osd-uuid ae7c4c10-2c98-4a49-ae1f-9dd3fbbb80e7 
2021-05-17T14:19:44.956-0700 7f8592b4bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:19:44.956-0700 7f8592b4bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:19:44.956-0700 7f8592b4bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:19:45.004-0700 7f8592b4bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T14:19:45.424-0700 7f8f0a505f00 -1 Falling back to public interface
2021-05-17T14:19:45.436-0700 7f8f0a505f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T14:19:49,269775745-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:19:49,280191256-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T14:19:49,373159225-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:19:49,381723630-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T14:19:52,196561599-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:19:52,203165650-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T14:19:55,339466749-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:19:55,345910572-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T14:19:58,311497866-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:19:58,317988108-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T14:20:04,021162026-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:20:04,027649792-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T14:20:07,078506065-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:20:07,085033903-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T14:20:10,400829878-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:20:10,407395867-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T14:20:13,866909664-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:20:13,873490761-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:20:17,141535675-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:20:17,148185844-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:20:20,456785623-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:20:20,463185759-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T14:20:23,863087891-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:20:23,869690561-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T14:20:26,606507196-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:20:26,613569395-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T14:20:29,384390359-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:20:52,511168531-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   202 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:21:02,635351454-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:21:10,858605090-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 25s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:21:18,872826115-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 47s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:21:27,082904760-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (30s)
      [=========...................] (remaining: 61s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:21:35,089641836-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 71s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:21:43,015658090-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (45s)
      [=========...................] (remaining: 92s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:21:51,013472473-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:21:51,030066786-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:21:59,000389224-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:21:59,016882285-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:22:07,199364812-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:22:07,216002303-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:22:15,088393019-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:22:15,107138129-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:22:23,128401306-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:22:23,144917051-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:22:23,158259305-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:22:23,166082313-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:22:23,183430430-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1203756
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T14:22:23,198064614-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-17T14:22:23,238016153-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:22:23,244519507-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:22:24.731+0000 ffff85f17010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:22:24.735+0000 ffff85f17010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:22:24.739+0000 ffff85f17010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:22:24.754962+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-17T21:22:24.755008+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T21:22:24.768858+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:22:24.768858+0000     0       0         0         0         0         0           -           0
2021-05-17T21:22:25.769073+0000     1     255      8510      8255   515.904   515.938   0.0956243   0.0300222
2021-05-17T21:22:26.769247+0000     2     255     16712     16457    514.22   512.625   0.0351355   0.0307589
2021-05-17T21:22:27.769500+0000     3     255     25127     24872   518.082   525.938   0.0225618   0.0305524
2021-05-17T21:22:28.769720+0000     4     255     33959     33704   526.531       552   0.0383194   0.0302209
2021-05-17T21:22:29.769981+0000     5     256     43051     42795   534.834   568.188   0.0146552   0.0298335
2021-05-17T21:22:30.770416+0000     6     255     51712     51457   535.885   541.375    0.021282   0.0297151
2021-05-17T21:22:31.770744+0000     7     256     60643     60387   539.036   558.125  0.00748379   0.0296039
2021-05-17T21:22:32.771055+0000     8     255     69396     69141   540.026   547.125  0.00760769   0.0294866
2021-05-17T21:22:33.771317+0000     9     256     77747     77491   537.994   521.875  0.00103572   0.0293588
2021-05-17T21:22:34.771520+0000    10     256     86035     85779   535.984       518  0.00856898   0.0297659
2021-05-17T21:22:35.771708+0000    11     256     94315     94059   534.295     517.5  0.00552352   0.0298838
2021-05-17T21:22:36.771886+0000    12     255    102437    102182    532.07   507.688   0.0298289     0.03001
2021-05-17T21:22:37.772067+0000    13     256    110942    110686   532.019     531.5  0.00130051   0.0298968
2021-05-17T21:22:38.772292+0000    14     255    119528    119273   532.344   536.688  0.00613983   0.0300046
2021-05-17T21:22:39.772470+0000    15     256    127981    127725   532.065    528.25  0.00385963   0.0299974
2021-05-17T21:22:40.772666+0000    16     255    136380    136125   531.617       525   0.0408009   0.0300528
2021-05-17T21:22:41.772969+0000    17     255    145043    144788   532.185   541.438   0.0328814   0.0300259
2021-05-17T21:22:42.773225+0000    18     256    153523    153267   532.052   529.938  0.00130803   0.0299369
2021-05-17T21:22:43.773415+0000    19     255    162482    162227   533.518       560   0.0274721   0.0299418
2021-05-17T21:22:44.773583+0000 min lat: 0.000989929 max lat: 0.182947 avg lat: 0.0300235
2021-05-17T21:22:44.773583+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:22:44.773583+0000    20     222    169823    169601   529.882   460.875   0.0822312   0.0300235
2021-05-17T21:22:45.773811+0000 Total time run:         20.0225
Total writes made:      169823
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     530.101
Stddev Bandwidth:       23.4188
Max bandwidth (MB/sec): 568.188
Min bandwidth (MB/sec): 460.875
Average IOPS:           8481
Stddev IOPS:            374.7
Max IOPS:               9091
Min IOPS:               7374
Average Latency(s):     0.0301625
Stddev Latency(s):      0.0245491
Max latency(s):         0.183961
Min latency(s):         0.000989929

[1;32mlocalhost.localdomain	[2021-05-17T14:22:46,145113853-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1203756


[1;33mlocalhost.localdomain	[2021-05-17T14:22:46,153982884-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:23:09,565988852-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 169.82k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:23:09,585256260-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:23:17,585816637-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 169.82k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:23:17,602489954-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:23:25,584672292-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 169.82k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:23:25,601474780-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:23:33,586374055-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 169.82k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:23:33,603154370-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:23:41,775139944-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 169.82k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:23:41,791927430-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:23:41,805083262-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:23:41,812806983-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:23:41,830526743-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1207090
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T14:23:41,845353230-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-17T14:23:41,884637905-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:23:41,891044597-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2fa1825-9b0a-4d4a-8492-5a2de7fa3c2b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tVpCwl:/tmp/ceph-asok.tVpCwl -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:23:43.605+0000 ffff91ebf010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:23:43.613+0000 ffff91ebf010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:23:43.613+0000 ffff91ebf010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:23:43.629996+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:23:43.629996+0000     0       0         0         0         0         0           -           0
2021-05-17T21:23:44.630229+0000     1     255     13022     12767    797.65   797.938   0.0362894   0.0196396
2021-05-17T21:23:45.630483+0000     2     255     26172     25917   809.657   821.875   0.0247232   0.0195992
2021-05-17T21:23:46.630694+0000     3     255     39529     39274   817.983   834.812   0.0183259   0.0194415
2021-05-17T21:23:47.630887+0000     4     255     52878     52623   822.025   834.312   0.0024771   0.0193443
2021-05-17T21:23:48.631073+0000     5     255     65353     65098   813.529   779.688  0.00420846   0.0195235
2021-05-17T21:23:49.631276+0000     6     255     77192     76937   801.239   739.938   0.0038019    0.019856
2021-05-17T21:23:50.631480+0000     7     256     89064     88808   792.746   741.938    0.011295   0.0201082
2021-05-17T21:23:51.631673+0000     8     255    101987    101732   794.602    807.75  0.00908441   0.0200715
2021-05-17T21:23:52.631986+0000     9     255    114830    114575   795.473   802.688   0.0147329   0.0200552
2021-05-17T21:23:53.632226+0000    10     255    125015    124760   779.566   636.562   0.0121505   0.0204539
2021-05-17T21:23:54.632443+0000    11     256    135948    135692   770.797    683.25   0.0133145   0.0207003
2021-05-17T21:23:55.632734+0000    12     255    148262    148007   770.686   769.688   0.0302684   0.0207133
2021-05-17T21:23:56.632971+0000    13     256    160631    160375    770.85       773   0.0323134   0.0207002
2021-05-17T21:23:57.633226+0000 Total time run:       13.977
Total reads made:     169823
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   759.383
Average IOPS:         12150
Stddev IOPS:          939.938
Max IOPS:             13357
Min IOPS:             10185
Average Latency(s):   0.0210149
Max latency(s):       0.140964
Min latency(s):       0.000487223

[1;32mlocalhost.localdomain	[2021-05-17T14:23:57,957405639-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1207090


[1;33mlocalhost.localdomain	[2021-05-17T14:23:57,965719470-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:24:21,113143166-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 169.82k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:24:21,129911868-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:24:29,212531514-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 169.82k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:24:29,228862022-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:24:37,286944992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 169.82k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:24:37,303487831-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:24:45,311333934-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 169.82k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:24:45,328079083-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:24:53,525493972-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 169.82k objects, 10 GiB
    usage:   21 GiB used, 279 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:24:53,542049414-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:24:53,555275818-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:24:53,560312971-07:00][RUNNING][ROUND 4/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:24:53,567985774-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T14:24:53,584692591-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40421\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.441333\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b1983bab-0aed-4ce7-92ff-89970367e484\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid b1983bab-0aed-4ce7-92ff-89970367e484\nlast_changed 2021-05-17T14:25:22.912267-0700\ncreated 2021-05-17T14:25:22.912267-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40421/0,v1:10.10.1.2:40422/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.441333 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 939ea1c2-165b-4eb0-a433-8dee197ad7d8\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 cdbc0dae-4430-4f37-ada3-346b53ffb6a8\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 a23689aa-e882-486f-93d4-304883252e5b\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42421\n  w/ user/pass: admin / 9cc01d1d-4173-48b2-9eb4-3e96b6159373\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:25:38 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40421
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.441333
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b1983bab-0aed-4ce7-92ff-89970367e484
setting min_mon_release = octopus
epoch 0
fsid b1983bab-0aed-4ce7-92ff-89970367e484
last_changed 2021-05-17T14:25:22.912267-0700
created 2021-05-17T14:25:22.912267-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40421/0,v1:10.10.1.2:40422/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.441333 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 939ea1c2-165b-4eb0-a433-8dee197ad7d8
0
start osd.0
add osd1 cdbc0dae-4430-4f37-ada3-346b53ffb6a8
1
start osd.1
add osd2 a23689aa-e882-486f-93d4-304883252e5b
2
start osd.2


restful urls: https://10.10.1.2:42421
  w/ user/pass: admin / 9cc01d1d-4173-48b2-9eb4-3e96b6159373


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T14:24:54.599-0700 7f7dfad3a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:24:54.599-0700 7f7dfad3a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:24:54.615-0700 7f91a69201c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:24:54.619-0700 7f91a69201c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40421,v1:10.10.1.2:40422] --print /tmp/ceph_monmap.441333 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.441333 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.441333 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42421 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.IHaxfCHY3y 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 939ea1c2-165b-4eb0-a433-8dee197ad7d8 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBM36JgOfeTChAAdfeHvHNi1iGdypiGdQNNVA== --osd-uuid 939ea1c2-165b-4eb0-a433-8dee197ad7d8 
2021-05-17T14:25:32.503-0700 7f2c3c3c2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:25:32.503-0700 7f2c3c3c2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:25:32.503-0700 7f2c3c3c2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:25:32.559-0700 7f2c3c3c2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new cdbc0dae-4430-4f37-ada3-346b53ffb6a8 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T14:25:32.875-0700 7f95eb2fcf00 -1 Falling back to public interface
2021-05-17T14:25:32.891-0700 7f95eb2fcf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBM36JgLlc7NBAAH8WeaDr10rH55Rjc/nU1Tw== --osd-uuid cdbc0dae-4430-4f37-ada3-346b53ffb6a8 
2021-05-17T14:25:33.215-0700 7f5e80e4cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:25:33.215-0700 7f5e80e4cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:25:33.215-0700 7f5e80e4cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:25:33.311-0700 7f5e80e4cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a23689aa-e882-486f-93d4-304883252e5b -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T14:25:33.663-0700 7ff51066df00 -1 Falling back to public interface
2021-05-17T14:25:33.679-0700 7ff51066df00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBN36Jg4pp+JxAApUxmJ+RFWj88OZgt2nly4w== --osd-uuid a23689aa-e882-486f-93d4-304883252e5b 
2021-05-17T14:25:34.063-0700 7f7cab47af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:25:34.063-0700 7f7cab47af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:25:34.063-0700 7f7cab47af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:25:34.115-0700 7f7cab47af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T14:25:34.427-0700 7ff1fc0d7f00 -1 Falling back to public interface
2021-05-17T14:25:34.439-0700 7ff1fc0d7f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T14:25:38,392579171-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:25:38,400590732-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T14:25:38,484325791-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:25:38,490955466-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b86ac18c-cb2c-4216-af15-f19d7e5be928', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b86ac18c-cb2c-4216-af15-f19d7e5be928 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T14:25:41,369952520-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:25:41,378582411-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b86ac18c-cb2c-4216-af15-f19d7e5be928', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b86ac18c-cb2c-4216-af15-f19d7e5be928 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T14:25:44,214068599-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:25:44,220480272-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b86ac18c-cb2c-4216-af15-f19d7e5be928', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b86ac18c-cb2c-4216-af15-f19d7e5be928 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T14:25:47,059908606-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:25:47,066359406-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b86ac18c-cb2c-4216-af15-f19d7e5be928', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b86ac18c-cb2c-4216-af15-f19d7e5be928 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T14:25:52,817551378-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:25:52,824114906-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b86ac18c-cb2c-4216-af15-f19d7e5be928', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b86ac18c-cb2c-4216-af15-f19d7e5be928 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T14:25:55,827948318-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:25:55,834381490-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b86ac18c-cb2c-4216-af15-f19d7e5be928', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b86ac18c-cb2c-4216-af15-f19d7e5be928 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T14:25:59,277110626-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:25:59,283658581-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b86ac18c-cb2c-4216-af15-f19d7e5be928', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b86ac18c-cb2c-4216-af15-f19d7e5be928 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T14:26:02,571419018-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:26:02,578073582-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b86ac18c-cb2c-4216-af15-f19d7e5be928', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b86ac18c-cb2c-4216-af15-f19d7e5be928 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:26:05,899301046-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:26:05,905827248-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b86ac18c-cb2c-4216-af15-f19d7e5be928', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b86ac18c-cb2c-4216-af15-f19d7e5be928 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:26:09,124836022-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:26:09,131280160-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b86ac18c-cb2c-4216-af15-f19d7e5be928', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b86ac18c-cb2c-4216-af15-f19d7e5be928 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T14:26:12,646132085-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:26:12,652699799-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b86ac18c-cb2c-4216-af15-f19d7e5be928', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b86ac18c-cb2c-4216-af15-f19d7e5be928 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T14:26:15,497380398-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:26:15,503799737-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b86ac18c-cb2c-4216-af15-f19d7e5be928', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b86ac18c-cb2c-4216-af15-f19d7e5be928 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  147 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  147 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.04   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.98   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.98   70      up          osd.2  
                       TOTAL  300 GiB  148 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.98/1.04  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T14:26:18,071780445-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:26:41,296528573-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (4s)
      [============................] (remaining: 6s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:26:49,121304485-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:26:57,155494578-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:27:05,201333398-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:27:13,270646982-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:27:21,172527483-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:27:29,117130847-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:27:37,155947002-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:27:37,172370030-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:27:45,238632951-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:27:45,255117669-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:27:53,252773578-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:27:53,269175843-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:28:01,420756157-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:28:01,437637933-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:28:09,403252944-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:28:09,420165086-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:28:09,433511045-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:28:09,441479067-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:28:09,459334225-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1220411
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T14:28:09,474386410-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-17T14:28:09,518449758-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:28:09,527452897-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b86ac18c-cb2c-4216-af15-f19d7e5be928', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b86ac18c-cb2c-4216-af15-f19d7e5be928 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:28:11.074+0000 ffff8b689010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:28:11.082+0000 ffff8b689010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:28:11.082+0000 ffff8b689010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:28:11.100644+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-17T21:28:11.100705+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-17T21:28:11.114760+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:28:11.114760+0000     0       0         0         0         0         0           -           0
2021-05-17T21:28:12.114968+0000     1     255      9799      9544   596.458     596.5   0.0137191   0.0257486
2021-05-17T21:28:13.115145+0000     2     255     19301     19046   595.114   593.875   0.0188239   0.0262326
2021-05-17T21:28:14.115379+0000     3     256     29581     29325   610.839   642.438  0.00246851   0.0258917
2021-05-17T21:28:15.115573+0000     4     255     39293     39038   609.866   607.062   0.0138081   0.0260432
2021-05-17T21:28:16.115780+0000     5     256     49137     48881   610.905   615.188  0.00640767   0.0260035
2021-05-17T21:28:17.116425+0000     6     256     59071     58815     612.5   620.875  0.00219033   0.0259662
2021-05-17T21:28:18.116799+0000     7     256     68036     67780   605.014   560.312  0.00193643   0.0262181
2021-05-17T21:28:19.117033+0000     8     256     77600     77344   604.089    597.75   0.0181301   0.0264095
2021-05-17T21:28:20.117297+0000     9     255     85714     85459   593.307   507.188  0.00350577   0.0268047
2021-05-17T21:28:21.117551+0000    10     255     95053     94798    592.33   583.688   0.0101656   0.0269406
2021-05-17T21:28:22.117911+0000    11     256    104508    104252   592.179   590.875   0.0100504     0.02695
2021-05-17T21:28:23.118314+0000    12     255    113669    113414    590.53   572.625   0.0182888    0.027021
2021-05-17T21:28:24.118621+0000    13     256    122167    121911   585.943   531.062  0.00952919   0.0271736
2021-05-17T21:28:25.118884+0000    14     255    131850    131595    587.31    605.25    0.050963   0.0271662
2021-05-17T21:28:26.119116+0000    15     255    140559    140304   584.436   544.312   0.0244627   0.0273188
2021-05-17T21:28:27.119370+0000    16     255    147915    147660   576.636    459.75   0.0241857   0.0276843
2021-05-17T21:28:28.119537+0000    17     256    157413    157157   577.625   593.562  0.00261378   0.0276224
2021-05-17T21:28:29.119704+0000    18     256    165841    165585   574.794    526.75   0.0104585   0.0277628
2021-05-17T21:28:30.119871+0000    19     256    175618    175362   576.698   611.062   0.0017977   0.0276863
2021-05-17T21:28:31.120027+0000 min lat: 0.0010249 max lat: 0.157491 avg lat: 0.0277838
2021-05-17T21:28:31.120027+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:28:31.120027+0000    20      71    184192    184121   575.231   547.438   0.0252594   0.0277838
2021-05-17T21:28:32.120345+0000 Total time run:         20.0085
Total writes made:      184192
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     575.357
Stddev Bandwidth:       44.4727
Max bandwidth (MB/sec): 642.438
Min bandwidth (MB/sec): 459.75
Average IOPS:           9205
Stddev IOPS:            711.564
Max IOPS:               10279
Min IOPS:               7356
Average Latency(s):     0.0277787
Stddev Latency(s):      0.0222993
Max latency(s):         0.157491
Min latency(s):         0.0010249

[1;32mlocalhost.localdomain	[2021-05-17T14:28:32,441422370-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1220411


[1;33mlocalhost.localdomain	[2021-05-17T14:28:32,449480948-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:28:55,431620051-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 184.19k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:28:55,448514040-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:29:03,293270727-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 184.19k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:29:03,309872811-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:29:11,369575886-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 184.19k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:29:11,386282432-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:29:19,486354635-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 184.19k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:29:19,503050005-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:29:27,453537250-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 184.19k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:29:27,472769012-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:29:27,489231496-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:29:27,500245068-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:29:27,520259318-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1223766
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T14:29:27,536357406-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-17T14:29:27,583264803-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:29:27,590349445-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b86ac18c-cb2c-4216-af15-f19d7e5be928', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b86ac18c-cb2c-4216-af15-f19d7e5be928 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.C0f58L:/tmp/ceph-asok.C0f58L -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:29:29.244+0000 ffff882e0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:29:29.252+0000 ffff882e0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:29:29.252+0000 ffff882e0010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:29:29.271664+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:29:29.271664+0000     0       0         0         0         0         0           -           0
2021-05-17T21:29:30.271861+0000     1     255     12264     12009   750.317   750.562  0.00319904   0.0207372
2021-05-17T21:29:31.272058+0000     2     256     23312     23056   720.311   690.438  0.00873296   0.0218537
2021-05-17T21:29:32.272328+0000     3     255     36297     36042   750.676   811.625  0.00139576   0.0210643
2021-05-17T21:29:33.272579+0000     4     255     49176     48921   764.191   804.938  0.00085123   0.0207484
2021-05-17T21:29:34.272773+0000     5     256     61600     61344    766.61   776.438  0.00237681    0.020708
2021-05-17T21:29:35.273053+0000     6     256     72317     72061   750.445   669.812  0.00296549    0.021189
2021-05-17T21:29:36.273789+0000     7     255     84954     84699   755.998   789.875  0.00460299   0.0210513
2021-05-17T21:29:37.273972+0000     8     255     96448     96193   751.279   718.375   0.0619172   0.0211756
2021-05-17T21:29:38.274313+0000     9     256    107762    107506   746.339   707.062    0.031459   0.0213661
2021-05-17T21:29:39.274628+0000    10     255    119377    119122   744.282       726  0.00197927    0.021386
2021-05-17T21:29:40.274842+0000    11     256    130899    130643   742.067   720.062   0.0602385    0.021463
2021-05-17T21:29:41.275036+0000    12     255    141836    141581   737.186   683.625   0.0966819   0.0216027
2021-05-17T21:29:42.275253+0000    13     255    152762    152507   732.997   682.875  0.00184678   0.0217395
2021-05-17T21:29:43.275507+0000    14     256    165026    164770   735.372   766.438  0.00788308   0.0216794
2021-05-17T21:29:44.275751+0000    15     256    174587    174331   726.175   597.562   0.0504731   0.0219702
2021-05-17T21:29:45.276087+0000 Total time run:       15.914
Total reads made:     184192
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   723.388
Average IOPS:         11574
Stddev IOPS:          931.466
Max IOPS:             12986
Min IOPS:             9561
Average Latency(s):   0.0220502
Max latency(s):       0.156398
Min latency(s):       0.000458944

[1;32mlocalhost.localdomain	[2021-05-17T14:29:45,577582140-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1223766


[1;33mlocalhost.localdomain	[2021-05-17T14:29:45,585987112-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:30:08,548385346-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 184.19k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:30:08,567885278-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:30:16,580212788-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 184.19k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:30:16,596843226-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:30:24,653366851-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 184.19k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:30:24,670313993-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:30:32,597147671-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 184.19k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:30:32,613893855-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:30:40,842840526-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 184.19k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:30:40,859491248-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:30:40,873056080-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:30:40,878186804-07:00][RUNNING][ROUND 5/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:30:40,886084209-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T14:30:40,903138365-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40015\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.444153\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 40fae42f-7e98-4f41-b9c1-87658f13aab1\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 40fae42f-7e98-4f41-b9c1-87658f13aab1\nlast_changed 2021-05-17T14:31:10.231482-0700\ncreated 2021-05-17T14:31:10.231482-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40015/0,v1:10.10.1.2:40016/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.444153 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 7ca9d8ed-e18b-479c-831c-39d430bb0201\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 fe881956-c812-4df9-a374-e75f8d62ff84\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 c14c1be5-6877-4c30-a42d-9a67a38943af\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42015\n  w/ user/pass: admin / 05dbe85f-ff2a-44af-8dcc-981e85c68d96\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:31:25 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40015
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.444153
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 40fae42f-7e98-4f41-b9c1-87658f13aab1
setting min_mon_release = octopus
epoch 0
fsid 40fae42f-7e98-4f41-b9c1-87658f13aab1
last_changed 2021-05-17T14:31:10.231482-0700
created 2021-05-17T14:31:10.231482-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40015/0,v1:10.10.1.2:40016/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.444153 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 7ca9d8ed-e18b-479c-831c-39d430bb0201
0
start osd.0
add osd1 fe881956-c812-4df9-a374-e75f8d62ff84
1
start osd.1
add osd2 c14c1be5-6877-4c30-a42d-9a67a38943af
2
start osd.2


restful urls: https://10.10.1.2:42015
  w/ user/pass: admin / 05dbe85f-ff2a-44af-8dcc-981e85c68d96


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T14:30:41.902-0700 7f1d117be1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:30:41.902-0700 7f1d117be1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:30:41.922-0700 7fb5e98741c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:30:41.922-0700 7fb5e98741c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40015,v1:10.10.1.2:40016] --print /tmp/ceph_monmap.444153 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.444153 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.444153 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42015 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.B8qtCUpjS3 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7ca9d8ed-e18b-479c-831c-39d430bb0201 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCn4KJgtgRDCBAAPWP9FPVEmX+MPZoR10rgDA== --osd-uuid 7ca9d8ed-e18b-479c-831c-39d430bb0201 
2021-05-17T14:31:19.494-0700 7f9ac8034f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:31:19.498-0700 7f9ac8034f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:31:19.498-0700 7f9ac8034f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:31:19.586-0700 7f9ac8034f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new fe881956-c812-4df9-a374-e75f8d62ff84 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T14:31:19.906-0700 7f0632796f00 -1 Falling back to public interface
2021-05-17T14:31:19.918-0700 7f0632796f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCn4KJgr4j1NRAAZGToKQh5X4JTCQ1PArEzEA== --osd-uuid fe881956-c812-4df9-a374-e75f8d62ff84 
2021-05-17T14:31:20.234-0700 7f2005496f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:31:20.234-0700 7f2005496f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:31:20.234-0700 7f2005496f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:31:20.278-0700 7f2005496f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c14c1be5-6877-4c30-a42d-9a67a38943af -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T14:31:20.606-0700 7fb064d9cf00 -1 Falling back to public interface
2021-05-17T14:31:20.618-0700 7fb064d9cf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCo4KJg0ff8IxAA6Za/i+5nnFQcMMeWrJM0ig== --osd-uuid c14c1be5-6877-4c30-a42d-9a67a38943af 
2021-05-17T14:31:20.938-0700 7f4a9eb40f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:31:20.938-0700 7f4a9eb40f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:31:20.938-0700 7f4a9eb40f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:31:20.990-0700 7f4a9eb40f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T14:31:21.346-0700 7fc0245cef00 -1 Falling back to public interface
2021-05-17T14:31:21.358-0700 7fc0245cef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T14:31:25,276482353-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:31:25,284618794-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T14:31:25,368756204-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:31:25,375112142-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14f70852-fb20-4b52-a0c6-86ca04ad2741', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14f70852-fb20-4b52-a0c6-86ca04ad2741 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T14:31:30,193148257-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:31:30,199444139-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14f70852-fb20-4b52-a0c6-86ca04ad2741', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14f70852-fb20-4b52-a0c6-86ca04ad2741 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T14:31:32,919081005-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:31:32,925665297-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14f70852-fb20-4b52-a0c6-86ca04ad2741', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14f70852-fb20-4b52-a0c6-86ca04ad2741 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T14:31:35,612613153-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:31:35,619202485-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14f70852-fb20-4b52-a0c6-86ca04ad2741', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14f70852-fb20-4b52-a0c6-86ca04ad2741 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T14:31:41,259559868-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:31:41,266088556-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14f70852-fb20-4b52-a0c6-86ca04ad2741', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14f70852-fb20-4b52-a0c6-86ca04ad2741 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T14:31:44,171235136-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:31:44,177944830-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14f70852-fb20-4b52-a0c6-86ca04ad2741', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14f70852-fb20-4b52-a0c6-86ca04ad2741 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T14:31:47,823809539-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:31:47,830522998-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14f70852-fb20-4b52-a0c6-86ca04ad2741', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14f70852-fb20-4b52-a0c6-86ca04ad2741 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T14:31:51,072672743-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:31:51,079169596-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14f70852-fb20-4b52-a0c6-86ca04ad2741', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14f70852-fb20-4b52-a0c6-86ca04ad2741 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:31:54,325230096-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:31:54,331802501-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14f70852-fb20-4b52-a0c6-86ca04ad2741', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14f70852-fb20-4b52-a0c6-86ca04ad2741 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:31:58,011880615-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:31:58,018234524-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14f70852-fb20-4b52-a0c6-86ca04ad2741', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14f70852-fb20-4b52-a0c6-86ca04ad2741 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T14:32:01,108491374-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:32:01,114930542-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14f70852-fb20-4b52-a0c6-86ca04ad2741', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14f70852-fb20-4b52-a0c6-86ca04ad2741 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T14:32:03,882874792-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:32:03,889334198-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14f70852-fb20-4b52-a0c6-86ca04ad2741', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14f70852-fb20-4b52-a0c6-86ca04ad2741 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T14:32:06,713562338-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:32:29,647828052-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   203 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=========...................] (remaining: 9s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:32:37,631816111-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:32:45,919894239-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:32:53,928064175-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:33:01,918063118-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:33:10,021854939-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:33:17,999422606-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:33:26,087593774-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:33:26,104567881-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:33:34,455392182-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:33:34,472090587-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:33:42,871780707-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:33:42,888277238-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:33:50,860012463-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:33:50,876803494-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:33:58,951074525-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:33:58,967909049-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:33:58,981428564-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:33:58,989314998-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:33:59,007656333-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1237235
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T14:33:59,022398196-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-17T14:33:59,063483543-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:33:59,069758418-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14f70852-fb20-4b52-a0c6-86ca04ad2741', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14f70852-fb20-4b52-a0c6-86ca04ad2741 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:34:00.614+0000 ffffa094a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:34:00.622+0000 ffffa094a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:34:00.622+0000 ffffa094a010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:34:00.638817+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-17T21:34:00.638863+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-17T21:34:00.652682+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:34:00.652682+0000     0       0         0         0         0         0           -           0
2021-05-17T21:34:01.652873+0000     1     255      8248      7993   499.537   499.562  0.00490589   0.0308157
2021-05-17T21:34:02.653380+0000     2     255     17242     16987   530.696   562.125   0.0360011   0.0297644
2021-05-17T21:34:03.653813+0000     3     256     26554     26298   547.694   581.938   0.0198211       0.029
2021-05-17T21:34:04.654265+0000     4     255     35465     35210   549.958       557   0.0184782   0.0288996
2021-05-17T21:34:05.654445+0000     5     255     44388     44133   551.484   557.688  0.00712097   0.0287852
2021-05-17T21:34:06.654651+0000     6     256     54425     54169   564.089    627.25   0.0147169   0.0282584
2021-05-17T21:34:07.654941+0000     7     255     64266     64011   571.354   615.125   0.0270328   0.0279055
2021-05-17T21:34:08.655161+0000     8     256     74007     73751   576.011    608.75  0.00688111   0.0276796
2021-05-17T21:34:09.655532+0000     9     256     83714     83458   579.395   606.688  0.00195564   0.0274786
2021-05-17T21:34:10.655975+0000    10     255     93189     92934   580.654    592.25    0.028444   0.0274905
2021-05-17T21:34:11.656383+0000    11     255    102818    102563   582.556   601.812   0.0190253   0.0273806
2021-05-17T21:34:12.656590+0000    12     256    111535    111279   579.396    544.75    0.010506   0.0275525
2021-05-17T21:34:13.656847+0000    13     255    120718    120463    578.97       574   0.0409797     0.02758
2021-05-17T21:34:14.657117+0000    14     255    130084    129829   579.416   585.375   0.0240952   0.0275592
2021-05-17T21:34:15.657305+0000    15     255    138335    138080   575.161   515.688     0.10597   0.0277077
2021-05-17T21:34:16.657487+0000    16     255    147471    147216   574.895       571   0.0244837   0.0277744
2021-05-17T21:34:17.657651+0000    17     255    157097    156842   576.461   601.625   0.0296201   0.0277112
2021-05-17T21:34:18.657843+0000    18     255    164615    164360   570.535   469.875     0.10571   0.0279574
2021-05-17T21:34:19.658072+0000    19     256    173561    173305   569.925   559.062  0.00484027   0.0280205
2021-05-17T21:34:20.658415+0000 min lat: 0.00100352 max lat: 0.212574 avg lat: 0.0281138
2021-05-17T21:34:20.658415+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:34:20.658415+0000    20      73    182079    182006    568.61   543.812   0.0232056   0.0281138
2021-05-17T21:34:21.658659+0000 Total time run:         20.008
Total writes made:      182079
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     568.77
Stddev Bandwidth:       40.2597
Max bandwidth (MB/sec): 627.25
Min bandwidth (MB/sec): 469.875
Average IOPS:           9100
Stddev IOPS:            644.156
Max IOPS:               10036
Min IOPS:               7518
Average Latency(s):     0.0281088
Stddev Latency(s):      0.0219028
Max latency(s):         0.212574
Min latency(s):         0.00100352

[1;32mlocalhost.localdomain	[2021-05-17T14:34:21,969326975-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1237235


[1;33mlocalhost.localdomain	[2021-05-17T14:34:21,978947673-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:34:45,126394377-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 182.08k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:34:45,143626724-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:34:53,098388214-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 182.08k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:34:53,115391367-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:35:01,187294031-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 182.08k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:35:01,204379743-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:35:09,378374648-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 182.08k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:35:09,395876081-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:35:17,513512506-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 182.08k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:35:17,530435020-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:35:17,544210855-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:35:17,552119869-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:35:17,570521485-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1240613
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T14:35:17,585626305-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-17T14:35:17,625404647-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:35:17,631936127-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '14f70852-fb20-4b52-a0c6-86ca04ad2741', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 14f70852-fb20-4b52-a0c6-86ca04ad2741 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.d81YpW:/tmp/ceph-asok.d81YpW -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:35:19.127+0000 ffff90ad2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:35:19.135+0000 ffff90ad2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:35:19.135+0000 ffff90ad2010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:35:19.155867+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:35:19.155867+0000     0       0         0         0         0         0           -           0
2021-05-17T21:35:20.156051+0000     1     256     12050     11794   736.862   737.125   0.0126558   0.0213177
2021-05-17T21:35:21.156349+0000     2     255     24950     24695   771.466   806.312   0.0101886   0.0204679
2021-05-17T21:35:22.156638+0000     3     256     37386     37130   773.298   777.188   0.0458077   0.0205191
2021-05-17T21:35:23.156895+0000     4     255     49951     49696   776.267   785.375  0.00183715    0.020439
2021-05-17T21:35:24.157233+0000     5     255     62230     61975   774.449   767.438   0.0513444   0.0205147
2021-05-17T21:35:25.157491+0000     6     256     74495     74239   773.091     766.5   0.0012444   0.0205421
2021-05-17T21:35:26.157798+0000     7     255     86613     86358   770.822   757.438  0.00755773   0.0206886
2021-05-17T21:35:27.158124+0000     8     256     99176     98920   772.578   785.125 0.000978106   0.0206011
2021-05-17T21:35:28.158421+0000     9     255    110533    110278   765.588   709.875   0.0121516   0.0208407
2021-05-17T21:35:29.158810+0000    10     255    121723    121468   758.939   699.375   0.0371768   0.0210131
2021-05-17T21:35:30.158999+0000    11     255    132591    132336   751.683    679.25   0.0156998   0.0212246
2021-05-17T21:35:31.159343+0000    12     256    143528    143272   745.982     683.5   0.0010602   0.0213565
2021-05-17T21:35:32.159677+0000    13     255    155189    154934   744.647   728.875   0.0487789   0.0214178
2021-05-17T21:35:33.159887+0000    14     255    167886    167631   748.129   793.562    0.002579   0.0213314
2021-05-17T21:35:34.160100+0000    15     256    180303    180047   749.976       776   0.0434151   0.0212722
2021-05-17T21:35:35.160387+0000 Total time run:       15.1823
Total reads made:     182079
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   749.554
Average IOPS:         11992
Stddev IOPS:          659.876
Max IOPS:             12901
Min IOPS:             10868
Average Latency(s):   0.0212846
Max latency(s):       0.0893694
Min latency(s):       0.000475356

[1;32mlocalhost.localdomain	[2021-05-17T14:35:35,516895216-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1240613


[1;33mlocalhost.localdomain	[2021-05-17T14:35:35,525374252-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:35:58,531013879-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 182.08k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:35:58,548385871-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:36:06,526760482-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 182.08k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:36:06,543902439-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:36:14,666822676-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 182.08k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:36:14,684172404-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:36:22,728891087-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 182.08k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:36:22,745833720-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:36:30,781988104-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 182.08k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:36:30,802479591-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:36:30,817731287-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:36:30,826759776-07:00][RUNNING][ROUND 1/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:36:30,835116676-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T14:36:30,853020689-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40136\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.447911\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 71cb1969-bf33-45c8-b09a-963e5b323444\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 71cb1969-bf33-45c8-b09a-963e5b323444\nlast_changed 2021-05-17T14:36:59.569582-0700\ncreated 2021-05-17T14:36:59.569582-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40136/0,v1:10.10.1.2:40137/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.447911 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 d706f4da-6e8e-4967-8fe2-93c7c678ad1b\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 bad3c944-8410-4067-8427-89e1104f2505\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 f0f08026-00e8-4da8-91bf-6f12f7bf71b6\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42136\n  w/ user/pass: admin / 1485e392-1d08-41bc-ac59-33000ec87943\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:37:14 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40136
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.447911
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 71cb1969-bf33-45c8-b09a-963e5b323444
setting min_mon_release = octopus
epoch 0
fsid 71cb1969-bf33-45c8-b09a-963e5b323444
last_changed 2021-05-17T14:36:59.569582-0700
created 2021-05-17T14:36:59.569582-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40136/0,v1:10.10.1.2:40137/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.447911 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 d706f4da-6e8e-4967-8fe2-93c7c678ad1b
0
start osd.0
add osd1 bad3c944-8410-4067-8427-89e1104f2505
1
start osd.1
add osd2 f0f08026-00e8-4da8-91bf-6f12f7bf71b6
2
start osd.2


restful urls: https://10.10.1.2:42136
  w/ user/pass: admin / 1485e392-1d08-41bc-ac59-33000ec87943


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T14:36:31.837-0700 7f86a5bcd1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:36:31.837-0700 7f86a5bcd1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:36:31.853-0700 7fb1e6d721c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:36:31.853-0700 7fb1e6d721c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40136,v1:10.10.1.2:40137] --print /tmp/ceph_monmap.447911 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.447911 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.447911 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42136 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.JqkvHnnFMW 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d706f4da-6e8e-4967-8fe2-93c7c678ad1b -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAE4qJgj/vMHRAAe7A+HutFnJ+AvF1naNjNPA== --osd-uuid d706f4da-6e8e-4967-8fe2-93c7c678ad1b 
2021-05-17T14:37:08.849-0700 7f077437ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:37:08.849-0700 7f077437ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:37:08.849-0700 7f077437ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:37:08.929-0700 7f077437ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new bad3c944-8410-4067-8427-89e1104f2505 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T14:37:09.209-0700 7fe1f446df00 -1 Falling back to public interface
2021-05-17T14:37:09.221-0700 7fe1f446df00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAF4qJgzAB3DBAAOV0cjFzxGnhqDV3T0OFdBw== --osd-uuid bad3c944-8410-4067-8427-89e1104f2505 
2021-05-17T14:37:09.537-0700 7faee821df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:37:09.537-0700 7faee821df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:37:09.537-0700 7faee821df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:37:09.589-0700 7faee821df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f0f08026-00e8-4da8-91bf-6f12f7bf71b6 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T14:37:09.933-0700 7f88fcc22f00 -1 Falling back to public interface
2021-05-17T14:37:09.945-0700 7f88fcc22f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAF4qJgM+XANhAAVDmzlIcPrTNgX9EdOpraXg== --osd-uuid f0f08026-00e8-4da8-91bf-6f12f7bf71b6 
2021-05-17T14:37:10.241-0700 7f834c3c3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:37:10.241-0700 7f834c3c3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:37:10.241-0700 7f834c3c3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:37:10.305-0700 7f834c3c3f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T14:37:10.697-0700 7f362b47ef00 -1 Falling back to public interface
2021-05-17T14:37:10.709-0700 7f362b47ef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T14:37:14,588018197-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:37:14,596370672-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T14:37:14,679493663-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:37:14,685874937-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3ae74ca9-c9d0-4463-8518-eacd7d6daab3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3ae74ca9-c9d0-4463-8518-eacd7d6daab3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T14:37:17,602956527-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:37:17,610643279-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3ae74ca9-c9d0-4463-8518-eacd7d6daab3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3ae74ca9-c9d0-4463-8518-eacd7d6daab3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T14:37:20,392651669-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:37:20,398907162-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3ae74ca9-c9d0-4463-8518-eacd7d6daab3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3ae74ca9-c9d0-4463-8518-eacd7d6daab3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T14:37:23,301698743-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:37:23,308210735-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3ae74ca9-c9d0-4463-8518-eacd7d6daab3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3ae74ca9-c9d0-4463-8518-eacd7d6daab3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T14:37:29,171496988-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:37:29,178172193-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3ae74ca9-c9d0-4463-8518-eacd7d6daab3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3ae74ca9-c9d0-4463-8518-eacd7d6daab3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T14:37:32,121376381-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:37:32,128092933-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3ae74ca9-c9d0-4463-8518-eacd7d6daab3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3ae74ca9-c9d0-4463-8518-eacd7d6daab3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T14:37:35,496646851-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:37:35,503035732-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3ae74ca9-c9d0-4463-8518-eacd7d6daab3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3ae74ca9-c9d0-4463-8518-eacd7d6daab3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T14:37:39,236626911-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:37:39,243258263-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3ae74ca9-c9d0-4463-8518-eacd7d6daab3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3ae74ca9-c9d0-4463-8518-eacd7d6daab3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:37:42,481323451-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:37:42,487810929-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3ae74ca9-c9d0-4463-8518-eacd7d6daab3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3ae74ca9-c9d0-4463-8518-eacd7d6daab3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:37:45,693838688-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:37:45,700270326-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3ae74ca9-c9d0-4463-8518-eacd7d6daab3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3ae74ca9-c9d0-4463-8518-eacd7d6daab3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T14:37:49,173659612-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:37:49,179933769-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3ae74ca9-c9d0-4463-8518-eacd7d6daab3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3ae74ca9-c9d0-4463-8518-eacd7d6daab3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T14:37:52,018525179-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:37:52,025009536-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3ae74ca9-c9d0-4463-8518-eacd7d6daab3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3ae74ca9-c9d0-4463-8518-eacd7d6daab3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T14:37:54,710827720-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:38:17,674380428-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:38:25,719160809-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:38:33,869478116-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:38:41,703980972-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:38:49,792211564-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 55s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:38:57,927009308-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 73s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:39:05,910285570-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 83s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:39:14,055328992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [=========...................] (remaining: 102s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:39:22,072102512-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:39:22,089119710-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:39:30,114889404-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:39:30,132354834-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:39:38,402091233-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:39:38,419271104-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:39:46,392141592-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:39:46,412842996-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:39:54,492221194-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:39:54,509440957-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:39:54,523370180-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:39:54,531523721-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:39:54,549317436-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1254426
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T14:39:54,564605486-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T14:39:54,605112688-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:39:54,611615513-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3ae74ca9-c9d0-4463-8518-eacd7d6daab3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3ae74ca9-c9d0-4463-8518-eacd7d6daab3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:39:56.134+0000 ffff8b8a0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:39:56.142+0000 ffff8b8a0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:39:56.142+0000 ffff8b8a0010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:39:56.159950+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-17T21:39:56.160009+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-17T21:39:56.211067+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:39:56.211067+0000     0       0         0         0         0         0           -           0
2021-05-17T21:39:57.211294+0000     1     255      2715      2460   614.947       615   0.0689732   0.0955408
2021-05-17T21:39:58.211533+0000     2     255      5806      5551   693.762    772.75   0.0719749   0.0893224
2021-05-17T21:39:59.211746+0000     3     255      8618      8363   696.792       703    0.079581   0.0892209
2021-05-17T21:40:00.211968+0000     4     255     11280     11025   688.932     665.5   0.0509834   0.0911603
2021-05-17T21:40:01.212489+0000     5     255     14097     13842   691.923    704.25   0.0983832   0.0910756
2021-05-17T21:40:02.212804+0000     6     255     16728     16473   686.192    657.75   0.0838799   0.0921937
2021-05-17T21:40:03.213221+0000     7     255     19298     19043   679.912     642.5   0.0627032   0.0931124
2021-05-17T21:40:04.213923+0000     8     255     21851     21596   674.646    638.25   0.0620417   0.0939704
2021-05-17T21:40:05.214170+0000     9     255     24558     24303   674.861    676.75    0.202412   0.0939775
2021-05-17T21:40:06.214790+0000    10     255     26997     26742   668.311    609.75    0.102638   0.0950712
2021-05-17T21:40:07.215154+0000    11     255     29464     29209   663.603    616.75   0.0916645   0.0956141
2021-05-17T21:40:08.215362+0000    12     255     32027     31772   661.688    640.75   0.0727116   0.0959923
2021-05-17T21:40:09.215631+0000    13     255     34801     34546    664.12     693.5    0.163285   0.0957929
2021-05-17T21:40:10.215920+0000    14     255     37716     37461   668.721    728.75   0.0445731   0.0952682
2021-05-17T21:40:11.216219+0000    15     255     40373     40118    668.41    664.25   0.0738064   0.0953724
2021-05-17T21:40:12.216486+0000    16     255     42632     42377   661.922    564.75   0.0939947    0.096133
2021-05-17T21:40:13.216717+0000    17     255     44906     44651    656.42     568.5   0.0978024    0.096806
2021-05-17T21:40:14.218749+0000    18     256     47185     46929   651.519     569.5     0.09311   0.0977642
2021-05-17T21:40:15.219232+0000    19     255     49611     49356   649.147    606.75      0.1052   0.0980582
2021-05-17T21:40:16.219753+0000 min lat: 0.00733104 max lat: 0.277468 avg lat: 0.0975326
2021-05-17T21:40:16.219753+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:40:16.219753+0000    20     108     52511     52403   654.758    761.75    0.047291   0.0975326
2021-05-17T21:40:17.220137+0000 Total time run:         20.0461
Total writes made:      52511
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     654.879
Stddev Bandwidth:       60.0894
Max bandwidth (MB/sec): 772.75
Min bandwidth (MB/sec): 564.75
Average IOPS:           2619
Stddev IOPS:            240.358
Max IOPS:               3091
Min IOPS:               2259
Average Latency(s):     0.0974174
Stddev Latency(s):      0.0359386
Max latency(s):         0.277468
Min latency(s):         0.00733104

[1;32mlocalhost.localdomain	[2021-05-17T14:40:17,556019248-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1254426


[1;33mlocalhost.localdomain	[2021-05-17T14:40:17,565811218-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:40:40,555135803-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.51k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:40:40,573316284-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:40:48,462913182-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.51k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:40:48,480585332-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:40:56,692640561-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.51k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:40:56,710035044-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:41:04,686325431-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.51k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:41:04,705212128-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:41:12,588634586-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.51k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:41:12,605764814-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:41:12,619986963-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:41:12,628321270-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:41:12,647064737-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1257736
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T14:41:12,662514289-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-17T14:41:12,702523678-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:41:12,708894616-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3ae74ca9-c9d0-4463-8518-eacd7d6daab3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3ae74ca9-c9d0-4463-8518-eacd7d6daab3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4dCFEu:/tmp/ceph-asok.4dCFEu -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:41:14.179+0000 ffffa5944010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:41:14.187+0000 ffffa5944010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:41:14.187+0000 ffffa5944010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:41:14.207772+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:41:14.207772+0000     0       9         9         0         0         0           -           0
2021-05-17T21:41:15.208485+0000     1     255      4476      4221   1053.96   1055.25   0.0258583   0.0580503
2021-05-17T21:41:16.209010+0000     2     255      9216      8961   1119.14      1185   0.0688234   0.0559771
2021-05-17T21:41:17.209370+0000     3     256     13493     13237   1102.31      1069   0.0342039   0.0572652
2021-05-17T21:41:18.209587+0000     4     255     18036     17781   1110.67      1136    0.121804   0.0566401
2021-05-17T21:41:19.210109+0000     5     256     22432     22176   1108.17   1098.75   0.0442908   0.0570981
2021-05-17T21:41:20.210324+0000     6     255     27156     26901    1120.3   1181.25    0.121756   0.0564919
2021-05-17T21:41:21.210845+0000     7     255     31803     31548   1126.14   1161.75   0.0284229   0.0562754
2021-05-17T21:41:22.211067+0000     8     255     35951     35696   1114.97      1037    0.041045   0.0570494
2021-05-17T21:41:23.211365+0000     9     255     40384     40129   1114.19   1108.25   0.0635353   0.0570571
2021-05-17T21:41:24.211693+0000    10     255     44234     43979   1098.99     962.5    0.154726   0.0577182
2021-05-17T21:41:25.212088+0000    11     255     48618     48363   1098.68      1096    0.158959    0.057717
2021-05-17T21:41:26.212391+0000 Total time run:       12
Total reads made:     52511
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1093.98
Average IOPS:         4375
Stddev IOPS:          267.887
Max IOPS:             4740
Min IOPS:             3850
Average Latency(s):   0.0580635
Max latency(s):       0.243121
Min latency(s):       0.000934058

[1;32mlocalhost.localdomain	[2021-05-17T14:41:26,539517160-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1257736


[1;33mlocalhost.localdomain	[2021-05-17T14:41:26,547905867-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:41:49,511870736-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.51k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:41:49,529437763-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:41:57,807909524-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.51k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:41:57,825421492-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:42:05,814292529-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.51k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:42:05,831753083-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:42:13,811753027-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.51k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:42:13,829506956-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:42:21,929674538-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.51k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:42:21,947164242-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:42:21,961571477-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:42:21,966997810-07:00][RUNNING][ROUND 2/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:42:21,975521180-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T14:42:21,993169671-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40354\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.452456\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ea1d7430-2290-4ceb-b183-2548b25d585c\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid ea1d7430-2290-4ceb-b183-2548b25d585c\nlast_changed 2021-05-17T14:42:55.993636-0700\ncreated 2021-05-17T14:42:55.993636-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40354/0,v1:10.10.1.2:40355/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.452456 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 dc4db992-7332-4881-abed-7ce3ae91562a\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 a5dda4ed-a86e-4ac1-b940-8a4e215f1d98\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 1f468d0a-c23d-4210-9fe4-925e782254ef\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42354\n  w/ user/pass: admin / a4e4c8e0-7e84-4b07-88c1-7ad85853d09a\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:43:10 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40354
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.452456
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ea1d7430-2290-4ceb-b183-2548b25d585c
setting min_mon_release = octopus
epoch 0
fsid ea1d7430-2290-4ceb-b183-2548b25d585c
last_changed 2021-05-17T14:42:55.993636-0700
created 2021-05-17T14:42:55.993636-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40354/0,v1:10.10.1.2:40355/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.452456 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 dc4db992-7332-4881-abed-7ce3ae91562a
0
start osd.0
add osd1 a5dda4ed-a86e-4ac1-b940-8a4e215f1d98
1
start osd.1
add osd2 1f468d0a-c23d-4210-9fe4-925e782254ef
2
start osd.2


restful urls: https://10.10.1.2:42354
  w/ user/pass: admin / a4e4c8e0-7e84-4b07-88c1-7ad85853d09a


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T14:42:22.996-0700 7fa7252701c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:42:22.996-0700 7fa7252701c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:42:23.012-0700 7f4a7ab1f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:42:23.012-0700 7f4a7ab1f1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40354,v1:10.10.1.2:40355] --print /tmp/ceph_monmap.452456 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.452456 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.452456 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42354 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.ilaMAayGRL 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new dc4db992-7332-4881-abed-7ce3ae91562a -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBo46JgUpY3MxAAJuDdq61NqG+E2nGFfATuPw== --osd-uuid dc4db992-7332-4881-abed-7ce3ae91562a 
2021-05-17T14:43:05.193-0700 7f7c50100f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:43:05.193-0700 7f7c50100f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:43:05.193-0700 7f7c50100f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:43:05.261-0700 7f7c50100f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a5dda4ed-a86e-4ac1-b940-8a4e215f1d98 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T14:43:05.557-0700 7fab39aa8f00 -1 Falling back to public interface
2021-05-17T14:43:05.569-0700 7fab39aa8f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBp46JgKd84IRAABnYcrjTj+FZqx2TA/mYcEA== --osd-uuid a5dda4ed-a86e-4ac1-b940-8a4e215f1d98 
2021-05-17T14:43:05.897-0700 7f58c70e5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:43:05.897-0700 7f58c70e5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:43:05.897-0700 7f58c70e5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:43:05.941-0700 7f58c70e5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 1f468d0a-c23d-4210-9fe4-925e782254ef -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T14:43:06.257-0700 7f88ed646f00 -1 Falling back to public interface
2021-05-17T14:43:06.269-0700 7f88ed646f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBq46JgQycTDhAAXhePYMIGWlKBdvzLv5qcew== --osd-uuid 1f468d0a-c23d-4210-9fe4-925e782254ef 
2021-05-17T14:43:06.557-0700 7fc6dcd62f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:43:06.557-0700 7fc6dcd62f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:43:06.557-0700 7fc6dcd62f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:43:06.621-0700 7fc6dcd62f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T14:43:07.013-0700 7f2e17fdbf00 -1 Falling back to public interface
2021-05-17T14:43:07.029-0700 7f2e17fdbf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T14:43:10,937264589-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:43:10,945757338-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T14:43:11,027209358-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:43:11,033829248-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bef777c9-0f6f-477f-9600-8dde7a3d1249', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bef777c9-0f6f-477f-9600-8dde7a3d1249 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T14:43:14,049854596-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:43:14,056299152-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bef777c9-0f6f-477f-9600-8dde7a3d1249', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bef777c9-0f6f-477f-9600-8dde7a3d1249 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T14:43:16,849582757-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:43:16,855918589-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bef777c9-0f6f-477f-9600-8dde7a3d1249', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bef777c9-0f6f-477f-9600-8dde7a3d1249 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T14:43:19,705189304-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:43:19,711599879-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bef777c9-0f6f-477f-9600-8dde7a3d1249', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bef777c9-0f6f-477f-9600-8dde7a3d1249 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T14:43:25,317005818-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:43:25,323438057-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bef777c9-0f6f-477f-9600-8dde7a3d1249', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bef777c9-0f6f-477f-9600-8dde7a3d1249 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T14:43:28,571752683-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:43:28,578181291-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bef777c9-0f6f-477f-9600-8dde7a3d1249', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bef777c9-0f6f-477f-9600-8dde7a3d1249 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T14:43:32,037594457-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:43:32,043916217-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bef777c9-0f6f-477f-9600-8dde7a3d1249', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bef777c9-0f6f-477f-9600-8dde7a3d1249 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T14:43:35,559860156-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:43:35,566603878-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bef777c9-0f6f-477f-9600-8dde7a3d1249', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bef777c9-0f6f-477f-9600-8dde7a3d1249 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:43:38,959452183-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:43:38,965992035-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bef777c9-0f6f-477f-9600-8dde7a3d1249', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bef777c9-0f6f-477f-9600-8dde7a3d1249 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:43:42,097638263-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:43:42,104122636-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bef777c9-0f6f-477f-9600-8dde7a3d1249', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bef777c9-0f6f-477f-9600-8dde7a3d1249 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T14:43:45,550631954-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:43:45,557261987-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bef777c9-0f6f-477f-9600-8dde7a3d1249', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bef777c9-0f6f-477f-9600-8dde7a3d1249 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T14:43:48,284089306-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:43:48,290502447-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bef777c9-0f6f-477f-9600-8dde7a3d1249', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bef777c9-0f6f-477f-9600-8dde7a3d1249 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T14:43:50,935329360-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:44:13,930599064-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   189 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:44:21,947700475-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:44:29,995881519-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 28s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:44:38,004640523-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 52s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:44:46,128049751-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 62s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:44:54,287526435-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 82s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:45:02,269556308-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 94s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:45:10,363167047-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   228 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [==============..............] (remaining: 46s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:45:18,361261007-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:45:18,379404509-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:45:26,341942810-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:45:26,359666900-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:45:34,465290031-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:45:34,482823094-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:45:42,679560821-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:45:42,697074651-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:45:50,699618319-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:45:50,716870879-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:45:50,730749335-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:45:50,738731173-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:45:50,756769147-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1271664
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T14:45:50,772168607-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T14:45:50,813159015-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:45:50,819595710-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bef777c9-0f6f-477f-9600-8dde7a3d1249', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bef777c9-0f6f-477f-9600-8dde7a3d1249 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:45:52.335+0000 ffff959f2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:45:52.343+0000 ffff959f2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:45:52.343+0000 ffff959f2010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:45:52.363775+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-17T21:45:52.363835+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T21:45:52.418694+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:45:52.418694+0000     0       0         0         0         0         0           -           0
2021-05-17T21:45:53.418937+0000     1     255      2905      2650   662.428     662.5   0.0855186   0.0882412
2021-05-17T21:45:54.419179+0000     2     255      5666      5411   676.257    690.25   0.0864368   0.0910225
2021-05-17T21:45:55.419433+0000     3     255      8459      8204   683.529    698.25   0.0888664   0.0907942
2021-05-17T21:45:56.419637+0000     4     255     11248     10993   686.924    697.25     0.04853    0.091369
2021-05-17T21:45:57.420133+0000     5     255     14069     13814    690.52    705.25   0.0863365   0.0911193
2021-05-17T21:45:58.420456+0000     6     255     16837     16582   690.729       692    0.075962    0.091361
2021-05-17T21:45:59.420980+0000     7     255     19562     19307   689.324    681.25   0.0608968   0.0915149
2021-05-17T21:46:00.421859+0000     8     255     22263     22008    687.49    675.25   0.0955088   0.0921235
2021-05-17T21:46:01.422182+0000     9     255     25116     24861   690.326    713.25    0.081847    0.091755
2021-05-17T21:46:02.422468+0000    10     255     27741     27486     686.9    656.25    0.103546   0.0922426
2021-05-17T21:46:03.422791+0000    11     255     30325     30070   683.163       646   0.0971844   0.0924999
2021-05-17T21:46:04.422998+0000    12     255     32868     32613   679.201    635.75    0.209629    0.093435
2021-05-17T21:46:05.423306+0000    13     255     35491     35236   677.382    655.75   0.0630054   0.0939924
2021-05-17T21:46:06.423611+0000    14     255     38271     38016   678.625       695    0.133472   0.0937467
2021-05-17T21:46:07.423918+0000    15     255     40741     40486   674.538     617.5    0.102068   0.0941722
2021-05-17T21:46:08.424592+0000    16     255     43461     43206   674.851       680    0.174249   0.0943473
2021-05-17T21:46:09.424834+0000    17     255     46280     46025   676.599    704.75   0.0744807   0.0942242
2021-05-17T21:46:10.425191+0000    18     255     48745     48490   673.234    616.25   0.0929256   0.0945651
2021-05-17T21:46:11.425660+0000    19     255     51312     51057   671.561    641.75   0.0796372   0.0949288
2021-05-17T21:46:12.425881+0000 min lat: 0.00624178 max lat: 0.293815 avg lat: 0.0952486
2021-05-17T21:46:12.425881+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:46:12.425881+0000    20      78     53717     53639   670.251     645.5  0.00701138   0.0952486
2021-05-17T21:46:13.426224+0000 Total time run:         20.0271
Total writes made:      53717
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     670.554
Stddev Bandwidth:       29.7983
Max bandwidth (MB/sec): 713.25
Min bandwidth (MB/sec): 616.25
Average IOPS:           2682
Stddev IOPS:            119.193
Max IOPS:               2853
Min IOPS:               2465
Average Latency(s):     0.0951872
Stddev Latency(s):      0.0336743
Max latency(s):         0.293815
Min latency(s):         0.00624178

[1;32mlocalhost.localdomain	[2021-05-17T14:46:13,905329416-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1271664


[1;33mlocalhost.localdomain	[2021-05-17T14:46:13,913951592-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:46:36,818759002-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.72k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:46:36,836122776-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:46:44,695494952-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.72k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:46:44,712686167-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:46:52,688628953-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.72k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:46:52,705898571-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:47:00,843744580-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.72k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:47:00,863210347-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:47:09,357245217-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.72k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:47:09,374769608-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:47:09,388731779-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:47:09,396962509-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:47:09,415657550-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1274974
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T14:47:09,430931904-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T14:47:09,471508768-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:47:09,478008414-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bef777c9-0f6f-477f-9600-8dde7a3d1249', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bef777c9-0f6f-477f-9600-8dde7a3d1249 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Q6L4wF:/tmp/ceph-asok.Q6L4wF -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:47:10.976+0000 ffffaf581010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:47:10.984+0000 ffffaf581010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:47:10.984+0000 ffffaf581010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:47:11.004223+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:47:11.004223+0000     0       0         0         0         0         0           -           0
2021-05-17T21:47:12.004461+0000     1     256      3497      3241   809.891    810.25   0.0589597   0.0747696
2021-05-17T21:47:13.004655+0000     2     256      6656      6400   799.745    789.75   0.0621851   0.0778721
2021-05-17T21:47:14.004849+0000     3     256      9862      9606   800.278     801.5   0.0423696   0.0776981
2021-05-17T21:47:15.005063+0000     4     255     12928     12673   791.856    766.75    0.167074   0.0792391
2021-05-17T21:47:16.005272+0000     5     255     15973     15718   785.703    761.25   0.0431038   0.0799204
2021-05-17T21:47:17.005468+0000     6     255     19347     19092   795.308     843.5    0.146725   0.0795125
2021-05-17T21:47:18.011119+0000     7     255     23880     23625   842.895   1133.25    0.172361   0.0749621
2021-05-17T21:47:19.011352+0000     8     255     28405     28150   878.882   1131.25   0.0335121   0.0722275
2021-05-17T21:47:20.011706+0000     9     255     32761     32506   902.174      1089   0.0127293   0.0703976
2021-05-17T21:47:21.011968+0000    10     255     37267     37012   924.565    1126.5  0.00770305   0.0686403
2021-05-17T21:47:22.012477+0000    11     255     41233     40978   930.603     991.5    0.140547   0.0683038
2021-05-17T21:47:23.012789+0000    12     255     45638     45383   944.789   1101.25  0.00533185   0.0672464
2021-05-17T21:47:24.013016+0000    13     255     50118     49863   958.241      1120  0.00526275   0.0663142
2021-05-17T21:47:25.013313+0000 Total time run:       13.9987
Total reads made:     53717
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   959.324
Average IOPS:         3837
Stddev IOPS:          650.318
Max IOPS:             4533
Min IOPS:             3045
Average Latency(s):   0.0662845
Max latency(s):       0.247785
Min latency(s):       0.00091394

[1;32mlocalhost.localdomain	[2021-05-17T14:47:25,625450602-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1274974


[1;33mlocalhost.localdomain	[2021-05-17T14:47:25,634529502-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:47:48,620580849-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.72k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:47:48,638962022-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:47:56,601901975-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.72k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:47:56,619548128-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:48:04,656032720-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.72k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:48:04,673417903-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:48:12,754845938-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.72k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:48:12,772774844-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:48:20,832436315-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.72k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:48:20,850066205-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:48:20,863761915-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:48:20,868978138-07:00][RUNNING][ROUND 3/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:48:20,877440868-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T14:48:20,895084875-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40438\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.454966\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6239cd57-79dd-48ee-8b92-bea7ad679306\nsetting min_mon_release = octopus\nepoch 0\nfsid 6239cd57-79dd-48ee-8b92-bea7ad679306\nlast_changed 2021-05-17T14:48:49.900074-0700\ncreated 2021-05-17T14:48:49.900074-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40438/0,v1:10.10.1.2:40439/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.454966 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 fb836cba-f018-4f33-9dde-75716212f274\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 68acda33-4eee-498d-b7d3-49bd48a5f19e\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 b20156e5-4f3b-4bb2-9ff4-315bc752fa7f\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42438\n  w/ user/pass: admin / 5e82f1ea-cccd-409d-af91-adb27fc36562\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:49:04 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40438
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.454966
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6239cd57-79dd-48ee-8b92-bea7ad679306
setting min_mon_release = octopus
epoch 0
fsid 6239cd57-79dd-48ee-8b92-bea7ad679306
last_changed 2021-05-17T14:48:49.900074-0700
created 2021-05-17T14:48:49.900074-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40438/0,v1:10.10.1.2:40439/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.454966 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 fb836cba-f018-4f33-9dde-75716212f274
0
start osd.0
add osd1 68acda33-4eee-498d-b7d3-49bd48a5f19e
1
start osd.1
add osd2 b20156e5-4f3b-4bb2-9ff4-315bc752fa7f
2
start osd.2


restful urls: https://10.10.1.2:42438
  w/ user/pass: admin / 5e82f1ea-cccd-409d-af91-adb27fc36562


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T14:48:21.899-0700 7f8dd5f581c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:48:21.899-0700 7f8dd5f581c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:48:21.915-0700 7f256d5ce1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:48:21.915-0700 7f256d5ce1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40438,v1:10.10.1.2:40439] --print /tmp/ceph_monmap.454966 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.454966 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.454966 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42438 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.AKDVWnS2eX 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new fb836cba-f018-4f33-9dde-75716212f274 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDK5KJgGXevLBAAqmBpzFrYZdN5QMlHprOe2A== --osd-uuid fb836cba-f018-4f33-9dde-75716212f274 
2021-05-17T14:48:59.096-0700 7f9d5c1a3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:48:59.096-0700 7f9d5c1a3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:48:59.096-0700 7f9d5c1a3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:48:59.168-0700 7f9d5c1a3f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 68acda33-4eee-498d-b7d3-49bd48a5f19e -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T14:48:59.456-0700 7fa819801f00 -1 Falling back to public interface
2021-05-17T14:48:59.468-0700 7fa819801f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDL5KJgaaM4GxAAi0TB/UofkaGT/2CbStkd6w== --osd-uuid 68acda33-4eee-498d-b7d3-49bd48a5f19e 
2021-05-17T14:48:59.784-0700 7f19ccc99f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:48:59.784-0700 7f19ccc99f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:48:59.784-0700 7f19ccc99f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:48:59.840-0700 7f19ccc99f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b20156e5-4f3b-4bb2-9ff4-315bc752fa7f -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T14:49:00.116-0700 7f3c3c6d4f00 -1 Falling back to public interface
2021-05-17T14:49:00.128-0700 7f3c3c6d4f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDM5KJgbS0ABxAA3bMUyjRZfVWFFDjaCPxYWQ== --osd-uuid b20156e5-4f3b-4bb2-9ff4-315bc752fa7f 
2021-05-17T14:49:00.464-0700 7faaa31aff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:49:00.464-0700 7faaa31aff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:49:00.464-0700 7faaa31aff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:49:00.532-0700 7faaa31aff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T14:49:00.920-0700 7f9f055bdf00 -1 Falling back to public interface
2021-05-17T14:49:00.932-0700 7f9f055bdf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T14:49:04,805719855-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:49:04,814314608-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T14:49:04,897143357-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:49:04,903624595-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '126b61db-8960-48c0-abfa-cead224c5a76', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 126b61db-8960-48c0-abfa-cead224c5a76 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T14:49:07,879243937-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:49:07,885807404-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '126b61db-8960-48c0-abfa-cead224c5a76', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 126b61db-8960-48c0-abfa-cead224c5a76 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T14:49:10,996652977-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:49:11,003031882-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '126b61db-8960-48c0-abfa-cead224c5a76', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 126b61db-8960-48c0-abfa-cead224c5a76 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T14:49:13,794805473-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:49:13,801265167-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '126b61db-8960-48c0-abfa-cead224c5a76', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 126b61db-8960-48c0-abfa-cead224c5a76 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T14:49:19,552101228-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:49:19,558582061-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '126b61db-8960-48c0-abfa-cead224c5a76', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 126b61db-8960-48c0-abfa-cead224c5a76 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T14:49:23,670371609-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:49:23,677116742-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '126b61db-8960-48c0-abfa-cead224c5a76', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 126b61db-8960-48c0-abfa-cead224c5a76 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T14:49:27,300062074-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:49:27,306516802-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '126b61db-8960-48c0-abfa-cead224c5a76', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 126b61db-8960-48c0-abfa-cead224c5a76 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T14:49:30,105368843-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:49:30,112120952-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '126b61db-8960-48c0-abfa-cead224c5a76', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 126b61db-8960-48c0-abfa-cead224c5a76 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:49:35,707407602-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:49:35,713806521-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '126b61db-8960-48c0-abfa-cead224c5a76', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 126b61db-8960-48c0-abfa-cead224c5a76 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:49:38,865335219-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:49:38,871778441-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '126b61db-8960-48c0-abfa-cead224c5a76', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 126b61db-8960-48c0-abfa-cead224c5a76 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T14:49:42,407798748-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:49:42,413980445-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '126b61db-8960-48c0-abfa-cead224c5a76', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 126b61db-8960-48c0-abfa-cead224c5a76 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T14:49:45,431692240-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:49:45,438100145-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '126b61db-8960-48c0-abfa-cead224c5a76', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 126b61db-8960-48c0-abfa-cead224c5a76 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T14:49:48,049499570-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:50:11,014915602-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (4s)
      [=============...............] (remaining: 5s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:50:19,264152092-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:50:27,130885540-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:50:34,989733578-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:50:42,987719337-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:50:50,904612207-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:50:58,992647070-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:50:59,010009930-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:51:06,996026448-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:51:07,013644502-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:51:15,060831927-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:51:15,078822899-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:51:23,100512593-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:51:23,118036085-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:51:31,091021783-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:51:31,108879127-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:51:31,123685926-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:51:31,131880621-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:51:31,151071047-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1287963
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T14:51:31,168038379-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-17T14:51:31,215172576-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:51:31,222178708-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '126b61db-8960-48c0-abfa-cead224c5a76', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 126b61db-8960-48c0-abfa-cead224c5a76 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:51:32.753+0000 ffffa4de3010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:51:32.761+0000 ffffa4de3010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:51:32.761+0000 ffffa4de3010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:51:32.776762+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-17T21:51:32.776832+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T21:51:32.827241+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:51:32.827241+0000     0       0         0         0         0         0           -           0
2021-05-17T21:51:33.827481+0000     1     255      3041      2786   696.421     696.5   0.0518787   0.0841872
2021-05-17T21:51:34.827730+0000     2     255      6221      5966   745.615       795   0.0551081   0.0823312
2021-05-17T21:51:35.827913+0000     3     255      9288      9033   752.613    766.75   0.0405708    0.082951
2021-05-17T21:51:36.828151+0000     4     255     12446     12191   761.788     789.5   0.0713923   0.0822607
2021-05-17T21:51:37.828475+0000     5     255     15393     15138   756.732    736.75   0.0745993   0.0833946
2021-05-17T21:51:38.828713+0000     6     255     18463     18208   758.497     767.5   0.0775654   0.0832262
2021-05-17T21:51:39.829093+0000     7     255     21695     21440   765.526       808   0.0743403   0.0829114
2021-05-17T21:51:40.829852+0000     8     255     24871     24616   769.011       794   0.0751059   0.0824927
2021-05-17T21:51:41.830135+0000     9     256     27946     27690    768.93     768.5   0.0332802     0.08262
2021-05-17T21:51:42.830338+0000    10     255     31024     30769   768.997    769.75    0.150216   0.0825463
2021-05-17T21:51:43.830535+0000    11     255     34166     33911   770.483     785.5    0.087238   0.0824509
2021-05-17T21:51:44.830753+0000    12     255     37387     37132   773.365    805.25    0.109233   0.0822635
2021-05-17T21:51:45.830958+0000    13     255     40506     40251   773.844    779.75   0.0860809   0.0821975
2021-05-17T21:51:46.831344+0000    14     255     43664     43409   774.941     789.5   0.0747981   0.0821863
2021-05-17T21:51:47.831602+0000    15     255     46768     46513   774.998       776   0.0603447   0.0821342
2021-05-17T21:51:48.832222+0000    16     255     49757     49502   773.234    747.25   0.0749864   0.0823931
2021-05-17T21:51:49.832512+0000    17     255     52707     52452    771.12     737.5   0.0816098   0.0825824
2021-05-17T21:51:50.832972+0000    18     255     55860     55605   772.051    788.25   0.0582904   0.0825398
2021-05-17T21:51:51.833183+0000    19     255     58898     58643   771.382     759.5   0.0711022     0.08258
2021-05-17T21:51:52.833405+0000 min lat: 0.0257432 max lat: 0.216349 avg lat: 0.0829278
2021-05-17T21:51:52.833405+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:51:52.833405+0000    20     255     61792     61537    768.98     723.5   0.0664803   0.0829278
2021-05-17T21:51:53.833746+0000 Total time run:         20.0497
Total writes made:      61793
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     770.497
Stddev Bandwidth:       28.7579
Max bandwidth (MB/sec): 808
Min bandwidth (MB/sec): 696.5
Average IOPS:           3081
Stddev IOPS:            115.031
Max IOPS:               3232
Min IOPS:               2786
Average Latency(s):     0.0828456
Stddev Latency(s):      0.0278318
Max latency(s):         0.216349
Min latency(s):         0.0226588

[1;32mlocalhost.localdomain	[2021-05-17T14:51:54,167961759-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1287963


[1;33mlocalhost.localdomain	[2021-05-17T14:51:54,176810445-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:52:17,249340011-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.79k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:52:17,267319520-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:52:25,369226223-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.79k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:52:25,387036909-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:52:33,394745106-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.79k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:52:33,412373751-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:52:41,648400292-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.79k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:52:41,665940947-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:52:49,640659907-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.79k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:52:49,662777122-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:52:49,678013805-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:52:49,687465848-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:52:49,709772875-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1291323
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T14:52:49,728447198-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-17T14:52:49,769262514-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:52:49,775882841-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '126b61db-8960-48c0-abfa-cead224c5a76', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 126b61db-8960-48c0-abfa-cead224c5a76 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IDa7YO:/tmp/ceph-asok.IDa7YO -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:52:51.294+0000 ffff9cea9010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:52:51.302+0000 ffff9cea9010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:52:51.302+0000 ffff9cea9010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:52:51.338585+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:52:51.338585+0000     0       0         0         0         0         0           -           0
2021-05-17T21:52:52.338793+0000     1     255      3587      3332   832.699       833    0.131183   0.0723243
2021-05-17T21:52:53.338974+0000     2     255      6805      6550   818.528     804.5   0.0321651   0.0750053
2021-05-17T21:52:54.339183+0000     3     256     10092      9836   819.462     821.5  0.00140571   0.0747151
2021-05-17T21:52:55.339395+0000     4     256     13480     13224   826.301       847  0.00363642   0.0752394
2021-05-17T21:52:56.339601+0000     5     256     16864     16608   830.206       846  0.00169275   0.0759009
2021-05-17T21:52:57.339804+0000     6     255     20718     20463    852.43    963.75    0.100696   0.0744238
2021-05-17T21:52:58.340215+0000     7     256     25500     25244   901.342   1195.25     0.03282   0.0703753
2021-05-17T21:52:59.340460+0000     8     256     30210     29954   935.825    1177.5   0.0205692    0.067997
2021-05-17T21:53:00.340806+0000     9     256     34463     34207   949.944   1063.25    0.124155   0.0668895
2021-05-17T21:53:01.346032+0000    10     256     38877     38621   964.792    1103.5   0.0559033   0.0658973
2021-05-17T21:53:02.346615+0000    11     255     43320     43065   978.022      1111  0.00686695   0.0649137
2021-05-17T21:53:03.346901+0000    12     255     47562     47307   984.868    1060.5  0.00626178   0.0644225
2021-05-17T21:53:04.347287+0000    13     255     51731     51476   989.249   1042.25    0.162685   0.0642217
2021-05-17T21:53:05.347492+0000    14     256     56150     55894   997.462    1104.5    0.159617   0.0637062
2021-05-17T21:53:06.347747+0000    15     255     60426     60171   1002.23   1069.25    0.139974   0.0635137
2021-05-17T21:53:07.348018+0000 Total time run:       15.4231
Total reads made:     61793
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1001.63
Average IOPS:         4006
Stddev IOPS:          549.402
Max IOPS:             4781
Min IOPS:             3218
Average Latency(s):   0.0635479
Max latency(s):       0.270096
Min latency(s):       0.000874664

[1;32mlocalhost.localdomain	[2021-05-17T14:53:07,655068796-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1291323


[1;33mlocalhost.localdomain	[2021-05-17T14:53:07,663812359-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:53:30,573386120-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.79k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:53:30,594600076-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:53:38,591386252-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.79k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:53:38,609221782-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:53:46,667046091-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.79k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:53:46,684820096-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:53:54,547700513-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.79k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:53:54,565392514-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:54:02,679572515-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.79k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:54:02,697369384-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:54:02,711822469-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:54:02,717301326-07:00][RUNNING][ROUND 4/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:54:02,725510709-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T14:54:02,743384571-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40222\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.456080\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f4be7d66-120f-4057-8c1b-3425352a531e\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid f4be7d66-120f-4057-8c1b-3425352a531e\nlast_changed 2021-05-17T14:54:31.904105-0700\ncreated 2021-05-17T14:54:31.904105-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40222/0,v1:10.10.1.2:40223/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.456080 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 3739406d-8575-4181-960f-dbdd42b4f223\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 caa28f54-35b9-4176-985d-d76a9508165d\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 e022fa60-8243-4c21-b091-13ad950624e5\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42222\n  w/ user/pass: admin / 50acc186-8641-41c0-befa-b26302f2c09d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 14:54:46 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40222
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.456080
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f4be7d66-120f-4057-8c1b-3425352a531e
setting min_mon_release = octopus
epoch 0
fsid f4be7d66-120f-4057-8c1b-3425352a531e
last_changed 2021-05-17T14:54:31.904105-0700
created 2021-05-17T14:54:31.904105-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40222/0,v1:10.10.1.2:40223/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.456080 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 3739406d-8575-4181-960f-dbdd42b4f223
0
start osd.0
add osd1 caa28f54-35b9-4176-985d-d76a9508165d
1
start osd.1
add osd2 e022fa60-8243-4c21-b091-13ad950624e5
2
start osd.2


restful urls: https://10.10.1.2:42222
  w/ user/pass: admin / 50acc186-8641-41c0-befa-b26302f2c09d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T14:54:03.731-0700 7faeb4d681c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:54:03.731-0700 7faeb4d681c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:54:03.751-0700 7fcc2ec211c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T14:54:03.751-0700 7fcc2ec211c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40222,v1:10.10.1.2:40223] --print /tmp/ceph_monmap.456080 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.456080 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.456080 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42222 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.FcEleOJWo2 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3739406d-8575-4181-960f-dbdd42b4f223 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAg5qJgxCeOMhAACIUHGxa814UHuuuhFcyxWQ== --osd-uuid 3739406d-8575-4181-960f-dbdd42b4f223 
2021-05-17T14:54:41.199-0700 7f1bc165df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:54:41.203-0700 7f1bc165df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:54:41.203-0700 7f1bc165df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T14:54:41.247-0700 7f1bc165df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new caa28f54-35b9-4176-985d-d76a9508165d -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T14:54:41.539-0700 7f93cca08f00 -1 Falling back to public interface
2021-05-17T14:54:41.555-0700 7f93cca08f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAh5qJg4a4uIBAAARmQjop/9iQbXeOreVY4dQ== --osd-uuid caa28f54-35b9-4176-985d-d76a9508165d 
2021-05-17T14:54:41.887-0700 7f20b89def00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:54:41.887-0700 7f20b89def00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:54:41.887-0700 7f20b89def00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T14:54:41.931-0700 7f20b89def00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e022fa60-8243-4c21-b091-13ad950624e5 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T14:54:42.283-0700 7f42353c0f00 -1 Falling back to public interface
2021-05-17T14:54:42.299-0700 7f42353c0f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAi5qJg+4X0EBAA1ZUkcRPmahx/x26VGLZ9Dw== --osd-uuid e022fa60-8243-4c21-b091-13ad950624e5 
2021-05-17T14:54:42.615-0700 7f0e9e544f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:54:42.615-0700 7f0e9e544f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:54:42.615-0700 7f0e9e544f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T14:54:42.679-0700 7f0e9e544f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T14:54:42.971-0700 7f972508bf00 -1 Falling back to public interface
2021-05-17T14:54:42.987-0700 7f972508bf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T14:54:46,968687367-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:54:46,977271633-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T14:54:47,062451226-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:54:47,068993299-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '710423ed-d834-4777-aacf-713bb4bf4060', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 710423ed-d834-4777-aacf-713bb4bf4060 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T14:54:50,105607113-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:54:50,112153267-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '710423ed-d834-4777-aacf-713bb4bf4060', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 710423ed-d834-4777-aacf-713bb4bf4060 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T14:54:52,969159257-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:54:52,976000722-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '710423ed-d834-4777-aacf-713bb4bf4060', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 710423ed-d834-4777-aacf-713bb4bf4060 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T14:54:55,849410025-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:54:55,855925229-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '710423ed-d834-4777-aacf-713bb4bf4060', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 710423ed-d834-4777-aacf-713bb4bf4060 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T14:55:01,424308132-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:55:01,432094367-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '710423ed-d834-4777-aacf-713bb4bf4060', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 710423ed-d834-4777-aacf-713bb4bf4060 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T14:55:04,872248831-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:55:04,883367764-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '710423ed-d834-4777-aacf-713bb4bf4060', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 710423ed-d834-4777-aacf-713bb4bf4060 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T14:55:08,529241307-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:55:08,535868760-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '710423ed-d834-4777-aacf-713bb4bf4060', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 710423ed-d834-4777-aacf-713bb4bf4060 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T14:55:11,838210670-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:55:11,845459078-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '710423ed-d834-4777-aacf-713bb4bf4060', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 710423ed-d834-4777-aacf-713bb4bf4060 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:55:14,780467414-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:55:14,787084380-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '710423ed-d834-4777-aacf-713bb4bf4060', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 710423ed-d834-4777-aacf-713bb4bf4060 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T14:55:18,114621370-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:55:18,121113050-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '710423ed-d834-4777-aacf-713bb4bf4060', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 710423ed-d834-4777-aacf-713bb4bf4060 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T14:55:21,441789868-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:55:21,448014097-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '710423ed-d834-4777-aacf-713bb4bf4060', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 710423ed-d834-4777-aacf-713bb4bf4060 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T14:55:24,083420198-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:55:24,089838110-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '710423ed-d834-4777-aacf-713bb4bf4060', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 710423ed-d834-4777-aacf-713bb4bf4060 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T14:55:26,938615702-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:55:49,839379907-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   198 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:55:57,844489687-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:56:05,979009547-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 29s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:56:14,276192211-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 52s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:56:22,498075354-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (30s)
      [=======.....................] (remaining: 76s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:56:30,579622590-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 80s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:56:38,547724361-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 92s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:56:46,560608502-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   233 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [==============..............] (remaining: 45s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:56:54,591883603-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:56:54,609429780-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:57:02,446664062-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:57:02,464623259-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:57:10,492148449-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:57:10,510061873-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:57:18,631451548-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:57:18,649412215-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:57:26,826222451-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:57:26,845818711-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:57:26,862279633-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:57:26,871730927-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:57:26,899636277-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1305167
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T14:57:26,916632144-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-17T14:57:26,958456098-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:57:26,965117652-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '710423ed-d834-4777-aacf-713bb4bf4060', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 710423ed-d834-4777-aacf-713bb4bf4060 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:57:28.505+0000 ffff9eab0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:57:28.513+0000 ffff9eab0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:57:28.513+0000 ffff9eab0010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:57:28.529983+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-17T21:57:28.530039+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T21:57:28.580536+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:57:28.580536+0000     0       0         0         0         0         0           -           0
2021-05-17T21:57:29.580776+0000     1     255      2859      2604   650.938       651    0.171273   0.0901507
2021-05-17T21:57:30.580995+0000     2     255      5518      5263   657.771    664.75   0.0595548   0.0938393
2021-05-17T21:57:31.581276+0000     3     255      8222      7967   663.785       676   0.0785931   0.0937605
2021-05-17T21:57:32.581723+0000     4     255     11014     10759   672.262       698   0.0579359   0.0934021
2021-05-17T21:57:33.582008+0000     5     255     13787     13532    676.42    693.25   0.0634859   0.0932187
2021-05-17T21:57:34.582222+0000     6     255     16308     16053   668.703    630.25   0.0775463   0.0944656
2021-05-17T21:57:35.582472+0000     7     255     19240     18985   677.862       733   0.0612502   0.0933204
2021-05-17T21:57:36.582748+0000     8     255     22046     21791   680.793     701.5   0.0531702   0.0929919
2021-05-17T21:57:37.582972+0000     9     255     24576     24321   675.411     632.5    0.143444   0.0939153
2021-05-17T21:57:38.583279+0000    10     255     26885     26630   665.577    577.25   0.0389253   0.0950338
2021-05-17T21:57:39.583553+0000    11     255     29326     29071   660.532    610.25   0.0707431   0.0961627
2021-05-17T21:57:40.583767+0000    12     255     31918     31663   659.476       648   0.0622572   0.0964496
2021-05-17T21:57:41.584132+0000    13     255     34329     34074   655.095    602.75   0.0808484   0.0970484
2021-05-17T21:57:42.584424+0000    14     255     36766     36511   651.808    609.25   0.0912663   0.0976453
2021-05-17T21:57:43.584717+0000    15     255     39268     39013   650.042     625.5    0.137982   0.0979813
2021-05-17T21:57:44.584965+0000    16     255     41877     41622    650.17    652.25   0.0849029   0.0979095
2021-05-17T21:57:45.585313+0000    17     255     44295     44040   647.471     604.5   0.0891815   0.0982982
2021-05-17T21:57:46.585716+0000    18     255     46930     46675   648.083    658.75    0.187537   0.0981603
2021-05-17T21:57:47.585960+0000    19     255     49666     49411   649.964       684    0.139673   0.0981497
2021-05-17T21:57:48.586179+0000 min lat: 0.0297641 max lat: 0.359903 avg lat: 0.0981487
2021-05-17T21:57:48.586179+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:57:48.586179+0000    20     255     52167     51912   648.722    625.25   0.0676401   0.0981487
2021-05-17T21:57:49.586453+0000 Total time run:         20.0704
Total writes made:      52168
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     649.813
Stddev Bandwidth:       40.1136
Max bandwidth (MB/sec): 733
Min bandwidth (MB/sec): 577.25
Average IOPS:           2599
Stddev IOPS:            160.454
Max IOPS:               2932
Min IOPS:               2309
Average Latency(s):     0.0981283
Stddev Latency(s):      0.0433432
Max latency(s):         0.359903
Min latency(s):         0.0297641

[1;32mlocalhost.localdomain	[2021-05-17T14:57:49,892708190-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1305167


[1;33mlocalhost.localdomain	[2021-05-17T14:57:49,901479313-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:58:12,948482794-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.17k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:58:12,966676675-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:58:20,920506711-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.17k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:58:20,938703248-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:58:29,209501598-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.17k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:58:29,227783935-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:58:37,360159967-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.17k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:58:37,378426567-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:58:45,505779522-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.17k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:58:45,523907857-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:58:45,538530426-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:58:45,547176953-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:58:45,566627299-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1308485
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T14:58:45,583310603-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-17T14:58:45,624427352-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T14:58:45,630925738-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '710423ed-d834-4777-aacf-713bb4bf4060', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 710423ed-d834-4777-aacf-713bb4bf4060 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.iJRFEy:/tmp/ceph-asok.iJRFEy -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T21:58:47.135+0000 ffffadbbf010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:58:47.143+0000 ffffadbbf010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T21:58:47.143+0000 ffffadbbf010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T21:58:47.163105+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T21:58:47.163105+0000     0       0         0         0         0         0           -           0
2021-05-17T21:58:48.163354+0000     1     255      3479      3224   805.698       806   0.0584678   0.0757854
2021-05-17T21:58:49.163644+0000     2     255      6806      6551   818.602    831.75    0.141183   0.0757278
2021-05-17T21:58:50.163846+0000     3     255     10152      9897   824.511     836.5    0.192249   0.0753308
2021-05-17T21:58:51.164028+0000     4     255     13444     13189   824.096       823  0.00449131   0.0756878
2021-05-17T21:58:52.164246+0000     5     256     16740     16484   823.991    823.75    0.215717   0.0759909
2021-05-17T21:58:53.164471+0000     6     255     20131     19876   827.961       848   0.0097373   0.0758985
2021-05-17T21:58:54.164737+0000     7     256     23622     23366    834.29     872.5     0.19697   0.0755231
2021-05-17T21:58:55.165011+0000     8     255     26993     26738    835.35       843  0.00641355   0.0755789
2021-05-17T21:58:56.165349+0000     9     255     30252     29997   833.031    814.75    0.186303   0.0759139
2021-05-17T21:58:57.165590+0000    10     255     33834     33579   839.256     895.5  0.00604672   0.0755725
2021-05-17T21:58:58.165911+0000    11     255     38127     37872   860.498   1073.25    0.159574   0.0738037
2021-05-17T21:58:59.166282+0000    12     255     42371     42116   877.175      1061  0.00923368   0.0724195
2021-05-17T21:59:00.166512+0000    13     255     46465     46210   888.412    1023.5  0.00265539   0.0714848
2021-05-17T21:59:01.167865+0000    14     255     50614     50359   898.954   1037.25    0.181246   0.0706837
2021-05-17T21:59:02.168112+0000 Total time run:       14.4913
Total reads made:     52168
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   899.989
Average IOPS:         3599
Stddev IOPS:          405.023
Max IOPS:             4293
Min IOPS:             3224
Average Latency(s):   0.0707087
Max latency(s):       0.245622
Min latency(s):       0.000961829

[1;32mlocalhost.localdomain	[2021-05-17T14:59:02,528025041-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1308485


[1;33mlocalhost.localdomain	[2021-05-17T14:59:02,537490152-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T14:59:25,524213706-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.17k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:59:25,542478250-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:59:35,572820912-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.17k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:59:35,591153177-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:59:43,591589530-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.17k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:59:43,609971769-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:59:51,689623559-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.17k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:59:51,707667445-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:59:59,866170964-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 52.17k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T14:59:59,884807721-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T14:59:59,899546980-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:59:59,904986096-07:00][RUNNING][ROUND 5/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T14:59:59,913452637-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T14:59:59,930734138-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40411\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.457193\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 121c782c-f5de-48ba-a0ab-f5bc256fb095\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 121c782c-f5de-48ba-a0ab-f5bc256fb095\nlast_changed 2021-05-17T15:00:28.457335-0700\ncreated 2021-05-17T15:00:28.457335-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40411/0,v1:10.10.1.2:40412/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.457193 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 eee541da-d455-4bf3-9f51-a868c1958bb2\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 dd867ded-68cd-48f6-8844-3e1b4d7b160f\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 04355f94-9f21-4005-aae9-39323bd14193\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42411\n  w/ user/pass: admin / b4622ab1-7992-4fb5-adf5-2595147fe472\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:00:43 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40411
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.457193
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 121c782c-f5de-48ba-a0ab-f5bc256fb095
setting min_mon_release = octopus
epoch 0
fsid 121c782c-f5de-48ba-a0ab-f5bc256fb095
last_changed 2021-05-17T15:00:28.457335-0700
created 2021-05-17T15:00:28.457335-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40411/0,v1:10.10.1.2:40412/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.457193 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 eee541da-d455-4bf3-9f51-a868c1958bb2
0
start osd.0
add osd1 dd867ded-68cd-48f6-8844-3e1b4d7b160f
1
start osd.1
add osd2 04355f94-9f21-4005-aae9-39323bd14193
2
start osd.2


restful urls: https://10.10.1.2:42411
  w/ user/pass: admin / b4622ab1-7992-4fb5-adf5-2595147fe472


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T15:00:00.934-0700 7f9ed04511c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:00:00.934-0700 7f9ed04511c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:00:00.950-0700 7effb52421c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:00:00.950-0700 7effb52421c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40411,v1:10.10.1.2:40412] --print /tmp/ceph_monmap.457193 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.457193 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.457193 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42411 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.0WEp2khrIo 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new eee541da-d455-4bf3-9f51-a868c1958bb2 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCF56JgIEdaFhAAIjSzZ3PoKrOC7zZy4lvcew== --osd-uuid eee541da-d455-4bf3-9f51-a868c1958bb2 
2021-05-17T15:00:37.726-0700 7f3420a26f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:00:37.726-0700 7f3420a26f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:00:37.726-0700 7f3420a26f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:00:37.806-0700 7f3420a26f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new dd867ded-68cd-48f6-8844-3e1b4d7b160f -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T15:00:38.082-0700 7f84cffa8f00 -1 Falling back to public interface
2021-05-17T15:00:38.094-0700 7f84cffa8f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCG56Jg0wryBBAAdUtMfVDL7jjQ9DjuhefmEQ== --osd-uuid dd867ded-68cd-48f6-8844-3e1b4d7b160f 
2021-05-17T15:00:38.430-0700 7f0af021af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:00:38.430-0700 7f0af021af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:00:38.430-0700 7f0af021af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:00:38.506-0700 7f0af021af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 04355f94-9f21-4005-aae9-39323bd14193 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T15:00:38.806-0700 7f15d127cf00 -1 Falling back to public interface
2021-05-17T15:00:38.822-0700 7f15d127cf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCG56JgT5phLhAA5LXJg+WvGMCUdAtuQlC+Eg== --osd-uuid 04355f94-9f21-4005-aae9-39323bd14193 
2021-05-17T15:00:39.134-0700 7f8a2094ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:00:39.134-0700 7f8a2094ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:00:39.134-0700 7f8a2094ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:00:39.230-0700 7f8a2094ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T15:00:39.590-0700 7fd555371f00 -1 Falling back to public interface
2021-05-17T15:00:39.606-0700 7fd555371f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T15:00:43,464978072-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:00:43,473675412-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T15:00:43,557134768-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:00:43,563680578-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e990859-ecb3-40d1-b633-02fbd4cb2aa5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e990859-ecb3-40d1-b633-02fbd4cb2aa5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T15:00:46,489091485-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:00:46,496611050-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e990859-ecb3-40d1-b633-02fbd4cb2aa5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e990859-ecb3-40d1-b633-02fbd4cb2aa5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T15:00:49,249372194-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:00:49,255921424-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e990859-ecb3-40d1-b633-02fbd4cb2aa5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e990859-ecb3-40d1-b633-02fbd4cb2aa5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T15:00:52,258651233-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:00:52,265145793-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e990859-ecb3-40d1-b633-02fbd4cb2aa5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e990859-ecb3-40d1-b633-02fbd4cb2aa5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T15:00:58,143580845-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:00:58,150333270-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e990859-ecb3-40d1-b633-02fbd4cb2aa5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e990859-ecb3-40d1-b633-02fbd4cb2aa5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T15:01:01,303621477-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:01:01,310270819-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e990859-ecb3-40d1-b633-02fbd4cb2aa5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e990859-ecb3-40d1-b633-02fbd4cb2aa5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T15:01:04,849046793-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:01:04,855521175-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e990859-ecb3-40d1-b633-02fbd4cb2aa5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e990859-ecb3-40d1-b633-02fbd4cb2aa5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T15:01:08,005879894-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:01:08,012583245-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e990859-ecb3-40d1-b633-02fbd4cb2aa5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e990859-ecb3-40d1-b633-02fbd4cb2aa5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:01:11,392254288-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:01:11,398857572-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e990859-ecb3-40d1-b633-02fbd4cb2aa5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e990859-ecb3-40d1-b633-02fbd4cb2aa5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:01:14,696545765-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:01:14,703024453-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e990859-ecb3-40d1-b633-02fbd4cb2aa5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e990859-ecb3-40d1-b633-02fbd4cb2aa5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T15:01:17,992423645-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:01:17,998934693-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e990859-ecb3-40d1-b633-02fbd4cb2aa5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e990859-ecb3-40d1-b633-02fbd4cb2aa5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T15:01:20,712297530-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:01:20,718805518-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e990859-ecb3-40d1-b633-02fbd4cb2aa5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e990859-ecb3-40d1-b633-02fbd4cb2aa5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T15:01:23,381464134-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:01:46,357316269-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:01:54,442273043-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:02:02,621013152-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 27s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:02:10,560662051-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (19s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:02:18,636181053-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (24s)
      [========....................] (remaining: 56s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:02:26,652310304-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 73s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:02:34,771293614-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 83s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:02:43,106335844-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 42s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:02:51,087757713-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:02:51,105762863-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:02:59,131655483-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:02:59,149900043-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:03:07,320840491-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:03:07,338894519-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:03:15,362390710-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:03:15,380870975-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:03:23,384976847-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:03:23,403419141-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:03:23,418221201-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:03:23,426672260-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:03:23,446408269-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1322271
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T15:03:23,462681179-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-17T15:03:23,503516210-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:03:23,510162011-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e990859-ecb3-40d1-b633-02fbd4cb2aa5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e990859-ecb3-40d1-b633-02fbd4cb2aa5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:03:24.986+0000 ffff89330010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:03:24.994+0000 ffff89330010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:03:24.994+0000 ffff89330010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:03:25.009492+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-17T22:03:25.009552+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T22:03:25.063766+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:03:25.063766+0000     0       6         6         0         0         0           -           0
2021-05-17T22:03:26.064409+0000     1     255      2817      2562   639.737     640.5    0.112178   0.0925582
2021-05-17T22:03:27.065352+0000     2     255      5241      4986   622.585       606   0.0712188   0.0991433
2021-05-17T22:03:28.065596+0000     3     255      7916      7661    637.91    668.75    0.237972   0.0967139
2021-05-17T22:03:29.066135+0000     4     255     10537     10282   642.156    655.25   0.0893747   0.0978597
2021-05-17T22:03:30.066423+0000     5     255     13104     12849   642.038    641.75   0.0817572   0.0974568
2021-05-17T22:03:31.066830+0000     6     255     15812     15557   647.818       677   0.0693414    0.097455
2021-05-17T22:03:32.067273+0000     7     255     18397     18142   647.553    646.25   0.0676188   0.0976784
2021-05-17T22:03:33.067514+0000     8     255     20826     20571   642.499    607.25   0.0850122   0.0984876
2021-05-17T22:03:34.067776+0000     9     255     23506     23251   645.534       670    0.049075   0.0980908
2021-05-17T22:03:35.068677+0000    10     255     26003     25748   643.349    624.25     0.14531    0.097999
2021-05-17T22:03:36.068908+0000    11     255     28397     28142    639.26     598.5    0.117584   0.0990899
2021-05-17T22:03:37.069297+0000    12     255     30673     30418   633.387       569    0.163213    0.100126
2021-05-17T22:03:38.069593+0000    13     255     33021     32766   629.806       587   0.0948926    0.101032
2021-05-17T22:03:39.069884+0000    14     255     35638     35383   631.538    654.25   0.0747739    0.100828
2021-05-17T22:03:40.070320+0000    15     255     38108     37853   630.585     617.5   0.0701147    0.100814
2021-05-17T22:03:41.070538+0000    16     255     40503     40248   628.587    598.75   0.0818476     0.10122
2021-05-17T22:03:42.070731+0000    17     255     43201     42946    631.28     674.5    0.095048    0.100665
2021-05-17T22:03:43.070977+0000    18     255     45770     45515    631.88    642.25     0.14321    0.100776
2021-05-17T22:03:44.071183+0000    19     255     47957     47702   627.395    546.75   0.0948947    0.101446
2021-05-17T22:03:45.071387+0000 min lat: 0.00908045 max lat: 0.383245 avg lat: 0.101643
2021-05-17T22:03:45.071387+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:03:45.071387+0000    20     102     50367     50265   628.056    640.75   0.0101251    0.101643
2021-05-17T22:03:46.071639+0000 Total time run:         20.226
Total writes made:      50367
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     622.552
Stddev Bandwidth:       36.2664
Max bandwidth (MB/sec): 677
Min bandwidth (MB/sec): 546.75
Average IOPS:           2490
Stddev IOPS:            145.065
Max IOPS:               2708
Min IOPS:               2187
Average Latency(s):     0.101742
Stddev Latency(s):      0.0462074
Max latency(s):         0.383245
Min latency(s):         0.00908045

[1;32mlocalhost.localdomain	[2021-05-17T15:03:46,383797659-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1322271


[1;33mlocalhost.localdomain	[2021-05-17T15:03:46,394748458-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:04:09,347662684-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.37k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:04:09,365778688-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:04:17,306471746-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.37k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:04:17,325170764-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:04:25,440632894-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.37k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:04:25,459227645-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:04:33,338122472-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.37k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:04:33,359495710-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:04:41,357595340-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.37k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:04:41,376369026-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:04:41,391350816-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:04:41,399741924-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:04:41,419612162-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1325626
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T15:04:41,435937911-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-17T15:04:41,475854185-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:04:41,482067838-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e990859-ecb3-40d1-b633-02fbd4cb2aa5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e990859-ecb3-40d1-b633-02fbd4cb2aa5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.foWDJP:/tmp/ceph-asok.foWDJP -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:04:42.967+0000 ffff89d69010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:04:42.975+0000 ffff89d69010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:04:42.975+0000 ffff89d69010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:04:42.994813+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:04:42.994813+0000     0       0         0         0         0         0           -           0
2021-05-17T22:04:43.995714+0000     1     255      4425      4170   1041.42    1042.5   0.0558294   0.0580766
2021-05-17T22:04:44.995981+0000     2     256      8896      8640    1079.3    1117.5    0.136192   0.0574027
2021-05-17T22:04:45.996191+0000     3     255     13134     12879   1072.71   1059.75    0.161837   0.0581308
2021-05-17T22:04:46.996415+0000     4     255     17488     17233   1076.59    1088.5    0.155542   0.0582378
2021-05-17T22:04:47.996741+0000     5     255     21846     21591    1079.1    1089.5   0.0129266   0.0586542
2021-05-17T22:04:48.996961+0000     6     255     26280     26025   1083.96    1108.5   0.0100384   0.0585061
2021-05-17T22:04:49.997277+0000     7     255     30798     30543   1090.42    1129.5    0.020648    0.058032
2021-05-17T22:04:50.997636+0000     8     256     34897     34641   1082.13    1024.5    0.147616    0.058579
2021-05-17T22:04:51.997869+0000     9     255     38934     38679   1074.04    1009.5   0.0116395   0.0589811
2021-05-17T22:04:52.998797+0000    10     255     43290     43035   1075.43      1089  0.00635505   0.0589657
2021-05-17T22:04:53.999451+0000    11     256     46784     46528      1057    873.25  0.00353651   0.0601743
2021-05-17T22:04:55.000944+0000 Total time run:       11.9867
Total reads made:     50367
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1050.48
Average IOPS:         4201
Stddev IOPS:          288.259
Max IOPS:             4518
Min IOPS:             3493
Average Latency(s):   0.0605385
Max latency(s):       0.230197
Min latency(s):       0.000888812

[1;32mlocalhost.localdomain	[2021-05-17T15:04:55,313822939-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1325626


[1;33mlocalhost.localdomain	[2021-05-17T15:04:55,323156237-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:05:18,475267943-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.37k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:05:18,494247824-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:05:26,633419041-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.37k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:05:26,651921136-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:05:34,500310441-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.37k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:05:34,525058998-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:05:42,633492716-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.37k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:05:42,651813939-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:05:50,650360238-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.37k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:05:50,669186267-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:05:50,684040206-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T15:05:50,692969726-07:00][RUNNING][ROUND 1/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:05:50,701558538-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T15:05:50,719143961-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40381\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.458300\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 1710aa82-791c-49e3-9b97-6e49e282a1b3\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 1710aa82-791c-49e3-9b97-6e49e282a1b3\nlast_changed 2021-05-17T15:06:19.786118-0700\ncreated 2021-05-17T15:06:19.786118-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40381/0,v1:10.10.1.2:40382/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.458300 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 cf18ab22-3214-4125-a74e-237421c3f873\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 03efa8c3-73f6-4678-a5a6-9ea2892b25d9\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 cefe55d5-2711-4ae3-88bf-74be8803a47f\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42381\n  w/ user/pass: admin / ef669230-a353-49a6-be9b-4911d0db8971\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:06:34 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40381
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.458300
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 1710aa82-791c-49e3-9b97-6e49e282a1b3
setting min_mon_release = octopus
epoch 0
fsid 1710aa82-791c-49e3-9b97-6e49e282a1b3
last_changed 2021-05-17T15:06:19.786118-0700
created 2021-05-17T15:06:19.786118-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40381/0,v1:10.10.1.2:40382/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.458300 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 cf18ab22-3214-4125-a74e-237421c3f873
0
start osd.0
add osd1 03efa8c3-73f6-4678-a5a6-9ea2892b25d9
1
start osd.1
add osd2 cefe55d5-2711-4ae3-88bf-74be8803a47f
2
start osd.2


restful urls: https://10.10.1.2:42381
  w/ user/pass: admin / ef669230-a353-49a6-be9b-4911d0db8971


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T15:05:51.753-0700 7f8d76dab1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:05:51.753-0700 7f8d76dab1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:05:51.773-0700 7fd086b5e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:05:51.773-0700 7fd086b5e1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40381,v1:10.10.1.2:40382] --print /tmp/ceph_monmap.458300 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.458300 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.458300 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42381 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.qgUdzPpWcU 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new cf18ab22-3214-4125-a74e-237421c3f873 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDk6KJg26HDLBAAGYSRD4liHYadEUN7aQvlkQ== --osd-uuid cf18ab22-3214-4125-a74e-237421c3f873 
2021-05-17T15:06:29.085-0700 7ff7fcb05f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:06:29.085-0700 7ff7fcb05f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:06:29.085-0700 7ff7fcb05f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:06:29.129-0700 7ff7fcb05f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 03efa8c3-73f6-4678-a5a6-9ea2892b25d9 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T15:06:29.417-0700 7f6d5e6caf00 -1 Falling back to public interface
2021-05-17T15:06:29.429-0700 7f6d5e6caf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDl6KJgigHpGBAATdWOSEHGeBa9OllnGVBT3g== --osd-uuid 03efa8c3-73f6-4678-a5a6-9ea2892b25d9 
2021-05-17T15:06:29.745-0700 7f0540eb8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:06:29.745-0700 7f0540eb8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:06:29.745-0700 7f0540eb8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:06:29.817-0700 7f0540eb8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new cefe55d5-2711-4ae3-88bf-74be8803a47f -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T15:06:30.153-0700 7fc3587fdf00 -1 Falling back to public interface
2021-05-17T15:06:30.165-0700 7fc3587fdf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDm6KJg4pIOCRAAwf1YWY3S2Tm7CkrpdP+3Cg== --osd-uuid cefe55d5-2711-4ae3-88bf-74be8803a47f 
2021-05-17T15:06:30.505-0700 7fcbea065f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:06:30.509-0700 7fcbea065f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:06:30.509-0700 7fcbea065f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:06:30.613-0700 7fcbea065f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T15:06:30.937-0700 7fc841931f00 -1 Falling back to public interface
2021-05-17T15:06:30.953-0700 7fc841931f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T15:06:34,883596989-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:06:34,893924050-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T15:06:34,992881613-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:06:35,000815318-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b78925a-095c-4d27-b42b-870f6f9c063b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b78925a-095c-4d27-b42b-870f6f9c063b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T15:06:37,817577094-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:06:37,823988656-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b78925a-095c-4d27-b42b-870f6f9c063b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b78925a-095c-4d27-b42b-870f6f9c063b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T15:06:40,852186475-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:06:40,858495780-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b78925a-095c-4d27-b42b-870f6f9c063b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b78925a-095c-4d27-b42b-870f6f9c063b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T15:06:43,683688537-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:06:43,690077176-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b78925a-095c-4d27-b42b-870f6f9c063b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b78925a-095c-4d27-b42b-870f6f9c063b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T15:06:49,300750740-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:06:49,307277792-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b78925a-095c-4d27-b42b-870f6f9c063b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b78925a-095c-4d27-b42b-870f6f9c063b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T15:06:52,256621627-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:06:52,264526948-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b78925a-095c-4d27-b42b-870f6f9c063b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b78925a-095c-4d27-b42b-870f6f9c063b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T15:06:55,566390849-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:06:55,573409744-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b78925a-095c-4d27-b42b-870f6f9c063b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b78925a-095c-4d27-b42b-870f6f9c063b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T15:06:58,874581469-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:06:58,881675032-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b78925a-095c-4d27-b42b-870f6f9c063b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b78925a-095c-4d27-b42b-870f6f9c063b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:07:02,610716884-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:07:02,618059325-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b78925a-095c-4d27-b42b-870f6f9c063b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b78925a-095c-4d27-b42b-870f6f9c063b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:07:05,520262428-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:07:05,526774973-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b78925a-095c-4d27-b42b-870f6f9c063b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b78925a-095c-4d27-b42b-870f6f9c063b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T15:07:08,941723345-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:07:08,948346113-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b78925a-095c-4d27-b42b-870f6f9c063b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b78925a-095c-4d27-b42b-870f6f9c063b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T15:07:11,739119317-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:07:11,745462798-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b78925a-095c-4d27-b42b-870f6f9c063b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b78925a-095c-4d27-b42b-870f6f9c063b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T15:07:14,441941054-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:07:37,430857940-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=====.......................] (remaining: 18s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:07:45,494762345-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:07:53,497781843-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:08:01,689558942-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:08:09,658429582-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:08:17,783566225-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:08:25,759610008-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:08:33,688583567-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:08:33,707005449-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:08:41,955543194-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:08:41,973971392-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:08:49,895360520-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:08:49,913549427-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:08:57,939574390-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:08:57,957515305-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:09:06,190184779-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:09:06,208585162-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:09:06,223611863-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:09:06,232225440-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:09:06,251938081-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1338892
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T15:09:06,267882892-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T15:09:06,309189193-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:09:06,315673039-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b78925a-095c-4d27-b42b-870f6f9c063b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b78925a-095c-4d27-b42b-870f6f9c063b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:09:08.096+0000 ffff9aad8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:09:08.100+0000 ffff9aad8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:09:08.104+0000 ffff9aad8010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:09:08.121208+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T22:09:08.121280+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T22:09:08.332598+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:09:08.332598+0000     0       0         0         0         0         0           -           0
2021-05-17T22:09:09.332831+0000     1     255       738       483   482.958       483    0.331698    0.351238
2021-05-17T22:09:10.333072+0000     2     255      1530      1275   637.395       792    0.323896    0.333782
2021-05-17T22:09:11.333285+0000     3     255      2294      2039   679.544       764    0.314013     0.33489
2021-05-17T22:09:12.333545+0000     4     255      3105      2850   712.357       811    0.319775    0.329015
2021-05-17T22:09:13.333786+0000     5     255      3939      3684   736.646       834    0.305467    0.323908
2021-05-17T22:09:14.334356+0000     6     255      4767      4512   751.798       828     0.31997    0.320843
2021-05-17T22:09:15.334613+0000     7     255      5546      5291   755.655       779     0.31866    0.322066
2021-05-17T22:09:16.335364+0000     8     255      6309      6054   756.502       763    0.372276    0.323279
2021-05-17T22:09:17.335673+0000     9     255      7035      6780   753.088       726    0.384718    0.326092
2021-05-17T22:09:18.336046+0000    10     255      7694      7439   743.654       659    0.385299    0.331714
2021-05-17T22:09:19.336308+0000    11     255      8335      8080   734.307       641    0.386205     0.33636
2021-05-17T22:09:20.336586+0000    12     255      9015      8760   729.766       680    0.372792    0.340397
2021-05-17T22:09:21.336907+0000    13     255      9636      9381   721.384       621    0.342316    0.345053
2021-05-17T22:09:22.337123+0000    14     255     10316     10061   718.418       680    0.361307    0.347231
2021-05-17T22:09:23.337326+0000    15     255     11003     10748   716.315       687    0.391329    0.348253
2021-05-17T22:09:24.337538+0000    16     255     11638     11383   711.224       635    0.411247    0.351122
2021-05-17T22:09:25.337877+0000    17     255     12269     12014   706.493       631    0.426498    0.354246
2021-05-17T22:09:26.338119+0000    18     255     12920     12665   703.401       651    0.382696    0.356355
2021-05-17T22:09:27.338443+0000    19     255     13575     13320   700.842       655    0.389388    0.357896
2021-05-17T22:09:28.338703+0000 min lat: 0.280223 max lat: 0.506918 avg lat: 0.359897
2021-05-17T22:09:28.338703+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:09:28.338703+0000    20     255     14212     13957   697.642       637    0.397114    0.359897
2021-05-17T22:09:29.338995+0000 Total time run:         20.0483
Total writes made:      14213
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     708.937
Stddev Bandwidth:       87.9164
Max bandwidth (MB/sec): 834
Min bandwidth (MB/sec): 483
Average IOPS:           708
Stddev IOPS:            87.9164
Max IOPS:               834
Min IOPS:               483
Average Latency(s):     0.357447
Stddev Latency(s):      0.0468932
Max latency(s):         0.506918
Min latency(s):         0.0335243

[1;32mlocalhost.localdomain	[2021-05-17T15:09:29,708325868-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1338892


[1;33mlocalhost.localdomain	[2021-05-17T15:09:29,717502791-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:09:52,669398963-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.21k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:09:52,687859132-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:10:00,681091158-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.21k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:10:00,699729452-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:10:08,749568294-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.21k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:10:08,769334720-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:10:16,775611505-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.21k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:10:16,793991261-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:10:24,774971440-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.21k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:10:24,793344670-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:10:24,808420715-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:10:24,817171640-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:10:24,837169181-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1342193
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T15:10:24,853887224-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-17T15:10:24,894775962-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:10:24,901072186-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b78925a-095c-4d27-b42b-870f6f9c063b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b78925a-095c-4d27-b42b-870f6f9c063b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FJ3fvF:/tmp/ceph-asok.FJ3fvF -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:10:26.501+0000 ffff832b8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:10:26.509+0000 ffff832b8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:10:26.509+0000 ffff832b8010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:10:26.528886+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:10:26.528886+0000     0       0         0         0         0         0           -           0
2021-05-17T22:10:27.529092+0000     1     255       953       698   697.752       698    0.313758    0.291004
2021-05-17T22:10:28.529634+0000     2     255      1749      1494   746.665       796    0.316529     0.30667
2021-05-17T22:10:29.530165+0000     3     255      2529      2274   757.639       780    0.335259    0.313024
2021-05-17T22:10:30.530515+0000     4     255      3316      3061    764.91       787    0.322143    0.316582
2021-05-17T22:10:31.530751+0000     5     255      4114      3859   771.489       798    0.318932    0.317474
2021-05-17T22:10:32.531041+0000     6     255      4905      4650   774.702       791    0.321539    0.318147
2021-05-17T22:10:33.531647+0000     7     255      5665      5410   772.536       760    0.329874    0.320753
2021-05-17T22:10:34.531899+0000     8     255      6452      6197   774.319       787    0.329705    0.321288
2021-05-17T22:10:35.532128+0000     9     255      7223      6968   773.931       771    0.322367    0.322496
2021-05-17T22:10:36.532360+0000    10     255      7991      7736    773.32       768    0.350032    0.323086
2021-05-17T22:10:37.533729+0000    11     255      8788      8533   775.375       797    0.326333    0.323231
2021-05-17T22:10:38.534357+0000    12     255      9574      9319    776.22       786    0.323547    0.323483
2021-05-17T22:10:39.534799+0000    13     255     10360     10105   776.945       786     0.33358    0.323435
2021-05-17T22:10:40.535030+0000    14     255     11097     10842   774.081       737    0.334687    0.325086
2021-05-17T22:10:41.535250+0000    15     255     11863     11608   773.531       766    0.333623    0.325663
2021-05-17T22:10:42.535909+0000    16     255     12644     12389   773.966       781    0.323917    0.325857
2021-05-17T22:10:43.538642+0000    17     255     13407     13152   773.197       763     0.34864    0.326179
2021-05-17T22:10:44.538857+0000    18     255     14156     13901   771.844       749    0.337904     0.32707
2021-05-17T22:10:45.539183+0000 Total time run:       18.2896
Total reads made:     14213
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   777.11
Average IOPS:         777
Stddev IOPS:          25.1251
Max IOPS:             798
Min IOPS:             698
Average Latency(s):   0.325566
Max latency(s):       0.368628
Min latency(s):       0.0921367

[1;32mlocalhost.localdomain	[2021-05-17T15:10:45,906349618-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1342193


[1;33mlocalhost.localdomain	[2021-05-17T15:10:45,915917163-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:11:09,047273558-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.21k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:11:09,065803910-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:11:17,105223752-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.21k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:11:17,124929178-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:11:25,122750384-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.21k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:11:25,141253672-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:11:33,098441041-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.21k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:11:33,116987025-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:11:41,106928474-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.21k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:11:41,125411943-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:11:41,140541503-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T15:11:41,146290224-07:00][RUNNING][ROUND 2/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:11:41,155315457-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T15:11:41,173075803-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40681\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.460372\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 2de982bc-360f-4f10-8267-09021c8aeffa\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 2de982bc-360f-4f10-8267-09021c8aeffa\nlast_changed 2021-05-17T15:12:09.475307-0700\ncreated 2021-05-17T15:12:09.475307-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40681/0,v1:10.10.1.2:40682/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.460372 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 fb8b019f-d444-4b97-8b53-0cf43a1631b4\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 8d1912d2-2277-4219-a64b-0733d0f5f81f\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 0ad1ff03-42cb-4fe5-a20a-e8ecc480a6ea\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42681\n'
10.10.1.2: b'  w/ user/pass: admin / 9d1a9b27-3076-4d5c-bf8f-3b7fae82df10\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:12:24 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40681
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.460372
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 2de982bc-360f-4f10-8267-09021c8aeffa
setting min_mon_release = octopus
epoch 0
fsid 2de982bc-360f-4f10-8267-09021c8aeffa
last_changed 2021-05-17T15:12:09.475307-0700
created 2021-05-17T15:12:09.475307-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40681/0,v1:10.10.1.2:40682/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.460372 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 fb8b019f-d444-4b97-8b53-0cf43a1631b4
0
start osd.0
add osd1 8d1912d2-2277-4219-a64b-0733d0f5f81f
1
start osd.1
add osd2 0ad1ff03-42cb-4fe5-a20a-e8ecc480a6ea
2
start osd.2


restful urls: https://10.10.1.2:42681
  w/ user/pass: admin / 9d1a9b27-3076-4d5c-bf8f-3b7fae82df10


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T15:11:42.156-0700 7f14088101c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:11:42.156-0700 7f14088101c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:11:42.172-0700 7f7b5c53e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:11:42.172-0700 7f7b5c53e1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40681,v1:10.10.1.2:40682] --print /tmp/ceph_monmap.460372 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.460372 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.460372 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42681 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.5wQAtzZnJE 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new fb8b019f-d444-4b97-8b53-0cf43a1631b4 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBC6qJgtsu8FxAACYDgXy+hct3zF+yJX4bwHA== --osd-uuid fb8b019f-d444-4b97-8b53-0cf43a1631b4 
2021-05-17T15:12:18.765-0700 7f7b7f2b0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:12:18.765-0700 7f7b7f2b0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:12:18.765-0700 7f7b7f2b0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:12:18.821-0700 7f7b7f2b0f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8d1912d2-2277-4219-a64b-0733d0f5f81f -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T15:12:19.089-0700 7f6be42e2f00 -1 Falling back to public interface
2021-05-17T15:12:19.101-0700 7f6be42e2f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBD6qJgZ1NhBRAApT4TApBM/O7RW6+QU9fCiA== --osd-uuid 8d1912d2-2277-4219-a64b-0733d0f5f81f 
2021-05-17T15:12:19.417-0700 7f60f938df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:12:19.417-0700 7f60f938df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:12:19.417-0700 7f60f938df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:12:19.465-0700 7f60f938df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0ad1ff03-42cb-4fe5-a20a-e8ecc480a6ea -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T15:12:19.793-0700 7ff2d12edf00 -1 Falling back to public interface
2021-05-17T15:12:19.805-0700 7ff2d12edf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBD6qJgKRQ1LxAA/WGTZtWfGrvc8PyIm4qdAQ== --osd-uuid 0ad1ff03-42cb-4fe5-a20a-e8ecc480a6ea 
2021-05-17T15:12:20.121-0700 7fe33e3e3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:12:20.121-0700 7fe33e3e3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:12:20.121-0700 7fe33e3e3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:12:20.165-0700 7fe33e3e3f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T15:12:20.521-0700 7ffb903eef00 -1 Falling back to public interface
2021-05-17T15:12:20.533-0700 7ffb903eef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T15:12:24,500400762-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:12:24,509456735-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T15:12:24,593078029-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:12:24,600132873-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c9629167-076e-4162-aee1-28ff96428f65', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c9629167-076e-4162-aee1-28ff96428f65 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T15:12:27,280496292-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:12:27,287414853-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c9629167-076e-4162-aee1-28ff96428f65', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c9629167-076e-4162-aee1-28ff96428f65 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T15:12:30,041008145-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:12:30,047360539-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c9629167-076e-4162-aee1-28ff96428f65', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c9629167-076e-4162-aee1-28ff96428f65 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T15:12:32,960003863-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:12:32,966361582-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c9629167-076e-4162-aee1-28ff96428f65', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c9629167-076e-4162-aee1-28ff96428f65 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T15:12:38,569874631-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:12:38,576424848-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c9629167-076e-4162-aee1-28ff96428f65', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c9629167-076e-4162-aee1-28ff96428f65 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T15:12:41,913678653-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:12:41,920037978-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c9629167-076e-4162-aee1-28ff96428f65', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c9629167-076e-4162-aee1-28ff96428f65 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T15:12:45,347203547-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:12:45,353732791-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c9629167-076e-4162-aee1-28ff96428f65', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c9629167-076e-4162-aee1-28ff96428f65 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T15:12:49,091240305-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:12:49,097752266-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c9629167-076e-4162-aee1-28ff96428f65', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c9629167-076e-4162-aee1-28ff96428f65 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:12:52,421922771-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:12:52,428431777-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c9629167-076e-4162-aee1-28ff96428f65', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c9629167-076e-4162-aee1-28ff96428f65 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:12:55,648537814-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:12:55,655099959-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c9629167-076e-4162-aee1-28ff96428f65', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c9629167-076e-4162-aee1-28ff96428f65 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T15:12:58,970296310-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:12:58,976846903-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c9629167-076e-4162-aee1-28ff96428f65', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c9629167-076e-4162-aee1-28ff96428f65 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T15:13:01,711182114-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:13:01,717740028-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c9629167-076e-4162-aee1-28ff96428f65', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c9629167-076e-4162-aee1-28ff96428f65 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T15:13:04,435819485-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:13:27,398993420-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   202 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=====================.......] (remaining: 1s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:13:35,539426108-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:13:44,066084882-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:13:52,268926886-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:14:00,262181225-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:14:08,254039994-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:14:16,200046171-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:14:24,312148312-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:14:24,330510202-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:14:32,250795747-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:14:32,271608484-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:14:40,283231855-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:14:40,301594886-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:14:48,284783292-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:14:48,303278825-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:14:56,319536446-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:14:56,338105926-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:14:56,353423409-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:14:56,362130574-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:14:56,382315529-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1355707
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T15:14:56,398777151-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T15:14:56,439872074-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:14:56,446118566-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c9629167-076e-4162-aee1-28ff96428f65', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c9629167-076e-4162-aee1-28ff96428f65 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:14:58.028+0000 ffffa30ec010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:14:58.036+0000 ffffa30ec010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:14:58.036+0000 ffffa30ec010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:14:58.053312+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T22:14:58.053384+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T22:14:58.265375+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:14:58.265375+0000     0       0         0         0         0         0           -           0
2021-05-17T22:14:59.265630+0000     1     255       697       442   441.955       442    0.356052    0.373238
2021-05-17T22:15:00.266216+0000     2     255      1374      1119   559.307       677    0.390308     0.37191
2021-05-17T22:15:01.266451+0000     3     255      2085      1830   609.812       711    0.331697    0.371797
2021-05-17T22:15:02.266737+0000     4     255      2791      2536   633.808       706    0.329823     0.36908
2021-05-17T22:15:03.266939+0000     5     255      3471      3216   643.019       680    0.394216    0.368183
2021-05-17T22:15:04.267162+0000     6     255      4139      3884   647.157       668    0.361062    0.371252
2021-05-17T22:15:05.267458+0000     7     255      4774      4519   645.393       635    0.392336    0.375613
2021-05-17T22:15:06.267757+0000     8     255      5394      5139   642.196       620    0.388113    0.380105
2021-05-17T22:15:07.268031+0000     9     255      5980      5725   635.934       586    0.451645    0.385022
2021-05-17T22:15:08.268350+0000    10     255      6705      6450   644.818       725    0.323433    0.383894
2021-05-17T22:15:09.268593+0000    11     255      7434      7179   652.454       729    0.340779    0.379937
2021-05-17T22:15:10.268897+0000    12     255      8181      7926   660.315       747     0.34011    0.376319
2021-05-17T22:15:11.269101+0000    13     255      8974      8719   670.508       793    0.322615    0.371841
2021-05-17T22:15:12.269320+0000    14     255      9768      9513   679.316       794    0.331276    0.367566
2021-05-17T22:15:13.269624+0000    15     255     10589     10334   688.745       821    0.310898    0.363697
2021-05-17T22:15:14.269953+0000    16     255     11230     10975   685.748       641    0.397086    0.364648
2021-05-17T22:15:15.270284+0000    17     255     11907     11652    685.22       677    0.371193    0.365547
2021-05-17T22:15:16.270625+0000    18     255     12583     12328   684.695       676    0.385502    0.366326
2021-05-17T22:15:17.270959+0000    19     255     13178     12923   679.963       595    0.438373    0.368948
2021-05-17T22:15:18.271171+0000 min lat: 0.010122 max lat: 0.489494 avg lat: 0.367685
2021-05-17T22:15:18.271171+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:15:18.271171+0000    20      32     13833     13801   689.855       878   0.0188298    0.367685
2021-05-17T22:15:19.271523+0000 Total time run:         20.0512
Total writes made:      13833
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     689.883
Stddev Bandwidth:       95.5348
Max bandwidth (MB/sec): 878
Min bandwidth (MB/sec): 442
Average IOPS:           689
Stddev IOPS:            95.5348
Max IOPS:               878
Min IOPS:               442
Average Latency(s):     0.366998
Stddev Latency(s):      0.0453095
Max latency(s):         0.489494
Min latency(s):         0.010122

[1;32mlocalhost.localdomain	[2021-05-17T15:15:19,634997846-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1355707


[1;33mlocalhost.localdomain	[2021-05-17T15:15:19,643915361-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:15:42,825220867-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.83k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:15:42,843878277-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:15:50,803574441-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.83k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:15:50,826827718-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:15:58,667453077-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.83k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:15:58,686191967-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:16:06,794814390-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.83k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:16:06,813881221-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:16:14,965044072-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.83k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:16:14,983602871-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:16:14,998765048-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:16:15,007489706-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:16:15,027750935-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1359008
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T15:16:15,044487629-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T15:16:15,085197783-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:16:15,091678780-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c9629167-076e-4162-aee1-28ff96428f65', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c9629167-076e-4162-aee1-28ff96428f65 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.4E2umL:/tmp/ceph-asok.4E2umL -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:16:16.617+0000 ffffa8664010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:16:16.625+0000 ffffa8664010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:16:16.625+0000 ffffa8664010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:16:16.643105+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:16:16.643105+0000     0       0         0         0         0         0           -           0
2021-05-17T22:16:17.643516+0000     1     255       903       648   647.637       648    0.343155     0.30795
2021-05-17T22:16:18.643751+0000     2     255      1617      1362   680.729       714    0.350816    0.333497
2021-05-17T22:16:19.643988+0000     3     255      2371      2116   705.091       754    0.337497     0.33631
2021-05-17T22:16:20.644874+0000     4     256      3120      2864   715.657       748    0.349178    0.337386
2021-05-17T22:16:21.645531+0000     5     256      3840      3584   716.431       720    0.411034    0.337667
2021-05-17T22:16:22.645764+0000     6     255      4585      4330   721.329       746      0.3351    0.341205
2021-05-17T22:16:23.646010+0000     7     255      5319      5064   723.113       734    0.271898    0.341155
2021-05-17T22:16:24.646621+0000     8     255      6091      5836   729.166       772    0.242285     0.34017
2021-05-17T22:16:25.647211+0000     9     255      6829      6574   730.099       738    0.337154    0.341664
2021-05-17T22:16:26.647434+0000    10     255      7594      7339   733.571       765    0.337988    0.340814
2021-05-17T22:16:27.647794+0000    11     255      8343      8088   734.949       749    0.331371    0.340977
2021-05-17T22:16:28.648703+0000    12     255      9100      8845    736.73       757     0.31903    0.340348
2021-05-17T22:16:29.648925+0000    13     255      9844      9589   737.277       744    0.506817    0.340264
2021-05-17T22:16:30.649208+0000    14     255     10593     10338   738.099       749    0.276113    0.340294
2021-05-17T22:16:31.649432+0000    15     255     11338     11083   738.548       745    0.248144     0.34063
2021-05-17T22:16:32.649710+0000    16     255     12067     11812   737.938       729    0.256968     0.34113
2021-05-17T22:16:33.650661+0000    17     255     12812     12557   738.312       745    0.345232    0.341687
2021-05-17T22:16:34.650899+0000    18     255     13552     13297   738.396       740    0.332398    0.341928
2021-05-17T22:16:35.651184+0000 Total time run:       18.6228
Total reads made:     13833
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   742.801
Average IOPS:         742
Stddev IOPS:          26.7159
Max IOPS:             772
Min IOPS:             648
Average Latency(s):   0.339944
Max latency(s):       0.6321
Min latency(s):       0.0800573

[1;32mlocalhost.localdomain	[2021-05-17T15:16:36,036452255-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1359008


[1;33mlocalhost.localdomain	[2021-05-17T15:16:36,045773999-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:16:59,139249272-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.83k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:16:59,161622785-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:17:07,244096209-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.83k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:17:07,263321542-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:17:15,283545325-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.83k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:17:15,302040334-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:17:23,263643847-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.83k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:17:23,282953494-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:17:31,346303991-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.83k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:17:31,365522573-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:17:31,381058914-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T15:17:31,387325029-07:00][RUNNING][ROUND 3/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:17:31,396170383-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T15:17:31,414494092-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40522\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.461481\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 96fbe28a-8d2a-482f-936f-848aaca9b5fc\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 96fbe28a-8d2a-482f-936f-848aaca9b5fc\nlast_changed 2021-05-17T15:18:00.318041-0700\ncreated 2021-05-17T15:18:00.318041-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40522/0,v1:10.10.1.2:40523/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.461481 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 24cdb358-7537-4027-a4ea-eace022ca371\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 82f61d80-2442-428d-ab83-14a8dd95d9ce\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 01efb029-d2d7-4fd7-82f9-60b7772d676f\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42522\n  w/ user/pass: admin / d1b7a99d-98dd-425c-8582-0ae3871dd7d1\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:18:15 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40522
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.461481
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 96fbe28a-8d2a-482f-936f-848aaca9b5fc
setting min_mon_release = octopus
epoch 0
fsid 96fbe28a-8d2a-482f-936f-848aaca9b5fc
last_changed 2021-05-17T15:18:00.318041-0700
created 2021-05-17T15:18:00.318041-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40522/0,v1:10.10.1.2:40523/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.461481 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 24cdb358-7537-4027-a4ea-eace022ca371
0
start osd.0
add osd1 82f61d80-2442-428d-ab83-14a8dd95d9ce
1
start osd.1
add osd2 01efb029-d2d7-4fd7-82f9-60b7772d676f
2
start osd.2


restful urls: https://10.10.1.2:42522
  w/ user/pass: admin / d1b7a99d-98dd-425c-8582-0ae3871dd7d1


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T15:17:32.416-0700 7f0549feb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:17:32.416-0700 7f0549feb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:17:32.436-0700 7fbb2d2d71c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:17:32.436-0700 7fbb2d2d71c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40522,v1:10.10.1.2:40523] --print /tmp/ceph_monmap.461481 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.461481 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.461481 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42522 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.3NFIFOebky 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 24cdb358-7537-4027-a4ea-eace022ca371 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCh66JgQe+uCBAA4pZvbI2EKslasoXM/UBNcQ== --osd-uuid 24cdb358-7537-4027-a4ea-eace022ca371 
2021-05-17T15:18:09.468-0700 7fdd88b3af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:18:09.468-0700 7fdd88b3af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:18:09.468-0700 7fdd88b3af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:18:09.540-0700 7fdd88b3af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 82f61d80-2442-428d-ab83-14a8dd95d9ce -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T15:18:09.844-0700 7f126db27f00 -1 Falling back to public interface
2021-05-17T15:18:09.856-0700 7f126db27f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCh66JgT+pcMhAAT/PFiIEbFD8/ClGonY5zHA== --osd-uuid 82f61d80-2442-428d-ab83-14a8dd95d9ce 
2021-05-17T15:18:10.196-0700 7fd6fd785f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:18:10.196-0700 7fd6fd785f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:18:10.196-0700 7fd6fd785f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:18:10.240-0700 7fd6fd785f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 01efb029-d2d7-4fd7-82f9-60b7772d676f -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T15:18:10.540-0700 7fdfc23dff00 -1 Falling back to public interface
2021-05-17T15:18:10.552-0700 7fdfc23dff00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCi66Jgl0+iHxAA7S+9a6fEi90LACogiBfUYQ== --osd-uuid 01efb029-d2d7-4fd7-82f9-60b7772d676f 
2021-05-17T15:18:10.856-0700 7f8040895f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:18:10.856-0700 7f8040895f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:18:10.860-0700 7f8040895f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:18:10.912-0700 7f8040895f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T15:18:11.296-0700 7fbb880caf00 -1 Falling back to public interface
2021-05-17T15:18:11.312-0700 7fbb880caf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T15:18:15,235583121-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:18:15,244835089-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T15:18:15,329457920-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:18:15,336091177-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1c046e8d-1912-42a0-a504-5fc771975ec9', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1c046e8d-1912-42a0-a504-5fc771975ec9 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T15:18:18,306043066-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:18:18,312606472-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1c046e8d-1912-42a0-a504-5fc771975ec9', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1c046e8d-1912-42a0-a504-5fc771975ec9 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T15:18:21,250358839-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:18:21,256758252-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1c046e8d-1912-42a0-a504-5fc771975ec9', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1c046e8d-1912-42a0-a504-5fc771975ec9 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T15:18:24,002625961-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:18:24,009159497-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1c046e8d-1912-42a0-a504-5fc771975ec9', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1c046e8d-1912-42a0-a504-5fc771975ec9 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T15:18:29,596636998-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:18:29,603284313-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1c046e8d-1912-42a0-a504-5fc771975ec9', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1c046e8d-1912-42a0-a504-5fc771975ec9 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T15:18:33,449056873-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:18:33,457417074-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1c046e8d-1912-42a0-a504-5fc771975ec9', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1c046e8d-1912-42a0-a504-5fc771975ec9 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T15:18:36,863777867-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:18:36,871010941-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1c046e8d-1912-42a0-a504-5fc771975ec9', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1c046e8d-1912-42a0-a504-5fc771975ec9 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T15:18:40,019212424-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:18:40,029336370-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1c046e8d-1912-42a0-a504-5fc771975ec9', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1c046e8d-1912-42a0-a504-5fc771975ec9 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:18:43,962167840-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:18:43,968626018-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1c046e8d-1912-42a0-a504-5fc771975ec9', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1c046e8d-1912-42a0-a504-5fc771975ec9 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:18:48,631225370-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:18:48,637762237-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1c046e8d-1912-42a0-a504-5fc771975ec9', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1c046e8d-1912-42a0-a504-5fc771975ec9 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T15:18:51,798369833-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:18:51,804965284-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1c046e8d-1912-42a0-a504-5fc771975ec9', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1c046e8d-1912-42a0-a504-5fc771975ec9 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T15:18:54,484843847-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:18:54,491078532-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1c046e8d-1912-42a0-a504-5fc771975ec9', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1c046e8d-1912-42a0-a504-5fc771975ec9 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T15:18:57,152577157-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:19:20,154109123-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   206 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=============...............] (remaining: 5s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:19:28,066403101-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:19:36,333021919-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:19:44,342105557-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:19:52,322215189-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:20:00,276367017-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:20:08,269474526-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:20:16,278264051-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:20:16,297012543-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:20:24,302846924-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:20:24,321847055-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:20:32,409786539-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:20:32,428918587-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:20:40,616666425-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:20:40,635676442-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:20:48,635015178-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:20:48,653846650-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:20:48,669976178-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:20:48,679296993-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:20:48,699664366-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1372491
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T15:20:48,716602636-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-17T15:20:48,758548373-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:20:48,764975135-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1c046e8d-1912-42a0-a504-5fc771975ec9', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1c046e8d-1912-42a0-a504-5fc771975ec9 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:20:50.279+0000 ffffbaa49010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:20:50.287+0000 ffffbaa49010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:20:50.287+0000 ffffbaa49010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:20:50.305150+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T22:20:50.305222+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T22:20:50.529594+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:20:50.529594+0000     0       0         0         0         0         0           -           0
2021-05-17T22:20:51.529841+0000     1     255       678       423   422.959       423    0.376421    0.392242
2021-05-17T22:20:52.530040+0000     2     255      1337      1082    540.92       659    0.381296    0.387291
2021-05-17T22:20:53.530337+0000     3     255      1982      1727   575.553       645    0.408288    0.390583
2021-05-17T22:20:54.530609+0000     4     255      2614      2359   589.622       632    0.415132    0.393336
2021-05-17T22:20:55.530852+0000     5     255      3260      3005   600.867       646    0.419872     0.39348
2021-05-17T22:20:56.531149+0000     6     255      3903      3648   607.858       643    0.389095    0.395544
2021-05-17T22:20:57.531497+0000     7     255      4563      4308   615.274       660    0.390178    0.394474
2021-05-17T22:20:58.531787+0000     8     255      5215      4960   619.842       652    0.380189    0.394707
2021-05-17T22:20:59.532029+0000     9     255      5830      5575   619.287       615    0.407214    0.396308
2021-05-17T22:21:00.532400+0000    10     255      6458      6203   620.135       628     0.40161    0.397501
2021-05-17T22:21:01.532698+0000    11     255      7076      6821   619.924       618    0.391428    0.399178
2021-05-17T22:21:02.532965+0000    12     255      7695      7440   619.834       619    0.442219    0.399804
2021-05-17T22:21:03.533257+0000    13     255      8342      8087   621.909       647    0.387421    0.399611
2021-05-17T22:21:04.533503+0000    14     255      8979      8724   622.976       637    0.383925    0.400343
2021-05-17T22:21:05.533789+0000    15     255      9649      9394   626.098       670    0.356986    0.399517
2021-05-17T22:21:06.534061+0000    16     255     10338     10083   630.017       689    0.331774    0.397872
2021-05-17T22:21:07.534294+0000    17     255     11098     10843   637.653       760    0.329578    0.393415
2021-05-17T22:21:08.534528+0000    18     255     11817     11562   642.163       719    0.340383    0.391012
2021-05-17T22:21:09.535615+0000    19     255     12519     12264   645.274       702    0.344147    0.389398
2021-05-17T22:21:10.535829+0000 min lat: 0.0112144 max lat: 0.450728 avg lat: 0.386803
2021-05-17T22:21:10.535829+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:21:10.535829+0000    20      30     13152     13122     655.9       858   0.0112144    0.386803
2021-05-17T22:21:11.536098+0000 Total time run:         20.0616
Total writes made:      13152
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     655.58
Stddev Bandwidth:       79.4739
Max bandwidth (MB/sec): 858
Min bandwidth (MB/sec): 423
Average IOPS:           655
Stddev IOPS:            79.4739
Max IOPS:               858
Min IOPS:               423
Average Latency(s):     0.386101
Stddev Latency(s):      0.0390246
Max latency(s):         0.450728
Min latency(s):         0.0112144

[1;32mlocalhost.localdomain	[2021-05-17T15:21:11,903914371-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1372491


[1;33mlocalhost.localdomain	[2021-05-17T15:21:11,913335388-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:21:35,133036218-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.15k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:21:35,152241374-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:21:43,135903727-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.15k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:21:43,155321814-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:21:51,129107939-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.15k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:21:51,148133304-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:21:59,474068801-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.15k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:21:59,493492694-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:22:07,468152680-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.15k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:22:07,487461564-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:22:07,503257330-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:22:07,512116936-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:22:07,533111551-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1375824
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T15:22:07,550449655-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-17T15:22:07,590936109-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:22:07,597261035-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1c046e8d-1912-42a0-a504-5fc771975ec9', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1c046e8d-1912-42a0-a504-5fc771975ec9 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ginWyJ:/tmp/ceph-asok.ginWyJ -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:22:09.129+0000 ffffa3aaf010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:22:09.137+0000 ffffa3aaf010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:22:09.137+0000 ffffa3aaf010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:22:09.157284+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:22:09.157284+0000     0       7         7         0         0         0           -           0
2021-05-17T22:22:10.157483+0000     1     255       918       663   662.534       663    0.326329    0.303594
2021-05-17T22:22:11.157710+0000     2     255      1682      1427   713.168       764    0.327343    0.319796
2021-05-17T22:22:12.157945+0000     3     255      2434      2179   726.051       752    0.342157    0.325998
2021-05-17T22:22:13.158451+0000     4     255      3211      2956   738.691       777    0.324439    0.327525
2021-05-17T22:22:14.158812+0000     5     256      3956      3700   739.699       744    0.363134    0.328727
2021-05-17T22:22:15.159148+0000     6     255      4691      4436   739.042       736    0.340403    0.332961
2021-05-17T22:22:16.159644+0000     7     255      5434      5179   739.555       743    0.334725    0.334836
2021-05-17T22:22:17.159858+0000     8     255      6222      5967   745.588       788    0.323219    0.333688
2021-05-17T22:22:18.160099+0000     9     255      6961      6706   744.836       739    0.340198    0.334745
2021-05-17T22:22:19.160328+0000    10     255      7720      7465   746.235       759    0.335215    0.334956
2021-05-17T22:22:20.160878+0000    11     255      8483      8228   747.721       763    0.333064    0.335072
2021-05-17T22:22:21.161091+0000    12     255      9242      8987   748.648       759    0.345352    0.334914
2021-05-17T22:22:22.161710+0000    13     255      9981      9726    747.87       739    0.344477    0.335955
2021-05-17T22:22:23.161986+0000    14     255     10713     10458   746.722       732    0.341679    0.336851
2021-05-17T22:22:24.162201+0000    15     255     11465     11210   747.063       752    0.333271    0.337159
2021-05-17T22:22:25.163102+0000    16     255     12196     11941   746.018       731    0.350732    0.337821
2021-05-17T22:22:26.163366+0000    17     255     12959     12704   747.005       763    0.331175    0.337805
2021-05-17T22:22:27.163658+0000 Total time run:       17.4396
Total reads made:     13152
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   754.145
Average IOPS:         754
Stddev IOPS:          26.8439
Max IOPS:             788
Min IOPS:             663
Average Latency(s):   0.335712
Max latency(s):       0.377854
Min latency(s):       0.105778

[1;32mlocalhost.localdomain	[2021-05-17T15:22:27,547712705-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1375824


[1;33mlocalhost.localdomain	[2021-05-17T15:22:27,557292164-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:22:50,551878080-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.15k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:22:50,571374255-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:22:58,561891555-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.15k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:22:58,582500860-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:23:06,580711267-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.15k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:23:06,599917368-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:23:14,455163058-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.15k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:23:14,474850831-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:23:22,581389505-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.15k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:23:22,600732144-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:23:22,616555995-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T15:23:22,622429854-07:00][RUNNING][ROUND 4/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:23:22,631416682-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T15:23:22,649397600-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40893\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.464796\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6b54ccaf-71cf-45cd-a092-09a7b5ef113c\nsetting min_mon_release = octopus\nepoch 0\nfsid 6b54ccaf-71cf-45cd-a092-09a7b5ef113c\nlast_changed 2021-05-17T15:23:50.684925-0700\ncreated 2021-05-17T15:23:50.684925-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40893/0,v1:10.10.1.2:40894/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.464796 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 227be7c6-239f-4105-a532-79aa675dfde4\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 4067903b-4e3c-4f50-9765-e97659644eb1\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 43739b27-6024-46d7-b9bd-548650724ad9\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42893\n  w/ user/pass: admin / c80ff586-8f35-49a0-b843-772ba4e89524\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:24:05 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40893
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.464796
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6b54ccaf-71cf-45cd-a092-09a7b5ef113c
setting min_mon_release = octopus
epoch 0
fsid 6b54ccaf-71cf-45cd-a092-09a7b5ef113c
last_changed 2021-05-17T15:23:50.684925-0700
created 2021-05-17T15:23:50.684925-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40893/0,v1:10.10.1.2:40894/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.464796 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 227be7c6-239f-4105-a532-79aa675dfde4
0
start osd.0
add osd1 4067903b-4e3c-4f50-9765-e97659644eb1
1
start osd.1
add osd2 43739b27-6024-46d7-b9bd-548650724ad9
2
start osd.2


restful urls: https://10.10.1.2:42893
  w/ user/pass: admin / c80ff586-8f35-49a0-b843-772ba4e89524


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T15:23:23.639-0700 7f2f3e1501c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:23:23.639-0700 7f2f3e1501c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:23:23.655-0700 7f9d1a6c21c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:23:23.655-0700 7f9d1a6c21c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40893,v1:10.10.1.2:40894] --print /tmp/ceph_monmap.464796 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.464796 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.464796 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42893 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.gLriCo36zj 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 227be7c6-239f-4105-a532-79aa675dfde4 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD/7KJgG8JZIRAAJ7vVkoJcKx/+oeevAIhgng== --osd-uuid 227be7c6-239f-4105-a532-79aa675dfde4 
2021-05-17T15:23:59.903-0700 7f43da431f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:23:59.903-0700 7f43da431f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:23:59.903-0700 7f43da431f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:23:59.983-0700 7f43da431f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4067903b-4e3c-4f50-9765-e97659644eb1 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T15:24:00.267-0700 7f6c8c320f00 -1 Falling back to public interface
2021-05-17T15:24:00.279-0700 7f6c8c320f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAA7aJg9gf4DxAA2lLM+0HYbWZDNXdGVhkcyg== --osd-uuid 4067903b-4e3c-4f50-9765-e97659644eb1 
2021-05-17T15:24:00.623-0700 7f1eda045f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:24:00.623-0700 7f1eda045f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:24:00.623-0700 7f1eda045f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:24:00.667-0700 7f1eda045f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 43739b27-6024-46d7-b9bd-548650724ad9 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T15:24:00.947-0700 7f66ad2a0f00 -1 Falling back to public interface
2021-05-17T15:24:00.959-0700 7f66ad2a0f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAA7aJgomdXOBAARVqT7vRO9loH10y4ubn3Vg== --osd-uuid 43739b27-6024-46d7-b9bd-548650724ad9 
2021-05-17T15:24:01.283-0700 7f5ffc39df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:24:01.283-0700 7f5ffc39df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:24:01.283-0700 7f5ffc39df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:24:01.347-0700 7f5ffc39df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T15:24:01.779-0700 7f149b6d6f00 -1 Falling back to public interface
2021-05-17T15:24:01.795-0700 7f149b6d6f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T15:24:05,597596398-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:24:05,608384807-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T15:24:05,706292253-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:24:05,713514811-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fe2fcae-e673-4532-9efe-756b018ebf4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fe2fcae-e673-4532-9efe-756b018ebf4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T15:24:08,397054903-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:24:08,403320988-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fe2fcae-e673-4532-9efe-756b018ebf4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fe2fcae-e673-4532-9efe-756b018ebf4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T15:24:11,468831815-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:24:11,475481861-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fe2fcae-e673-4532-9efe-756b018ebf4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fe2fcae-e673-4532-9efe-756b018ebf4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T15:24:14,379369648-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:24:14,385860352-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fe2fcae-e673-4532-9efe-756b018ebf4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fe2fcae-e673-4532-9efe-756b018ebf4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T15:24:19,910509864-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:24:19,917205142-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fe2fcae-e673-4532-9efe-756b018ebf4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fe2fcae-e673-4532-9efe-756b018ebf4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T15:24:23,635508011-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:24:23,642090365-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fe2fcae-e673-4532-9efe-756b018ebf4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fe2fcae-e673-4532-9efe-756b018ebf4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T15:24:26,928137939-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:24:26,934786605-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fe2fcae-e673-4532-9efe-756b018ebf4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fe2fcae-e673-4532-9efe-756b018ebf4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T15:24:30,300458346-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:24:30,307059829-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fe2fcae-e673-4532-9efe-756b018ebf4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fe2fcae-e673-4532-9efe-756b018ebf4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:24:33,692111553-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:24:33,698606202-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fe2fcae-e673-4532-9efe-756b018ebf4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fe2fcae-e673-4532-9efe-756b018ebf4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:24:37,032071865-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:24:37,038489506-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fe2fcae-e673-4532-9efe-756b018ebf4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fe2fcae-e673-4532-9efe-756b018ebf4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T15:24:40,241469968-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:24:40,248030749-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fe2fcae-e673-4532-9efe-756b018ebf4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fe2fcae-e673-4532-9efe-756b018ebf4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T15:24:43,129661481-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:24:43,135837117-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fe2fcae-e673-4532-9efe-756b018ebf4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fe2fcae-e673-4532-9efe-756b018ebf4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T15:24:45,827119318-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:25:08,779818000-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:25:17,025003521-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:25:25,088939339-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 27s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:25:32,968701217-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:25:40,920387060-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [=========...................] (remaining: 51s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:25:49,135775309-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 71s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:25:57,131290324-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 81s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:26:05,136914922-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 40s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:26:13,154267785-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:26:13,173258525-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:26:21,339447510-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:26:21,358423922-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:26:29,542397899-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:26:29,561670492-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:26:37,511071058-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:26:37,532433732-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:26:45,627936579-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:26:45,647275213-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:26:45,662974768-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:26:45,672065909-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:26:45,692531040-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1389650
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T15:26:45,709739117-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-17T15:26:45,751745872-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:26:45,758355141-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fe2fcae-e673-4532-9efe-756b018ebf4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fe2fcae-e673-4532-9efe-756b018ebf4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:26:47.280+0000 ffff91322010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:26:47.288+0000 ffff91322010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:26:47.288+0000 ffff91322010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:26:47.304840+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T22:26:47.304913+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T22:26:47.523245+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:26:47.523245+0000     0       0         0         0         0         0           -           0
2021-05-17T22:26:48.523480+0000     1     255       678       423   422.967       423    0.345126    0.391893
2021-05-17T22:26:49.523682+0000     2     255      1433      1178   588.918       755    0.336953    0.356362
2021-05-17T22:26:50.523893+0000     3     255      2212      1957   652.227       779    0.307958    0.347711
2021-05-17T22:26:51.524128+0000     4     255      2983      2728   681.876       771    0.318861    0.343364
2021-05-17T22:26:52.524436+0000     5     255      3753      3498   699.455       770    0.314254    0.340985
2021-05-17T22:26:53.524693+0000     6     255      4506      4251   708.348       753    0.364747    0.338707
2021-05-17T22:26:54.525378+0000     7     255      5202      4947   706.515       696    0.351622    0.343612
2021-05-17T22:26:55.525600+0000     8     255      5863      5608   700.808       661    0.347255    0.348923
2021-05-17T22:26:56.525832+0000     9     255      6583      6328   702.921       720    0.372489     0.34918
2021-05-17T22:26:57.526054+0000    10     255      7268      7013   701.114       685    0.375335    0.351661
2021-05-17T22:26:58.526326+0000    11     255      7939      7684    698.36       671    0.383483    0.354395
2021-05-17T22:26:59.526603+0000    12     255      8663      8408    700.48       724    0.340108    0.354625
2021-05-17T22:27:00.527072+0000    13     255      9417      9162    704.57       754    0.329283    0.353626
2021-05-17T22:27:01.527520+0000    14     255     10164      9909   707.578       747    0.323453     0.35267
2021-05-17T22:27:02.527744+0000    15     255     10861     10606   706.862       697    0.355907    0.353398
2021-05-17T22:27:03.528252+0000    16     255     11535     11280   704.786       674    0.360674    0.355137
2021-05-17T22:27:04.528784+0000    17     255     12273     12018   706.717       738    0.350396    0.354602
2021-05-17T22:27:05.529060+0000    18     255     12888     12633   701.613       615    0.425196    0.356314
2021-05-17T22:27:06.529438+0000    19     255     13579     13324    701.04       691    0.368746     0.35815
2021-05-17T22:27:07.529754+0000 min lat: 0.297764 max lat: 0.464166 avg lat: 0.357813
2021-05-17T22:27:07.529754+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:27:07.529754+0000    20     255     14309     14054   702.477       730    0.347904    0.357813
2021-05-17T22:27:08.530109+0000 Total time run:         20.0348
Total writes made:      14310
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     714.258
Stddev Bandwidth:       78.686
Max bandwidth (MB/sec): 779
Min bandwidth (MB/sec): 423
Average IOPS:           714
Stddev IOPS:            78.686
Max IOPS:               779
Min IOPS:               423
Average Latency(s):     0.35501
Stddev Latency(s):      0.0375116
Max latency(s):         0.464166
Min latency(s):         0.0330403

[1;32mlocalhost.localdomain	[2021-05-17T15:27:09,056888278-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1389650


[1;33mlocalhost.localdomain	[2021-05-17T15:27:09,066475508-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:27:32,038676147-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.31k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:27:32,058274430-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:27:40,141887272-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.31k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:27:40,161376202-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:27:48,128929894-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.31k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:27:48,148697676-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:27:56,130457440-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.31k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:27:56,149779422-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:28:04,430398904-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.31k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:28:04,449703602-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:28:04,465380609-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:28:04,474745442-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:28:04,495983054-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1392941
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T15:28:04,512965598-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-17T15:28:04,554252890-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:28:04,560671372-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fe2fcae-e673-4532-9efe-756b018ebf4c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fe2fcae-e673-4532-9efe-756b018ebf4c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.M8PolZ:/tmp/ceph-asok.M8PolZ -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:28:06.130+0000 ffff98de2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:28:06.134+0000 ffff98de2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:28:06.134+0000 ffff98de2010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:28:06.153848+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:28:06.153848+0000     0       0         0         0         0         0           -           0
2021-05-17T22:28:07.154048+0000     1     255       871       616     615.8       616    0.363451    0.316634
2021-05-17T22:28:08.154281+0000     2     255      1637      1382   690.807       766    0.325545    0.328975
2021-05-17T22:28:09.154828+0000     3     255      2420      2165   721.401       783    0.238399    0.325401
2021-05-17T22:28:10.155291+0000     4     255      3181      2926   731.213       761    0.251132    0.328623
2021-05-17T22:28:11.155502+0000     5     255      3959      3704   740.536       778    0.246625     0.32795
2021-05-17T22:28:12.155734+0000     6     255      4716      4461   743.251       757    0.284335    0.329636
2021-05-17T22:28:13.157366+0000     7     255      5472      5217   744.898       756    0.344913    0.331709
2021-05-17T22:28:14.157577+0000     8     255      6233      5978    746.89       761    0.335744    0.332396
2021-05-17T22:28:15.157792+0000     9     255      6988      6733   747.773       755    0.339015    0.333222
2021-05-17T22:28:16.159535+0000    10     255      7758      7503   749.864       770    0.329246    0.333214
2021-05-17T22:28:17.159756+0000    11     255      8491      8236   748.317       733    0.337739    0.334552
2021-05-17T22:28:18.160101+0000    12     255      9258      9003   749.851       767    0.335744    0.334488
2021-05-17T22:28:19.160360+0000    13     255     10024      9769   751.078       766    0.337418    0.334338
2021-05-17T22:28:20.160979+0000    14     255     10774     10519   750.968       750    0.354013    0.334538
2021-05-17T22:28:21.161437+0000    15     255     11534     11279   751.547       760    0.326454    0.335073
2021-05-17T22:28:22.161891+0000    16     255     12299     12044   752.366       765    0.338792    0.334866
2021-05-17T22:28:23.162129+0000    17     255     13076     12821   753.804       777     0.32546    0.334662
2021-05-17T22:28:24.162366+0000    18     255     13830     13575   753.805       754    0.348343    0.334693
2021-05-17T22:28:25.162633+0000 Total time run:       18.873
Total reads made:     14310
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   758.228
Average IOPS:         758
Stddev IOPS:          36.3225
Max IOPS:             783
Min IOPS:             616
Average Latency(s):   0.333291
Max latency(s):       0.604329
Min latency(s):       0.0833361

[1;32mlocalhost.localdomain	[2021-05-17T15:28:25,528220454-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1392941


[1;33mlocalhost.localdomain	[2021-05-17T15:28:25,537981040-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:28:48,524638392-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.31k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:28:48,543968325-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:28:56,539896823-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.31k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:28:56,559443543-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:29:06,593197363-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.31k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:29:06,612575349-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:29:14,691836058-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.31k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:29:14,712690025-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:29:22,665218362-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.31k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:29:22,684523420-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:29:22,700403112-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T15:29:22,706443364-07:00][RUNNING][ROUND 5/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:29:22,715604342-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T15:29:22,733589506-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40925\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.467331\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 41c3109b-55c0-4087-8080-cf2aa0fe8f84\nsetting min_mon_release = octopus\nepoch 0\nfsid 41c3109b-55c0-4087-8080-cf2aa0fe8f84\nlast_changed 2021-05-17T15:29:51.685292-0700\ncreated 2021-05-17T15:29:51.685292-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40925/0,v1:10.10.1.2:40926/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.467331 (1 monitors)\n'
[1] 15:30:02 [FAILURE] ljishen@10.10.1.2 Exited with error code 1
ip 10.10.1.2
port 40925
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.467331
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 41c3109b-55c0-4087-8080-cf2aa0fe8f84
setting min_mon_release = octopus
epoch 0
fsid 41c3109b-55c0-4087-8080-cf2aa0fe8f84
last_changed 2021-05-17T15:29:51.685292-0700
created 2021-05-17T15:29:51.685292-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40925/0,v1:10.10.1.2:40926/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.467331 (1 monitors)
Stderr: 2021-05-17T15:29:23.726-0700 7f4aa2f591c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:29:23.726-0700 7f4aa2f591c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:29:23.742-0700 7fadaeb8c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:29:23.742-0700 7fadaeb8c1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40925,v1:10.10.1.2:40926] --print /tmp/ceph_monmap.467331 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.467331 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.467331 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T15:29:52.442-0700 7f5c55258580 -1  Processor -- bind unable to bind to v1:10.10.1.2:40926/0: (98) Address already in use
2021-05-17T15:29:52.442-0700 7f5c55258580 -1  Processor -- bind was unable to bind. Trying again in 5 seconds 
2021-05-17T15:29:57.442-0700 7f5c55258580 -1  Processor -- bind unable to bind to v1:10.10.1.2:40926/0: (98) Address already in use
2021-05-17T15:29:57.442-0700 7f5c55258580 -1  Processor -- bind was unable to bind. Trying again in 5 seconds 
2021-05-17T15:30:02.442-0700 7f5c55258580 -1  Processor -- bind unable to bind to v1:10.10.1.2:40926/0: (98) Address already in use
2021-05-17T15:30:02.442-0700 7f5c55258580 -1  Processor -- bind was unable to bind after 3 attempts: (98) Address already in use
2021-05-17T15:30:02.442-0700 7f5c55258580 -1 unable to bind monitor to [v2:10.10.1.2:40925/0,v1:10.10.1.2:40926/0]


[1;7;39;49m[2021-05-17T15:35:33,901091066-07:00][RUNNING][ROUND 1/1/20] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:35:33,906051345-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T15:35:33,921917639-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40804\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.469636\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c61568ea-bc5e-4280-957f-b1262ba58a29\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid c61568ea-bc5e-4280-957f-b1262ba58a29\nlast_changed 2021-05-17T15:35:36.984846-0700\ncreated 2021-05-17T15:35:36.984846-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40804/0,v1:10.10.1.2:40805/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.469636 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 ff9ed083-e817-416f-a6ca-ec75a07129ec\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 c1868872-c249-4e38-9d34-7146d9c0a227\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 6d64dc29-8ae3-47fa-9ee1-cf9af9d5561f\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42804\n  w/ user/pass: admin / 58764124-5d05-428b-8d73-fad31064e1e8\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:35:52 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40804
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.469636
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c61568ea-bc5e-4280-957f-b1262ba58a29
setting min_mon_release = octopus
epoch 0
fsid c61568ea-bc5e-4280-957f-b1262ba58a29
last_changed 2021-05-17T15:35:36.984846-0700
created 2021-05-17T15:35:36.984846-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40804/0,v1:10.10.1.2:40805/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.469636 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 ff9ed083-e817-416f-a6ca-ec75a07129ec
0
start osd.0
add osd1 c1868872-c249-4e38-9d34-7146d9c0a227
1
start osd.1
add osd2 6d64dc29-8ae3-47fa-9ee1-cf9af9d5561f
2
start osd.2


restful urls: https://10.10.1.2:42804
  w/ user/pass: admin / 58764124-5d05-428b-8d73-fad31064e1e8


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T15:35:35.294-0700 7fbfac3ee1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:35:35.294-0700 7fbfac3ee1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:35:35.314-0700 7f64a17881c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:35:35.314-0700 7f64a17881c0 -1 WARNING: all dangerous and experimental features are enabled.
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40804,v1:10.10.1.2:40805] --print /tmp/ceph_monmap.469636 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.469636 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.469636 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42804 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.54zwoKvlPo 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ff9ed083-e817-416f-a6ca-ec75a07129ec -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDC76JgmgxyKxAAnE6eDSyySLfLUyxYJKLp6w== --osd-uuid ff9ed083-e817-416f-a6ca-ec75a07129ec 
2021-05-17T15:35:47.098-0700 7f3f2ea18f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:35:47.098-0700 7f3f2ea18f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:35:47.098-0700 7f3f2ea18f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:35:47.142-0700 7f3f2ea18f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c1868872-c249-4e38-9d34-7146d9c0a227 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T15:35:47.438-0700 7f99a0282f00 -1 Falling back to public interface
2021-05-17T15:35:47.450-0700 7f99a0282f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDD76JgrAoJGhAAThLVDtYO8a0qJIOtD1TI0A== --osd-uuid c1868872-c249-4e38-9d34-7146d9c0a227 
2021-05-17T15:35:47.758-0700 7f41dad67f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:35:47.758-0700 7f41dad67f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:35:47.758-0700 7f41dad67f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:35:47.830-0700 7f41dad67f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6d64dc29-8ae3-47fa-9ee1-cf9af9d5561f -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T15:35:48.266-0700 7fea76952f00 -1 Falling back to public interface
2021-05-17T15:35:48.278-0700 7fea76952f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDE76JgiWzIDxAAMqHImTOX/zBojBnPF+ewhA== --osd-uuid 6d64dc29-8ae3-47fa-9ee1-cf9af9d5561f 
2021-05-17T15:35:48.618-0700 7f4c5d3b2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:35:48.618-0700 7f4c5d3b2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:35:48.618-0700 7f4c5d3b2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:35:48.674-0700 7f4c5d3b2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T15:35:48.962-0700 7fbcc908bf00 -1 Falling back to public interface
2021-05-17T15:35:48.974-0700 7fbcc908bf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T15:35:52,885221270-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:35:52,890268683-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T15:35:52,973073020-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:35:52,979837265-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94b47616-6ec0-464b-b5f6-bfab774d659d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94b47616-6ec0-464b-b5f6-bfab774d659d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T15:35:55,921042382-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:35:55,927647301-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94b47616-6ec0-464b-b5f6-bfab774d659d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94b47616-6ec0-464b-b5f6-bfab774d659d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T15:35:58,863219455-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:35:58,869846098-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94b47616-6ec0-464b-b5f6-bfab774d659d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94b47616-6ec0-464b-b5f6-bfab774d659d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T15:36:01,833057094-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:36:01,839483122-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94b47616-6ec0-464b-b5f6-bfab774d659d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94b47616-6ec0-464b-b5f6-bfab774d659d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T15:36:07,464299343-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:36:07,471100975-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94b47616-6ec0-464b-b5f6-bfab774d659d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94b47616-6ec0-464b-b5f6-bfab774d659d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T15:36:10,781104007-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:36:10,787500285-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94b47616-6ec0-464b-b5f6-bfab774d659d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94b47616-6ec0-464b-b5f6-bfab774d659d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T15:36:14,334917925-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:36:14,341367523-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94b47616-6ec0-464b-b5f6-bfab774d659d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94b47616-6ec0-464b-b5f6-bfab774d659d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T15:36:17,567264987-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:36:17,573679104-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94b47616-6ec0-464b-b5f6-bfab774d659d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94b47616-6ec0-464b-b5f6-bfab774d659d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:36:20,864438828-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:36:20,871129276-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94b47616-6ec0-464b-b5f6-bfab774d659d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94b47616-6ec0-464b-b5f6-bfab774d659d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:36:24,480657108-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:36:24,487041099-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94b47616-6ec0-464b-b5f6-bfab774d659d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94b47616-6ec0-464b-b5f6-bfab774d659d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T15:36:27,615958603-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:36:27,622296371-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94b47616-6ec0-464b-b5f6-bfab774d659d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94b47616-6ec0-464b-b5f6-bfab774d659d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T15:36:30,363235888-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:36:30,370483456-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94b47616-6ec0-464b-b5f6-bfab774d659d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94b47616-6ec0-464b-b5f6-bfab774d659d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T15:36:33,042399666-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:36:56,050169064-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   198 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:37:03,988651828-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:37:11,967640210-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 27s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:37:20,170408583-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 52s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:37:28,285334509-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 62s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:37:36,279360164-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 80s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:37:44,279307824-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 92s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:37:52,182341144-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 38s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:38:00,327147195-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:38:00,338819210-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:38:08,369343247-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:38:08,382306157-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:38:16,226110681-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:38:16,237559684-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:38:24,328111635-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:38:24,339490666-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:38:32,283049353-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:38:32,294398679-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:38:32,303109326-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:38:32,308349882-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:38:32,320273116-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1416443
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T15:38:32,330001401-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T15:38:32,369141814-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:38:32,375691553-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94b47616-6ec0-464b-b5f6-bfab774d659d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94b47616-6ec0-464b-b5f6-bfab774d659d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:38:33.920+0000 ffff916bb010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:38:33.924+0000 ffff916bb010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:38:33.928+0000 ffff916bb010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:38:33.944516+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T22:38:33.944586+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-17T22:38:34.158129+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:38:34.158129+0000     0       0         0         0         0         0           -           0
2021-05-17T22:38:35.158372+0000     1     255       714       459   458.959       459    0.360815    0.361331
2021-05-17T22:38:36.158773+0000     2     255      1311      1056   527.871       597    0.409116    0.392815
2021-05-17T22:38:37.159052+0000     3     255      1961      1706   568.521       650    0.381713    0.396647
2021-05-17T22:38:38.159697+0000     4     255      2638      2383   595.539       677    0.368681    0.391136
2021-05-17T22:38:39.160488+0000     5     255      3296      3041   607.932       658    0.399373    0.390078
2021-05-17T22:38:40.161167+0000     6     255      3958      3703    616.87       662    0.361914    0.390338
2021-05-17T22:38:41.161467+0000     7     255      4601      4346   620.575       643    0.393987    0.390608
2021-05-17T22:38:42.162044+0000     8     255      5272      5017    626.83       671    0.366158    0.390318
2021-05-17T22:38:43.162654+0000     9     255      5925      5670   629.694       653    0.370998    0.390031
2021-05-17T22:38:44.162971+0000    10     255      6614      6359   635.602       689    0.386971    0.388095
2021-05-17T22:38:45.163968+0000    11     255      7266      7011   637.034       652    0.376805     0.38873
2021-05-17T22:38:46.164327+0000    12     255      7913      7658   637.845       647    0.400995    0.388866
2021-05-17T22:38:47.164597+0000    13     255      8505      8250   634.307       592    0.387632     0.39205
2021-05-17T22:38:48.164819+0000    14     255      9141      8886   634.418       636     0.39709    0.392876
2021-05-17T22:38:49.165073+0000    15     255      9800      9545   636.045       659     0.39397    0.392504
2021-05-17T22:38:50.165447+0000    16     255     10431     10176   635.715       631    0.396046    0.393452
2021-05-17T22:38:51.165799+0000    17     255     11104     10849   637.894       673    0.391862    0.392654
2021-05-17T22:38:52.166452+0000    18     255     11835     11580   643.041       731    0.343706    0.390566
2021-05-17T22:38:53.166797+0000    19     255     12467     12212   642.449       632    0.424373    0.390476
2021-05-17T22:38:54.167058+0000 min lat: 0.0277 max lat: 0.499147 avg lat: 0.387627
2021-05-17T22:38:54.167058+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:38:54.167058+0000    20      32     13125     13093   654.363       881      0.0277    0.387627
2021-05-17T22:38:55.167487+0000 Total time run:         20.0542
Total writes made:      13125
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     654.476
Stddev Bandwidth:       74.905
Max bandwidth (MB/sec): 881
Min bandwidth (MB/sec): 459
Average IOPS:           654
Stddev IOPS:            74.905
Max IOPS:               881
Min IOPS:               459
Average Latency(s):     0.38683
Stddev Latency(s):      0.0404875
Max latency(s):         0.499147
Min latency(s):         0.0237815

[1;32mlocalhost.localdomain	[2021-05-17T15:38:55,509670071-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1416443


[1;33mlocalhost.localdomain	[2021-05-17T15:38:55,515224946-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:39:18,738672112-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.13k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:39:18,750387576-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:39:26,658102370-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.13k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:39:26,669766779-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:39:34,628589825-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.13k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:39:34,640453498-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:39:42,851800269-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.13k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:39:42,863460118-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:39:50,878399523-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.13k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:39:50,889885071-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:39:50,898471647-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:39:50,903573370-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:39:50,915717921-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1419756
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T15:39:50,927549804-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-17T15:39:50,966083289-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:39:50,972501426-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94b47616-6ec0-464b-b5f6-bfab774d659d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94b47616-6ec0-464b-b5f6-bfab774d659d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8FAhaG:/tmp/ceph-asok.8FAhaG -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:39:52.442+0000 ffff91584010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:39:52.450+0000 ffff91584010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:39:52.450+0000 ffff91584010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:39:52.470104+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:39:52.470104+0000     0       0         0         0         0         0           -           0
2021-05-17T22:39:53.470909+0000     1     255       919       664   663.374       664    0.334141    0.300682
2021-05-17T22:39:54.471158+0000     2     255      1671      1416   707.578       752     0.33865    0.321579
2021-05-17T22:39:55.471370+0000     3     255      2461      2206   734.989       790    0.320289     0.32335
2021-05-17T22:39:56.471592+0000     4     255      3262      3007   751.444       801    0.323849    0.322066
2021-05-17T22:39:57.472090+0000     5     255      4034      3779   755.479       772    0.345319    0.322824
2021-05-17T22:39:58.472364+0000     6     255      4759      4504   750.367       725    0.369402    0.327715
2021-05-17T22:39:59.473839+0000     7     255      5522      5267   752.012       763    0.335355     0.32927
2021-05-17T22:40:00.474527+0000     8     255      6311      6056   756.569       789    0.221299    0.327434
2021-05-17T22:40:01.475099+0000     9     255      7064      6809   756.124       753    0.530252    0.329071
2021-05-17T22:40:02.475367+0000    10     255      7808      7553   754.892       744    0.344246    0.331032
2021-05-17T22:40:03.475579+0000    11     255      8567      8312   755.251       759     0.32782    0.331806
2021-05-17T22:40:04.476010+0000    12     255      9331      9076   755.953       764    0.333691    0.331963
2021-05-17T22:40:05.476594+0000    13     255     10081      9826   755.461       750    0.332179    0.332691
2021-05-17T22:40:06.476810+0000    14     255     10828     10573   754.845       747    0.345418    0.333239
2021-05-17T22:40:07.477705+0000    15     255     11598     11343    755.81       770    0.325943    0.333316
2021-05-17T22:40:08.478087+0000    16     255     12338     12083   754.804       740    0.354738    0.333806
2021-05-17T22:40:09.478798+0000    17     255     13107     12852   755.607       769    0.337735    0.333872
2021-05-17T22:40:10.479072+0000 Total time run:       17.2065
Total reads made:     13125
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   762.793
Average IOPS:         762
Stddev IOPS:          30.5982
Max IOPS:             801
Min IOPS:             664
Average Latency(s):   0.331767
Max latency(s):       0.623723
Min latency(s):       0.0856412

[1;32mlocalhost.localdomain	[2021-05-17T15:40:10,873052404-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1419756


[1;33mlocalhost.localdomain	[2021-05-17T15:40:10,880218118-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:40:33,847495038-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.13k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:40:33,861501596-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:40:41,827373033-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.13k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:40:41,843733564-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:40:49,839031572-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.13k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:40:49,853254557-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:40:57,929648681-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.13k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:40:57,943705168-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:41:05,991858740-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 13.13k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:41:06,005996076-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:41:06,016930627-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T15:41:06,020776061-07:00][RUNNING][ROUND 2/1/20] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:41:06,027129798-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T15:41:06,043453378-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40009\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.473307\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f7abe917-1811-45e6-9ca6-c4402bb75717\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid f7abe917-1811-45e6-9ca6-c4402bb75717\nlast_changed 2021-05-17T15:41:34.390934-0700\ncreated 2021-05-17T15:41:34.390934-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40009/0,v1:10.10.1.2:40010/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.473307 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 3fddb18e-2f27-4d2f-ba69-b7f1557969ed\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 f2ffa3f3-34cf-4dbd-8761-518f80926d9b\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 ea89cbf3-6d5d-456f-bbe3-776216a2536d\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42009\n  w/ user/pass: admin / 015d2932-162d-4f49-850e-aaedae6d03ca\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:41:49 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40009
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.473307
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f7abe917-1811-45e6-9ca6-c4402bb75717
setting min_mon_release = octopus
epoch 0
fsid f7abe917-1811-45e6-9ca6-c4402bb75717
last_changed 2021-05-17T15:41:34.390934-0700
created 2021-05-17T15:41:34.390934-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40009/0,v1:10.10.1.2:40010/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.473307 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 3fddb18e-2f27-4d2f-ba69-b7f1557969ed
0
start osd.0
add osd1 f2ffa3f3-34cf-4dbd-8761-518f80926d9b
1
start osd.1
add osd2 ea89cbf3-6d5d-456f-bbe3-776216a2536d
2
start osd.2


restful urls: https://10.10.1.2:42009
  w/ user/pass: admin / 015d2932-162d-4f49-850e-aaedae6d03ca


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T15:41:07.049-0700 7f54966cb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:41:07.049-0700 7f54966cb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:41:07.065-0700 7fa0fda041c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:41:07.065-0700 7fa0fda041c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40009,v1:10.10.1.2:40010] --print /tmp/ceph_monmap.473307 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.473307 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.473307 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42009 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.hry6etmnAZ 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3fddb18e-2f27-4d2f-ba69-b7f1557969ed -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAn8aJgTmH+DRAA92HQDZ3AjIgnGTMAS8ZeqQ== --osd-uuid 3fddb18e-2f27-4d2f-ba69-b7f1557969ed 
2021-05-17T15:41:43.589-0700 7f5fe5eb8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:41:43.593-0700 7f5fe5eb8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:41:43.593-0700 7f5fe5eb8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:41:43.669-0700 7f5fe5eb8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f2ffa3f3-34cf-4dbd-8761-518f80926d9b -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T15:41:43.993-0700 7f6ecab83f00 -1 Falling back to public interface
2021-05-17T15:41:44.005-0700 7f6ecab83f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAn8aJg7uorOxAAjMaCfCoG6lv/T8q3ynGGfw== --osd-uuid f2ffa3f3-34cf-4dbd-8761-518f80926d9b 
2021-05-17T15:41:44.337-0700 7fb03dab3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:41:44.337-0700 7fb03dab3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:41:44.337-0700 7fb03dab3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:41:44.381-0700 7fb03dab3f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ea89cbf3-6d5d-456f-bbe3-776216a2536d -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T15:41:44.689-0700 7efd85c6ef00 -1 Falling back to public interface
2021-05-17T15:41:44.701-0700 7efd85c6ef00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAo8aJgmrgZKRAAQbUz7JNsqbxuCwq1HfvIJw== --osd-uuid ea89cbf3-6d5d-456f-bbe3-776216a2536d 
2021-05-17T15:41:45.013-0700 7f34b42bcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:41:45.013-0700 7f34b42bcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:41:45.013-0700 7f34b42bcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:41:45.069-0700 7f34b42bcf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T15:41:45.453-0700 7fd75a93ff00 -1 Falling back to public interface
2021-05-17T15:41:45.469-0700 7fd75a93ff00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T15:41:49,394887828-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:41:49,401439705-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T15:41:49,485180591-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:41:49,491920345-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '48398da7-b8d7-4f91-ae5e-8bfb11f9212b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 48398da7-b8d7-4f91-ae5e-8bfb11f9212b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T15:41:52,312603666-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:41:52,319558121-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '48398da7-b8d7-4f91-ae5e-8bfb11f9212b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 48398da7-b8d7-4f91-ae5e-8bfb11f9212b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T15:41:55,163125664-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:41:55,169613389-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '48398da7-b8d7-4f91-ae5e-8bfb11f9212b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 48398da7-b8d7-4f91-ae5e-8bfb11f9212b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T15:41:57,936105897-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:41:57,942468532-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '48398da7-b8d7-4f91-ae5e-8bfb11f9212b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 48398da7-b8d7-4f91-ae5e-8bfb11f9212b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T15:42:03,767686576-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:42:03,774293587-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '48398da7-b8d7-4f91-ae5e-8bfb11f9212b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 48398da7-b8d7-4f91-ae5e-8bfb11f9212b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T15:42:06,866057437-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:42:06,872441300-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '48398da7-b8d7-4f91-ae5e-8bfb11f9212b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 48398da7-b8d7-4f91-ae5e-8bfb11f9212b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T15:42:10,278465700-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:42:10,284806746-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '48398da7-b8d7-4f91-ae5e-8bfb11f9212b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 48398da7-b8d7-4f91-ae5e-8bfb11f9212b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T15:42:14,014840617-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:42:14,021362909-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '48398da7-b8d7-4f91-ae5e-8bfb11f9212b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 48398da7-b8d7-4f91-ae5e-8bfb11f9212b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:42:17,268370845-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:42:17,275013547-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '48398da7-b8d7-4f91-ae5e-8bfb11f9212b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 48398da7-b8d7-4f91-ae5e-8bfb11f9212b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:42:20,587818226-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:42:20,594260599-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '48398da7-b8d7-4f91-ae5e-8bfb11f9212b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 48398da7-b8d7-4f91-ae5e-8bfb11f9212b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T15:42:26,189269895-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:42:26,195904421-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '48398da7-b8d7-4f91-ae5e-8bfb11f9212b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 48398da7-b8d7-4f91-ae5e-8bfb11f9212b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T15:42:28,978374429-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:42:28,984824814-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '48398da7-b8d7-4f91-ae5e-8bfb11f9212b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 48398da7-b8d7-4f91-ae5e-8bfb11f9212b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  154 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T15:42:31,591687992-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:42:54,651617294-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   205 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (5s)
      [=============...............] (remaining: 5s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:43:02,633881120-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:43:10,634739374-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:43:18,629146924-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:43:26,647344591-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:43:34,483643697-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:43:42,509681059-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:43:50,411030181-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:43:50,425290738-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:43:58,521172135-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:43:58,535590113-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:44:06,555660558-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:44:06,569685290-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:44:14,534498223-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:44:14,548763371-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:44:22,657868625-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:44:22,671896883-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:44:22,683090277-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:44:22,689701400-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:44:22,704292598-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1433219
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T15:44:22,716781446-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T15:44:22,757524243-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:44:22,763866326-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '48398da7-b8d7-4f91-ae5e-8bfb11f9212b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 48398da7-b8d7-4f91-ae5e-8bfb11f9212b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:44:24.260+0000 ffff966b5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:44:24.268+0000 ffff966b5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:44:24.268+0000 ffff966b5010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:44:24.284061+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T22:44:24.284145+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T22:44:24.502730+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:44:24.502730+0000     0       0         0         0         0         0           -           0
2021-05-17T22:44:25.502959+0000     1     255       628       373   372.972       373    0.436265    0.409403
2021-05-17T22:44:26.503256+0000     2     255      1249       994   496.907       621    0.416705    0.412254
2021-05-17T22:44:27.503478+0000     3     255      1853      1598   532.561       604    0.406636    0.417325
2021-05-17T22:44:28.503764+0000     4     255      2463      2208   551.878       610    0.416597    0.417509
2021-05-17T22:44:29.503999+0000     5     255      3064      2809   561.675       601    0.438803    0.418569
2021-05-17T22:44:30.504240+0000     6     255      3664      3409   568.038       600     0.42742    0.420633
2021-05-17T22:44:31.504511+0000     7     255      4254      3999   571.153       590    0.445051    0.421192
2021-05-17T22:44:32.504775+0000     8     255      4813      4558   569.615       559    0.458437     0.42561
2021-05-17T22:44:33.505061+0000     9     255      5406      5151   572.195       593    0.421319    0.427561
2021-05-17T22:44:34.505398+0000    10     255      5990      5735   573.356       584    0.450614    0.427918
2021-05-17T22:44:35.505813+0000    11     255      6520      6265   569.394       530    0.525303    0.431038
2021-05-17T22:44:36.506477+0000    12     255      7088      6833   569.246       568     0.42463    0.434775
2021-05-17T22:44:37.506744+0000    13     255      7710      7455   573.291       622     0.40722     0.43283
2021-05-17T22:44:38.507179+0000    14     255      8356      8101   578.465       646    0.400994    0.430134
2021-05-17T22:44:39.507949+0000    15     255      8977      8722    581.27       621    0.411271    0.428756
2021-05-17T22:44:40.508256+0000    16     255      9621      9366   585.179       644     0.38061    0.427276
2021-05-17T22:44:41.508550+0000    17     255     10278     10023   589.392       657    0.377432    0.424817
2021-05-17T22:44:42.509352+0000    18     255     10923     10668   592.454       645    0.397596    0.422651
2021-05-17T22:44:43.509682+0000    19     255     11537     11282   593.577       614    0.426627    0.422062
2021-05-17T22:44:44.510109+0000 min lat: 0.0338582 max lat: 0.550164 avg lat: 0.418433
2021-05-17T22:44:44.510109+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:44:44.510109+0000    20      28     12146     12118   605.681       836   0.0829901    0.418433
2021-05-17T22:44:45.510514+0000 Total time run:         20.0567
Total writes made:      12146
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     605.582
Stddev Bandwidth:       81.287
Max bandwidth (MB/sec): 836
Min bandwidth (MB/sec): 373
Average IOPS:           605
Stddev IOPS:            81.287
Max IOPS:               836
Min IOPS:               373
Average Latency(s):     0.417573
Stddev Latency(s):      0.0438409
Max latency(s):         0.550164
Min latency(s):         0.012941

[1;32mlocalhost.localdomain	[2021-05-17T15:44:45,878122724-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1433219


[1;33mlocalhost.localdomain	[2021-05-17T15:44:45,885029908-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:45:08,928509146-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.15k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:45:08,942894765-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:45:17,193193636-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.15k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:45:17,207593928-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:45:25,297619764-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.15k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:45:25,311915909-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:45:33,287160829-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.15k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:45:33,301524335-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:45:41,274040817-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.15k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:45:41,287900823-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:45:41,298965211-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:45:41,305435762-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:45:41,319923159-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1436527
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T15:45:41,332217815-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T15:45:41,370712501-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:45:41,377194198-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '48398da7-b8d7-4f91-ae5e-8bfb11f9212b', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 48398da7-b8d7-4f91-ae5e-8bfb11f9212b --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZzfjFR:/tmp/ceph-asok.ZzfjFR -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:45:42.833+0000 ffffae0e8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:45:42.841+0000 ffffae0e8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:45:42.841+0000 ffffae0e8010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:45:42.861857+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:45:42.861857+0000     0       0         0         0         0         0           -           0
2021-05-17T22:45:43.862042+0000     1     255       941       686   685.777       686    0.329329    0.290992
2021-05-17T22:45:44.862271+0000     2     255      1733      1478   738.795       792    0.313826    0.309872
2021-05-17T22:45:45.862533+0000     3     255      2523      2268   755.794       790    0.462919    0.312149
2021-05-17T22:45:46.862789+0000     4     255      3293      3038   759.296       770     0.23827    0.316762
2021-05-17T22:45:47.863002+0000     5     255      4041      3786   757.005       748    0.504043    0.321264
2021-05-17T22:45:48.863343+0000     6     255      4824      4569   761.294       783     0.48807    0.322572
2021-05-17T22:45:49.864493+0000     7     255      5604      5349    763.84       780    0.470885    0.323206
2021-05-17T22:45:50.864722+0000     8     255      6355      6100   762.214       751    0.503009    0.324815
2021-05-17T22:45:51.864987+0000     9     255      7060      6805   755.836       705    0.253063    0.329098
2021-05-17T22:45:52.865214+0000    10     255      7814      7559   755.636       754    0.239141    0.329942
2021-05-17T22:45:53.865453+0000    11     255      8541      8286   753.017       727    0.253062     0.33184
2021-05-17T22:45:54.865658+0000    12     255      9283      9028   752.086       742    0.235463    0.332547
2021-05-17T22:45:55.866188+0000    13     255     10035      9780   752.049       752    0.488556    0.333452
2021-05-17T22:45:56.866582+0000    14     255     10793     10538   752.453       758    0.487436    0.333775
2021-05-17T22:45:57.867187+0000    15     255     11554     11299   752.992       761    0.504515    0.333952
2021-05-17T22:45:58.867428+0000    16       5     12146     12141   758.542       842    0.206982    0.333025
2021-05-17T22:45:59.867693+0000 Total time run:       16.0169
Total reads made:     12146
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   758.322
Average IOPS:         758
Stddev IOPS:          36.422
Max IOPS:             842
Min IOPS:             686
Average Latency(s):   0.332972
Max latency(s):       0.548574
Min latency(s):       0.0746728

[1;32mlocalhost.localdomain	[2021-05-17T15:46:00,235963578-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1436527


[1;33mlocalhost.localdomain	[2021-05-17T15:46:00,242989447-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:46:23,211826080-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.15k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:46:23,226338217-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:46:31,250882035-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.15k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:46:31,265003146-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:46:39,379537354-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.15k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:46:39,394001919-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:46:47,468317395-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.15k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:46:47,482465991-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:46:55,649749787-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.15k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:46:55,664113130-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:46:55,675383128-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T15:46:55,679387127-07:00][RUNNING][ROUND 3/1/20] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:46:55,685856162-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T15:46:55,701944392-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40739\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.474417\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 3f3fdc4a-de27-419b-83c0-24ad19eb5020\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 3f3fdc4a-de27-419b-83c0-24ad19eb5020\nlast_changed 2021-05-17T15:47:24.873900-0700\ncreated 2021-05-17T15:47:24.873900-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40739/0,v1:10.10.1.2:40740/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.474417 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 7020639d-7f43-4a4e-b4a2-8388f00dfc6e\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 7a22b810-ec13-4040-900b-3bb1d461068b\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 0ae4308a-38e0-41d2-9a15-4b2d803b7d0a\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42739\n  w/ user/pass: admin / a688293d-7ad6-404d-a6ba-a090abf7e197\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:47:39 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40739
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.474417
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 3f3fdc4a-de27-419b-83c0-24ad19eb5020
setting min_mon_release = octopus
epoch 0
fsid 3f3fdc4a-de27-419b-83c0-24ad19eb5020
last_changed 2021-05-17T15:47:24.873900-0700
created 2021-05-17T15:47:24.873900-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40739/0,v1:10.10.1.2:40740/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.474417 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 7020639d-7f43-4a4e-b4a2-8388f00dfc6e
0
start osd.0
add osd1 7a22b810-ec13-4040-900b-3bb1d461068b
1
start osd.1
add osd2 0ae4308a-38e0-41d2-9a15-4b2d803b7d0a
2
start osd.2


restful urls: https://10.10.1.2:42739
  w/ user/pass: admin / a688293d-7ad6-404d-a6ba-a090abf7e197


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T15:46:56.720-0700 7fc0ce36b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:46:56.720-0700 7fc0ce36b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:46:56.740-0700 7f872707c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:46:56.740-0700 7f872707c1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40739,v1:10.10.1.2:40740] --print /tmp/ceph_monmap.474417 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.474417 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.474417 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42739 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.4iF2fZ8h7y 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7020639d-7f43-4a4e-b4a2-8388f00dfc6e -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCF8qJgjAQMORAA9WhPNf3dX+oS/b3pJXvoGQ== --osd-uuid 7020639d-7f43-4a4e-b4a2-8388f00dfc6e 
2021-05-17T15:47:34.304-0700 7f472acf7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:47:34.304-0700 7f472acf7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:47:34.304-0700 7f472acf7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:47:34.384-0700 7f472acf7f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7a22b810-ec13-4040-900b-3bb1d461068b -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T15:47:34.688-0700 7f53e170af00 -1 Falling back to public interface
2021-05-17T15:47:34.700-0700 7f53e170af00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCG8qJgpS/7KBAAUmcnNnqynZYI9qipnXSAjg== --osd-uuid 7a22b810-ec13-4040-900b-3bb1d461068b 
2021-05-17T15:47:35.008-0700 7f35d77ecf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:47:35.008-0700 7f35d77ecf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:47:35.008-0700 7f35d77ecf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:47:35.052-0700 7f35d77ecf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0ae4308a-38e0-41d2-9a15-4b2d803b7d0a -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T15:47:35.328-0700 7fda8d1dff00 -1 Falling back to public interface
2021-05-17T15:47:35.344-0700 7fda8d1dff00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCH8qJgOJDXExAAu1PuQRppSculm+Un+6b+bw== --osd-uuid 0ae4308a-38e0-41d2-9a15-4b2d803b7d0a 
2021-05-17T15:47:35.676-0700 7ff4e2c1df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:47:35.676-0700 7ff4e2c1df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:47:35.676-0700 7ff4e2c1df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:47:35.784-0700 7ff4e2c1df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T15:47:36.084-0700 7fc75bdfdf00 -1 Falling back to public interface
2021-05-17T15:47:36.100-0700 7fc75bdfdf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T15:47:39,993908071-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:47:40,001245034-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T15:47:40,085238069-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:47:40,091767685-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5270e19b-d2cd-44ef-a681-e8d1f9d5d77c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5270e19b-d2cd-44ef-a681-e8d1f9d5d77c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T15:47:43,067107815-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:47:43,074187724-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5270e19b-d2cd-44ef-a681-e8d1f9d5d77c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5270e19b-d2cd-44ef-a681-e8d1f9d5d77c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T15:47:46,065811952-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:47:46,072187462-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5270e19b-d2cd-44ef-a681-e8d1f9d5d77c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5270e19b-d2cd-44ef-a681-e8d1f9d5d77c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T15:47:48,935719833-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:47:48,943425708-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5270e19b-d2cd-44ef-a681-e8d1f9d5d77c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5270e19b-d2cd-44ef-a681-e8d1f9d5d77c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T15:47:54,777101209-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:47:54,783802364-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5270e19b-d2cd-44ef-a681-e8d1f9d5d77c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5270e19b-d2cd-44ef-a681-e8d1f9d5d77c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T15:47:57,983491288-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:47:57,989849110-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5270e19b-d2cd-44ef-a681-e8d1f9d5d77c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5270e19b-d2cd-44ef-a681-e8d1f9d5d77c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T15:48:01,491721558-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:48:01,498117817-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5270e19b-d2cd-44ef-a681-e8d1f9d5d77c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5270e19b-d2cd-44ef-a681-e8d1f9d5d77c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T15:48:04,706488525-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:48:04,713088473-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5270e19b-d2cd-44ef-a681-e8d1f9d5d77c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5270e19b-d2cd-44ef-a681-e8d1f9d5d77c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:48:07,953049849-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:48:07,959546310-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5270e19b-d2cd-44ef-a681-e8d1f9d5d77c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5270e19b-d2cd-44ef-a681-e8d1f9d5d77c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:48:11,507995866-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:48:11,514353613-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5270e19b-d2cd-44ef-a681-e8d1f9d5d77c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5270e19b-d2cd-44ef-a681-e8d1f9d5d77c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T15:48:14,547812894-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:48:14,554296828-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5270e19b-d2cd-44ef-a681-e8d1f9d5d77c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5270e19b-d2cd-44ef-a681-e8d1f9d5d77c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T15:48:17,264312258-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:48:17,270950058-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5270e19b-d2cd-44ef-a681-e8d1f9d5d77c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5270e19b-d2cd-44ef-a681-e8d1f9d5d77c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T15:48:20,103054550-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:48:43,312782775-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   202 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:48:51,249300053-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:48:59,183069255-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 28s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:49:07,287009780-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 48s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:49:15,507665440-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 56s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:49:23,492617344-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 71s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:49:31,470340569-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 81s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:49:39,730454669-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 42s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:49:47,881213892-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:49:47,895479751-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:49:55,907299963-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:49:55,921720003-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:50:04,040357180-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:50:04,054985440-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:50:12,018631706-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:50:12,032976298-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:50:20,093689689-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:50:20,108040717-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:50:20,118988419-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:50:20,125401931-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:50:20,140053911-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1450365
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T15:50:20,152739252-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-17T15:50:20,192786292-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:50:20,199261015-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5270e19b-d2cd-44ef-a681-e8d1f9d5d77c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5270e19b-d2cd-44ef-a681-e8d1f9d5d77c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:50:22.040+0000 ffff9b816010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:50:22.048+0000 ffff9b816010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:50:22.048+0000 ffff9b816010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:50:22.064835+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T22:50:22.064908+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T22:50:22.279475+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:50:22.279475+0000     0       0         0         0         0         0           -           0
2021-05-17T22:50:23.279723+0000     1     255       705       450   449.953       450    0.353738    0.366386
2021-05-17T22:50:24.279958+0000     2     255      1390      1135   567.404       685    0.373371    0.368317
2021-05-17T22:50:25.280326+0000     3     255      2044      1789   596.193       654    0.404265    0.375794
2021-05-17T22:50:26.280656+0000     4     255      2688      2433   608.092       644    0.388704    0.381792
2021-05-17T22:50:27.281045+0000     5     255      3322      3067   613.225       634    0.391477    0.385336
2021-05-17T22:50:28.281418+0000     6     255      3935      3680    613.15       613    0.418266    0.391188
2021-05-17T22:50:29.281681+0000     7     255      4524      4269   609.678       589     0.44354    0.395752
2021-05-17T22:50:30.281941+0000     8     255      5108      4853   606.449       584    0.438477     0.40219
2021-05-17T22:50:31.282193+0000     9     255      5743      5488   609.603       635    0.395261    0.402384
2021-05-17T22:50:32.282490+0000    10     255      6313      6058   605.626       570    0.406101    0.406338
2021-05-17T22:50:33.282755+0000    11     255      6972      6717   610.462       659    0.378506    0.405303
2021-05-17T22:50:34.283004+0000    12     255      7575      7320   609.828       603    0.434797    0.406466
2021-05-17T22:50:35.283243+0000    13     255      8163      7908   608.138       588    0.417559    0.407972
2021-05-17T22:50:36.283487+0000    14     255      8798      8543   610.046       635    0.391872    0.408577
2021-05-17T22:50:37.283709+0000    15     255      9414      9159   610.434       616    0.421949    0.408592
2021-05-17T22:50:38.284016+0000    16     255     10037      9782   611.207       623    0.378925    0.409302
2021-05-17T22:50:39.284404+0000    17     255     10612     10357   609.064       575    0.476302    0.410008
2021-05-17T22:50:40.284675+0000    18     255     11231     10976   609.607       619    0.394039    0.411228
2021-05-17T22:50:41.285229+0000    19     255     11874     11619   611.346       643    0.389783     0.41057
2021-05-17T22:50:42.285553+0000 min lat: 0.0116926 max lat: 0.513113 avg lat: 0.406656
2021-05-17T22:50:42.285553+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:50:42.285553+0000    20      33     12510     12477   623.665       858   0.0116926    0.406656
2021-05-17T22:50:43.285924+0000 Total time run:         20.0675
Total writes made:      12510
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     623.395
Stddev Bandwidth:       73.2948
Max bandwidth (MB/sec): 858
Min bandwidth (MB/sec): 450
Average IOPS:           623
Stddev IOPS:            73.2948
Max IOPS:               858
Min IOPS:               450
Average Latency(s):     0.405812
Stddev Latency(s):      0.0416105
Max latency(s):         0.513113
Min latency(s):         0.0116926

[1;32mlocalhost.localdomain	[2021-05-17T15:50:43,633374820-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1450365


[1;33mlocalhost.localdomain	[2021-05-17T15:50:43,640694051-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:51:07,018413751-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.51k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:51:07,032775192-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:51:15,266522625-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.51k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:51:15,281039898-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:51:23,258665018-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.51k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:51:23,273512766-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:51:31,415812733-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.51k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:51:31,430213225-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:51:39,387718207-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.51k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:51:39,402268665-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:51:39,413640969-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:51:39,420371531-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:51:39,435312040-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1453679
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T15:51:39,448052651-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-17T15:51:39,487053026-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:51:39,494074893-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5270e19b-d2cd-44ef-a681-e8d1f9d5d77c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5270e19b-d2cd-44ef-a681-e8d1f9d5d77c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CASp8m:/tmp/ceph-asok.CASp8m -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:51:43.134+0000 ffff9dd6d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:51:43.138+0000 ffff9dd6d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:51:43.142+0000 ffff9dd6d010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:51:43.157878+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:51:43.157878+0000     0       0         0         0         0         0           -           0
2021-05-17T22:51:44.158079+0000     1     255       956       701   700.756       701    0.310564    0.287098
2021-05-17T22:51:45.158702+0000     2     255      1784      1529   764.129       828    0.309565    0.299105
2021-05-17T22:51:46.158899+0000     3     255      2598      2343   780.696       814    0.318272    0.303793
2021-05-17T22:51:47.159756+0000     4     255      3377      3122   780.105       779    0.310814    0.310381
2021-05-17T22:51:48.159986+0000     5     255      4192      3937   787.045       815    0.316381    0.310791
2021-05-17T22:51:49.160476+0000     6     255      4999      4744   790.305       807    0.323132    0.311572
2021-05-17T22:51:50.163523+0000     7     255      5777      5522   788.205       778    0.343484    0.313602
2021-05-17T22:51:51.163785+0000     8     255      6538      6283   784.781       761    0.329087    0.316675
2021-05-17T22:51:52.163997+0000     9     255      7308      7053   783.121       770    0.336038    0.318234
2021-05-17T22:51:53.164208+0000    10     255      8064      7809   780.395       756    0.336787    0.320095
2021-05-17T22:51:54.165176+0000    11     255      8813      8558   777.474       749    0.328938    0.322122
2021-05-17T22:51:55.165572+0000    12     255      9577      9322   776.326       764    0.496621    0.322433
2021-05-17T22:51:56.166343+0000    13     255     10330     10075   774.487       753    0.487742    0.323572
2021-05-17T22:51:57.166797+0000    14     255     11051     10796   770.644       721    0.512263     0.32546
2021-05-17T22:51:58.168048+0000    15     255     11785     11530   768.138       734    0.229549    0.327156
2021-05-17T22:51:59.186895+0000    16     120     12510     12390   772.966       860    0.123971    0.327387
2021-05-17T22:52:00.187148+0000 Total time run:       16.2089
Total reads made:     12510
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   771.798
Average IOPS:         771
Stddev IOPS:          41.7403
Max IOPS:             860
Min IOPS:             701
Average Latency(s):   0.326583
Max latency(s):       0.547567
Min latency(s):       0.0828028

[1;32mlocalhost.localdomain	[2021-05-17T15:52:00,555460416-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1453679


[1;33mlocalhost.localdomain	[2021-05-17T15:52:00,562733275-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:52:23,470109200-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.51k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:52:23,484686663-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:52:31,358813962-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.51k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:52:31,373288058-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:52:39,611537283-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.51k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:52:39,626036508-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:52:47,570315187-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.51k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:52:47,584909483-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:52:55,424329122-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 12.51k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:52:55,438862223-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:52:55,450448494-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T15:52:55,454844525-07:00][RUNNING][ROUND 4/1/20] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:52:55,461253132-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T15:52:55,477632491-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40581\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.477018\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 53f72fb2-721b-4065-bf9d-eebf72f3a196\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 53f72fb2-721b-4065-bf9d-eebf72f3a196\nlast_changed 2021-05-17T15:53:26.071034-0700\ncreated 2021-05-17T15:53:26.071034-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40581/0,v1:10.10.1.2:40582/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.477018 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 c85bae2b-a104-4c28-b127-2a16af3efa68\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 639a79b1-9b56-4241-9e0f-9733a1c741d7\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 ac045065-394a-452a-a690-38d5f4be5fe9\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42581\n  w/ user/pass: admin / 733c3ec4-5ee0-4d0d-b93a-437c4d6500b4\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:53:41 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40581
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.477018
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 53f72fb2-721b-4065-bf9d-eebf72f3a196
setting min_mon_release = octopus
epoch 0
fsid 53f72fb2-721b-4065-bf9d-eebf72f3a196
last_changed 2021-05-17T15:53:26.071034-0700
created 2021-05-17T15:53:26.071034-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40581/0,v1:10.10.1.2:40582/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.477018 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 c85bae2b-a104-4c28-b127-2a16af3efa68
0
start osd.0
add osd1 639a79b1-9b56-4241-9e0f-9733a1c741d7
1
start osd.1
add osd2 ac045065-394a-452a-a690-38d5f4be5fe9
2
start osd.2


restful urls: https://10.10.1.2:42581
  w/ user/pass: admin / 733c3ec4-5ee0-4d0d-b93a-437c4d6500b4


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T15:52:56.459-0700 7f1052c481c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:52:56.459-0700 7f1052c481c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:52:56.479-0700 7fb0ec4861c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:52:56.479-0700 7fb0ec4861c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40581,v1:10.10.1.2:40582] --print /tmp/ceph_monmap.477018 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.477018 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.477018 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42581 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.eGOUlUckcz 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c85bae2b-a104-4c28-b127-2a16af3efa68 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDu86Jg+o9rORAA7eIGMrqI3qW4E+GhHoj+wg== --osd-uuid c85bae2b-a104-4c28-b127-2a16af3efa68 
2021-05-17T15:53:35.312-0700 7fc77b088f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:53:35.312-0700 7fc77b088f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:53:35.312-0700 7fc77b088f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:53:35.364-0700 7fc77b088f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 639a79b1-9b56-4241-9e0f-9733a1c741d7 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T15:53:35.668-0700 7f119d9d9f00 -1 Falling back to public interface
2021-05-17T15:53:35.680-0700 7f119d9d9f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDv86JgJrv7JxAAoWUWHh3VwNwxrpyh2uPmMg== --osd-uuid 639a79b1-9b56-4241-9e0f-9733a1c741d7 
2021-05-17T15:53:36.032-0700 7fc757376f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:53:36.032-0700 7fc757376f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:53:36.032-0700 7fc757376f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:53:36.084-0700 7fc757376f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ac045065-394a-452a-a690-38d5f4be5fe9 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T15:53:36.480-0700 7f00609c7f00 -1 Falling back to public interface
2021-05-17T15:53:36.492-0700 7f00609c7f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDw86Jgs2u9HBAAaYKLo1JHG3Vz8JfgFd3Trw== --osd-uuid ac045065-394a-452a-a690-38d5f4be5fe9 
2021-05-17T15:53:36.816-0700 7ff5e669ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:53:36.816-0700 7ff5e669ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:53:36.816-0700 7ff5e669ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:53:36.888-0700 7ff5e669ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T15:53:37.184-0700 7ffb07d05f00 -1 Falling back to public interface
2021-05-17T15:53:37.196-0700 7ffb07d05f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T15:53:41,185447583-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:53:41,192084574-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T15:53:41,275536476-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:53:41,282027522-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '97aa48f4-82a2-424e-ba48-91f49da449e0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 97aa48f4-82a2-424e-ba48-91f49da449e0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T15:53:44,368982594-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:53:44,375585334-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '97aa48f4-82a2-424e-ba48-91f49da449e0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 97aa48f4-82a2-424e-ba48-91f49da449e0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T15:53:47,226149119-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:53:47,232759286-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '97aa48f4-82a2-424e-ba48-91f49da449e0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 97aa48f4-82a2-424e-ba48-91f49da449e0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T15:53:50,021692361-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:53:50,028168165-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '97aa48f4-82a2-424e-ba48-91f49da449e0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 97aa48f4-82a2-424e-ba48-91f49da449e0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T15:53:55,995040550-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:53:56,001825542-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '97aa48f4-82a2-424e-ba48-91f49da449e0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 97aa48f4-82a2-424e-ba48-91f49da449e0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T15:53:58,819238128-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:53:58,825710542-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '97aa48f4-82a2-424e-ba48-91f49da449e0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 97aa48f4-82a2-424e-ba48-91f49da449e0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T15:54:02,294000748-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:54:02,301378085-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '97aa48f4-82a2-424e-ba48-91f49da449e0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 97aa48f4-82a2-424e-ba48-91f49da449e0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T15:54:05,789677703-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:54:05,797209551-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '97aa48f4-82a2-424e-ba48-91f49da449e0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 97aa48f4-82a2-424e-ba48-91f49da449e0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:54:09,059756307-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:54:09,066694039-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '97aa48f4-82a2-424e-ba48-91f49da449e0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 97aa48f4-82a2-424e-ba48-91f49da449e0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T15:54:12,455287182-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:54:12,462788020-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '97aa48f4-82a2-424e-ba48-91f49da449e0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 97aa48f4-82a2-424e-ba48-91f49da449e0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T15:54:15,734986270-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:54:15,743157482-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '97aa48f4-82a2-424e-ba48-91f49da449e0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 97aa48f4-82a2-424e-ba48-91f49da449e0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T15:54:18,472337917-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:54:18,478998087-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '97aa48f4-82a2-424e-ba48-91f49da449e0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 97aa48f4-82a2-424e-ba48-91f49da449e0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T15:54:21,184112101-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:54:44,087306566-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:54:52,093391204-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:54:59,975890175-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 27s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:55:08,152582706-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (19s)
      [========....................] (remaining: 48s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:55:16,154053894-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 56s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:55:24,121881075-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 75s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:55:32,380528285-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 85s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:55:40,397940664-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [========....................] (remaining: 107s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:55:48,725789805-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:55:48,740182168-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:55:56,735566212-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:55:56,749922554-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:56:04,633532670-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:56:04,647606304-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:56:12,795388732-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:56:12,809991251-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:56:20,783381804-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:56:20,799460634-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:56:20,812356268-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:56:20,820966140-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:56:20,839912902-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1467731
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T15:56:20,853520917-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-17T15:56:20,900045090-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:56:20,910739671-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '97aa48f4-82a2-424e-ba48-91f49da449e0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 97aa48f4-82a2-424e-ba48-91f49da449e0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:56:22.437+0000 ffff89fc2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:56:22.445+0000 ffff89fc2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:56:22.445+0000 ffff89fc2010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:56:22.462442+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T22:56:22.462512+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T22:56:22.679893+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:56:22.679893+0000     0       0         0         0         0         0           -           0
2021-05-17T22:56:23.680172+0000     1     255       731       476   475.937       476    0.322279    0.351038
2021-05-17T22:56:24.680574+0000     2     255      1503      1248   623.833       772     0.31461    0.341098
2021-05-17T22:56:25.680929+0000     3     255      2258      2003   667.469       755    0.361005    0.336572
2021-05-17T22:56:26.681318+0000     4     255      2949      2694   673.285       691    0.309417    0.348294
2021-05-17T22:56:27.681644+0000     5     255      3676      3421    683.98       727    0.342547     0.34745
2021-05-17T22:56:28.681997+0000     6     255      4388      4133   688.609       712    0.333983     0.35013
2021-05-17T22:56:29.682349+0000     7     255      5189      4934   704.625       801    0.310189    0.346314
2021-05-17T22:56:30.682656+0000     8     255      5990      5735   716.641       801    0.326979    0.341941
2021-05-17T22:56:31.683022+0000     9     255      6730      6475   719.206       740    0.352562    0.341451
2021-05-17T22:56:32.683373+0000    10     255      7468      7213    721.06       738    0.355888    0.342568
2021-05-17T22:56:33.683610+0000    11     255      8180      7925   720.221       712    0.348626    0.344031
2021-05-17T22:56:34.683894+0000    12     255      8924      8669   722.185       744    0.326125    0.344211
2021-05-17T22:56:35.684415+0000    13     255      9647      9392   722.218       723    0.331437    0.344576
2021-05-17T22:56:36.684798+0000    14     255     10362     10107   721.683       715    0.330323    0.345944
2021-05-17T22:56:37.685056+0000    15     255     11134     10879   725.024       772    0.328594    0.345144
2021-05-17T22:56:38.685421+0000    16     255     11882     11627   726.443       748    0.346542    0.344745
2021-05-17T22:56:39.685661+0000    17     255     12616     12361   726.877       734    0.367139     0.34454
2021-05-17T22:56:40.685984+0000    18     255     13383     13128   729.093       767    0.340816    0.344218
2021-05-17T22:56:41.686354+0000    19     255     14130     13875   730.021       747    0.337425     0.34407
2021-05-17T22:56:42.686799+0000 min lat: 0.0146677 max lat: 0.435385 avg lat: 0.342952
2021-05-17T22:56:42.686799+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:56:42.686799+0000    20      17     14813     14796    739.55       921   0.0235465    0.342952
2021-05-17T22:56:43.687055+0000 Total time run:         20.0129
Total writes made:      14813
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     740.171
Stddev Bandwidth:       78.8474
Max bandwidth (MB/sec): 921
Min bandwidth (MB/sec): 476
Average IOPS:           740
Stddev IOPS:            78.8474
Max IOPS:               921
Min IOPS:               476
Average Latency(s):     0.342599
Stddev Latency(s):      0.0326771
Max latency(s):         0.435385
Min latency(s):         0.0146677

[1;32mlocalhost.localdomain	[2021-05-17T15:56:44,049412025-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1467731


[1;33mlocalhost.localdomain	[2021-05-17T15:56:44,057006194-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:57:06,957504233-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.81k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:57:06,971858730-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:57:15,226422700-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.81k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:57:15,240948407-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:57:23,249134927-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.81k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:57:23,264044414-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:57:31,465214875-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.81k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:57:31,479619706-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:57:39,458802716-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.81k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:57:39,473596848-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:57:39,485068531-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:57:39,492026083-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:57:39,507467980-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1470989
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T15:57:39,520022621-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-17T15:57:39,558918466-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:57:39,565376358-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '97aa48f4-82a2-424e-ba48-91f49da449e0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 97aa48f4-82a2-424e-ba48-91f49da449e0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XGzuiF:/tmp/ceph-asok.XGzuiF -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T22:57:41.123+0000 ffffad71e010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:57:41.131+0000 ffffad71e010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T22:57:41.131+0000 ffffad71e010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T22:57:41.152432+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:57:41.152432+0000     0       0         0         0         0         0           -           0
2021-05-17T22:57:42.153055+0000     1     255       893       638   637.512       638    0.339897     0.30967
2021-05-17T22:57:43.153269+0000     2     255      1618      1363   681.167       725    0.352919    0.331813
2021-05-17T22:57:44.153486+0000     3     255      2344      2089   696.056       726    0.341334    0.338964
2021-05-17T22:57:45.154150+0000     4     255      3081      2826   706.172       737    0.508895    0.338886
2021-05-17T22:57:46.154374+0000     5     255      3787      3532   706.106       706    0.246956    0.343241
2021-05-17T22:57:47.154922+0000     6     255      4498      4243   706.857       711    0.264217    0.345419
2021-05-17T22:57:48.155156+0000     7     255      5228      4973   710.138       730    0.528083    0.346704
2021-05-17T22:57:49.155362+0000     8     255      5905      5650   705.979       677    0.271406    0.349741
2021-05-17T22:57:50.155949+0000     9     255      6612      6357   706.046       707    0.257039    0.351363
2021-05-17T22:57:51.156543+0000    10     255      7294      7039   703.601       682    0.264422    0.353817
2021-05-17T22:57:52.156775+0000    11     255      8005      7750   704.258       711     0.51832    0.354615
2021-05-17T22:57:53.157004+0000    12     255      8715      8460   704.723       710    0.275027    0.355216
2021-05-17T22:57:54.157223+0000    13     255      9412      9157   704.117       697    0.562566    0.355481
2021-05-17T22:57:55.157439+0000    14     255     10143      9888   706.026       731    0.505288    0.355578
2021-05-17T22:57:56.157652+0000    15     255     10899     10644   709.346       756    0.247146    0.354273
2021-05-17T22:57:57.157867+0000    16     256     11629     11373   710.565       729    0.509758    0.353853
2021-05-17T22:57:58.158295+0000    17     255     12350     12095   711.219       722    0.258114    0.354095
2021-05-17T22:57:59.158533+0000    18     255     13107     12852   713.752       757    0.241856    0.353245
2021-05-17T22:58:00.158798+0000    19     255     13876     13621   716.649       769    0.255395    0.352029
2021-05-17T22:58:01.159398+0000 min lat: 0.177849 max lat: 0.576902 avg lat: 0.351828
2021-05-17T22:58:01.159398+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T22:58:01.159398+0000    20     255     14603     14348   717.145       727    0.249425    0.351828
2021-05-17T22:58:02.159679+0000 Total time run:       20.5246
Total reads made:     14813
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   721.718
Average IOPS:         721
Stddev IOPS:          29.7435
Max IOPS:             769
Min IOPS:             638
Average Latency(s):   0.350262
Max latency(s):       0.576902
Min latency(s):       0.0614169

[1;32mlocalhost.localdomain	[2021-05-17T15:58:02,496945172-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1470989


[1;33mlocalhost.localdomain	[2021-05-17T15:58:02,504159657-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T15:58:25,530475519-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.81k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:58:25,545163508-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:58:33,553982855-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.81k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:58:33,568686942-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:58:41,503914010-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.81k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:58:41,521196600-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:58:49,563240852-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.81k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:58:49,577969963-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:58:57,711984623-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.81k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T15:58:57,727022021-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T15:58:57,739126302-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T15:58:57,743311608-07:00][RUNNING][ROUND 5/1/20] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:58:57,750162941-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T15:58:57,766600783-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40362\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.479025\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6a0abfe6-06a3-4e11-9bec-837f5d5cc51a\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 6a0abfe6-06a3-4e11-9bec-837f5d5cc51a\nlast_changed 2021-05-17T15:59:25.479264-0700\ncreated 2021-05-17T15:59:25.479264-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40362/0,v1:10.10.1.2:40363/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.479025 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 77581b1e-8f4f-4ef6-8770-7942b5b918ca\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 2b8b7446-6d70-47ea-8de1-a566cb76bfa2\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 8cb74fdb-1bd1-4025-b224-841b02d73671\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42362\n  w/ user/pass: admin / e5648a1d-9d58-4ac4-86fa-eef6d02aead5\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:59:40 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40362
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.479025
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6a0abfe6-06a3-4e11-9bec-837f5d5cc51a
setting min_mon_release = octopus
epoch 0
fsid 6a0abfe6-06a3-4e11-9bec-837f5d5cc51a
last_changed 2021-05-17T15:59:25.479264-0700
created 2021-05-17T15:59:25.479264-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40362/0,v1:10.10.1.2:40363/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.479025 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 77581b1e-8f4f-4ef6-8770-7942b5b918ca
0
start osd.0
add osd1 2b8b7446-6d70-47ea-8de1-a566cb76bfa2
1
start osd.1
add osd2 8cb74fdb-1bd1-4025-b224-841b02d73671
2
start osd.2


restful urls: https://10.10.1.2:42362
  w/ user/pass: admin / e5648a1d-9d58-4ac4-86fa-eef6d02aead5


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T15:58:58.799-0700 7fe86cdf91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:58:58.799-0700 7fe86cdf91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:58:58.815-0700 7f9df9ee61c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T15:58:58.815-0700 7f9df9ee61c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40362,v1:10.10.1.2:40363] --print /tmp/ceph_monmap.479025 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.479025 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.479025 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42362 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.vxvAo3c0NW 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 77581b1e-8f4f-4ef6-8770-7942b5b918ca -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBW9aJgPHTEGRAAgyk45ftXP/TZifvMxp6WwQ== --osd-uuid 77581b1e-8f4f-4ef6-8770-7942b5b918ca 
2021-05-17T15:59:34.783-0700 7f25ff765f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:59:34.783-0700 7f25ff765f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:59:34.783-0700 7f25ff765f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T15:59:34.871-0700 7f25ff765f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 2b8b7446-6d70-47ea-8de1-a566cb76bfa2 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T15:59:35.175-0700 7fb437940f00 -1 Falling back to public interface
2021-05-17T15:59:35.187-0700 7fb437940f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBX9aJgweJeChAAHJHy2Oc9u7aNoo2N2E7AQg== --osd-uuid 2b8b7446-6d70-47ea-8de1-a566cb76bfa2 
2021-05-17T15:59:35.511-0700 7f7f433ccf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:59:35.511-0700 7f7f433ccf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:59:35.511-0700 7f7f433ccf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T15:59:35.559-0700 7f7f433ccf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8cb74fdb-1bd1-4025-b224-841b02d73671 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T15:59:35.843-0700 7f3bdf75bf00 -1 Falling back to public interface
2021-05-17T15:59:35.859-0700 7f3bdf75bf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBX9aJgSIE+MhAA5EzGr//YcNo01CE2MUOoZQ== --osd-uuid 8cb74fdb-1bd1-4025-b224-841b02d73671 
2021-05-17T15:59:36.215-0700 7fae50910f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:59:36.215-0700 7fae50910f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:59:36.215-0700 7fae50910f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T15:59:36.303-0700 7fae50910f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T15:59:36.647-0700 7f288c334f00 -1 Falling back to public interface
2021-05-17T15:59:36.663-0700 7f288c334f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T15:59:40,501103156-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T15:59:40,507941122-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T15:59:40,591250481-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:59:40,597875201-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c7a0b722-3769-48d1-9153-289cb2eccb1c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c7a0b722-3769-48d1-9153-289cb2eccb1c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T15:59:43,570813237-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:59:43,577378007-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c7a0b722-3769-48d1-9153-289cb2eccb1c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c7a0b722-3769-48d1-9153-289cb2eccb1c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T15:59:46,347697737-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:59:46,354155914-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c7a0b722-3769-48d1-9153-289cb2eccb1c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c7a0b722-3769-48d1-9153-289cb2eccb1c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T15:59:49,150747718-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:59:49,157303727-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c7a0b722-3769-48d1-9153-289cb2eccb1c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c7a0b722-3769-48d1-9153-289cb2eccb1c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T15:59:54,726449720-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:59:54,733012179-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c7a0b722-3769-48d1-9153-289cb2eccb1c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c7a0b722-3769-48d1-9153-289cb2eccb1c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T15:59:58,898306552-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T15:59:58,905056799-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c7a0b722-3769-48d1-9153-289cb2eccb1c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c7a0b722-3769-48d1-9153-289cb2eccb1c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T16:00:02,018467996-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:00:02,025642500-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c7a0b722-3769-48d1-9153-289cb2eccb1c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c7a0b722-3769-48d1-9153-289cb2eccb1c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T16:00:05,316843257-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:00:05,323020571-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c7a0b722-3769-48d1-9153-289cb2eccb1c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c7a0b722-3769-48d1-9153-289cb2eccb1c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:00:08,611863785-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:00:08,618257901-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c7a0b722-3769-48d1-9153-289cb2eccb1c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c7a0b722-3769-48d1-9153-289cb2eccb1c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:00:11,902332513-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:00:11,908879895-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c7a0b722-3769-48d1-9153-289cb2eccb1c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c7a0b722-3769-48d1-9153-289cb2eccb1c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T16:00:15,068619422-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:00:15,075299726-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c7a0b722-3769-48d1-9153-289cb2eccb1c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c7a0b722-3769-48d1-9153-289cb2eccb1c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T16:00:17,964627927-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:00:17,970994138-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c7a0b722-3769-48d1-9153-289cb2eccb1c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c7a0b722-3769-48d1-9153-289cb2eccb1c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T16:00:20,938201694-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:00:43,763252960-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   202 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:00:51,758503081-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:00:59,714023873-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 28s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:01:07,730272355-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:01:15,699256417-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 53s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:01:23,515225108-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 73s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:01:33,653282969-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (45s)
      [=========...................] (remaining: 94s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:01:41,609401056-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [=========...................] (remaining: 104s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:01:49,810785543-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:01:49,825672137-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:01:57,902175890-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:01:57,916742134-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:02:06,114904522-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:02:06,129754479-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:02:14,054693947-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:02:14,069776849-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:02:22,072107891-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:02:22,087176991-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:02:22,098521513-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:02:22,105376718-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:02:22,120658434-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1485032
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T16:02:22,134309913-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-17T16:02:22,173967350-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:02:22,180466034-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c7a0b722-3769-48d1-9153-289cb2eccb1c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c7a0b722-3769-48d1-9153-289cb2eccb1c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:02:23.803+0000 ffffa57e2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:02:23.811+0000 ffffa57e2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:02:23.811+0000 ffffa57e2010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:02:23.831530+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T23:02:23.831599+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-17T23:02:24.052610+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:02:24.052610+0000     0       0         0         0         0         0           -           0
2021-05-17T23:02:25.052967+0000     1     255       643       388    387.92       388    0.403866    0.408628
2021-05-17T23:02:26.053665+0000     2     255      1417      1162   580.737       774    0.321098    0.367569
2021-05-17T23:02:27.054970+0000     3     255      2199      1944   647.523       782    0.333112    0.350744
2021-05-17T23:02:28.055637+0000     4     255      2976      2721   679.761       777    0.338587    0.344585
2021-05-17T23:02:29.055863+0000     5     255      3715      3460   691.571       739     0.31963    0.345234
2021-05-17T23:02:30.056091+0000     6     255      4532      4277   712.438       817    0.320903    0.339285
2021-05-17T23:02:31.056313+0000     7     255      5352      5097   727.773       820     0.30013    0.336089
2021-05-17T23:02:32.056528+0000     8     255      6095      5840   729.656       743    0.365211    0.335419
2021-05-17T23:02:33.056763+0000     9     255      6843      6588   731.675       748    0.353116     0.33658
2021-05-17T23:02:34.056973+0000    10     255      7578      7323   731.992       735    0.367388    0.337311
2021-05-17T23:02:35.057246+0000    11     255      8328      8073    733.61       750    0.346568    0.338197
2021-05-17T23:02:36.057537+0000    12     255      9132      8877   739.456       804    0.317725     0.33686
2021-05-17T23:02:37.057946+0000    13     255      9928      9673    743.78       796    0.318214    0.335357
2021-05-17T23:02:38.058379+0000    14     255     10732     10477   748.057       804    0.306476    0.334312
2021-05-17T23:02:39.058686+0000    15     255     11503     11248   749.571       771    0.317389    0.334163
2021-05-17T23:02:40.058973+0000    16     255     12300     12045    752.52       797    0.299348    0.333414
2021-05-17T23:02:41.059204+0000    17     255     13102     12847    755.42       802      0.3036    0.332344
2021-05-17T23:02:42.059455+0000    18     255     13908     13653   758.218       806    0.315702    0.331486
2021-05-17T23:02:43.059753+0000    19     255     14696     14441   759.773       788    0.295889    0.331133
2021-05-17T23:02:44.060070+0000 min lat: 0.00848738 max lat: 0.433252 avg lat: 0.328264
2021-05-17T23:02:44.060070+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:02:44.060070+0000    20      30     15492     15462   772.818      1021  0.00848738    0.328264
2021-05-17T23:02:45.060337+0000 Total time run:         20.037
Total writes made:      15492
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     773.169
Stddev Bandwidth:       108.534
Max bandwidth (MB/sec): 1021
Min bandwidth (MB/sec): 388
Average IOPS:           773
Stddev IOPS:            108.534
Max IOPS:               1021
Min IOPS:               388
Average Latency(s):     0.327789
Stddev Latency(s):      0.0328752
Max latency(s):         0.433252
Min latency(s):         0.00848738

[1;32mlocalhost.localdomain	[2021-05-17T16:02:45,405807878-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1485032


[1;33mlocalhost.localdomain	[2021-05-17T16:02:45,413079479-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:03:08,414588858-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.49k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:03:08,429670845-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:03:16,436010708-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.49k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:03:16,450978241-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:03:24,697503084-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.49k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:03:24,712414897-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:03:32,659432365-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.49k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:03:32,676666423-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:03:40,678549422-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.49k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:03:40,693706257-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:03:40,705411366-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:03:40,712328396-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:03:40,727822848-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1488322
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T16:03:40,740653643-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-17T16:03:40,779407012-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:03:40,785747179-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c7a0b722-3769-48d1-9153-289cb2eccb1c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c7a0b722-3769-48d1-9153-289cb2eccb1c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.N958VK:/tmp/ceph-asok.N958VK -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:03:42.392+0000 ffff8fb2b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:03:42.400+0000 ffff8fb2b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:03:42.400+0000 ffff8fb2b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:03:42.420019+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:03:42.420019+0000     0       0         0         0         0         0           -           0
2021-05-17T23:03:43.420823+0000     1     255       856       601   600.427       601    0.214058    0.313482
2021-05-17T23:03:44.421084+0000     2     255      1621      1366   682.585       765    0.252357    0.328491
2021-05-17T23:03:45.421571+0000     3     255      2386      2131    709.93       765    0.485005    0.331235
2021-05-17T23:03:46.421791+0000     4     255      3117      2862   715.156       731     0.27559    0.332475
2021-05-17T23:03:47.422415+0000     5     255      3852      3597   719.034       735    0.256355    0.338294
2021-05-17T23:03:48.422619+0000     6     255      4624      4369   727.833       772     0.25083    0.336734
2021-05-17T23:03:49.422833+0000     7     255      5411      5156    736.26       787    0.479057    0.335389
2021-05-17T23:03:50.423629+0000     8     255      6136      5881    734.78       725    0.242957    0.336633
2021-05-17T23:03:51.424298+0000     9     255      6901      6646   738.081       765    0.239572    0.336823
2021-05-17T23:03:52.425643+0000    10     255      7663      7408   740.372       762    0.236447    0.336863
2021-05-17T23:03:53.429016+0000    11     255      8385      8130   738.477       722    0.271158    0.337622
2021-05-17T23:03:54.429233+0000    12     255      9148      8893   740.505       763    0.244273    0.338069
2021-05-17T23:03:55.429447+0000    13     255      9923      9668   743.145       775    0.325273    0.338133
2021-05-17T23:03:56.429657+0000    14     255     10682     10427   744.265       759    0.592213     0.33769
2021-05-17T23:03:57.429933+0000    15     255     11422     11167   743.967       740    0.231352    0.337795
2021-05-17T23:03:58.430345+0000    16     255     12176     11921   744.575       754    0.250093    0.337919
2021-05-17T23:03:59.430605+0000    17     255     12940     12685   745.706       764     0.33892    0.338172
2021-05-17T23:04:00.430838+0000    18     255     13690     13435   745.934       750    0.261189    0.337464
2021-05-17T23:04:01.431078+0000    19     255     14443     14188   746.297       753     0.49263    0.338053
2021-05-17T23:04:02.431490+0000 min lat: 0.178519 max lat: 0.628034 avg lat: 0.338115
2021-05-17T23:04:02.431490+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:04:02.431490+0000    20     255     15188     14933   746.216       745    0.265353    0.338115
2021-05-17T23:04:03.431753+0000 Total time run:       20.6775
Total reads made:     15492
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   749.221
Average IOPS:         749
Stddev IOPS:          38.2956
Max IOPS:             787
Min IOPS:             601
Average Latency(s):   0.337262
Max latency(s):       0.628034
Min latency(s):       0.0575368

[1;32mlocalhost.localdomain	[2021-05-17T16:04:03,801566926-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1488322


[1;33mlocalhost.localdomain	[2021-05-17T16:04:03,809799665-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:04:26,804519251-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.49k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:04:26,819821491-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:04:34,801685262-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.49k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:04:34,816905858-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:04:42,820087601-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.49k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:04:42,835488302-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:04:50,867443968-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.49k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:04:50,883991240-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:04:58,754011847-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.49k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:04:58,769370107-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:04:58,781325788-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T16:04:58,788570519-07:00][RUNNING][ROUND 1/2/20] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:04:58,795348761-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T16:04:58,811787218-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40169\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.481773\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ca06f79a-67d2-42b4-8f82-fbe393b6ab1c\nsetting min_mon_release = octopus\nepoch 0\nfsid ca06f79a-67d2-42b4-8f82-fbe393b6ab1c\nlast_changed 2021-05-17T16:05:27.583236-0700\ncreated 2021-05-17T16:05:27.583236-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40169/0,v1:10.10.1.2:40170/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.481773 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 a64e215c-932b-46ea-b461-74773ed6e11f\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 3e21f180-5e72-442f-a912-238bbe117564\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 ec9617ce-2d54-4c5e-aced-39fa8ca25259\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42169\n  w/ user/pass: admin / 2d1c616f-26b0-4383-aaba-ae925459c279\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:05:43 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40169
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.481773
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ca06f79a-67d2-42b4-8f82-fbe393b6ab1c
setting min_mon_release = octopus
epoch 0
fsid ca06f79a-67d2-42b4-8f82-fbe393b6ab1c
last_changed 2021-05-17T16:05:27.583236-0700
created 2021-05-17T16:05:27.583236-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40169/0,v1:10.10.1.2:40170/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.481773 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 a64e215c-932b-46ea-b461-74773ed6e11f
0
start osd.0
add osd1 3e21f180-5e72-442f-a912-238bbe117564
1
start osd.1
add osd2 ec9617ce-2d54-4c5e-aced-39fa8ca25259
2
start osd.2


restful urls: https://10.10.1.2:42169
  w/ user/pass: admin / 2d1c616f-26b0-4383-aaba-ae925459c279


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T16:04:59.794-0700 7f585dd111c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:04:59.794-0700 7f585dd111c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:04:59.810-0700 7f56f107d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:04:59.810-0700 7f56f107d1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40169,v1:10.10.1.2:40170] --print /tmp/ceph_monmap.481773 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.481773 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.481773 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42169 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.LIEfqo45UD 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a64e215c-932b-46ea-b461-74773ed6e11f -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDB9qJg5ykjFRAAq/GEXAFFsMavYJGMvZ4UmQ== --osd-uuid a64e215c-932b-46ea-b461-74773ed6e11f 
2021-05-17T16:05:37.682-0700 7f4f4f662f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:05:37.682-0700 7f4f4f662f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:05:37.682-0700 7f4f4f662f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:05:37.774-0700 7f4f4f662f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3e21f180-5e72-442f-a912-238bbe117564 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T16:05:38.122-0700 7fda2843af00 -1 Falling back to public interface
2021-05-17T16:05:38.134-0700 7fda2843af00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDC9qJgF4wXBxAAtqDN8YnnLcR76lm3FeZsuQ== --osd-uuid 3e21f180-5e72-442f-a912-238bbe117564 
2021-05-17T16:05:38.534-0700 7fe66231ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:05:38.534-0700 7fe66231ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:05:38.534-0700 7fe66231ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:05:38.586-0700 7fe66231ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ec9617ce-2d54-4c5e-aced-39fa8ca25259 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T16:05:38.906-0700 7f00a73fef00 -1 Falling back to public interface
2021-05-17T16:05:38.922-0700 7f00a73fef00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDC9qJgvpP9NRAAJL1H/hZqZP+KYYYEd4qM8g== --osd-uuid ec9617ce-2d54-4c5e-aced-39fa8ca25259 
2021-05-17T16:05:39.254-0700 7f794756df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:05:39.254-0700 7f794756df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:05:39.254-0700 7f794756df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:05:39.306-0700 7f794756df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T16:05:39.634-0700 7f8f44461f00 -1 Falling back to public interface
2021-05-17T16:05:39.650-0700 7f8f44461f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T16:05:43,540608411-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:05:43,547818171-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T16:05:43,631545207-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:05:43,638043831-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8d241bff-bc6e-477a-b422-3c1b2800fdc5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8d241bff-bc6e-477a-b422-3c1b2800fdc5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T16:05:46,449136822-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:05:46,455812191-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8d241bff-bc6e-477a-b422-3c1b2800fdc5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8d241bff-bc6e-477a-b422-3c1b2800fdc5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T16:05:49,279327638-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:05:49,285803429-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8d241bff-bc6e-477a-b422-3c1b2800fdc5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8d241bff-bc6e-477a-b422-3c1b2800fdc5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T16:05:52,061016970-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:05:52,067632014-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8d241bff-bc6e-477a-b422-3c1b2800fdc5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8d241bff-bc6e-477a-b422-3c1b2800fdc5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T16:05:57,607994175-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:05:57,614389867-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8d241bff-bc6e-477a-b422-3c1b2800fdc5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8d241bff-bc6e-477a-b422-3c1b2800fdc5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T16:06:01,260501789-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:06:01,267120343-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8d241bff-bc6e-477a-b422-3c1b2800fdc5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8d241bff-bc6e-477a-b422-3c1b2800fdc5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T16:06:05,216989688-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:06:05,223522638-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8d241bff-bc6e-477a-b422-3c1b2800fdc5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8d241bff-bc6e-477a-b422-3c1b2800fdc5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T16:06:08,279389786-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:06:08,286524383-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8d241bff-bc6e-477a-b422-3c1b2800fdc5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8d241bff-bc6e-477a-b422-3c1b2800fdc5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:06:11,723769561-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:06:11,731220802-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8d241bff-bc6e-477a-b422-3c1b2800fdc5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8d241bff-bc6e-477a-b422-3c1b2800fdc5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:06:15,549615403-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:06:15,556123869-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8d241bff-bc6e-477a-b422-3c1b2800fdc5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8d241bff-bc6e-477a-b422-3c1b2800fdc5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T16:06:18,800759394-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:06:18,807141478-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8d241bff-bc6e-477a-b422-3c1b2800fdc5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8d241bff-bc6e-477a-b422-3c1b2800fdc5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T16:06:21,599132306-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:06:21,605623639-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8d241bff-bc6e-477a-b422-3c1b2800fdc5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8d241bff-bc6e-477a-b422-3c1b2800fdc5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T16:06:24,297946412-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:06:47,304355536-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   188 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=======================.....] (remaining: 1s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:06:55,156758677-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:07:03,216796806-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:07:11,366493656-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:07:19,372717460-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:07:27,666363423-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:07:35,600021981-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:07:43,608124004-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:07:43,623119352-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:07:51,770355609-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:07:51,785768733-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:07:59,982083212-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:07:59,997265296-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:08:08,621761869-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:08:08,637217810-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:08:16,602673523-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:08:16,621118407-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:08:16,634105214-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:08:16,641485973-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:08:16,657888501-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1501869
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T16:08:16,671335240-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T16:08:16,713828806-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:08:16,721119205-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8d241bff-bc6e-477a-b422-3c1b2800fdc5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8d241bff-bc6e-477a-b422-3c1b2800fdc5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:08:18.247+0000 ffffaecb0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:08:18.255+0000 ffffaecb0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:08:18.255+0000 ffffaecb0010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:08:18.275554+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-17T23:08:18.275652+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-17T23:08:19.238687+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:08:19.238687+0000     0       0         0         0         0         0           -           0
2021-05-17T23:08:20.239221+0000     1     158       158         0         0         0           -           0
2021-05-17T23:08:21.239483+0000     2     255       296        41   81.9742        82     1.75341     1.70184
2021-05-17T23:08:22.239724+0000     3     255       438       183   243.929       568     1.83969     1.79002
2021-05-17T23:08:23.239959+0000     4     255       578       323   322.911       560     1.80645     1.81658
2021-05-17T23:08:24.240233+0000     5     255       706       451   360.701       512     1.88895     1.82312
2021-05-17T23:08:25.240460+0000     6     255       844       589   392.562       552     1.92646     1.84641
2021-05-17T23:08:26.240734+0000     7     255       982       727   415.317       552     1.87123     1.86278
2021-05-17T23:08:27.241071+0000     8     255      1124       869    434.38       568     1.81399     1.85901
2021-05-17T23:08:28.241520+0000     9     255      1261      1006   446.979       548      1.8305      1.8512
2021-05-17T23:08:29.241904+0000    10     255      1385      1130   451.862       496     1.98107     1.85325
2021-05-17T23:08:30.242247+0000    11     255      1539      1284   466.765       616     1.84436     1.85882
2021-05-17T23:08:31.242625+0000    12     255      1679      1424   474.518       560     1.74485     1.85419
2021-05-17T23:08:32.242878+0000    13     255      1828      1573    483.85       596     1.76514     1.84545
2021-05-17T23:08:33.243097+0000    14     255      1968      1713    489.28       560     1.77735     1.83557
2021-05-17T23:08:34.244137+0000    15     255      2107      1852   493.693       556     1.86259     1.83477
2021-05-17T23:08:35.244796+0000    16     255      2248      1993   498.065       564     1.80828      1.8356
2021-05-17T23:08:36.245358+0000    17     255      2397      2142   503.807       596     1.76886     1.83171
2021-05-17T23:08:37.245632+0000    18     255      2541      2286   507.809       576     1.75753     1.82695
2021-05-17T23:08:38.246496+0000    19     255      2679      2424   510.111       552      1.8026     1.82558
2021-05-17T23:08:39.246817+0000 min lat: 0.0286721 max lat: 1.99702 avg lat: 1.74327
2021-05-17T23:08:39.246817+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:08:39.246817+0000    20       6      2826      2820   563.776      1584   0.0483992     1.74327
2021-05-17T23:08:40.247120+0000 Total time run:         20.078
Total writes made:      2826
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     563.004
Stddev Bandwidth:       290.511
Max bandwidth (MB/sec): 1584
Min bandwidth (MB/sec): 0
Average IOPS:           140
Stddev IOPS:            72.6712
Max IOPS:               396
Min IOPS:               0
Average Latency(s):     1.73971
Stddev Latency(s):      0.317634
Max latency(s):         1.99702
Min latency(s):         0.0286721

[1;32mlocalhost.localdomain	[2021-05-17T16:08:40,722482714-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1501869


[1;33mlocalhost.localdomain	[2021-05-17T16:08:40,730277453-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:09:03,677517746-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:09:03,692765107-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:09:11,773772041-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:09:11,789211255-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:09:19,891715602-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:09:19,907149655-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:09:27,844922041-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:09:27,860131115-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:09:35,924215475-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:09:35,939701092-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:09:35,951622898-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:09:35,958671126-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:09:35,974644100-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1505206
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T16:09:35,988024829-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-17T16:09:36,027416377-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:09:36,034051600-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8d241bff-bc6e-477a-b422-3c1b2800fdc5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8d241bff-bc6e-477a-b422-3c1b2800fdc5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bohmHy:/tmp/ceph-asok.bohmHy -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:09:37.496+0000 ffffb08ce010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:09:37.505+0000 ffffb08ce010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:09:37.505+0000 ffffb08ce010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:09:37.527392+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:09:37.527392+0000     0       0         0         0         0         0           -           0
2021-05-17T23:09:38.528864+0000     1     255       288        33   131.785       132    0.958033    0.915372
2021-05-17T23:09:39.529117+0000     2     255       480       225   449.576       768     1.28433     1.09812
2021-05-17T23:09:40.529431+0000     3     255       680       425   566.251       800      1.2861     1.19322
2021-05-17T23:09:41.532418+0000     4     255       877       622   621.194       788     1.29615     1.22205
2021-05-17T23:09:42.532706+0000     5     255      1062       807   644.894       740     1.36686     1.24988
2021-05-17T23:09:43.532951+0000     6     255      1249       994   662.035       748     1.34659      1.2722
2021-05-17T23:09:44.533183+0000     7     255      1438      1183   675.426       756     1.34984     1.28613
2021-05-17T23:09:45.533446+0000     8     255      1622      1367   682.969       736     1.36905     1.29623
2021-05-17T23:09:46.533745+0000     9     255      1807      1552   689.279       740     1.39514     1.30725
2021-05-17T23:09:47.533994+0000    10     255      1994      1739    695.13       748     1.35929     1.31435
2021-05-17T23:09:48.534229+0000    11     255      2179      1924   699.191       740     1.39182     1.32034
2021-05-17T23:09:49.534594+0000    12     255      2361      2106   701.569       728     1.38488     1.32587
2021-05-17T23:09:50.535953+0000    13     255      2546      2291    704.45       740     1.39426     1.33168
2021-05-17T23:09:51.536313+0000    14     255      2731      2476    706.97       740     1.38565     1.33567
2021-05-17T23:09:52.536589+0000 Total time run:       14.9201
Total reads made:     2826
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   757.637
Average IOPS:         189
Stddev IOPS:          41.7204
Max IOPS:             200
Min IOPS:             33
Average Latency(s):   1.29719
Max latency(s):       1.41163
Min latency(s):       0.409026

[1;32mlocalhost.localdomain	[2021-05-17T16:09:52,996313295-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1505206


[1;33mlocalhost.localdomain	[2021-05-17T16:09:53,003925123-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:10:15,959841109-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:10:15,977161313-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:10:24,013599861-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:10:24,028485945-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:10:32,155935604-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:10:32,170877063-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:10:40,199806095-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:10:40,214632605-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:10:48,191168283-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.83k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:10:48,206319643-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:10:48,218282615-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T16:10:48,222764042-07:00][RUNNING][ROUND 2/2/20] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:10:48,229740693-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T16:10:48,246437900-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40684\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.482880\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8db2f6da-61e1-4364-9bf8-88e579d35651\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 8db2f6da-61e1-4364-9bf8-88e579d35651\nlast_changed 2021-05-17T16:11:16.829506-0700\ncreated 2021-05-17T16:11:16.829506-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40684/0,v1:10.10.1.2:40685/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.482880 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 c36cbcc7-abe3-409e-b7bb-d54ef0e96c17\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 744ffc84-196c-4151-812d-552bf3e93c3e\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 025796d1-b256-46c9-9877-9f4b81b4dbca\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42684\n  w/ user/pass: admin / a0f105cb-ef5d-4c06-94d1-36891b8e392d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:11:32 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40684
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.482880
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8db2f6da-61e1-4364-9bf8-88e579d35651
setting min_mon_release = octopus
epoch 0
fsid 8db2f6da-61e1-4364-9bf8-88e579d35651
last_changed 2021-05-17T16:11:16.829506-0700
created 2021-05-17T16:11:16.829506-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40684/0,v1:10.10.1.2:40685/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.482880 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 c36cbcc7-abe3-409e-b7bb-d54ef0e96c17
0
start osd.0
add osd1 744ffc84-196c-4151-812d-552bf3e93c3e
1
start osd.1
add osd2 025796d1-b256-46c9-9877-9f4b81b4dbca
2
start osd.2


restful urls: https://10.10.1.2:42684
  w/ user/pass: admin / a0f105cb-ef5d-4c06-94d1-36891b8e392d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T16:10:49.269-0700 7f75f852f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:10:49.269-0700 7f75f852f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:10:49.289-0700 7fed74a7f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:10:49.289-0700 7fed74a7f1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40684,v1:10.10.1.2:40685] --print /tmp/ceph_monmap.482880 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.482880 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.482880 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42684 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.KEsH2ywIU9 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c36cbcc7-abe3-409e-b7bb-d54ef0e96c17 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAd+KJg+wJoOhAACVQoufUxoEP5SwKGCT/bPA== --osd-uuid c36cbcc7-abe3-409e-b7bb-d54ef0e96c17 
2021-05-17T16:11:26.322-0700 7fe145a6af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:11:26.322-0700 7fe145a6af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:11:26.322-0700 7fe145a6af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:11:26.402-0700 7fe145a6af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 744ffc84-196c-4151-812d-552bf3e93c3e -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T16:11:26.686-0700 7f3a99bd6f00 -1 Falling back to public interface
2021-05-17T16:11:26.702-0700 7f3a99bd6f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAe+KJgRY8CKRAAC/2EX+oecfQ9e6s4JXDYvA== --osd-uuid 744ffc84-196c-4151-812d-552bf3e93c3e 
2021-05-17T16:11:27.010-0700 7f4b30c85f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:11:27.010-0700 7f4b30c85f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:11:27.010-0700 7f4b30c85f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:11:27.054-0700 7f4b30c85f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 025796d1-b256-46c9-9877-9f4b81b4dbca -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T16:11:27.358-0700 7f0892cc1f00 -1 Falling back to public interface
2021-05-17T16:11:27.370-0700 7f0892cc1f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAf+KJgUbZXFRAAv0FC7oAwWXsLFcRv1FBEvw== --osd-uuid 025796d1-b256-46c9-9877-9f4b81b4dbca 
2021-05-17T16:11:27.702-0700 7fb660d37f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:11:27.706-0700 7fb660d37f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:11:27.706-0700 7fb660d37f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:11:27.818-0700 7fb660d37f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T16:11:28.162-0700 7fbe26026f00 -1 Falling back to public interface
2021-05-17T16:11:28.174-0700 7fbe26026f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T16:11:32,052494649-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:11:32,059624625-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T16:11:32,142825089-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:11:32,149395006-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b58a213-b234-4e67-9a90-42c83adcde1d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b58a213-b234-4e67-9a90-42c83adcde1d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T16:11:35,129589460-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:11:35,135882955-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b58a213-b234-4e67-9a90-42c83adcde1d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b58a213-b234-4e67-9a90-42c83adcde1d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T16:11:37,911566286-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:11:37,917948190-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b58a213-b234-4e67-9a90-42c83adcde1d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b58a213-b234-4e67-9a90-42c83adcde1d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T16:11:40,619453006-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:11:40,625560424-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b58a213-b234-4e67-9a90-42c83adcde1d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b58a213-b234-4e67-9a90-42c83adcde1d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T16:11:48,191134971-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:11:48,197591243-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b58a213-b234-4e67-9a90-42c83adcde1d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b58a213-b234-4e67-9a90-42c83adcde1d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T16:11:52,248455745-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:11:52,255281905-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b58a213-b234-4e67-9a90-42c83adcde1d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b58a213-b234-4e67-9a90-42c83adcde1d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T16:11:55,711755417-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:11:55,718282697-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b58a213-b234-4e67-9a90-42c83adcde1d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b58a213-b234-4e67-9a90-42c83adcde1d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T16:11:59,641810421-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:11:59,648536184-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b58a213-b234-4e67-9a90-42c83adcde1d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b58a213-b234-4e67-9a90-42c83adcde1d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:12:03,052522018-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:12:03,058921731-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b58a213-b234-4e67-9a90-42c83adcde1d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b58a213-b234-4e67-9a90-42c83adcde1d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:12:06,670259903-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:12:06,676857589-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b58a213-b234-4e67-9a90-42c83adcde1d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b58a213-b234-4e67-9a90-42c83adcde1d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T16:12:09,960069705-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:12:09,966457101-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b58a213-b234-4e67-9a90-42c83adcde1d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b58a213-b234-4e67-9a90-42c83adcde1d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T16:12:12,580368396-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:12:12,586757472-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b58a213-b234-4e67-9a90-42c83adcde1d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b58a213-b234-4e67-9a90-42c83adcde1d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T16:12:15,342879975-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:12:38,413931435-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (5s)
      [=============...............] (remaining: 5s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:12:46,566076367-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:12:54,484847807-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:13:02,519271866-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:13:10,642188992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:13:18,620787926-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:13:26,815660077-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   214 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:13:26,830872408-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:13:34,804078472-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:13:34,820521446-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:13:43,008877037-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:13:43,024359128-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:13:51,270792233-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:13:51,285986185-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:13:59,181884090-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:13:59,196876618-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:13:59,208953461-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:13:59,216133111-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:13:59,231712358-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1518266
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T16:13:59,244663159-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T16:13:59,284730574-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:13:59,291188271-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b58a213-b234-4e67-9a90-42c83adcde1d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b58a213-b234-4e67-9a90-42c83adcde1d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:14:00.713+0000 ffff84a59010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:14:00.721+0000 ffff84a59010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:14:00.721+0000 ffff84a59010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:14:00.741290+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-17T23:14:00.741393+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T23:14:01.689911+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:14:01.689911+0000     0       8         8         0         0         0           -           0
2021-05-17T23:14:02.690100+0000     1     156       156         0         0         0           -           0
2021-05-17T23:14:03.690358+0000     2     255       301        46   91.9472        92     1.69624     1.71622
2021-05-17T23:14:04.690643+0000     3     255       444       189    251.88       572     1.77271     1.74139
2021-05-17T23:14:05.690878+0000     4     255       579       324   323.865       540     1.83503     1.76583
2021-05-17T23:14:06.691310+0000     5     255       725       470   375.842       584     1.80503     1.78547
2021-05-17T23:14:07.691829+0000     6     255       869       614   409.155       576     1.76383     1.78291
2021-05-17T23:14:08.692153+0000     7     255      1003       748   427.249       536     1.85301     1.78612
2021-05-17T23:14:09.692384+0000     8     255      1146       891   445.323       572     1.83395     1.79724
2021-05-17T23:14:10.692807+0000     9     255      1293      1038   461.149       588      1.7481     1.79847
2021-05-17T23:14:11.693062+0000    10     255      1439      1184   473.418       584     1.73589     1.79146
2021-05-17T23:14:12.693348+0000    11     255      1579      1324   481.274       560     1.78869     1.78822
2021-05-17T23:14:13.693635+0000    12     255      1723      1468   489.153       576     1.82482     1.79179
2021-05-17T23:14:14.694285+0000    13     255      1872      1617   497.344       596     1.74644     1.79097
2021-05-17T23:14:15.694612+0000    14     255      2029      1774   506.662       628      1.6489     1.78089
2021-05-17T23:14:16.694845+0000    15     255      2165      1910   509.142       544     1.76136     1.77634
2021-05-17T23:14:17.695110+0000    16     255      2311      2056   513.811       584      1.7947     1.77787
2021-05-17T23:14:18.695550+0000    17     255      2463      2208   519.336       608     1.72575     1.77581
2021-05-17T23:14:19.696086+0000    18     255      2611      2356   523.356       592     1.68375     1.77121
2021-05-17T23:14:20.696615+0000    19     255      2757      2502   526.532       584     1.74282     1.76908
2021-05-17T23:14:21.696905+0000 min lat: 1.64 max lat: 1.88083 avg lat: 1.76809
2021-05-17T23:14:21.696905+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:14:21.696905+0000    20     255      2905      2650   529.796       592     1.73122     1.76809
2021-05-17T23:14:22.697234+0000 Total time run:         20.0646
Total writes made:      2906
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     579.328
Stddev Bandwidth:       166.081
Max bandwidth (MB/sec): 628
Min bandwidth (MB/sec): 0
Average IOPS:           144
Stddev IOPS:            41.5202
Max IOPS:               157
Min IOPS:               0
Average Latency(s):     1.68944
Stddev Latency(s):      0.297329
Max latency(s):         1.88083
Min latency(s):         0.0353686

[1;32mlocalhost.localdomain	[2021-05-17T16:14:23,319283767-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1518266


[1;33mlocalhost.localdomain	[2021-05-17T16:14:23,327123064-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:14:46,017715916-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.91k objects, 11 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:14:46,033101251-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:14:54,028739044-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.91k objects, 11 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:14:54,043753231-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:15:02,128512824-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.91k objects, 11 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:15:02,144222725-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:15:09,885852973-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.91k objects, 11 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:15:09,901229981-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:15:17,628381605-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.91k objects, 11 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:15:17,643709105-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:15:17,656291043-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:15:17,663462363-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:15:17,679513242-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1521607
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T16:15:17,692214195-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T16:15:17,731313992-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:15:17,737747906-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b58a213-b234-4e67-9a90-42c83adcde1d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b58a213-b234-4e67-9a90-42c83adcde1d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BdezEc:/tmp/ceph-asok.BdezEc -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:15:19.163+0000 ffff92ed0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:15:19.171+0000 ffff92ed0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:15:19.171+0000 ffff92ed0010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:15:19.195367+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:15:19.195367+0000     0       0         0         0         0         0           -           0
2021-05-17T23:15:20.195597+0000     1     248       248         0         0         0           -           0
2021-05-17T23:15:21.195840+0000     2     255       432       177   353.885       354     1.32572     1.22319
2021-05-17T23:15:22.196665+0000     3     255       616       361   481.097       736     1.39022     1.30482
2021-05-17T23:15:23.196931+0000     4     255       799       544   543.763       732     1.38932     1.33544
2021-05-17T23:15:24.197178+0000     5     255       975       720   575.771       704     1.42702     1.35307
2021-05-17T23:15:25.197400+0000     6     255      1146       891   593.781       684     1.47792     1.37593
2021-05-17T23:15:26.197687+0000     7     255      1324      1069   610.639       712     1.44384     1.38988
2021-05-17T23:15:27.197984+0000     8     255      1500      1245   622.283       704     1.45991     1.39846
2021-05-17T23:15:28.198207+0000     9     255      1679      1424   632.677       716     1.43054      1.4069
2021-05-17T23:15:29.198474+0000    10     255      1861      1606   642.189       728     1.38835      1.4062
2021-05-17T23:15:30.198758+0000    11     255      2037      1782    647.79       704     1.43855      1.4086
2021-05-17T23:15:31.199897+0000    12     255      2216      1961    653.41       716     1.43019     1.41148
2021-05-17T23:15:32.200213+0000    13     255      2396      2141   658.515       720     1.40511      1.4122
2021-05-17T23:15:33.200529+0000    14     255      2576      2321    662.89       720     1.42566     1.41244
2021-05-17T23:15:34.200767+0000    15     255      2758      2503   667.219       728     1.41697     1.41269
2021-05-17T23:15:35.202162+0000    16     150      2906      2756     688.7      1012      1.0184     1.40634
2021-05-17T23:15:36.202448+0000 Total time run:       16.2297
Total reads made:     2906
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   716.219
Average IOPS:         179
Stddev IOPS:          53.8674
Max IOPS:             253
Min IOPS:             0
Average Latency(s):   1.36998
Max latency(s):       1.50322
Min latency(s):       0.397126

[1;32mlocalhost.localdomain	[2021-05-17T16:15:36,659247756-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1521607


[1;33mlocalhost.localdomain	[2021-05-17T16:15:36,667018339-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:15:59,762801173-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.91k objects, 11 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:15:59,778082267-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:16:07,661310345-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.91k objects, 11 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:16:07,676632277-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:16:15,447376908-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.91k objects, 11 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:16:15,462541402-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:16:23,619164377-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.91k objects, 11 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:16:23,634647466-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:16:31,453073421-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.91k objects, 11 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:16:31,468315732-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:16:31,480716902-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T16:16:31,485241408-07:00][RUNNING][ROUND 3/2/20] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:16:31,492182497-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T16:16:31,509749727-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40768\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.483991\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 0c7ae3b1-c763-43be-8ad6-0577eb8fd6ee\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 0c7ae3b1-c763-43be-8ad6-0577eb8fd6ee\nlast_changed 2021-05-17T16:17:02.372724-0700\ncreated 2021-05-17T16:17:02.372724-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40768/0,v1:10.10.1.2:40769/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.483991 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 a2289fd6-92d4-42ad-82bd-bc225f8f116a\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 39bb8df3-0054-4562-a25b-f0fc1be14e6d\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 026765ef-234d-479e-9629-e70aec4ae144\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42768\n  w/ user/pass: admin / 7fb39d71-9bfb-4526-97ae-5644d9c6fe1b\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:17:17 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40768
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.483991
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 0c7ae3b1-c763-43be-8ad6-0577eb8fd6ee
setting min_mon_release = octopus
epoch 0
fsid 0c7ae3b1-c763-43be-8ad6-0577eb8fd6ee
last_changed 2021-05-17T16:17:02.372724-0700
created 2021-05-17T16:17:02.372724-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40768/0,v1:10.10.1.2:40769/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.483991 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 a2289fd6-92d4-42ad-82bd-bc225f8f116a
0
start osd.0
add osd1 39bb8df3-0054-4562-a25b-f0fc1be14e6d
1
start osd.1
add osd2 026765ef-234d-479e-9629-e70aec4ae144
2
start osd.2


restful urls: https://10.10.1.2:42768
  w/ user/pass: admin / 7fb39d71-9bfb-4526-97ae-5644d9c6fe1b


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T16:16:32.540-0700 7f569bef01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:16:32.540-0700 7f569bef01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:16:32.556-0700 7fed48a2d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:16:32.556-0700 7fed48a2d1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40768,v1:10.10.1.2:40769] --print /tmp/ceph_monmap.483991 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.483991 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.483991 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42768 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.uMEiC0C5kV 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a2289fd6-92d4-42ad-82bd-bc225f8f116a -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB3+aJgbaHiDBAAXPguzSARpHYk4UXa6qgtRQ== --osd-uuid a2289fd6-92d4-42ad-82bd-bc225f8f116a 
2021-05-17T16:17:11.557-0700 7f18f4c69f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:17:11.557-0700 7f18f4c69f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:17:11.557-0700 7f18f4c69f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:17:11.637-0700 7f18f4c69f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 39bb8df3-0054-4562-a25b-f0fc1be14e6d -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T16:17:11.949-0700 7f4311e3cf00 -1 Falling back to public interface
2021-05-17T16:17:11.961-0700 7f4311e3cf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB3+aJgVNyROBAAmYqKAQ/BhBqWmpkmmlDxAA== --osd-uuid 39bb8df3-0054-4562-a25b-f0fc1be14e6d 
2021-05-17T16:17:12.301-0700 7fbd88ce2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:17:12.301-0700 7fbd88ce2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:17:12.301-0700 7fbd88ce2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:17:12.349-0700 7fbd88ce2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 026765ef-234d-479e-9629-e70aec4ae144 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T16:17:12.637-0700 7f737395df00 -1 Falling back to public interface
2021-05-17T16:17:12.653-0700 7f737395df00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB4+aJgI75hJBAAjE1V2sSY0lT3uZbEQUm6Fg== --osd-uuid 026765ef-234d-479e-9629-e70aec4ae144 
2021-05-17T16:17:12.989-0700 7f31774c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:17:12.989-0700 7f31774c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:17:12.989-0700 7f31774c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:17:13.053-0700 7f31774c5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T16:17:13.513-0700 7effa5fe1f00 -1 Falling back to public interface
2021-05-17T16:17:13.529-0700 7effa5fe1f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T16:17:17,328700675-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:17:17,335915590-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T16:17:17,433528847-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:17:17,439885975-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5323700f-844b-4a85-9624-5accdffdfe54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5323700f-844b-4a85-9624-5accdffdfe54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T16:17:20,107288827-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:17:20,114004142-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5323700f-844b-4a85-9624-5accdffdfe54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5323700f-844b-4a85-9624-5accdffdfe54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T16:17:23,072680581-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:17:23,078967663-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5323700f-844b-4a85-9624-5accdffdfe54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5323700f-844b-4a85-9624-5accdffdfe54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T16:17:25,868946001-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:17:25,875416624-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5323700f-844b-4a85-9624-5accdffdfe54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5323700f-844b-4a85-9624-5accdffdfe54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T16:17:31,289834953-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:17:31,296533405-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5323700f-844b-4a85-9624-5accdffdfe54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5323700f-844b-4a85-9624-5accdffdfe54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T16:17:35,388923929-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:17:35,395258269-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5323700f-844b-4a85-9624-5accdffdfe54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5323700f-844b-4a85-9624-5accdffdfe54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T16:17:38,646492082-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:17:38,652946712-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5323700f-844b-4a85-9624-5accdffdfe54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5323700f-844b-4a85-9624-5accdffdfe54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T16:17:41,940755445-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:17:41,947075487-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5323700f-844b-4a85-9624-5accdffdfe54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5323700f-844b-4a85-9624-5accdffdfe54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:17:45,290679949-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:17:45,297467516-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5323700f-844b-4a85-9624-5accdffdfe54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5323700f-844b-4a85-9624-5accdffdfe54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:17:48,580261926-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:17:48,586682290-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5323700f-844b-4a85-9624-5accdffdfe54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5323700f-844b-4a85-9624-5accdffdfe54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T16:17:51,852359618-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:17:51,858755243-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5323700f-844b-4a85-9624-5accdffdfe54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5323700f-844b-4a85-9624-5accdffdfe54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T16:17:54,667799385-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:17:54,674357698-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5323700f-844b-4a85-9624-5accdffdfe54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5323700f-844b-4a85-9624-5accdffdfe54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T16:17:57,477495509-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:18:20,324526721-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:18:28,317792830-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:18:36,092514568-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:18:44,119302074-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 52s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:18:52,241956628-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 60s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:19:00,132566778-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 82s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:19:07,972968302-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 94s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:19:16,089509917-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [==============..............] (remaining: 46s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:19:24,251723280-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:19:24,267036489-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:19:32,060862227-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:19:32,076650447-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:19:40,096025476-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:19:40,111218574-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:19:47,978700194-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:19:47,994050909-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:19:56,065431313-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:19:56,080995529-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:19:56,093118832-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:19:56,100160091-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:19:56,116405933-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1535381
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T16:19:56,129963946-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-17T16:19:56,170666759-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:19:56,177065713-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5323700f-844b-4a85-9624-5accdffdfe54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5323700f-844b-4a85-9624-5accdffdfe54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6 -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:19:57.630+0000 ffffa35c2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:19:57.642+0000 ffffa35c2010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:19:57.642+0000 ffffa35c2010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:19:57.665326+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-17T23:19:57.665426+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T23:19:58.620339+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:19:58.620339+0000     0       0         0         0         0         0           -           0
2021-05-17T23:19:59.620637+0000     1     146       146         0         0         0           -           0
2021-05-17T23:20:00.621159+0000     2     255       292        37   73.9757        74     1.76437     1.76778
2021-05-17T23:20:01.621632+0000     3     255       434       179   238.577       568     1.79282     1.78105
2021-05-17T23:20:02.621993+0000     4     255       580       325   324.879       584     1.78986     1.78153
2021-05-17T23:20:03.622630+0000     5     255       732       477   381.438       608      1.7199     1.76381
2021-05-17T23:20:04.623421+0000     6     255       880       625   416.464       592     1.71077     1.75089
2021-05-17T23:20:05.624229+0000     7     255      1022       767   438.052       568     1.77015      1.7512
2021-05-17T23:20:06.624667+0000     8     255      1175       920   459.761       612     1.73067     1.75116
2021-05-17T23:20:07.625139+0000     9     255      1324      1069   474.866       596     1.70302     1.74385
2021-05-17T23:20:08.625503+0000    10     255      1459      1204   481.359       540     1.79684     1.74338
2021-05-17T23:20:09.626043+0000    11     255      1609      1354   492.116       600     1.81262     1.74979
2021-05-17T23:20:10.626351+0000    12     255      1757      1502   500.423       592     1.72303     1.75097
2021-05-17T23:20:11.626668+0000    13     255      1908      1653   508.374       604     1.70227     1.74903
2021-05-17T23:20:12.627010+0000    14     255      2055      1800   514.047       588     1.71265      1.7462
2021-05-17T23:20:13.627461+0000    15     255      2202      1947   518.959       588     1.73339     1.74485
2021-05-17T23:20:14.627691+0000    16     255      2352      2097   524.015       600     1.71382     1.74378
2021-05-17T23:20:15.627941+0000    17     255      2493      2238   526.358       564     1.75204     1.74337
2021-05-17T23:20:16.628250+0000    18     255      2646      2391   531.105       612     1.73526     1.74352
2021-05-17T23:20:17.628686+0000    19     255      2796      2541   534.717       600     1.69698     1.74206
2021-05-17T23:20:18.628922+0000 min lat: 0.0338686 max lat: 1.83725 avg lat: 1.66819
2021-05-17T23:20:18.628922+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:20:18.628922+0000    20       8      2945      2937   587.153      1584   0.0338686     1.66819
2021-05-17T23:20:19.629196+0000 Total time run:         20.0457
Total writes made:      2945
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     587.658
Stddev Bandwidth:       290.881
Max bandwidth (MB/sec): 1584
Min bandwidth (MB/sec): 0
Average IOPS:           146
Stddev IOPS:            72.7663
Max IOPS:               396
Min IOPS:               0
Average Latency(s):     1.66381
Stddev Latency(s):      0.283866
Max latency(s):         1.83725
Min latency(s):         0.0338686

[1;32mlocalhost.localdomain	[2021-05-17T16:20:20,084480871-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1535381


[1;33mlocalhost.localdomain	[2021-05-17T16:20:20,092460360-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:20:42,882526212-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.95k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:20:42,898174457-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:20:51,082928646-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.95k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:20:51,099081711-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:20:59,013651009-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.95k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:20:59,029348611-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:21:07,032122063-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.95k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:21:07,047984859-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:21:14,955380358-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.95k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:21:14,970934640-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:21:14,983181128-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:21:14,990489506-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:21:15,007359318-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1538719
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T16:21:15,021418490-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-17T16:21:15,060032792-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:21:15,066268473-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5323700f-844b-4a85-9624-5accdffdfe54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5323700f-844b-4a85-9624-5accdffdfe54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2fctq6:/tmp/ceph-asok.2fctq6 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:21:16.640+0000 ffff85393010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:21:16.648+0000 ffff85393010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:21:16.648+0000 ffff85393010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:21:16.668751+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:21:16.668751+0000     0       0         0         0         0         0           -           0
2021-05-17T23:21:17.669414+0000     1     255       284        29     115.9       116    0.984482    0.930109
2021-05-17T23:21:18.671703+0000     2     255       479       224   447.296       780      1.2562     1.09874
2021-05-17T23:21:19.674104+0000     3     255       670       415   552.312       764     1.33315     1.19999
2021-05-17T23:21:20.675606+0000     4     255       861       606   604.934       764     1.35165     1.24684
2021-05-17T23:21:21.676721+0000     5     255      1049       794   634.164       752     1.34083     1.27043
2021-05-17T23:21:22.676968+0000     6     255      1239       984   655.081       760      1.3371     1.28581
2021-05-17T23:21:23.677277+0000     7     255      1420      1165   664.886       724     1.39772     1.29716
2021-05-17T23:21:24.679467+0000     8     255      1612      1357   677.576       768     1.35559     1.31031
2021-05-17T23:21:25.679714+0000     9     255      1793      1538   682.709       724     1.38063     1.31609
2021-05-17T23:21:26.681302+0000    10     255      1989      1734   692.717       784     1.33869      1.3217
2021-05-17T23:21:27.681822+0000    11     255      2169      1914   695.162       720     1.38788     1.32528
2021-05-17T23:21:28.682972+0000    12     255      2354      2099   698.827       740     1.38298     1.33144
2021-05-17T23:21:29.683174+0000    13     255      2539      2284    701.98       740     1.38906      1.3356
2021-05-17T23:21:30.684627+0000    14     255      2721      2466   703.763       728     1.40971     1.34024
2021-05-17T23:21:31.685147+0000    15     255      2903      2648   705.353       728     1.40114     1.34498
2021-05-17T23:21:32.686550+0000 Total time run:       15.6602
Total reads made:     2945
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   752.227
Average IOPS:         188
Stddev IOPS:          41.1545
Max IOPS:             196
Min IOPS:             29
Average Latency(s):   1.30974
Max latency(s):       1.42682
Min latency(s):       0.409873

[1;32mlocalhost.localdomain	[2021-05-17T16:21:33,135751487-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1538719


[1;33mlocalhost.localdomain	[2021-05-17T16:21:33,143592892-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:21:58,261918312-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.95k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:21:58,277889049-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:22:06,282953256-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.95k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:22:06,298790396-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:22:14,222422509-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.95k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:22:14,238360526-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:22:22,229775713-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.95k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:22:22,245191385-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:22:30,232104468-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.95k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:22:30,247839350-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:22:30,260370855-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T16:22:30,265038812-07:00][RUNNING][ROUND 4/2/20] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:22:30,272547729-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T16:22:30,289285322-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40118\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.485107\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid cc560d05-1bd9-41a9-a95e-3d472ed0ba16\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid cc560d05-1bd9-41a9-a95e-3d472ed0ba16\nlast_changed 2021-05-17T16:22:58.591026-0700\ncreated 2021-05-17T16:22:58.591026-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40118/0,v1:10.10.1.2:40119/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.485107 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 8f797157-9237-4fe9-9c08-cdefad570a61\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 9e8f9c12-803a-4008-8a0b-4e7f268734c6\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 6e5baeba-0e8c-4cbc-8ef7-98d50aaeba8a\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42118\n  w/ user/pass: admin / 39e5c7fc-b92d-4229-9cd9-984b1ec59e6d\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:23:13 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40118
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.485107
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid cc560d05-1bd9-41a9-a95e-3d472ed0ba16
setting min_mon_release = octopus
epoch 0
fsid cc560d05-1bd9-41a9-a95e-3d472ed0ba16
last_changed 2021-05-17T16:22:58.591026-0700
created 2021-05-17T16:22:58.591026-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40118/0,v1:10.10.1.2:40119/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.485107 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 8f797157-9237-4fe9-9c08-cdefad570a61
0
start osd.0
add osd1 9e8f9c12-803a-4008-8a0b-4e7f268734c6
1
start osd.1
add osd2 6e5baeba-0e8c-4cbc-8ef7-98d50aaeba8a
2
start osd.2


restful urls: https://10.10.1.2:42118
  w/ user/pass: admin / 39e5c7fc-b92d-4229-9cd9-984b1ec59e6d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T16:22:31.280-0700 7fe5bfb811c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:22:31.280-0700 7fe5bfb811c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:22:31.296-0700 7f78437441c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:22:31.296-0700 7f78437441c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40118,v1:10.10.1.2:40119] --print /tmp/ceph_monmap.485107 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.485107 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.485107 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42118 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.Wq7iTY3x5t 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8f797157-9237-4fe9-9c08-cdefad570a61 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDb+qJg9nX6FxAADe0cZXmkzBNrqtcHJxhP8g== --osd-uuid 8f797157-9237-4fe9-9c08-cdefad570a61 
2021-05-17T16:23:07.744-0700 7f628c3d5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:23:07.744-0700 7f628c3d5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:23:07.744-0700 7f628c3d5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:23:07.820-0700 7f628c3d5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9e8f9c12-803a-4008-8a0b-4e7f268734c6 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T16:23:08.124-0700 7f3965180f00 -1 Falling back to public interface
2021-05-17T16:23:08.136-0700 7f3965180f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDc+qJga02TBxAAy11/ibCam7Lfp8qV8jj2Pg== --osd-uuid 9e8f9c12-803a-4008-8a0b-4e7f268734c6 
2021-05-17T16:23:08.488-0700 7f76fb007f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:23:08.488-0700 7f76fb007f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:23:08.488-0700 7f76fb007f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:23:08.532-0700 7f76fb007f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6e5baeba-0e8c-4cbc-8ef7-98d50aaeba8a -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T16:23:08.852-0700 7f3d27730f00 -1 Falling back to public interface
2021-05-17T16:23:08.864-0700 7f3d27730f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDc+qJg9SvwMhAAlY/ilw4w2XXIf9sBRp3P2g== --osd-uuid 6e5baeba-0e8c-4cbc-8ef7-98d50aaeba8a 
2021-05-17T16:23:09.180-0700 7f1015ef4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:23:09.184-0700 7f1015ef4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:23:09.184-0700 7f1015ef4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:23:09.236-0700 7f1015ef4f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T16:23:09.640-0700 7fc90c88ef00 -1 Falling back to public interface
2021-05-17T16:23:09.652-0700 7fc90c88ef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T16:23:13,565364363-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:23:13,572826457-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T16:23:13,656163808-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:23:13,662670283-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '30ac6305-8789-47d8-b93f-a0b4bbd1d54d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 30ac6305-8789-47d8-b93f-a0b4bbd1d54d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T16:23:16,513698955-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:23:16,520399969-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '30ac6305-8789-47d8-b93f-a0b4bbd1d54d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 30ac6305-8789-47d8-b93f-a0b4bbd1d54d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T16:23:19,210597068-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:23:19,217132109-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '30ac6305-8789-47d8-b93f-a0b4bbd1d54d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 30ac6305-8789-47d8-b93f-a0b4bbd1d54d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T16:23:22,027946144-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:23:22,034416284-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '30ac6305-8789-47d8-b93f-a0b4bbd1d54d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 30ac6305-8789-47d8-b93f-a0b4bbd1d54d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T16:23:27,419489652-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:23:27,426166977-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '30ac6305-8789-47d8-b93f-a0b4bbd1d54d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 30ac6305-8789-47d8-b93f-a0b4bbd1d54d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T16:23:30,979657868-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:23:30,986100373-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '30ac6305-8789-47d8-b93f-a0b4bbd1d54d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 30ac6305-8789-47d8-b93f-a0b4bbd1d54d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T16:23:34,767271131-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:23:34,773799825-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '30ac6305-8789-47d8-b93f-a0b4bbd1d54d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 30ac6305-8789-47d8-b93f-a0b4bbd1d54d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T16:23:38,082501409-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:23:38,088971969-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '30ac6305-8789-47d8-b93f-a0b4bbd1d54d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 30ac6305-8789-47d8-b93f-a0b4bbd1d54d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:23:41,542452841-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:23:41,549008855-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '30ac6305-8789-47d8-b93f-a0b4bbd1d54d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 30ac6305-8789-47d8-b93f-a0b4bbd1d54d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:23:45,131200584-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:23:45,137654820-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '30ac6305-8789-47d8-b93f-a0b4bbd1d54d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 30ac6305-8789-47d8-b93f-a0b4bbd1d54d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T16:23:48,054105125-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:23:48,060592637-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '30ac6305-8789-47d8-b93f-a0b4bbd1d54d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 30ac6305-8789-47d8-b93f-a0b4bbd1d54d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T16:23:50,705329701-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:23:50,711832125-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '30ac6305-8789-47d8-b93f-a0b4bbd1d54d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 30ac6305-8789-47d8-b93f-a0b4bbd1d54d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T16:23:53,316962153-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:24:16,204983716-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:24:24,243242422-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:24:32,225667885-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:24:40,051311126-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:24:48,116028035-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 57s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:24:56,007387296-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 75s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:25:03,896289864-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 85s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:25:11,966959771-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   228 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [=========...................] (remaining: 104s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:25:19,854725067-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:25:19,870498142-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:25:27,808917667-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:25:27,824784854-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:25:35,759987643-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:25:35,775822154-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:25:43,561338256-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:25:43,578800274-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:25:51,501365678-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:25:51,516832086-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:25:51,529124597-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:25:51,536330927-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:25:51,552620240-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1552523
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T16:25:51,566325932-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-17T16:25:51,607147340-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:25:51,613719166-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '30ac6305-8789-47d8-b93f-a0b4bbd1d54d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 30ac6305-8789-47d8-b93f-a0b4bbd1d54d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2 -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:25:53.215+0000 ffffbb586010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:25:53.223+0000 ffffbb586010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:25:53.223+0000 ffffbb586010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:25:53.244680+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-17T23:25:53.244778+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T23:25:54.215700+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:25:54.215700+0000     0       0         0         0         0         0           -           0
2021-05-17T23:25:55.215937+0000     1     144       144         0         0         0           -           0
2021-05-17T23:25:56.216154+0000     2     255       292        37   73.9892        74     1.71085     1.73024
2021-05-17T23:25:57.216434+0000     3     255       435       180   239.954       572     1.76176     1.74426
2021-05-17T23:25:58.216691+0000     4     255       588       333   332.931       612     1.72338     1.75426
2021-05-17T23:25:59.216949+0000     5     255       743       488   390.315       620     1.63893     1.72838
2021-05-17T23:26:00.217197+0000     6     255       897       642   427.905       616     1.65775      1.7091
2021-05-17T23:26:01.217447+0000     7     255      1041       786   449.041       576     1.74125     1.70896
2021-05-17T23:26:02.218052+0000     8     255      1194       939   469.371       612     1.69936     1.71359
2021-05-17T23:26:03.218325+0000     9     255      1348      1093   485.645       616     1.65445     1.70737
2021-05-17T23:26:04.218672+0000    10     255      1507      1252   500.659       636     1.62533     1.69996
2021-05-17T23:26:05.218945+0000    11     255      1652      1397   507.858       580     1.68392     1.69646
2021-05-17T23:26:06.219227+0000    12     255      1809      1554   517.855       628     1.67724     1.69614
2021-05-17T23:26:07.219689+0000    13     255      1966      1711   526.307       628     1.63677     1.69184
2021-05-17T23:26:08.219893+0000    14     255      2120      1865   532.704       616     1.63379     1.68597
2021-05-17T23:26:09.220268+0000    15     255      2271      2016   537.442       604     1.69116      1.6847
2021-05-17T23:26:10.220658+0000    16     255      2395      2140    534.84       496     1.84103     1.68922
2021-05-17T23:26:11.221001+0000    17     255      2547      2292   539.131       608     1.86584     1.70207
2021-05-17T23:26:12.223112+0000    18     255      2693      2438    541.56       584     1.69033     1.70746
2021-05-17T23:26:13.223513+0000    19     255      2847      2592   545.465       616     1.69582      1.7077
2021-05-17T23:26:14.224281+0000 min lat: 0.0625368 max lat: 1.91459 avg lat: 1.63934
2021-05-17T23:26:14.224281+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:26:14.224281+0000    20       7      2992      2985   596.749      1572   0.0625368     1.63934
2021-05-17T23:26:15.224733+0000 Total time run:         20.0479
Total writes made:      2992
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     596.971
Stddev Bandwidth:       290.014
Max bandwidth (MB/sec): 1572
Min bandwidth (MB/sec): 0
Average IOPS:           149
Stddev IOPS:            72.5506
Max IOPS:               393
Min IOPS:               0
Average Latency(s):     1.63562
Stddev Latency(s):      0.284183
Max latency(s):         1.91459
Min latency(s):         0.0395082

[1;32mlocalhost.localdomain	[2021-05-17T16:26:15,673859162-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1552523


[1;33mlocalhost.localdomain	[2021-05-17T16:26:15,681987848-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:26:38,838461710-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:26:38,854812728-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:26:46,689485227-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:26:46,705606751-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:26:54,779536238-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:26:54,795481572-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:27:02,720915289-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:27:02,737103589-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:27:10,811691495-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:27:10,827727549-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:27:10,840424424-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:27:10,847970426-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:27:10,864705884-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1555886
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T16:27:10,878823411-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-17T16:27:10,918691254-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:27:10,925121020-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '30ac6305-8789-47d8-b93f-a0b4bbd1d54d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 30ac6305-8789-47d8-b93f-a0b4bbd1d54d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.X5oGP2:/tmp/ceph-asok.X5oGP2 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:27:12.385+0000 ffffa3b9f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:27:12.393+0000 ffffa3b9f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:27:12.393+0000 ffffa3b9f010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:27:12.415575+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:27:12.415575+0000     0       0         0         0         0         0           -           0
2021-05-17T23:27:13.415807+0000     1     255       290        35   139.941       140    0.946478     0.91174
2021-05-17T23:27:14.416497+0000     2     255       483       228   455.747       772     1.25257      1.0732
2021-05-17T23:27:15.416814+0000     3     255       677       422   562.399       776     1.30983     1.18266
2021-05-17T23:27:16.417118+0000     4     255       857       602   601.739       720     1.37842     1.23055
2021-05-17T23:27:17.419100+0000     5     255      1026       771   616.342       676     1.50537     1.27852
2021-05-17T23:27:18.419350+0000     6     255      1210       955   636.246       736     1.42379     1.31543
2021-05-17T23:27:19.426369+0000     7     255      1388      1133   646.414       712      1.4274     1.32896
2021-05-17T23:27:20.426627+0000     8     255      1567      1312   655.079       716     1.45193     1.34505
2021-05-17T23:27:21.427797+0000     9     255      1744      1489   660.866       708     1.42938     1.35593
2021-05-17T23:27:22.428117+0000    10     255      1918      1663   664.354       696     1.45822     1.36558
2021-05-17T23:27:23.428532+0000    11     255      2104      1849   671.561       744     1.40879     1.37184
2021-05-17T23:27:24.428778+0000    12     255      2286      2031   676.245       728     1.41058     1.37393
2021-05-17T23:27:25.429094+0000    13     255      2457      2202   676.825       684     1.46917     1.37832
2021-05-17T23:27:26.429531+0000    14     255      2632      2377   678.457       700     1.47214     1.38564
2021-05-17T23:27:27.429860+0000    15     255      2812      2557   681.209       720     1.44409     1.39069
2021-05-17T23:27:28.430163+0000    16     255      2989      2734   682.869       708     1.42299     1.39314
2021-05-17T23:27:29.430532+0000 Total time run:       16.4638
Total reads made:     2992
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   726.929
Average IOPS:         181
Stddev IOPS:          36.878
Max IOPS:             194
Min IOPS:             35
Average Latency(s):   1.35723
Max latency(s):       1.50991
Min latency(s):       0.441452

[1;32mlocalhost.localdomain	[2021-05-17T16:27:29,890177763-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1555886


[1;33mlocalhost.localdomain	[2021-05-17T16:27:29,898531277-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:27:52,899089607-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:27:52,914952801-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:28:00,888250229-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:28:00,904524148-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:28:09,018938983-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:28:09,034937935-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:28:16,957095108-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:28:16,975195333-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:28:25,023325252-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:28:25,039358305-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:28:25,051719872-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T16:28:25,056296417-07:00][RUNNING][ROUND 5/2/20] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:28:25,063565726-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T16:28:25,080468357-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40592\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.486226\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ae8ae557-b833-4cda-b26d-6cf1c2175279\nsetting min_mon_release = octopus\nepoch 0\nfsid ae8ae557-b833-4cda-b26d-6cf1c2175279\nlast_changed 2021-05-17T16:28:54.724969-0700\ncreated 2021-05-17T16:28:54.724969-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40592/0,v1:10.10.1.2:40593/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.486226 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 18f613ab-22c9-44b8-9c41-7ed6039926b5\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 c41071b7-f694-4a1b-8891-963fc4a01d75\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 45fb79fa-c6f1-4437-9df1-bd3f21d0d9ac\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42592\n  w/ user/pass: admin / 0e74bcac-cdbd-4080-9853-30357da79b4c\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:29:09 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40592
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.486226
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ae8ae557-b833-4cda-b26d-6cf1c2175279
setting min_mon_release = octopus
epoch 0
fsid ae8ae557-b833-4cda-b26d-6cf1c2175279
last_changed 2021-05-17T16:28:54.724969-0700
created 2021-05-17T16:28:54.724969-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40592/0,v1:10.10.1.2:40593/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.486226 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 18f613ab-22c9-44b8-9c41-7ed6039926b5
0
start osd.0
add osd1 c41071b7-f694-4a1b-8891-963fc4a01d75
1
start osd.1
add osd2 45fb79fa-c6f1-4437-9df1-bd3f21d0d9ac
2
start osd.2


restful urls: https://10.10.1.2:42592
  w/ user/pass: admin / 0e74bcac-cdbd-4080-9853-30357da79b4c


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T16:28:26.091-0700 7f36009191c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:28:26.091-0700 7f36009191c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:28:26.107-0700 7fd7dd2c91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:28:26.107-0700 7fd7dd2c91c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40592,v1:10.10.1.2:40593] --print /tmp/ceph_monmap.486226 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.486226 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.486226 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42592 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.rBRa1rFtMw 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 18f613ab-22c9-44b8-9c41-7ed6039926b5 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA//KJg6igsIRAAYG0myGJqJba9TTT+SEZ19Q== --osd-uuid 18f613ab-22c9-44b8-9c41-7ed6039926b5 
2021-05-17T16:29:03.895-0700 7f516b657f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:29:03.895-0700 7f516b657f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:29:03.895-0700 7f516b657f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:29:03.967-0700 7f516b657f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c41071b7-f694-4a1b-8891-963fc4a01d75 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T16:29:04.263-0700 7f7315f01f00 -1 Falling back to public interface
2021-05-17T16:29:04.275-0700 7f7315f01f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBA/KJgQ/TQDxAAbyK9+D9XKLUuN1ljc8ALkQ== --osd-uuid c41071b7-f694-4a1b-8891-963fc4a01d75 
2021-05-17T16:29:04.615-0700 7f484b5b8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:29:04.615-0700 7f484b5b8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:29:04.615-0700 7f484b5b8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:29:04.663-0700 7f484b5b8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 45fb79fa-c6f1-4437-9df1-bd3f21d0d9ac -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T16:29:05.079-0700 7fdfdbd7df00 -1 Falling back to public interface
2021-05-17T16:29:05.095-0700 7fdfdbd7df00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBB/KJgbpEWBRAAVewI0/EusJFewIQYeDFMkw== --osd-uuid 45fb79fa-c6f1-4437-9df1-bd3f21d0d9ac 
2021-05-17T16:29:05.419-0700 7f617fc23f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:29:05.419-0700 7f617fc23f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:29:05.419-0700 7f617fc23f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:29:05.499-0700 7f617fc23f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T16:29:05.875-0700 7f8933f3ef00 -1 Falling back to public interface
2021-05-17T16:29:05.891-0700 7f8933f3ef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T16:29:09,769976012-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:29:09,777456557-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T16:29:09,860704560-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:29:09,867284111-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6dfd5882-b45a-4614-8eb2-5d44f8d9aa72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6dfd5882-b45a-4614-8eb2-5d44f8d9aa72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T16:29:12,638800854-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:29:12,645509067-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6dfd5882-b45a-4614-8eb2-5d44f8d9aa72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6dfd5882-b45a-4614-8eb2-5d44f8d9aa72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T16:29:15,469874068-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:29:15,476164475-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6dfd5882-b45a-4614-8eb2-5d44f8d9aa72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6dfd5882-b45a-4614-8eb2-5d44f8d9aa72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T16:29:18,248531345-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:29:18,255276960-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6dfd5882-b45a-4614-8eb2-5d44f8d9aa72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6dfd5882-b45a-4614-8eb2-5d44f8d9aa72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T16:29:23,711960801-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:29:23,718811194-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6dfd5882-b45a-4614-8eb2-5d44f8d9aa72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6dfd5882-b45a-4614-8eb2-5d44f8d9aa72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T16:29:27,787147407-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:29:27,794858497-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6dfd5882-b45a-4614-8eb2-5d44f8d9aa72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6dfd5882-b45a-4614-8eb2-5d44f8d9aa72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T16:29:30,955079442-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:29:30,961734591-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6dfd5882-b45a-4614-8eb2-5d44f8d9aa72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6dfd5882-b45a-4614-8eb2-5d44f8d9aa72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T16:29:34,298948178-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:29:34,305430978-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6dfd5882-b45a-4614-8eb2-5d44f8d9aa72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6dfd5882-b45a-4614-8eb2-5d44f8d9aa72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:29:37,675073951-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:29:37,681640285-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6dfd5882-b45a-4614-8eb2-5d44f8d9aa72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6dfd5882-b45a-4614-8eb2-5d44f8d9aa72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:29:41,030418672-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:29:41,036838686-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6dfd5882-b45a-4614-8eb2-5d44f8d9aa72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6dfd5882-b45a-4614-8eb2-5d44f8d9aa72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T16:29:44,244032900-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:29:44,250650858-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6dfd5882-b45a-4614-8eb2-5d44f8d9aa72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6dfd5882-b45a-4614-8eb2-5d44f8d9aa72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T16:29:46,896473237-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:29:46,902882179-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6dfd5882-b45a-4614-8eb2-5d44f8d9aa72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6dfd5882-b45a-4614-8eb2-5d44f8d9aa72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T16:29:49,813089494-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:30:12,734478651-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [======================......] (remaining: 1s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:30:20,549954319-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:30:28,492674770-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:30:36,487310159-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:30:44,328966433-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:30:52,283225626-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:31:00,357494048-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:31:08,289316953-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   223 KiB used, 300 GiB / 300 GiB avail
    pgs:     1.042% pgs not active
             190 active+clean
             2   activating
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:31:08,305312678-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:31:16,224430072-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:31:16,240535931-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:31:24,221815924-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:31:24,237868089-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:31:32,022332061-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:31:32,038062555-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:31:42,026965603-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:31:42,042879654-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:31:42,055559544-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:31:42,063021260-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:31:42,079639201-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1569319
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T16:31:42,093996917-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-17T16:31:42,133861514-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:31:42,140266315-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6dfd5882-b45a-4614-8eb2-5d44f8d9aa72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6dfd5882-b45a-4614-8eb2-5d44f8d9aa72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:31:43.635+0000 ffff83b22010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:31:43.643+0000 ffff83b22010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:31:43.643+0000 ffff83b22010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:31:43.662680+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-17T23:31:43.662773+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T23:31:44.615419+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:31:44.615419+0000     0       0         0         0         0         0           -           0
2021-05-17T23:31:45.615665+0000     1     141       141         0         0         0           -           0
2021-05-17T23:31:46.615903+0000     2     255       290        35   69.9899        70     1.72491     1.76173
2021-05-17T23:31:47.616165+0000     3     255       437       182   242.622       588      1.7227     1.74245
2021-05-17T23:31:48.616442+0000     4     255       592       337    336.93       620     1.67973     1.73277
2021-05-17T23:31:49.616737+0000     5     255       746       491   392.712       616     1.65667     1.71282
2021-05-17T23:31:50.617051+0000     6     255       906       651   433.896       640     1.60295     1.69524
2021-05-17T23:31:51.617363+0000     7     255      1064       809    462.17       632     1.61451     1.67642
2021-05-17T23:31:52.617867+0000     8     255      1228       973   486.363       656     1.57197     1.66411
2021-05-17T23:31:53.618133+0000     9     255      1371      1116   495.861       572     1.67292     1.66137
2021-05-17T23:31:54.618412+0000    10     255      1525      1270   507.858       616     1.68963     1.66765
2021-05-17T23:31:55.618687+0000    11     255      1671      1416   514.765       584     1.71969     1.67129
2021-05-17T23:31:56.618961+0000    12     255      1813      1558   519.189       568     1.77776     1.67733
2021-05-17T23:31:57.619219+0000    13     255      1962      1707   525.085       596     1.77017     1.68596
2021-05-17T23:31:58.619454+0000    14     255      2120      1865   532.711       632     1.63295     1.68711
2021-05-17T23:31:59.619714+0000    15     255      2265      2010   535.854       580     1.69618      1.6856
2021-05-17T23:32:00.619964+0000    16     255      2412      2157   539.103       588     1.73829     1.69054
2021-05-17T23:32:01.620863+0000    17     255      2563      2308   542.891       604     1.69233     1.69386
2021-05-17T23:32:02.621283+0000    18     255      2710      2455   545.384       588     1.71384     1.69378
2021-05-17T23:32:03.621561+0000    19     255      2856      2601   547.408       584     1.77047     1.69523
2021-05-17T23:32:04.621823+0000 min lat: 1.56515 max lat: 1.80989 avg lat: 1.69823
2021-05-17T23:32:04.621823+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:32:04.621823+0000    20     255      3004      2749   549.629       592     1.74395     1.69823
2021-05-17T23:32:05.622324+0000 Total time run:         20.0556
Total writes made:      3005
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     599.334
Stddev Bandwidth:       176.839
Max bandwidth (MB/sec): 656
Min bandwidth (MB/sec): 0
Average IOPS:           149
Stddev IOPS:            44.2808
Max IOPS:               164
Min IOPS:               0
Average Latency(s):     1.62852
Stddev Latency(s):      0.277635
Max latency(s):         1.80989
Min latency(s):         0.0372493

[1;32mlocalhost.localdomain	[2021-05-17T16:32:06,076736686-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1569319


[1;33mlocalhost.localdomain	[2021-05-17T16:32:06,084736604-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:32:29,144108318-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.01k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:32:29,159915776-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:32:37,171262107-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.01k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:32:37,188505969-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:32:45,137755486-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.01k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:32:45,153732277-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:32:53,030667744-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.01k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:32:53,046663664-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:33:01,139442524-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.01k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:33:01,155517927-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:33:01,168345142-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:33:01,175615155-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:33:01,192732036-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1572718
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T16:33:01,206950242-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-17T16:33:01,246615559-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:33:01,253116631-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6dfd5882-b45a-4614-8eb2-5d44f8d9aa72', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6dfd5882-b45a-4614-8eb2-5d44f8d9aa72 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GJgctk:/tmp/ceph-asok.GJgctk -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:33:02.840+0000 ffffa6af1010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:33:02.848+0000 ffffa6af1010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:33:02.848+0000 ffffa6af1010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:33:02.871325+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:33:02.871325+0000     0       0         0         0         0         0           -           0
2021-05-17T23:33:03.871538+0000     1     255       257         2   7.99699         8    0.996361      0.9953
2021-05-17T23:33:04.874086+0000     2     255       435       180   359.475       712     1.34543     1.20713
2021-05-17T23:33:05.876072+0000     3     255       616       361   480.547       724     1.41745     1.31672
2021-05-17T23:33:06.876549+0000     4     255       810       555   554.254       776     1.33397     1.33871
2021-05-17T23:33:07.876767+0000     5     255       993       738   589.739       732     1.35955     1.33679
2021-05-17T23:33:08.876974+0000     6     255      1168       913   608.078       700     1.45855     1.35194
2021-05-17T23:33:09.877226+0000     7     255      1344      1089   621.747       704     1.45048     1.37002
2021-05-17T23:33:10.877645+0000     8     255      1527      1272   635.485       732     1.39334     1.37899
2021-05-17T23:33:11.877966+0000     9     255      1700      1445   641.737       692     1.44723      1.3845
2021-05-17T23:33:12.878184+0000    10     255      1877      1622   648.345       708     1.44913     1.39235
2021-05-17T23:33:13.878460+0000    11     255      2063      1808   657.019       744     1.40304     1.39629
2021-05-17T23:33:14.879330+0000    12     255      2243      1988   662.216       720     1.39894     1.39462
2021-05-17T23:33:15.880476+0000    13     255      2431      2176   669.059       752     1.39246     1.39539
2021-05-17T23:33:16.880814+0000    14     255      2610      2355   672.394       716     1.41152     1.39468
2021-05-17T23:33:17.881234+0000    15     255      2795      2540   676.879       740     1.40507     1.39557
2021-05-17T23:33:18.881457+0000    16     255      2969      2714   678.064       696      1.4476     1.39757
2021-05-17T23:33:19.881735+0000 Total time run:       16.6431
Total reads made:     3005
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   722.219
Average IOPS:         180
Stddev IOPS:          45.0494
Max IOPS:             194
Min IOPS:             2
Average Latency(s):   1.36119
Max latency(s):       1.48498
Min latency(s):       0.444641

[1;32mlocalhost.localdomain	[2021-05-17T16:33:20,359513558-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1572718


[1;33mlocalhost.localdomain	[2021-05-17T16:33:20,367801225-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:33:43,311550569-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.01k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:33:43,327635109-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:33:51,250256176-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.01k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:33:51,266587838-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:33:59,194397880-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.01k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:33:59,210861444-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:34:07,593471329-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.01k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:34:07,609463378-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:34:15,548995388-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.01k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:34:15,566485306-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:34:15,580163571-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T16:34:15,588955833-07:00][RUNNING][ROUND 1/3/20] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:34:15,596620114-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T16:34:15,614456921-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40667\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.487337\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8b97a1a9-4c5c-496a-a2f0-3b90a879f3c6\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 8b97a1a9-4c5c-496a-a2f0-3b90a879f3c6\nlast_changed 2021-05-17T16:34:48.612944-0700\ncreated 2021-05-17T16:34:48.612944-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40667/0,v1:10.10.1.2:40668/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.487337 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 4508ba00-17f0-4816-9fe6-22a0421d0c6d\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 7620913f-2cf4-44ba-8e9f-dc6d73cf684b\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 233d65e0-24ba-4973-a807-d4f793a5c522\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42667\n  w/ user/pass: admin / 8383eb2c-76bc-49e5-8ecf-c9e4614b4f5d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:35:03 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40667
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.487337
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8b97a1a9-4c5c-496a-a2f0-3b90a879f3c6
setting min_mon_release = octopus
epoch 0
fsid 8b97a1a9-4c5c-496a-a2f0-3b90a879f3c6
last_changed 2021-05-17T16:34:48.612944-0700
created 2021-05-17T16:34:48.612944-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40667/0,v1:10.10.1.2:40668/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.487337 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 4508ba00-17f0-4816-9fe6-22a0421d0c6d
0
start osd.0
add osd1 7620913f-2cf4-44ba-8e9f-dc6d73cf684b
1
start osd.1
add osd2 233d65e0-24ba-4973-a807-d4f793a5c522
2
start osd.2


restful urls: https://10.10.1.2:42667
  w/ user/pass: admin / 8383eb2c-76bc-49e5-8ecf-c9e4614b4f5d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T16:34:16.642-0700 7f067e7281c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:34:16.642-0700 7f067e7281c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:34:16.658-0700 7f1673b311c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:34:16.658-0700 7f1673b311c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40667,v1:10.10.1.2:40668] --print /tmp/ceph_monmap.487337 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.487337 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.487337 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42667 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.5PfAI6P2ha 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4508ba00-17f0-4816-9fe6-22a0421d0c6d -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCh/aJgbix1IxAATdg/ZZu+FGxvW4URN8WhjQ== --osd-uuid 4508ba00-17f0-4816-9fe6-22a0421d0c6d 
2021-05-17T16:34:57.947-0700 7f060f65af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:34:57.947-0700 7f060f65af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:34:57.947-0700 7f060f65af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:34:58.007-0700 7f060f65af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7620913f-2cf4-44ba-8e9f-dc6d73cf684b -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T16:34:58.355-0700 7fe322f2cf00 -1 Falling back to public interface
2021-05-17T16:34:58.367-0700 7fe322f2cf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCi/aJgvEwSFRAA6bdu7SpnnWozcYB+MP0Pdw== --osd-uuid 7620913f-2cf4-44ba-8e9f-dc6d73cf684b 
2021-05-17T16:34:58.691-0700 7f32610d5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:34:58.691-0700 7f32610d5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:34:58.691-0700 7f32610d5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:34:58.811-0700 7f32610d5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 233d65e0-24ba-4973-a807-d4f793a5c522 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T16:34:59.155-0700 7f78b8640f00 -1 Falling back to public interface
2021-05-17T16:34:59.171-0700 7f78b8640f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCj/aJgtrtCCRAAirhwhz8NQv89S5qbN9JtOg== --osd-uuid 233d65e0-24ba-4973-a807-d4f793a5c522 
2021-05-17T16:34:59.503-0700 7f382fe17f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:34:59.503-0700 7f382fe17f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:34:59.503-0700 7f382fe17f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:34:59.567-0700 7f382fe17f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T16:34:59.851-0700 7ff414c98f00 -1 Falling back to public interface
2021-05-17T16:34:59.867-0700 7ff414c98f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T16:35:03,842243032-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:35:03,849907912-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T16:35:03,933311675-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:35:03,939648484-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef583f53-cba8-4c62-9442-aeec9e689fdc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef583f53-cba8-4c62-9442-aeec9e689fdc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T16:35:06,660277387-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:35:06,666787040-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef583f53-cba8-4c62-9442-aeec9e689fdc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef583f53-cba8-4c62-9442-aeec9e689fdc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T16:35:09,579278573-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:35:09,585466047-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef583f53-cba8-4c62-9442-aeec9e689fdc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef583f53-cba8-4c62-9442-aeec9e689fdc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T16:35:12,318513910-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:35:12,325158421-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef583f53-cba8-4c62-9442-aeec9e689fdc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef583f53-cba8-4c62-9442-aeec9e689fdc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T16:35:17,963303454-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:35:17,969927502-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef583f53-cba8-4c62-9442-aeec9e689fdc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef583f53-cba8-4c62-9442-aeec9e689fdc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T16:35:21,821297598-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:35:21,827718482-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef583f53-cba8-4c62-9442-aeec9e689fdc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef583f53-cba8-4c62-9442-aeec9e689fdc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T16:35:25,386200823-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:35:25,392617116-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef583f53-cba8-4c62-9442-aeec9e689fdc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef583f53-cba8-4c62-9442-aeec9e689fdc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T16:35:28,669576109-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:35:28,676056433-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef583f53-cba8-4c62-9442-aeec9e689fdc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef583f53-cba8-4c62-9442-aeec9e689fdc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:35:32,097649953-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:35:32,104043157-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef583f53-cba8-4c62-9442-aeec9e689fdc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef583f53-cba8-4c62-9442-aeec9e689fdc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:35:36,017546034-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:35:36,023746230-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef583f53-cba8-4c62-9442-aeec9e689fdc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef583f53-cba8-4c62-9442-aeec9e689fdc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T16:35:39,230579583-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:35:39,237196280-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef583f53-cba8-4c62-9442-aeec9e689fdc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef583f53-cba8-4c62-9442-aeec9e689fdc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T16:35:42,038336652-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:35:42,045032607-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef583f53-cba8-4c62-9442-aeec9e689fdc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef583f53-cba8-4c62-9442-aeec9e689fdc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T16:35:44,715755020-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:36:07,500653187-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=========...................] (remaining: 9s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:36:15,696043164-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:36:23,596644540-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:36:31,575872054-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:36:39,700212423-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:36:47,839648363-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:36:55,793671049-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:37:03,621123920-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:37:03,637397476-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:37:11,735291241-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:37:11,752093232-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:37:19,887258744-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:37:19,903353229-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:37:27,735560221-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:37:27,752814704-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:37:35,568100857-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:37:35,584200384-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:37:35,597065524-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:37:35,604541192-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:37:35,621654275-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1586190
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T16:37:35,636192051-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T16:37:35,677806436-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:37:35,684319645-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef583f53-cba8-4c62-9442-aeec9e689fdc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef583f53-cba8-4c62-9442-aeec9e689fdc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:37:37.263+0000 ffffa4af6010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:37:37.271+0000 ffffa4af6010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:37:37.271+0000 ffffa4af6010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:37:37.301009+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-17T23:37:37.301127+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T23:37:41.298882+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:37:41.298882+0000     0       0         0         0         0         0           -           0
2021-05-17T23:37:42.299117+0000     1      23        23         0         0         0           -           0
2021-05-17T23:37:43.299363+0000     2      46        46         0         0         0           -           0
2021-05-17T23:37:44.299591+0000     3      68        68         0         0         0           -           0
2021-05-17T23:37:45.299828+0000     4      90        90         0         0         0           -           0
2021-05-17T23:37:46.300056+0000     5     114       114         0         0         0           -           0
2021-05-17T23:37:47.300271+0000     6     137       137         0         0         0           -           0
2021-05-17T23:37:48.300489+0000     7     160       160         0         0         0           -           0
2021-05-17T23:37:49.300718+0000     8     182       182         0         0         0           -           0
2021-05-17T23:37:50.300933+0000     9     205       205         0         0         0           -           0
2021-05-17T23:37:51.301150+0000    10     228       228         0         0         0           -           0
2021-05-17T23:37:52.301367+0000    11     251       251         0         0         0           -           0
2021-05-17T23:37:53.301578+0000    12     255       274        19    25.328   25.3333      11.184     11.1976
2021-05-17T23:37:54.301808+0000    13     255       298        43   52.9118       384     11.1838     11.1964
2021-05-17T23:37:55.302036+0000    14     255       320        65   74.2698       352     11.1853     11.1869
2021-05-17T23:37:56.302265+0000    15     255       343        88   93.8464       368     11.1405     11.1903
2021-05-17T23:37:57.302512+0000    16     255       365       110   109.976       352     11.1959     11.1923
2021-05-17T23:37:58.302776+0000    17     255       387       132   124.208       352      11.284     11.2058
2021-05-17T23:37:59.303037+0000    18     255       409       154   136.858       352        11.3     11.2181
2021-05-17T23:38:00.303276+0000    19     255       431       176   148.177       352     11.2968     11.2286
2021-05-17T23:38:01.303479+0000 min lat: 0.102215 max lat: 11.3321 avg lat: 8.15141
2021-05-17T23:38:01.303479+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:38:01.303479+0000    20       1       454       453   362.319      4432    0.102215     8.15141
2021-05-17T23:38:02.303786+0000 Total time run:         20.0843
Total writes made:      454
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     361.676
Stddev Bandwidth:       976.489
Max bandwidth (MB/sec): 4432
Min bandwidth (MB/sec): 0
Average IOPS:           22
Stddev IOPS:            61.0409
Max IOPS:               277
Min IOPS:               0
Average Latency(s):     8.13382
Stddev Latency(s):      3.6756
Max latency(s):         11.3321
Min latency(s):         0.102215

[1;32mlocalhost.localdomain	[2021-05-17T16:38:03,192404571-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1586190


[1;33mlocalhost.localdomain	[2021-05-17T16:38:03,200652435-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:38:26,144361187-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 455 objects, 7.1 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:38:26,160820849-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:38:34,233582296-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 455 objects, 7.1 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:38:34,249807664-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:38:42,153992750-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 455 objects, 7.1 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:38:42,174274538-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:38:50,105385777-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 455 objects, 7.1 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:38:50,121650166-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:38:58,185499418-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 455 objects, 7.1 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:38:58,201663215-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:38:58,214779513-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:38:58,222262396-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:38:58,239383176-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1589591
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T16:38:58,253535370-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-17T16:38:58,293232084-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:38:58,299716668-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef583f53-cba8-4c62-9442-aeec9e689fdc', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef583f53-cba8-4c62-9442-aeec9e689fdc --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EtA4On:/tmp/ceph-asok.EtA4On -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:39:00.065+0000 ffffbf4a8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:39:00.073+0000 ffffbf4a8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:39:00.073+0000 ffffbf4a8010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:39:00.103806+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:39:00.103806+0000     0       0         0         0         0         0           -           0
2021-05-17T23:39:01.104025+0000     1      75        75         0         0         0           -           0
2021-05-17T23:39:02.104275+0000     2     142       142         0         0         0           -           0
2021-05-17T23:39:03.104502+0000     3     202       202         0         0         0           -           0
2021-05-17T23:39:04.109046+0000     4     255       267        12    47.935        48      3.9135     3.86724
2021-05-17T23:39:05.117469+0000     5     255       314        59   188.279       752     4.24411     4.03111
2021-05-17T23:39:06.119287+0000     6     255       361       106   281.931       752     4.54091     4.19079
2021-05-17T23:39:07.119552+0000     7     255       406       151   344.359       720      4.8193     4.35245
2021-05-17T23:39:08.251836+0000     8     230       454       224   439.851      1168     4.83694     4.54533
2021-05-17T23:39:09.252458+0000     9      55       454       399   697.794      2800     2.31533     4.16109
2021-05-17T23:39:10.252735+0000 Total time run:       9.46207
Total reads made:     454
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   767.696
Average IOPS:         47
Stddev IOPS:          56.4646
Max IOPS:             175
Min IOPS:             0
Average Latency(s):   3.88732
Max latency(s):       5.04453
Min latency(s):       1.49937

[1;32mlocalhost.localdomain	[2021-05-17T16:39:11,189443764-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1589591


[1;33mlocalhost.localdomain	[2021-05-17T16:39:11,197623891-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:39:34,096595299-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 455 objects, 7.1 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:39:34,112933500-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:39:42,116204737-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 455 objects, 7.1 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:39:42,132214427-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:39:50,256638598-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 455 objects, 7.1 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:39:50,274826435-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:39:58,223254765-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 455 objects, 7.1 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:39:58,240044303-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:40:06,097842466-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 455 objects, 7.1 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:40:06,114374650-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:40:06,127537471-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T16:40:06,132194444-07:00][RUNNING][ROUND 2/3/20] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:40:06,139628149-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T16:40:06,156734886-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40095\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.488447\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c7bd865d-cfbe-4b41-9d9d-b027d3675cab\nsetting min_mon_release = octopus\nepoch 0\nfsid c7bd865d-cfbe-4b41-9d9d-b027d3675cab\nlast_changed 2021-05-17T16:40:33.079896-0700\ncreated 2021-05-17T16:40:33.079896-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40095/0,v1:10.10.1.2:40096/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.488447 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 e99bfc1f-330d-4372-9be2-3a0b85f941a9\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 c055994e-4e53-4029-9fec-9e87db2a4144\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 e457b934-024a-4b6f-a28c-198c78655132\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42095\n  w/ user/pass: admin / 247648b8-c675-48fa-ba99-91ea7f7d9344\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:40:48 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40095
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.488447
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c7bd865d-cfbe-4b41-9d9d-b027d3675cab
setting min_mon_release = octopus
epoch 0
fsid c7bd865d-cfbe-4b41-9d9d-b027d3675cab
last_changed 2021-05-17T16:40:33.079896-0700
created 2021-05-17T16:40:33.079896-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40095/0,v1:10.10.1.2:40096/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.488447 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 e99bfc1f-330d-4372-9be2-3a0b85f941a9
0
start osd.0
add osd1 c055994e-4e53-4029-9fec-9e87db2a4144
1
start osd.1
add osd2 e457b934-024a-4b6f-a28c-198c78655132
2
start osd.2


restful urls: https://10.10.1.2:42095
  w/ user/pass: admin / 247648b8-c675-48fa-ba99-91ea7f7d9344


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T16:40:07.161-0700 7ff8df8fb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:40:07.161-0700 7ff8df8fb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:40:07.177-0700 7f050d8401c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:40:07.177-0700 7f050d8401c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40095,v1:10.10.1.2:40096] --print /tmp/ceph_monmap.488447 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.488447 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.488447 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42095 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.ner85D6653 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e99bfc1f-330d-4372-9be2-3a0b85f941a9 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD6/qJgrgmIKxAAE8ogtnFglbuHfToIt0r1JA== --osd-uuid e99bfc1f-330d-4372-9be2-3a0b85f941a9 
2021-05-17T16:40:43.062-0700 7f923a0b7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:40:43.062-0700 7f923a0b7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:40:43.062-0700 7f923a0b7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:40:43.186-0700 7f923a0b7f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c055994e-4e53-4029-9fec-9e87db2a4144 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T16:40:43.486-0700 7f7eb3090f00 -1 Falling back to public interface
2021-05-17T16:40:43.498-0700 7f7eb3090f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD7/qJg83QdHRAAGawOZg9MKoAtGduwcWyFlQ== --osd-uuid c055994e-4e53-4029-9fec-9e87db2a4144 
2021-05-17T16:40:43.834-0700 7f6c9063cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:40:43.834-0700 7f6c9063cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:40:43.834-0700 7f6c9063cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:40:43.914-0700 7f6c9063cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e457b934-024a-4b6f-a28c-198c78655132 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T16:40:44.290-0700 7f4bbb485f00 -1 Falling back to public interface
2021-05-17T16:40:44.306-0700 7f4bbb485f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD8/qJg+ET6DhAA5ASkjYWOy/YZm8CpkhgEEA== --osd-uuid e457b934-024a-4b6f-a28c-198c78655132 
2021-05-17T16:40:44.626-0700 7fdcc3878f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:40:44.626-0700 7fdcc3878f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:40:44.626-0700 7fdcc3878f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:40:44.682-0700 7fdcc3878f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T16:40:44.970-0700 7f281246df00 -1 Falling back to public interface
2021-05-17T16:40:44.986-0700 7f281246df00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T16:40:48,948756915-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:40:48,956301279-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T16:40:49,041462766-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:40:49,047898892-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df07fcf-7e5c-499f-8133-1e07ba4cbf2e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df07fcf-7e5c-499f-8133-1e07ba4cbf2e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T16:40:51,872521625-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:40:51,879073751-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df07fcf-7e5c-499f-8133-1e07ba4cbf2e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df07fcf-7e5c-499f-8133-1e07ba4cbf2e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T16:40:54,558431375-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:40:54,565039550-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df07fcf-7e5c-499f-8133-1e07ba4cbf2e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df07fcf-7e5c-499f-8133-1e07ba4cbf2e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T16:40:57,214603392-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:40:57,222289110-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df07fcf-7e5c-499f-8133-1e07ba4cbf2e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df07fcf-7e5c-499f-8133-1e07ba4cbf2e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T16:41:02,637923654-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:41:02,644734843-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df07fcf-7e5c-499f-8133-1e07ba4cbf2e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df07fcf-7e5c-499f-8133-1e07ba4cbf2e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T16:41:05,500489716-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:41:05,507059859-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df07fcf-7e5c-499f-8133-1e07ba4cbf2e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df07fcf-7e5c-499f-8133-1e07ba4cbf2e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T16:41:08,791070056-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:41:08,797788755-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df07fcf-7e5c-499f-8133-1e07ba4cbf2e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df07fcf-7e5c-499f-8133-1e07ba4cbf2e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T16:41:12,075626930-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:41:12,081934618-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df07fcf-7e5c-499f-8133-1e07ba4cbf2e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df07fcf-7e5c-499f-8133-1e07ba4cbf2e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:41:15,467347434-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:41:15,473586111-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df07fcf-7e5c-499f-8133-1e07ba4cbf2e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df07fcf-7e5c-499f-8133-1e07ba4cbf2e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:41:18,900930417-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:41:18,907241091-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df07fcf-7e5c-499f-8133-1e07ba4cbf2e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df07fcf-7e5c-499f-8133-1e07ba4cbf2e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T16:41:22,196229820-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:41:22,202676627-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df07fcf-7e5c-499f-8133-1e07ba4cbf2e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df07fcf-7e5c-499f-8133-1e07ba4cbf2e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T16:41:24,836662738-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:41:24,842904595-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df07fcf-7e5c-499f-8133-1e07ba4cbf2e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df07fcf-7e5c-499f-8133-1e07ba4cbf2e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  146 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  146 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   44 KiB   0 B   0 B   0 B  100 GiB     0  0.90   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.05   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.05   70      up          osd.2  
                       TOTAL  300 GiB  148 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.90/1.05  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T16:41:27,445492250-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:41:50,371895973-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   195 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (5s)
      [=====.......................] (remaining: 19s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:41:58,208669604-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=========================...] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:42:06,234393821-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:42:16,046028638-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:42:24,142784335-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:42:32,114005601-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:42:40,301977664-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:42:48,234682739-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:42:48,253404906-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:42:56,245835141-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:42:56,262239429-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:43:04,377065410-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:43:04,393766478-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:43:12,455343019-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:43:12,472169703-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:43:20,440000440-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:43:20,456679874-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:43:20,470053616-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:43:20,477758522-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:43:20,495330053-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1602848
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T16:43:20,509888891-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T16:43:20,549966218-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:43:20,556220662-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df07fcf-7e5c-499f-8133-1e07ba4cbf2e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df07fcf-7e5c-499f-8133-1e07ba4cbf2e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:43:22.090+0000 ffff8f89f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:43:22.110+0000 ffff8f89f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:43:22.110+0000 ffff8f89f010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:43:22.137637+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-17T23:43:22.137756+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T23:43:26.235888+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:43:26.235888+0000     0       0         0         0         0         0           -           0
2021-05-17T23:43:27.236142+0000     1      20        20         0         0         0           -           0
2021-05-17T23:43:28.236629+0000     2      41        41         0         0         0           -           0
2021-05-17T23:43:29.236844+0000     3      61        61         0         0         0           -           0
2021-05-17T23:43:30.237200+0000     4      81        81         0         0         0           -           0
2021-05-17T23:43:31.237421+0000     5     100       100         0         0         0           -           0
2021-05-17T23:43:32.237637+0000     6     121       121         0         0         0           -           0
2021-05-17T23:43:33.237844+0000     7     142       142         0         0         0           -           0
2021-05-17T23:43:34.238088+0000     8     163       163         0         0         0           -           0
2021-05-17T23:43:35.238310+0000     9     184       184         0         0         0           -           0
2021-05-17T23:43:36.238596+0000    10     204       204         0         0         0           -           0
2021-05-17T23:43:37.238809+0000    11     225       225         0         0         0           -           0
2021-05-17T23:43:38.239046+0000    12     247       247         0         0         0           -           0
2021-05-17T23:43:39.239423+0000    13     255       269        14   17.2263   17.2308     12.3809     12.4088
2021-05-17T23:43:40.239701+0000    14     255       289        34    38.847       320     12.4583     12.4101
2021-05-17T23:43:41.239968+0000    15     255       311        56   59.7178       352     12.3303     12.3886
2021-05-17T23:43:42.240268+0000    16     255       331        76     75.98       320     12.3175     12.3714
2021-05-17T23:43:43.240462+0000    17     255       352        97   91.2705       336     12.2147     12.3451
2021-05-17T23:43:44.240693+0000    18     255       373       118   104.862       336     12.1604     12.3136
2021-05-17T23:43:45.240949+0000    19     255       395       140   117.864       352      12.142     12.2897
2021-05-17T23:43:46.241181+0000 min lat: 12.1228 max lat: 12.4763 avg lat: 12.276
2021-05-17T23:43:46.241181+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:43:46.241181+0000    20     255       415       160   127.967       320     12.1859      12.276
2021-05-17T23:43:47.241476+0000 Total time run:         20.0987
Total writes made:      416
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     331.165
Stddev Bandwidth:       162.903
Max bandwidth (MB/sec): 352
Min bandwidth (MB/sec): 0
Average IOPS:           20
Stddev IOPS:            10.184
Max IOPS:               22
Min IOPS:               0
Average Latency(s):     8.48524
Stddev Latency(s):      4.07625
Max latency(s):         12.4763
Min latency(s):         0.128623

[1;32mlocalhost.localdomain	[2021-05-17T16:43:48,127169356-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1602848


[1;33mlocalhost.localdomain	[2021-05-17T16:43:48,135332021-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:44:11,175465390-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 417 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:44:11,192268325-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:44:19,196140642-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 417 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:44:19,212804984-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:44:27,138745916-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 417 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:44:27,155686259-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:44:35,397181324-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 417 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:44:35,413782730-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:44:43,397884710-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 417 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:44:43,416805121-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:44:43,432303058-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:44:43,441394366-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:44:43,462641012-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1606299
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T16:44:43,478707247-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T16:44:43,519546342-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:44:43,525831467-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4df07fcf-7e5c-499f-8133-1e07ba4cbf2e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4df07fcf-7e5c-499f-8133-1e07ba4cbf2e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5cmPtC:/tmp/ceph-asok.5cmPtC -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:44:45.004+0000 ffffa6779010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:44:45.012+0000 ffffa6779010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:44:45.012+0000 ffffa6779010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:44:45.044046+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:44:45.044046+0000     0       0         0         0         0         0           -           0
2021-05-17T23:44:46.044315+0000     1      74        74         0         0         0           -           0
2021-05-17T23:44:47.044597+0000     2     141       141         0         0         0           -           0
2021-05-17T23:44:48.044863+0000     3     210       210         0         0         0           -           0
2021-05-17T23:44:49.045126+0000     4     255       275        20   79.9748        80     3.79281     3.73813
2021-05-17T23:44:50.045373+0000     5     255       324        69   220.733       784     4.08708     3.89852
2021-05-17T23:44:51.045998+0000     6     255       368       113   301.226       704       4.378     4.02793
2021-05-17T23:44:52.658601+0000     7     152       416       264   554.714      2416     3.81456     4.22941
2021-05-17T23:44:53.658923+0000 Total time run:       8.47141
Total reads made:     416
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   785.702
Average IOPS:         49
Stddev IOPS:          55.2354
Max IOPS:             151
Min IOPS:             0
Average Latency(s):   3.66176
Max latency(s):       4.71901
Min latency(s):       1.5196

[1;32mlocalhost.localdomain	[2021-05-17T16:44:54,607163150-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1606299


[1;33mlocalhost.localdomain	[2021-05-17T16:44:54,615531184-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:45:17,585341750-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 417 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:45:17,604596748-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:45:25,650608747-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 417 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:45:25,666920393-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:45:33,704825923-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 417 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:45:33,721533232-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:45:41,593754997-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 417 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:45:41,610447693-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:45:49,866924123-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 417 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:45:49,883414270-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:45:49,896631189-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T16:45:49,901596269-07:00][RUNNING][ROUND 3/3/20] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:45:49,909164757-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T16:45:49,925893294-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40961\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.492555\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 7eab9a1b-67be-44df-a1b6-df00efdc9f0b\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 7eab9a1b-67be-44df-a1b6-df00efdc9f0b\nlast_changed 2021-05-17T16:46:16.834000-0700\ncreated 2021-05-17T16:46:16.834000-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40961/0,v1:10.10.1.2:40962/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.492555 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 9b32724a-f366-41a7-be5d-c89022d2829a\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 166779a6-39ec-4530-b8ec-e217d6bfd9fe\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 41b5dba2-a09c-43e1-9b65-f7e342ae14bf\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42961\n  w/ user/pass: admin / dc563755-3ef2-4495-80c1-e283458dcf42\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:46:31 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40961
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.492555
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 7eab9a1b-67be-44df-a1b6-df00efdc9f0b
setting min_mon_release = octopus
epoch 0
fsid 7eab9a1b-67be-44df-a1b6-df00efdc9f0b
last_changed 2021-05-17T16:46:16.834000-0700
created 2021-05-17T16:46:16.834000-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40961/0,v1:10.10.1.2:40962/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.492555 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 9b32724a-f366-41a7-be5d-c89022d2829a
0
start osd.0
add osd1 166779a6-39ec-4530-b8ec-e217d6bfd9fe
1
start osd.1
add osd2 41b5dba2-a09c-43e1-9b65-f7e342ae14bf
2
start osd.2


restful urls: https://10.10.1.2:42961
  w/ user/pass: admin / dc563755-3ef2-4495-80c1-e283458dcf42


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T16:45:50.949-0700 7f8bb49d61c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:45:50.949-0700 7f8bb49d61c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:45:50.965-0700 7f295f3661c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:45:50.965-0700 7f295f3661c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40961,v1:10.10.1.2:40962] --print /tmp/ceph_monmap.492555 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.492555 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.492555 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42961 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.bH8gfARs81 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9b32724a-f366-41a7-be5d-c89022d2829a -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBRAKNga3NULBAAqcBsfRub9XDynq2bRwlL7Q== --osd-uuid 9b32724a-f366-41a7-be5d-c89022d2829a 
2021-05-17T16:46:26.093-0700 7fdc6f5d6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:46:26.093-0700 7fdc6f5d6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:46:26.093-0700 7fdc6f5d6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:46:26.173-0700 7fdc6f5d6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 166779a6-39ec-4530-b8ec-e217d6bfd9fe -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T16:46:26.449-0700 7fedace1cf00 -1 Falling back to public interface
2021-05-17T16:46:26.461-0700 7fedace1cf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBSAKNgthDfGhAAY/sHvf149wm3p1mGLxp0jg== --osd-uuid 166779a6-39ec-4530-b8ec-e217d6bfd9fe 
2021-05-17T16:46:26.805-0700 7ff645ae2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:46:26.805-0700 7ff645ae2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:46:26.805-0700 7ff645ae2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:46:26.841-0700 7ff645ae2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 41b5dba2-a09c-43e1-9b65-f7e342ae14bf -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T16:46:27.121-0700 7fc1a2745f00 -1 Falling back to public interface
2021-05-17T16:46:27.133-0700 7fc1a2745f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBTAKNg2zpFBxAAn3HQrEDrRNsFABqhpT8ITA== --osd-uuid 41b5dba2-a09c-43e1-9b65-f7e342ae14bf 
2021-05-17T16:46:27.473-0700 7f4e33f1cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:46:27.473-0700 7f4e33f1cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:46:27.473-0700 7f4e33f1cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:46:27.537-0700 7f4e33f1cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T16:46:27.949-0700 7f90adedbf00 -1 Falling back to public interface
2021-05-17T16:46:27.961-0700 7f90adedbf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T16:46:31,857005398-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:46:31,864621144-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T16:46:31,949410790-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:46:31,955673951-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '26f77d2e-8995-4fd5-92ca-94901c976a37', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 26f77d2e-8995-4fd5-92ca-94901c976a37 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T16:46:34,727901552-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:46:34,735616870-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '26f77d2e-8995-4fd5-92ca-94901c976a37', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 26f77d2e-8995-4fd5-92ca-94901c976a37 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T16:46:37,500515858-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:46:37,507164000-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '26f77d2e-8995-4fd5-92ca-94901c976a37', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 26f77d2e-8995-4fd5-92ca-94901c976a37 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T16:46:40,517765834-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:46:40,524296791-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '26f77d2e-8995-4fd5-92ca-94901c976a37', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 26f77d2e-8995-4fd5-92ca-94901c976a37 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T16:46:46,059519328-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:46:46,066149646-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '26f77d2e-8995-4fd5-92ca-94901c976a37', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 26f77d2e-8995-4fd5-92ca-94901c976a37 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T16:46:49,274927572-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:46:49,281591286-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '26f77d2e-8995-4fd5-92ca-94901c976a37', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 26f77d2e-8995-4fd5-92ca-94901c976a37 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T16:46:52,560179678-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:46:52,566644428-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '26f77d2e-8995-4fd5-92ca-94901c976a37', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 26f77d2e-8995-4fd5-92ca-94901c976a37 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T16:46:55,930759615-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:46:55,937159224-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '26f77d2e-8995-4fd5-92ca-94901c976a37', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 26f77d2e-8995-4fd5-92ca-94901c976a37 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:46:59,649161208-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:46:59,656180723-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '26f77d2e-8995-4fd5-92ca-94901c976a37', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 26f77d2e-8995-4fd5-92ca-94901c976a37 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:47:02,926842858-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:47:02,933291436-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '26f77d2e-8995-4fd5-92ca-94901c976a37', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 26f77d2e-8995-4fd5-92ca-94901c976a37 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T16:47:06,352292861-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:47:06,358726586-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '26f77d2e-8995-4fd5-92ca-94901c976a37', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 26f77d2e-8995-4fd5-92ca-94901c976a37 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T16:47:09,008457718-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:47:09,015026391-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '26f77d2e-8995-4fd5-92ca-94901c976a37', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 26f77d2e-8995-4fd5-92ca-94901c976a37 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T16:47:11,543678936-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:47:34,476338468-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   188 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:47:42,442016804-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:47:50,534972927-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:47:58,562120992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:48:06,528365993-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:48:14,628719128-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:48:22,817632152-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:48:30,755842216-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   227 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:48:30,772785274-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:48:38,768887817-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:48:38,785926366-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:48:46,608171136-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:48:46,625073112-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:48:54,822268679-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:48:54,838939230-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:49:02,762767782-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:49:02,779631011-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:49:02,793031008-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:49:02,800816325-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:49:02,818241026-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1619514
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T16:49:02,832796254-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-17T16:49:02,873596800-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:49:02,880101959-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '26f77d2e-8995-4fd5-92ca-94901c976a37', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 26f77d2e-8995-4fd5-92ca-94901c976a37 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:49:04.420+0000 ffff9386a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:49:04.428+0000 ffff9386a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:49:04.428+0000 ffff9386a010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:49:04.457789+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-17T23:49:04.457905+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T23:49:08.304148+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:49:08.304148+0000     0       0         0         0         0         0           -           0
2021-05-17T23:49:09.304401+0000     1      22        22         0         0         0           -           0
2021-05-17T23:49:10.304634+0000     2      43        43         0         0         0           -           0
2021-05-17T23:49:11.304875+0000     3      64        64         0         0         0           -           0
2021-05-17T23:49:12.305107+0000     4      85        85         0         0         0           -           0
2021-05-17T23:49:13.305564+0000     5     106       106         0         0         0           -           0
2021-05-17T23:49:14.305834+0000     6     127       127         0         0         0           -           0
2021-05-17T23:49:15.306147+0000     7     148       148         0         0         0           -           0
2021-05-17T23:49:16.306385+0000     8     171       171         0         0         0           -           0
2021-05-17T23:49:17.306940+0000     9     191       191         0         0         0           -           0
2021-05-17T23:49:18.307181+0000    10     211       211         0         0         0           -           0
2021-05-17T23:49:19.307641+0000    11     234       234         0         0         0           -           0
2021-05-17T23:49:20.307949+0000    12     254       254         0         0         0           -           0
2021-05-17T23:49:21.308170+0000    13     255       275        20   24.6081   24.6154     12.1231      12.121
2021-05-17T23:49:22.308397+0000    14     255       297        42    47.986       352     12.0802     12.1278
2021-05-17T23:49:23.308633+0000    15     255       319        64    68.247       352     12.0452     12.1043
2021-05-17T23:49:24.308906+0000    16     255       341        86   85.9754       352     11.9878     12.0819
2021-05-17T23:49:25.309134+0000    17     255       364       109   102.559       368     11.8878     12.0542
2021-05-17T23:49:26.309370+0000    18     255       385       130   115.523       336     11.9347     12.0305
2021-05-17T23:49:27.309635+0000    19     255       406       151   127.122       336     11.9418     12.0158
2021-05-17T23:49:28.309869+0000 min lat: 11.8827 max lat: 12.1791 avg lat: 12.006
2021-05-17T23:49:28.309869+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:49:28.309869+0000    20     255       428       173   138.362       352     11.9288      12.006
2021-05-17T23:49:29.310145+0000 Total time run:         20.1225
Total writes made:      429
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     341.111
Stddev Bandwidth:       170.409
Max bandwidth (MB/sec): 368
Min bandwidth (MB/sec): 0
Average IOPS:           21
Stddev IOPS:            10.6677
Max IOPS:               23
Min IOPS:               0
Average Latency(s):     8.38678
Stddev Latency(s):      3.9871
Max latency(s):         12.1791
Min latency(s):         0.142464

[1;32mlocalhost.localdomain	[2021-05-17T16:49:30,196476449-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1619514


[1;33mlocalhost.localdomain	[2021-05-17T16:49:30,205646655-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:49:53,098856842-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:49:53,116165944-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:50:01,144099589-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:50:01,161022327-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:50:09,312396405-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:50:09,329184104-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:50:17,251475788-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:50:17,271629020-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:50:25,317226691-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:50:25,334165945-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:50:25,347873145-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:50:25,355788366-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:50:25,373643574-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1623008
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T16:50:25,388702028-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-17T16:50:25,428036982-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:50:25,434257944-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '26f77d2e-8995-4fd5-92ca-94901c976a37', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 26f77d2e-8995-4fd5-92ca-94901c976a37 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lUtt3h:/tmp/ceph-asok.lUtt3h -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:50:26.890+0000 ffff9696d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:50:26.898+0000 ffff9696d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:50:26.898+0000 ffff9696d010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:50:26.957384+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:50:26.957384+0000     0       0         0         0         0         0           -           0
2021-05-17T23:50:27.957615+0000     1      73        73         0         0         0           -           0
2021-05-17T23:50:28.957844+0000     2     143       143         0         0         0           -           0
2021-05-17T23:50:29.958076+0000     3     213       213         0         0         0           -           0
2021-05-17T23:50:30.972779+0000     4     255       277        22   87.6585        88     3.79634     3.70925
2021-05-17T23:50:31.973045+0000     5     255       320        65   207.343       688     4.15621     3.90526
2021-05-17T23:50:32.973328+0000     6     255       363       108   287.228       688      4.4801     4.06355
2021-05-17T23:50:33.973627+0000     7     255       410       155   353.456       752     4.83584     4.25037
2021-05-17T23:50:35.372040+0000     8      92       429       337   640.772      2912     3.05071     4.27188
2021-05-17T23:50:36.372313+0000 Total time run:       8.93423
Total reads made:     429
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   768.281
Average IOPS:         48
Stddev IOPS:          61.2442
Max IOPS:             182
Min IOPS:             0
Average Latency(s):   3.84135
Max latency(s):       5.02233
Min latency(s):       1.54648

[1;32mlocalhost.localdomain	[2021-05-17T16:50:37,308115613-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1623008


[1;33mlocalhost.localdomain	[2021-05-17T16:50:37,316509477-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:51:00,157579197-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:51:00,174446976-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:51:08,019003939-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:51:08,036962796-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:51:16,000066719-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:51:16,016966123-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:51:24,354876274-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:51:24,371663247-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:51:32,310734122-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 430 objects, 6.7 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:51:32,327238746-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:51:32,340509486-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T16:51:32,345438919-07:00][RUNNING][ROUND 4/3/20] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:51:32,352948407-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T16:51:32,369930144-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40345\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.498128\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid cb867101-e8e6-458c-a550-e1198f9dd19a\nsetting min_mon_release = octopus\nepoch 0\nfsid cb867101-e8e6-458c-a550-e1198f9dd19a\nlast_changed 2021-05-17T16:51:59.151860-0700\ncreated 2021-05-17T16:51:59.151860-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40345/0,v1:10.10.1.2:40346/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.498128 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 a81c4a9e-195c-45ef-88ab-94a84300621a\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 78536fb7-d412-4eab-aabe-4adfb80a58df\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 8c8d2293-02a1-4908-9ab8-e5f1f44bc8c4\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42345\n  w/ user/pass: admin / d0416fdf-ce97-4da5-b02d-0ff97a2349f9\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:52:14 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40345
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.498128
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid cb867101-e8e6-458c-a550-e1198f9dd19a
setting min_mon_release = octopus
epoch 0
fsid cb867101-e8e6-458c-a550-e1198f9dd19a
last_changed 2021-05-17T16:51:59.151860-0700
created 2021-05-17T16:51:59.151860-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40345/0,v1:10.10.1.2:40346/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.498128 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 a81c4a9e-195c-45ef-88ab-94a84300621a
0
start osd.0
add osd1 78536fb7-d412-4eab-aabe-4adfb80a58df
1
start osd.1
add osd2 8c8d2293-02a1-4908-9ab8-e5f1f44bc8c4
2
start osd.2


restful urls: https://10.10.1.2:42345
  w/ user/pass: admin / d0416fdf-ce97-4da5-b02d-0ff97a2349f9


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T16:51:33.368-0700 7f03432ef1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:51:33.368-0700 7f03432ef1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:51:33.384-0700 7f401162c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:51:33.384-0700 7f401162c1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40345,v1:10.10.1.2:40346] --print /tmp/ceph_monmap.498128 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.498128 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.498128 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42345 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.Xa4lbrplDB 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a81c4a9e-195c-45ef-88ab-94a84300621a -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCoAaNgYJ5IKhAAgzsXGbylWZoaRHJ1u9WVLQ== --osd-uuid a81c4a9e-195c-45ef-88ab-94a84300621a 
2021-05-17T16:52:09.032-0700 7f4852b5cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:52:09.032-0700 7f4852b5cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:52:09.032-0700 7f4852b5cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:52:09.120-0700 7f4852b5cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 78536fb7-d412-4eab-aabe-4adfb80a58df -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T16:52:09.404-0700 7fd9d7fbcf00 -1 Falling back to public interface
2021-05-17T16:52:09.416-0700 7fd9d7fbcf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCpAaNg2OJLGBAAC5RLygEUjbZToR8tB+g67w== --osd-uuid 78536fb7-d412-4eab-aabe-4adfb80a58df 
2021-05-17T16:52:09.784-0700 7fb0412b7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:52:09.788-0700 7fb0412b7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:52:09.788-0700 7fb0412b7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:52:09.908-0700 7fb0412b7f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8c8d2293-02a1-4908-9ab8-e5f1f44bc8c4 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T16:52:10.208-0700 7f28b1b9bf00 -1 Falling back to public interface
2021-05-17T16:52:10.220-0700 7f28b1b9bf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCqAaNg9+mSDBAAkeZLvF8uBmmT1t8jwqvrCw== --osd-uuid 8c8d2293-02a1-4908-9ab8-e5f1f44bc8c4 
2021-05-17T16:52:10.588-0700 7f76d341cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:52:10.588-0700 7f76d341cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:52:10.588-0700 7f76d341cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:52:10.644-0700 7f76d341cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T16:52:10.964-0700 7f45b1a4bf00 -1 Falling back to public interface
2021-05-17T16:52:10.976-0700 7f45b1a4bf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T16:52:14,941839456-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:52:14,950093087-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T16:52:15,035505872-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:52:15,042154180-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd05c52f2-8807-4d6b-a059-fc44d40d8e96', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d05c52f2-8807-4d6b-a059-fc44d40d8e96 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T16:52:18,037421733-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:52:18,043872007-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd05c52f2-8807-4d6b-a059-fc44d40d8e96', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d05c52f2-8807-4d6b-a059-fc44d40d8e96 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T16:52:20,949019514-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:52:20,955638972-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd05c52f2-8807-4d6b-a059-fc44d40d8e96', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d05c52f2-8807-4d6b-a059-fc44d40d8e96 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T16:52:25,687206247-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:52:25,693716352-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd05c52f2-8807-4d6b-a059-fc44d40d8e96', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d05c52f2-8807-4d6b-a059-fc44d40d8e96 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T16:52:31,371212368-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:52:31,377763339-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd05c52f2-8807-4d6b-a059-fc44d40d8e96', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d05c52f2-8807-4d6b-a059-fc44d40d8e96 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T16:52:34,994016158-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:52:35,000536899-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd05c52f2-8807-4d6b-a059-fc44d40d8e96', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d05c52f2-8807-4d6b-a059-fc44d40d8e96 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T16:52:38,596819479-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:52:38,603336529-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd05c52f2-8807-4d6b-a059-fc44d40d8e96', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d05c52f2-8807-4d6b-a059-fc44d40d8e96 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T16:52:41,607748084-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:52:41,614265584-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd05c52f2-8807-4d6b-a059-fc44d40d8e96', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d05c52f2-8807-4d6b-a059-fc44d40d8e96 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:52:44,428332405-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:52:44,434799467-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd05c52f2-8807-4d6b-a059-fc44d40d8e96', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d05c52f2-8807-4d6b-a059-fc44d40d8e96 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:52:47,919767386-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:52:47,926213279-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd05c52f2-8807-4d6b-a059-fc44d40d8e96', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d05c52f2-8807-4d6b-a059-fc44d40d8e96 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T16:52:51,724611742-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:52:51,730949121-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd05c52f2-8807-4d6b-a059-fc44d40d8e96', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d05c52f2-8807-4d6b-a059-fc44d40d8e96 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T16:52:54,428143055-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:52:54,434546400-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd05c52f2-8807-4d6b-a059-fc44d40d8e96', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d05c52f2-8807-4d6b-a059-fc44d40d8e96 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T16:52:57,103498128-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:53:20,162415499-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (5s)
      [=============...............] (remaining: 5s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:53:28,200603291-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:53:36,088679941-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:53:44,133852473-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:53:52,226193177-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:54:00,129903633-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:54:08,103881063-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:54:16,255767364-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:54:16,272576680-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:54:24,138886336-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:54:24,156781224-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:54:32,132390488-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:54:32,149383451-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:54:40,161408445-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:54:40,177790172-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:54:48,089793844-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:54:48,106229234-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:54:48,119828776-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:54:48,127788577-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:54:48,145875797-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1636223
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T16:54:48,160970977-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-17T16:54:48,202196892-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:54:48,208599095-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd05c52f2-8807-4d6b-a059-fc44d40d8e96', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d05c52f2-8807-4d6b-a059-fc44d40d8e96 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0 -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:54:49.748+0000 ffff9bb8b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:54:49.756+0000 ffff9bb8b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:54:49.756+0000 ffff9bb8b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:54:49.785649+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-17T23:54:49.785766+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T23:54:53.852434+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:54:53.852434+0000     0       0         0         0         0         0           -           0
2021-05-17T23:54:54.852676+0000     1      22        22         0         0         0           -           0
2021-05-17T23:54:55.852886+0000     2      43        43         0         0         0           -           0
2021-05-17T23:54:56.853102+0000     3      66        66         0         0         0           -           0
2021-05-17T23:54:57.853457+0000     4      88        88         0         0         0           -           0
2021-05-17T23:54:58.853694+0000     5     110       110         0         0         0           -           0
2021-05-17T23:54:59.854135+0000     6     130       130         0         0         0           -           0
2021-05-17T23:55:00.854390+0000     7     152       152         0         0         0           -           0
2021-05-17T23:55:01.854657+0000     8     173       173         0         0         0           -           0
2021-05-17T23:55:02.854890+0000     9     196       196         0         0         0           -           0
2021-05-17T23:55:03.855158+0000    10     218       218         0         0         0           -           0
2021-05-17T23:55:04.855380+0000    11     240       240         0         0         0           -           0
2021-05-17T23:55:05.855696+0000    12     255       261         6   7.99793         8     11.7806     11.7861
2021-05-17T23:55:06.855955+0000    13     255       284        29   35.6831       368     11.7142     11.7599
2021-05-17T23:55:07.856165+0000    14     255       306        51   58.2709       352     11.7101     11.7339
2021-05-17T23:55:08.856381+0000    15     255       329        74   78.9134       368     11.6528     11.7241
2021-05-17T23:55:09.856678+0000    16     255       352        97   96.9753       368     11.6565     11.7046
2021-05-17T23:55:10.857031+0000    17     255       374       119   111.971       352     11.6044     11.6916
2021-05-17T23:55:11.857253+0000    18     255       397       142    126.19       368     11.5246     11.6696
2021-05-17T23:55:12.857462+0000    19     255       419       164    138.07       352     11.5211      11.649
2021-05-17T23:55:13.857703+0000 min lat: 11.4351 max lat: 11.845 avg lat: 11.6325
2021-05-17T23:55:13.857703+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:55:13.857703+0000    20     255       441       186   148.762       352     11.4549     11.6325
2021-05-17T23:55:14.858032+0000 Total time run:         20.0843
Total writes made:      442
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     352.116
Stddev Bandwidth:       180.693
Max bandwidth (MB/sec): 368
Min bandwidth (MB/sec): 0
Average IOPS:           22
Stddev IOPS:            11.3137
Max IOPS:               23
Min IOPS:               0
Average Latency(s):     8.21116
Stddev Latency(s):      3.85494
Max latency(s):         11.845
Min latency(s):         0.11564

[1;32mlocalhost.localdomain	[2021-05-17T16:55:15,743751062-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1636223


[1;33mlocalhost.localdomain	[2021-05-17T16:55:15,752205932-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:55:38,740587168-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 443 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:55:38,757670638-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:55:46,799354041-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 443 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:55:46,816866974-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:55:54,976309008-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 443 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:55:54,993340733-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:56:02,826935318-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 443 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:56:02,843945304-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:56:10,941640178-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 443 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:56:10,959037501-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:56:10,973040774-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:56:10,981175683-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:56:10,998803160-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1639636
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T16:56:11,013507369-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-17T16:56:11,053055852-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:56:11,059499565-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd05c52f2-8807-4d6b-a059-fc44d40d8e96', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d05c52f2-8807-4d6b-a059-fc44d40d8e96 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gqRXB0:/tmp/ceph-asok.gqRXB0 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T23:56:12.754+0000 ffff999d8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:56:12.762+0000 ffff999d8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T23:56:12.762+0000 ffff999d8010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T23:56:12.791811+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T23:56:12.791811+0000     0       0         0         0         0         0           -           0
2021-05-17T23:56:13.792033+0000     1      70        70         0         0         0           -           0
2021-05-17T23:56:14.792258+0000     2     134       134         0         0         0           -           0
2021-05-17T23:56:15.792599+0000     3     199       199         0         0         0           -           0
2021-05-17T23:56:16.793152+0000     4     255       267        12   47.9816        48     3.86562     3.83746
2021-05-17T23:56:17.798515+0000     5     255       317        62   198.127       800     4.13178     3.98725
2021-05-17T23:56:18.802435+0000     6     255       364       109   290.144       752       4.411      4.1091
2021-05-17T23:56:19.805648+0000     7     255       409       154   351.296       720     4.73346     4.24746
2021-05-17T23:56:20.815788+0000     8     204       442       238   474.566      1344     4.51183     4.43314
2021-05-17T23:56:21.836055+0000     9      24       442       418    739.46      2880     1.84795     3.92967
2021-05-17T23:56:22.836342+0000 Total time run:       9.18016
Total reads made:     442
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   770.357
Average IOPS:         48
Stddev IOPS:          58.7497
Max IOPS:             180
Min IOPS:             0
Average Latency(s):   3.80701
Max latency(s):       4.98225
Min latency(s):       1.49121

[1;32mlocalhost.localdomain	[2021-05-17T16:56:23,751611348-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1639636


[1;33mlocalhost.localdomain	[2021-05-17T16:56:23,760092442-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:56:46,675046635-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 443 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:56:46,692297336-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:56:54,704331147-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 443 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:56:54,721245969-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:57:02,629567377-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 443 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:57:02,646977240-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:57:10,456457965-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 443 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:57:10,473451295-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:57:18,409305771-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 443 objects, 6.9 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:57:18,426190363-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T16:57:18,439858762-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T16:57:18,444825545-07:00][RUNNING][ROUND 5/3/20] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:57:18,452509206-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T16:57:18,469798056-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40243\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.501738\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 4bf118f1-b539-49cf-b5ff-219b3ed93e59\nsetting min_mon_release = octopus\nepoch 0\nfsid 4bf118f1-b539-49cf-b5ff-219b3ed93e59\nlast_changed 2021-05-17T16:57:45.955373-0700\ncreated 2021-05-17T16:57:45.955373-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40243/0,v1:10.10.1.2:40244/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.501738 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 c8304b6c-62f7-4ae0-bd5c-90525d051f81\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 2b38060f-6c90-4506-bb21-79e632d3e216\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 643d1c28-1166-4c56-b556-643add41fa3b\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42243\n'
10.10.1.2: b'  w/ user/pass: admin / 062a788a-b1d7-44de-a56f-2f13813009e0\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:58:00 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40243
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.501738
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 4bf118f1-b539-49cf-b5ff-219b3ed93e59
setting min_mon_release = octopus
epoch 0
fsid 4bf118f1-b539-49cf-b5ff-219b3ed93e59
last_changed 2021-05-17T16:57:45.955373-0700
created 2021-05-17T16:57:45.955373-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40243/0,v1:10.10.1.2:40244/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.501738 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 c8304b6c-62f7-4ae0-bd5c-90525d051f81
0
start osd.0
add osd1 2b38060f-6c90-4506-bb21-79e632d3e216
1
start osd.1
add osd2 643d1c28-1166-4c56-b556-643add41fa3b
2
start osd.2


restful urls: https://10.10.1.2:42243
  w/ user/pass: admin / 062a788a-b1d7-44de-a56f-2f13813009e0


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T16:57:19.475-0700 7fdb889061c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:57:19.475-0700 7fdb889061c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:57:19.491-0700 7fcd6d8f31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:57:19.491-0700 7fcd6d8f31c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40243,v1:10.10.1.2:40244] --print /tmp/ceph_monmap.501738 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.501738 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.501738 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42243 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.RQdEoJ1r51 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c8304b6c-62f7-4ae0-bd5c-90525d051f81 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQACA6NghTOJOxAAtVrbsm74eObCwEoJMqom3Q== --osd-uuid c8304b6c-62f7-4ae0-bd5c-90525d051f81 
2021-05-17T16:57:55.319-0700 7f5f1bed6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:57:55.319-0700 7f5f1bed6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:57:55.319-0700 7f5f1bed6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T16:57:55.383-0700 7f5f1bed6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 2b38060f-6c90-4506-bb21-79e632d3e216 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T16:57:55.663-0700 7f0186551f00 -1 Falling back to public interface
2021-05-17T16:57:55.675-0700 7f0186551f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQADA6NgqvyYJxAAJxOSvi0k9hcaWB1jkijy8A== --osd-uuid 2b38060f-6c90-4506-bb21-79e632d3e216 
2021-05-17T16:57:56.007-0700 7f7771e00f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:57:56.007-0700 7f7771e00f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:57:56.007-0700 7f7771e00f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T16:57:56.051-0700 7f7771e00f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 643d1c28-1166-4c56-b556-643add41fa3b -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T16:57:56.327-0700 7f1a1f8a0f00 -1 Falling back to public interface
2021-05-17T16:57:56.343-0700 7f1a1f8a0f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAEA6NgDc3kExAAu/DSesaRTdDpw9D55tY4Qw== --osd-uuid 643d1c28-1166-4c56-b556-643add41fa3b 
2021-05-17T16:57:56.675-0700 7f36eb196f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:57:56.679-0700 7f36eb196f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:57:56.679-0700 7f36eb196f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T16:57:56.783-0700 7f36eb196f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T16:57:57.107-0700 7f91d6737f00 -1 Falling back to public interface
2021-05-17T16:57:57.123-0700 7f91d6737f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T16:58:01,018282538-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T16:58:01,026290359-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T16:58:01,108299199-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:58:01,115010752-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6a8f10a0-545e-45ae-a25f-a32ec17a6d49', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6a8f10a0-545e-45ae-a25f-a32ec17a6d49 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T16:58:03,922404207-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:58:03,929075283-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6a8f10a0-545e-45ae-a25f-a32ec17a6d49', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6a8f10a0-545e-45ae-a25f-a32ec17a6d49 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T16:58:06,703024124-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:58:06,711097145-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6a8f10a0-545e-45ae-a25f-a32ec17a6d49', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6a8f10a0-545e-45ae-a25f-a32ec17a6d49 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T16:58:09,476797599-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:58:09,483470821-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6a8f10a0-545e-45ae-a25f-a32ec17a6d49', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6a8f10a0-545e-45ae-a25f-a32ec17a6d49 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T16:58:15,413243794-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:58:15,419391820-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6a8f10a0-545e-45ae-a25f-a32ec17a6d49', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6a8f10a0-545e-45ae-a25f-a32ec17a6d49 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T16:58:18,960039905-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:58:18,966604327-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6a8f10a0-545e-45ae-a25f-a32ec17a6d49', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6a8f10a0-545e-45ae-a25f-a32ec17a6d49 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T16:58:22,348916250-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:58:22,355675060-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6a8f10a0-545e-45ae-a25f-a32ec17a6d49', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6a8f10a0-545e-45ae-a25f-a32ec17a6d49 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T16:58:25,718068415-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:58:25,724649055-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6a8f10a0-545e-45ae-a25f-a32ec17a6d49', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6a8f10a0-545e-45ae-a25f-a32ec17a6d49 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:58:28,988231163-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:58:28,994399817-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6a8f10a0-545e-45ae-a25f-a32ec17a6d49', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6a8f10a0-545e-45ae-a25f-a32ec17a6d49 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T16:58:32,341933724-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:58:32,348284209-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6a8f10a0-545e-45ae-a25f-a32ec17a6d49', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6a8f10a0-545e-45ae-a25f-a32ec17a6d49 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T16:58:35,645959464-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:58:35,652371535-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6a8f10a0-545e-45ae-a25f-a32ec17a6d49', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6a8f10a0-545e-45ae-a25f-a32ec17a6d49 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T16:58:38,297948312-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T16:58:38,304304003-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6a8f10a0-545e-45ae-a25f-a32ec17a6d49', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6a8f10a0-545e-45ae-a25f-a32ec17a6d49 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T16:58:41,151646011-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T16:59:04,066745730-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   193 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (5s)
      [=======================.....] (remaining: 1s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:59:11,978250539-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:59:20,203076567-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:59:28,414095175-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:59:36,438341057-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:59:44,349371994-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T16:59:52,320521487-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:00:00,146646564-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:00:00,163892442-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:00:08,120522677-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:00:08,139913668-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:00:16,195150646-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:00:16,212129328-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:00:24,169688954-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:00:24,187190821-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:00:32,150391843-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:00:32,167477283-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:00:32,181821213-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T17:00:32,189808972-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T17:00:32,207807844-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1652836
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T17:00:32,222972115-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-17T17:00:32,264155077-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T17:00:32,270326745-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6a8f10a0-545e-45ae-a25f-a32ec17a6d49', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6a8f10a0-545e-45ae-a25f-a32ec17a6d49 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4 -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T00:00:33.763+0000 ffffbeb5b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:00:33.771+0000 ffffbeb5b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:00:33.775+0000 ffffbeb5b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T00:00:33.802688+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-18T00:00:33.802799+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-18T00:00:37.731934+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T00:00:37.731934+0000     0       0         0         0         0         0           -           0
2021-05-18T00:00:38.732196+0000     1      20        20         0         0         0           -           0
2021-05-18T00:00:39.732443+0000     2      41        41         0         0         0           -           0
2021-05-18T00:00:40.732671+0000     3      63        63         0         0         0           -           0
2021-05-18T00:00:41.732922+0000     4      84        84         0         0         0           -           0
2021-05-18T00:00:42.733178+0000     5     107       107         0         0         0           -           0
2021-05-18T00:00:43.733409+0000     6     129       129         0         0         0           -           0
2021-05-18T00:00:44.733653+0000     7     151       151         0         0         0           -           0
2021-05-18T00:00:45.733900+0000     8     173       173         0         0         0           -           0
2021-05-18T00:00:46.734172+0000     9     194       194         0         0         0           -           0
2021-05-18T00:00:47.734395+0000    10     216       216         0         0         0           -           0
2021-05-18T00:00:48.734751+0000    11     238       238         0         0         0           -           0
2021-05-18T00:00:49.734970+0000    12     255       260         5   6.66508   6.66667     11.8326     11.8486
2021-05-18T00:00:50.735196+0000    13     255       282        27   33.2229       352     11.7482     11.7792
2021-05-18T00:00:51.735523+0000    14     255       303        48   54.8438       336     11.7542     11.7611
2021-05-18T00:00:52.735722+0000    15     255       324        69   73.5823       336     11.7445     11.7541
2021-05-18T00:00:53.735963+0000    16     255       347        92   91.9778       368     11.6912     11.7475
2021-05-18T00:00:54.736270+0000    17     255       370       115   108.209       368     11.7213     11.7403
2021-05-18T00:00:55.736512+0000    18     255       392       137   121.748       352     11.6983     11.7374
2021-05-18T00:00:56.736830+0000    19     255       413       158    133.02       336     11.7267     11.7329
2021-05-18T00:00:57.737047+0000 min lat: 11.6763 max lat: 11.899 avg lat: 11.7307
2021-05-18T00:00:57.737047+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T00:00:57.737047+0000    20     255       435       180   143.964       352     11.6869     11.7307
2021-05-18T00:00:58.737596+0000 Total time run:         20.123
Total writes made:      436
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     346.668
Stddev Bandwidth:       175.832
Max bandwidth (MB/sec): 368
Min bandwidth (MB/sec): 0
Average IOPS:           21
Stddev IOPS:            11.0066
Max IOPS:               23
Min IOPS:               0
Average Latency(s):     8.29101
Stddev Latency(s):      3.87754
Max latency(s):         11.899
Min latency(s):         0.13165

[1;32mlocalhost.localdomain	[2021-05-17T17:00:59,653741515-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1652836


[1;33mlocalhost.localdomain	[2021-05-17T17:00:59,664219086-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T17:01:22,623322391-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 437 objects, 6.8 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:01:22,641164380-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:01:30,591394461-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 437 objects, 6.8 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:01:30,609130456-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:01:38,696319393-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 437 objects, 6.8 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:01:38,713843342-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:01:48,622313854-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 437 objects, 6.8 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:01:48,640000400-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:01:56,579960201-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 437 objects, 6.8 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:01:56,597533477-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:01:56,611654602-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T17:01:56,619695995-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T17:01:56,637794947-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1656370
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T17:01:56,652958976-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-17T17:01:56,692686174-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T17:01:56,699211977-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6a8f10a0-545e-45ae-a25f-a32ec17a6d49', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6a8f10a0-545e-45ae-a25f-a32ec17a6d49 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ruk4P4:/tmp/ceph-asok.ruk4P4 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T00:01:58.229+0000 ffffb9861010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:01:58.237+0000 ffffb9861010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:01:58.237+0000 ffffb9861010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T00:01:58.269460+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-18T00:01:58.269460+0000     0       0         0         0         0         0           -           0
2021-05-18T00:01:59.269692+0000     1      66        66         0         0         0           -           0
2021-05-18T00:02:00.269941+0000     2     125       125         0         0         0           -           0
2021-05-18T00:02:01.270227+0000     3     181       181         0         0         0           -           0
2021-05-18T00:02:02.270469+0000     4     242       242         0         0         0           -           0
2021-05-18T00:02:03.270714+0000     5     255       291        36   115.167     115.2     4.49527      4.3883
2021-05-18T00:02:04.270982+0000     6     255       333        78    207.94       672      4.8025      4.5268
2021-05-18T00:02:05.272875+0000     7     255       375       120   274.144       672     5.12088     4.67927
2021-05-18T00:02:06.273096+0000     8     255       421       166   331.841       736      5.2895     4.82802
2021-05-18T00:02:07.362990+0000     9     124       436       312    548.95      2336     3.61651     4.75112
2021-05-18T00:02:08.363250+0000 Total time run:       9.82544
Total reads made:     436
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   709.993
Average IOPS:         44
Stddev IOPS:          47.5792
Max IOPS:             146
Min IOPS:             0
Average Latency(s):   4.12045
Max latency(s):       5.35725
Min latency(s):       1.51033

[1;32mlocalhost.localdomain	[2021-05-17T17:02:09,309151551-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 1656370


[1;33mlocalhost.localdomain	[2021-05-17T17:02:09,317856373-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T17:02:32,352169964-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 437 objects, 6.8 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:02:32,370282072-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:02:40,473930021-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 437 objects, 6.8 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:02:40,491577124-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:02:48,395473623-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 437 objects, 6.8 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:02:48,413242442-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:02:56,528780933-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 437 objects, 6.8 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:02:56,546283861-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:03:04,380315002-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 437 objects, 6.8 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:03:04,397977513-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:03:04,412168039-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T17:03:04,420614215-07:00][RUNNING][ROUND 1/4/20] object_size=64MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T17:03:04,428786205-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T17:03:04,446327720-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40094\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.505491\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8980e86b-85b6-4568-a378-67cccbbc9dad\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 8980e86b-85b6-4568-a378-67cccbbc9dad\nlast_changed 2021-05-17T17:03:31.864186-0700\ncreated 2021-05-17T17:03:31.864186-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40094/0,v1:10.10.1.2:40095/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.505491 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 9573ba22-28bc-459d-a152-407c72f65635\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 a5ed6255-6aa1-409c-8bcf-3d2c687d1afb\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 f05a3f5b-712a-4f0d-b49f-f2483ee1ad1f\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42094\n  w/ user/pass: admin / a193b95e-ec20-4473-9f08-b074964935be\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 17:03:47 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40094
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.505491
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8980e86b-85b6-4568-a378-67cccbbc9dad
setting min_mon_release = octopus
epoch 0
fsid 8980e86b-85b6-4568-a378-67cccbbc9dad
last_changed 2021-05-17T17:03:31.864186-0700
created 2021-05-17T17:03:31.864186-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40094/0,v1:10.10.1.2:40095/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.505491 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 9573ba22-28bc-459d-a152-407c72f65635
0
start osd.0
add osd1 a5ed6255-6aa1-409c-8bcf-3d2c687d1afb
1
start osd.1
add osd2 f05a3f5b-712a-4f0d-b49f-f2483ee1ad1f
2
start osd.2


restful urls: https://10.10.1.2:42094
  w/ user/pass: admin / a193b95e-ec20-4473-9f08-b074964935be


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T17:03:05.438-0700 7fbf3c4b81c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:03:05.438-0700 7fbf3c4b81c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:03:05.454-0700 7f96176841c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:03:05.454-0700 7f96176841c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40094,v1:10.10.1.2:40095] --print /tmp/ceph_monmap.505491 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.505491 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.505491 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42094 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.yOaa9bWD4h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9573ba22-28bc-459d-a152-407c72f65635 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBdBKNgQJ1ZGhAA3W3nlkamTnOg94dEg3KLBQ== --osd-uuid 9573ba22-28bc-459d-a152-407c72f65635 
2021-05-17T17:03:41.766-0700 7f41c57a5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T17:03:41.766-0700 7f41c57a5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T17:03:41.766-0700 7f41c57a5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T17:03:41.846-0700 7f41c57a5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a5ed6255-6aa1-409c-8bcf-3d2c687d1afb -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T17:03:42.158-0700 7f0ceed36f00 -1 Falling back to public interface
2021-05-17T17:03:42.170-0700 7f0ceed36f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBeBKNgWXB8CRAA6M35bpCTUKTnkuMV5CgRdQ== --osd-uuid a5ed6255-6aa1-409c-8bcf-3d2c687d1afb 
2021-05-17T17:03:42.510-0700 7ff3437c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T17:03:42.514-0700 7ff3437c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T17:03:42.514-0700 7ff3437c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T17:03:42.562-0700 7ff3437c5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f05a3f5b-712a-4f0d-b49f-f2483ee1ad1f -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T17:03:42.938-0700 7fd7f3069f00 -1 Falling back to public interface
2021-05-17T17:03:42.950-0700 7fd7f3069f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBeBKNgvXXhNxAATKgvm79InPpwQ0CzOyJpBg== --osd-uuid f05a3f5b-712a-4f0d-b49f-f2483ee1ad1f 
2021-05-17T17:03:43.275-0700 7f4451f0df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T17:03:43.275-0700 7f4451f0df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T17:03:43.275-0700 7f4451f0df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T17:03:43.331-0700 7f4451f0df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T17:03:43.647-0700 7fde0dc10f00 -1 Falling back to public interface
2021-05-17T17:03:43.663-0700 7fde0dc10f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T17:03:47,579111114-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T17:03:47,587207625-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T17:03:47,671841514-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T17:03:47,678265494-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ffbe07fe-74be-4862-ab70-9c7f0370f7ba', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ffbe07fe-74be-4862-ab70-9c7f0370f7ba --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T17:03:50,553525062-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T17:03:50,560008677-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ffbe07fe-74be-4862-ab70-9c7f0370f7ba', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ffbe07fe-74be-4862-ab70-9c7f0370f7ba --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T17:03:53,422525790-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T17:03:53,429103711-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ffbe07fe-74be-4862-ab70-9c7f0370f7ba', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ffbe07fe-74be-4862-ab70-9c7f0370f7ba --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T17:03:56,240743758-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T17:03:56,247339518-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ffbe07fe-74be-4862-ab70-9c7f0370f7ba', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ffbe07fe-74be-4862-ab70-9c7f0370f7ba --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T17:04:01,830278955-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T17:04:01,841036682-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ffbe07fe-74be-4862-ab70-9c7f0370f7ba', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ffbe07fe-74be-4862-ab70-9c7f0370f7ba --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T17:04:05,511926951-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T17:04:05,518584401-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ffbe07fe-74be-4862-ab70-9c7f0370f7ba', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ffbe07fe-74be-4862-ab70-9c7f0370f7ba --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T17:04:08,959352083-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T17:04:08,966034812-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ffbe07fe-74be-4862-ab70-9c7f0370f7ba', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ffbe07fe-74be-4862-ab70-9c7f0370f7ba --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T17:04:12,227164874-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T17:04:12,234056559-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ffbe07fe-74be-4862-ab70-9c7f0370f7ba', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ffbe07fe-74be-4862-ab70-9c7f0370f7ba --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T17:04:15,963058160-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T17:04:15,969558488-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ffbe07fe-74be-4862-ab70-9c7f0370f7ba', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ffbe07fe-74be-4862-ab70-9c7f0370f7ba --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T17:04:19,579512862-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T17:04:19,586080387-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ffbe07fe-74be-4862-ab70-9c7f0370f7ba', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ffbe07fe-74be-4862-ab70-9c7f0370f7ba --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T17:04:22,830765440-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T17:04:22,837181258-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ffbe07fe-74be-4862-ab70-9c7f0370f7ba', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ffbe07fe-74be-4862-ab70-9c7f0370f7ba --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T17:04:25,484835184-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T17:04:25,491151280-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ffbe07fe-74be-4862-ab70-9c7f0370f7ba', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ffbe07fe-74be-4862-ab70-9c7f0370f7ba --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T17:04:28,315724668-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T17:04:51,354091402-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [======================......] (remaining: 1s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:04:59,160767850-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:05:07,064623079-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:05:15,155195368-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:05:23,083753385-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:05:31,151084050-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:05:39,267406431-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:05:47,265896028-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:05:47,283381625-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:05:55,245808763-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:05:55,263894958-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:06:03,246639782-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:06:03,264282441-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:06:11,223391481-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:06:11,240733894-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:06:19,348135835-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T17:06:19,365761001-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T17:06:19,380021301-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T17:06:19,388261911-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T17:06:19,407102300-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=1669611
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T17:06:19,422756698-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 67108864 -O 67108864 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T17:06:19,464013233-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T17:06:19,470644698-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ffbe07fe-74be-4862-ab70-9c7f0370f7ba', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '67108864', '-O', '67108864', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ffbe07fe-74be-4862-ab70-9c7f0370f7ba --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Qop8IO:/tmp/ceph-asok.Qop8IO -- rados bench 20 write --pool bench_rados -b 67108864 -O 67108864 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-18T00:06:21.250+0000 ffff9b358010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:06:21.262+0000 ffff9b358010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-18T00:06:21.262+0000 ffff9b358010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-18T00:06:21.326352+0000 Maintaining 256 concurrent writes of 67108864 bytes to objects of size 67108864 for up to 20 seconds or 0 objects
2021-05-18T00:06:21.326469+0000 Object prefix: benchmark_data_localhost.localdomain_6
