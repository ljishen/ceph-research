

[1;7;39;49m[2021-05-16T13:31:57,196028260-04:00][RUNNING][ROUND 1/1/40] object_size=4KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:31:57,198635749-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:31:57,206591878-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40216\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.130560\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 1270924e-eb7d-4456-a290-4a69c49db151\nsetting min_mon_release = octopus\nepoch 0\nfsid 1270924e-eb7d-4456-a290-4a69c49db151\nlast_changed 2021-05-16T10:32:08.315289-0700\ncreated 2021-05-16T10:32:08.315289-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40216/0,v1:10.10.1.2:40217/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.130560 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 cfe8f220-f93a-46d8-b24e-fef385351069\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 9f0861b4-4955-4d27-963e-721113b2052f\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 01913e3d-c249-4fa3-90c3-7d4ef99df75a\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42216\n  w/ user/pass: admin / 211377e8-ccd1-48e5-b36c-669f502edc87\n\n'
10.10.1.2: b'\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:32:25 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40216
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.130560
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 1270924e-eb7d-4456-a290-4a69c49db151
setting min_mon_release = octopus
epoch 0
fsid 1270924e-eb7d-4456-a290-4a69c49db151
last_changed 2021-05-16T10:32:08.315289-0700
created 2021-05-16T10:32:08.315289-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40216/0,v1:10.10.1.2:40217/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.130560 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 cfe8f220-f93a-46d8-b24e-fef385351069
0
start osd.0
add osd1 9f0861b4-4955-4d27-963e-721113b2052f
1
start osd.1
add osd2 01913e3d-c249-4fa3-90c3-7d4ef99df75a
2
start osd.2


restful urls: https://10.10.1.2:42216
  w/ user/pass: admin / 211377e8-ccd1-48e5-b36c-669f502edc87


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T10:31:59.099-0700 7f95524931c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T10:31:59.099-0700 7f95524931c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T10:31:59.115-0700 7f90a1cba1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T10:31:59.115-0700 7f90a1cba1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40216,v1:10.10.1.2:40217] --print /tmp/ceph_monmap.130560 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.130560 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.130560 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42216 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5591b3108000 @  0x7fd9f9b34680 0x7fd9f9b55824 0x7fd9fa2f0187 0x7fd9fa2f8355 0x7fd9fa2f0708 0x7fd9fa2f0877 0x7fd9fa2f1c24 0x7fd9fa309ec1 0x7fd9fa27c5f3 0x7fd9fa2dde97 0x7fd9fa2e5b1a 0x7fd9f99e8d84 0x7fd9f9b04609 0x7fd9f96d8293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.nqeoF3MSLe 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new cfe8f220-f93a-46d8-b24e-fef385351069 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAiV6FgtK4NNBAAHQWFA6ZzQ2tN9r2UxPLxUg== --osd-uuid cfe8f220-f93a-46d8-b24e-fef385351069 
2021-05-16T10:32:19.507-0700 7fd10ca81f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T10:32:19.527-0700 7fd10ca81f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T10:32:19.527-0700 7fd10ca81f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55559ab48000 @  0x7fd10d44a680 0x7fd10d46b824 0x55558ec23447 0x55558ec2b4b5 0x55558ec239c8 0x55558ec23b37 0x55558ec24ee4 0x55558e9f5ca1 0x55558ebcd423 0x55558e9e62a7 0x55558e9eb53a 0x7fd10cf9dd84 0x7fd10d122609 0x7fd10cc8b293
2021-05-16T10:32:19.831-0700 7fd10ca81f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9f0861b4-4955-4d27-963e-721113b2052f -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5643ca12a000 @  0x7fcdc7e72680 0x7fcdc7e93824 0x5643bf884447 0x5643bf88c4b5 0x5643bf8849c8 0x5643bf884b37 0x5643bf885ee4 0x5643bf656ca1 0x5643bf82e423 0x5643bf6472a7 0x5643bf64c53a 0x7fcdc79c5d84 0x7fcdc7b4a609 0x7fcdc76b3293
2021-05-16T10:32:20.467-0700 7fcdc74a9f00 -1 Falling back to public interface
2021-05-16T10:32:20.727-0700 7fcdc74a9f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAkV6FgzPTRCBAA6+YoStfwGHLV8JliYYWvuw== --osd-uuid 9f0861b4-4955-4d27-963e-721113b2052f 
2021-05-16T10:32:20.831-0700 7fcae10f7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T10:32:20.851-0700 7fcae10f7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T10:32:20.851-0700 7fcae10f7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5625eaa72000 @  0x7fcae1ac0680 0x7fcae1ae1824 0x5625deb3c447 0x5625deb444b5 0x5625deb3c9c8 0x5625deb3cb37 0x5625deb3dee4 0x5625de90eca1 0x5625deae6423 0x5625de8ff2a7 0x5625de90453a 0x7fcae1613d84 0x7fcae1798609 0x7fcae1301293
2021-05-16T10:32:21.227-0700 7fcae10f7f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 01913e3d-c249-4fa3-90c3-7d4ef99df75a -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5567e79b2000 @  0x7f3734026680 0x7f3734047824 0x5567dcda4447 0x5567dcdac4b5 0x5567dcda49c8 0x5567dcda4b37 0x5567dcda5ee4 0x5567dcb76ca1 0x5567dcd4e423 0x5567dcb672a7 0x5567dcb6c53a 0x7f3733b79d84 0x7f3733cfe609 0x7f3733867293
2021-05-16T10:32:21.863-0700 7f373365df00 -1 Falling back to public interface
2021-05-16T10:32:22.119-0700 7f373365df00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAlV6FgFSvkIBAADANd7m5sb74R/RTmvo7BNg== --osd-uuid 01913e3d-c249-4fa3-90c3-7d4ef99df75a 
2021-05-16T10:32:22.215-0700 7fb8f9776f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T10:32:22.235-0700 7fb8f9776f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T10:32:22.235-0700 7fb8f9776f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55f4629e6000 @  0x7fb8fa13f680 0x7fb8fa160824 0x55f457825447 0x55f45782d4b5 0x55f4578259c8 0x55f457825b37 0x55f457826ee4 0x55f4575f7ca1 0x55f4577cf423 0x55f4575e82a7 0x55f4575ed53a 0x7fb8f9c92d84 0x7fb8f9e17609 0x7fb8f9980293
2021-05-16T10:32:22.555-0700 7fb8f9776f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55cf86568000 @  0x7fed681bd680 0x7fed681de824 0x55cf7b05f447 0x55cf7b0674b5 0x55cf7b05f9c8 0x55cf7b05fb37 0x55cf7b060ee4 0x55cf7ae31ca1 0x55cf7b009423 0x55cf7ae222a7 0x55cf7ae2753a 0x7fed67d10d84 0x7fed67e95609 0x7fed679fe293
2021-05-16T10:32:23.219-0700 7fed677f4f00 -1 Falling back to public interface
2021-05-16T10:32:23.491-0700 7fed677f4f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:25,766247143-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:25,774644050-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:25,855567350-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:25,858794864-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eeec9283-5296-4dda-af8c-a5ba1e5468dc', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eeec9283-5296-4dda-af8c-a5ba1e5468dc --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:32,477883389-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:32,481946824-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eeec9283-5296-4dda-af8c-a5ba1e5468dc', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eeec9283-5296-4dda-af8c-a5ba1e5468dc --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:35,735490406-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:35,739100470-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eeec9283-5296-4dda-af8c-a5ba1e5468dc', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eeec9283-5296-4dda-af8c-a5ba1e5468dc --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:38,971233133-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:38,975478580-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eeec9283-5296-4dda-af8c-a5ba1e5468dc', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eeec9283-5296-4dda-af8c-a5ba1e5468dc --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:45,434854020-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:45,438530718-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eeec9283-5296-4dda-af8c-a5ba1e5468dc', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eeec9283-5296-4dda-af8c-a5ba1e5468dc --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:49,270523348-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:49,273927284-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eeec9283-5296-4dda-af8c-a5ba1e5468dc', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eeec9283-5296-4dda-af8c-a5ba1e5468dc --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:52,651901445-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:52,655652122-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eeec9283-5296-4dda-af8c-a5ba1e5468dc', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eeec9283-5296-4dda-af8c-a5ba1e5468dc --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:55,953787222-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:32:55,957383920-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eeec9283-5296-4dda-af8c-a5ba1e5468dc', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eeec9283-5296-4dda-af8c-a5ba1e5468dc --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:33:00,453791472-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:33:00,457927303-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eeec9283-5296-4dda-af8c-a5ba1e5468dc', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eeec9283-5296-4dda-af8c-a5ba1e5468dc --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:33:03,976040318-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:33:03,979977736-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eeec9283-5296-4dda-af8c-a5ba1e5468dc', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eeec9283-5296-4dda-af8c-a5ba1e5468dc --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:33:07,260155417-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:33:07,264405863-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eeec9283-5296-4dda-af8c-a5ba1e5468dc', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eeec9283-5296-4dda-af8c-a5ba1e5468dc --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:33:10,618538832-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:33:10,622243322-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eeec9283-5296-4dda-af8c-a5ba1e5468dc', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eeec9283-5296-4dda-af8c-a5ba1e5468dc --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  161 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  161 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  162 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.02  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:33:13,926665007-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:33:37,020789307-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:33:45,213809619-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:33:53,495897707-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:01,742782002-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:09,772734960-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:18,027841460-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:26,292354699-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:26,299368327-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:34,500250000-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:34,507671975-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:42,881765744-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:42,888451737-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:51,214484186-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:51,221880794-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:59,608708836-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:59,615313997-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:59,620469303-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:59,623308558-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:59,629853826-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=34745
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:59,634882866-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:59,654801021-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:34:59,658596262-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eeec9283-5296-4dda-af8c-a5ba1e5468dc', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eeec9283-5296-4dda-af8c-a5ba1e5468dc --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T17:35:01.936+0000 7f7b3323cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:35:02.200+0000 7f7b3323cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:35:02.200+0000 7f7b3323cd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T17:35:02.215748+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-16T17:35:02.215760+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T17:35:02.216422+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T17:35:02.216422+0000     0       0         0         0         0         0           -           0
2021-05-16T17:35:03.216540+0000     1     256     21484     21228   82.9111   82.9219  0.00370392   0.0117221
2021-05-16T17:35:04.216618+0000     2     256     42192     41936   81.8977   80.8906 0.000896154   0.0119116
2021-05-16T17:35:05.216694+0000     3     256     61523     61267   79.7672   75.5117   0.0989506   0.0122528
2021-05-16T17:35:06.216763+0000     4     256     80843     80587   78.6913   75.4688  0.00217025   0.0125926
2021-05-16T17:35:07.216861+0000     5     255    100620    100365   78.4031   77.2578    0.138456   0.0126151
2021-05-16T17:35:08.216952+0000     6     255    121197    120942   78.7312   80.3789  0.00502445   0.0126384
2021-05-16T17:35:09.217023+0000     7     255    141733    141478   78.9429   80.2188     0.11323   0.0125855
2021-05-16T17:35:10.217097+0000     8     255    161640    161385   78.7945   77.7617  0.00401855   0.0125957
2021-05-16T17:35:11.217197+0000     9     256    180548    180292   78.2449   73.8555  0.00282436   0.0126989
2021-05-16T17:35:12.217273+0000    10     256    197958    197702   77.2207   68.0078  0.00519685   0.0129254
2021-05-16T17:35:13.217343+0000    11     255    216879    216624   76.9196   73.9141  0.00152766   0.0129745
2021-05-16T17:35:14.217413+0000    12     256    235730    235474   76.6453   73.6328  0.00394285    0.012995
2021-05-16T17:35:15.217514+0000    13     256    254460    254204   76.3769   73.1641  0.00127542   0.0129769
2021-05-16T17:35:16.217587+0000    14     255    273095    272840   76.1208   72.7969   0.0224817   0.0131036
2021-05-16T17:35:17.217656+0000    15     256    290139    289883   75.4841   66.5742   0.0190939   0.0132247
2021-05-16T17:35:18.217723+0000    16     256    308256    308000   75.1891   70.7695  0.00194756   0.0132795
2021-05-16T17:35:19.217831+0000    17     256    325984    325728   74.8393     69.25  0.00101257     0.01334
2021-05-16T17:35:20.217914+0000    18     256    341123    340867   73.9667   59.1367    0.004375    0.013427
2021-05-16T17:35:21.218000+0000    19     255    357047    356792   73.3475    62.207  0.00270191   0.0136207
2021-05-16T17:35:22.218067+0000 min lat: 0.000563539 max lat: 0.289834 avg lat: 0.0137614
2021-05-16T17:35:22.218067+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T17:35:22.218067+0000    20     225    371870    371645   72.5809   58.0195  0.00333558   0.0137614
2021-05-16T17:35:23.218253+0000 Total time run:         20.0442
Total writes made:      371870
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     72.4705
Stddev Bandwidth:       7.02416
Max bandwidth (MB/sec): 82.9219
Min bandwidth (MB/sec): 58.0195
Average IOPS:           18552
Stddev IOPS:            1798.18
Max IOPS:               21228
Min IOPS:               14853
Average Latency(s):     0.0137751
Stddev Latency(s):      0.0292432
Max latency(s):         0.289834
Min latency(s):         0.000563539

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:35:23,755177492-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 34745


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:35:23,758732562-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:35:46,984547822-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.87k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:35:46,991769622-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:35:55,257996281-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.87k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:35:55,264508467-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:03,462929673-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.87k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:03,470132927-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:11,787722830-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.87k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:11,795369277-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:20,108228537-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.87k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:20,115232075-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:20,120639595-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:20,123409109-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:20,129926094-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=36158
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:20,135608350-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:20,154156991-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:20,157824732-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eeec9283-5296-4dda-af8c-a5ba1e5468dc', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eeec9283-5296-4dda-af8c-a5ba1e5468dc --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Eemeeb:/tmp/ceph-asok.Eemeeb -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T17:36:22.312+0000 7faba2613d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:36:22.596+0000 7faba2613d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:36:22.596+0000 7faba2613d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T17:36:22.615884+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T17:36:22.615884+0000     0       0         0         0         0         0           -           0
2021-05-16T17:36:23.615987+0000     1     255     46592     46337   180.968   181.004  0.00730583  0.00549875
2021-05-16T17:36:24.616065+0000     2     255     92314     92059   179.778   178.602   0.0047527  0.00554955
2021-05-16T17:36:25.616143+0000     3     256    142832    142576   185.624   197.332 0.000235523  0.00533488
2021-05-16T17:36:26.616216+0000     4     255    188393    188138   183.709   177.977  0.00406821  0.00543567
2021-05-16T17:36:27.616312+0000     5     255    233438    233183   182.155   175.957  0.00550236  0.00548267
2021-05-16T17:36:28.616387+0000     6     255    279578    279323   181.833   180.234  0.00538693  0.00549319
2021-05-16T17:36:29.616458+0000     7     255    318652    318397    177.66   152.633  0.00766833  0.00562208
2021-05-16T17:36:30.616538+0000     8     256    361703    361447   176.471   168.164 0.000247805  0.00565758
2021-05-16T17:36:31.616687+0000 Total time run:       8.23461
Total reads made:     371870
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   176.404
Average IOPS:         45159
Stddev IOPS:          3229.81
Max IOPS:             50517
Min IOPS:             39074
Average Latency(s):   0.00566476
Max latency(s):       0.137615
Min latency(s):       0.000194776

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:32,204088444-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 36158


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:32,207867314-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:55,623787868-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.87k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:36:55,632185656-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:37:03,848843916-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.87k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:37:03,855959596-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:37:12,205238359-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.87k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:37:12,212473994-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:37:20,446905466-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.87k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:37:20,454417681-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:37:28,759413498-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.87k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:37:28,767433718-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:37:28,773116455-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T13:37:28,775081627-04:00][RUNNING][ROUND 2/1/40] object_size=4KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:37:28,778517112-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:37:28,786845701-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40596\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.134897\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ef24ed2e-093d-4d83-baeb-b7822aab7484\nsetting min_mon_release = octopus\nepoch 0\nfsid ef24ed2e-093d-4d83-baeb-b7822aab7484\nlast_changed 2021-05-16T10:37:54.503527-0700\ncreated 2021-05-16T10:37:54.503527-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40596/0,v1:10.10.1.2:40597/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.134897 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 0834b0cd-a317-42cd-9792-a1b14c325b89\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 3c722311-6fbb-4bba-a787-f38a272c9c7e\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 63672c3d-9597-4792-989a-1d15617d539b\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42596\n  w/ user/pass: admin / 92223f22-36be-417e-86c2-10d91c11b51a\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 13:38:11 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40596
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.134897
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ef24ed2e-093d-4d83-baeb-b7822aab7484
setting min_mon_release = octopus
epoch 0
fsid ef24ed2e-093d-4d83-baeb-b7822aab7484
last_changed 2021-05-16T10:37:54.503527-0700
created 2021-05-16T10:37:54.503527-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40596/0,v1:10.10.1.2:40597/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.134897 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 0834b0cd-a317-42cd-9792-a1b14c325b89
0
start osd.0
add osd1 3c722311-6fbb-4bba-a787-f38a272c9c7e
1
start osd.1
add osd2 63672c3d-9597-4792-989a-1d15617d539b
2
start osd.2


restful urls: https://10.10.1.2:42596
  w/ user/pass: admin / 92223f22-36be-417e-86c2-10d91c11b51a


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T10:37:31.022-0700 7fe09c3d31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T10:37:31.022-0700 7fe09c3d31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T10:37:31.038-0700 7fdd8b8781c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T10:37:31.098-0700 7fdd8b8781c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40596,v1:10.10.1.2:40597] --print /tmp/ceph_monmap.134897 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.134897 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.134897 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42596 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55be1a42a000 @  0x7ff1a2ebb680 0x7ff1a2edc824 0x7ff1a3677187 0x7ff1a367f355 0x7ff1a3677708 0x7ff1a3677877 0x7ff1a3678c24 0x7ff1a3690ec1 0x7ff1a36035f3 0x7ff1a3664e97 0x7ff1a366cb1a 0x7ff1a2d6fd84 0x7ff1a2e8b609 0x7ff1a2a5f293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.PIlRg9NWM1 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0834b0cd-a317-42cd-9792-a1b14c325b89 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB9WKFg4ABOBRAAiKg9lFx1ic/NLM2X6IXBGA== --osd-uuid 0834b0cd-a317-42cd-9792-a1b14c325b89 
2021-05-16T10:38:05.722-0700 7f6103f60f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T10:38:05.742-0700 7f6103f60f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T10:38:05.742-0700 7f6103f60f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x558789a90000 @  0x7f6104929680 0x7f610494a824 0x55877f1a6447 0x55877f1ae4b5 0x55877f1a69c8 0x55877f1a6b37 0x55877f1a7ee4 0x55877ef78ca1 0x55877f150423 0x55877ef692a7 0x55877ef6e53a 0x7f610447cd84 0x7f6104601609 0x7f610416a293
2021-05-16T10:38:06.046-0700 7f6103f60f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3c722311-6fbb-4bba-a787-f38a272c9c7e -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55fb6a9ae000 @  0x7f359f5f2680 0x7f359f613824 0x55fb5f886447 0x55fb5f88e4b5 0x55fb5f8869c8 0x55fb5f886b37 0x55fb5f887ee4 0x55fb5f658ca1 0x55fb5f830423 0x55fb5f6492a7 0x55fb5f64e53a 0x7f359f145d84 0x7f359f2ca609 0x7f359ee33293
2021-05-16T10:38:06.678-0700 7f359ec29f00 -1 Falling back to public interface
2021-05-16T10:38:06.942-0700 7f359ec29f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB+WKFgbaUjFhAAkDZbgavXuVVGyHYr1vXOCA== --osd-uuid 3c722311-6fbb-4bba-a787-f38a272c9c7e 
2021-05-16T10:38:07.046-0700 7fc6eb66af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T10:38:07.066-0700 7fc6eb66af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T10:38:07.066-0700 7fc6eb66af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x563e4fab6000 @  0x7fc6ec033680 0x7fc6ec054824 0x563e4486d447 0x563e448754b5 0x563e4486d9c8 0x563e4486db37 0x563e4486eee4 0x563e4463fca1 0x563e44817423 0x563e446302a7 0x563e4463553a 0x7fc6ebb86d84 0x7fc6ebd0b609 0x7fc6eb874293
2021-05-16T10:38:07.382-0700 7fc6eb66af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 63672c3d-9597-4792-989a-1d15617d539b -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55fde7fd6000 @  0x7f176b19c680 0x7f176b1bd824 0x55fddd8df447 0x55fddd8e74b5 0x55fddd8df9c8 0x55fddd8dfb37 0x55fddd8e0ee4 0x55fddd6b1ca1 0x55fddd889423 0x55fddd6a22a7 0x55fddd6a753a 0x7f176acefd84 0x7f176ae74609 0x7f176a9dd293
2021-05-16T10:38:08.034-0700 7f176a7d3f00 -1 Falling back to public interface
2021-05-16T10:38:08.294-0700 7f176a7d3f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB/WKFgEbA+KhAAIjeUQEaW0yiwn8MkY/Q5EA== --osd-uuid 63672c3d-9597-4792-989a-1d15617d539b 
2021-05-16T10:38:08.374-0700 7f04a9c5ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T10:38:08.398-0700 7f04a9c5ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T10:38:08.398-0700 7f04a9c5ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55c156572000 @  0x7f04aa627680 0x7f04aa648824 0x55c14b1e2447 0x55c14b1ea4b5 0x55c14b1e29c8 0x55c14b1e2b37 0x55c14b1e3ee4 0x55c14afb4ca1 0x55c14b18c423 0x55c14afa52a7 0x55c14afaa53a 0x7f04aa17ad84 0x7f04aa2ff609 0x7f04a9e68293
2021-05-16T10:38:08.714-0700 7f04a9c5ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x560047df8000 @  0x7f715eb80680 0x7f715eba1824 0x56003c6a0447 0x56003c6a84b5 0x56003c6a09c8 0x56003c6a0b37 0x56003c6a1ee4 0x56003c472ca1 0x56003c64a423 0x56003c4632a7 0x56003c46853a 0x7f715e6d3d84 0x7f715e858609 0x7f715e3c1293
2021-05-16T10:38:09.334-0700 7f715e1b7f00 -1 Falling back to public interface
2021-05-16T10:38:09.594-0700 7f715e1b7f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:11,817586541-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:11,828603450-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:11,869993026-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:11,873814196-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'da41dd51-d22d-46b4-adb3-15b32f748776', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid da41dd51-d22d-46b4-adb3-15b32f748776 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:15,219814087-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:15,223515692-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'da41dd51-d22d-46b4-adb3-15b32f748776', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid da41dd51-d22d-46b4-adb3-15b32f748776 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:18,551670585-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:18,555065143-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'da41dd51-d22d-46b4-adb3-15b32f748776', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid da41dd51-d22d-46b4-adb3-15b32f748776 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:21,767133252-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:21,770670899-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'da41dd51-d22d-46b4-adb3-15b32f748776', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid da41dd51-d22d-46b4-adb3-15b32f748776 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:28,246084855-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:28,249967219-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'da41dd51-d22d-46b4-adb3-15b32f748776', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid da41dd51-d22d-46b4-adb3-15b32f748776 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:32,368638297-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:32,372210469-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'da41dd51-d22d-46b4-adb3-15b32f748776', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid da41dd51-d22d-46b4-adb3-15b32f748776 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:35,709512421-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:35,713051992-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'da41dd51-d22d-46b4-adb3-15b32f748776', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid da41dd51-d22d-46b4-adb3-15b32f748776 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:40,073808173-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:40,077809501-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'da41dd51-d22d-46b4-adb3-15b32f748776', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid da41dd51-d22d-46b4-adb3-15b32f748776 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:44,429920119-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:44,433410316-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'da41dd51-d22d-46b4-adb3-15b32f748776', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid da41dd51-d22d-46b4-adb3-15b32f748776 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:48,807800016-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:48,811754546-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'da41dd51-d22d-46b4-adb3-15b32f748776', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid da41dd51-d22d-46b4-adb3-15b32f748776 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:53,093344475-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:53,096801370-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'da41dd51-d22d-46b4-adb3-15b32f748776', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid da41dd51-d22d-46b4-adb3-15b32f748776 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:56,429906461-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:56,433677737-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'da41dd51-d22d-46b4-adb3-15b32f748776', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid da41dd51-d22d-46b4-adb3-15b32f748776 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:38:59,709759929-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:39:22,983493648-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:39:31,177408555-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:39:39,681475729-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:39:47,986080259-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:39:56,283705819-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:04,774413053-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:13,161693974-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:13,169690529-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:21,401053951-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:21,409485223-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:29,648507419-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:29,656199772-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:38,070797580-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:38,078636308-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:46,424652343-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:46,432261810-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:46,438250031-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:46,441994467-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:46,449614604-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=42868
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:46,455847054-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:46,475561526-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:40:46,479504113-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'da41dd51-d22d-46b4-adb3-15b32f748776', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid da41dd51-d22d-46b4-adb3-15b32f748776 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T17:40:48.617+0000 7f0e36f3bd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:40:48.865+0000 7f0e36f3bd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:40:48.865+0000 7f0e36f3bd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T17:40:48.879333+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-16T17:40:48.879342+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T17:40:48.880037+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T17:40:48.880037+0000     0       0         0         0         0         0           -           0
2021-05-16T17:40:49.880575+0000     1     256     19826     19570   76.4035   76.4453  0.00373684   0.0125827
2021-05-16T17:40:50.880688+0000     2     256     38894     38638   75.4399   74.4844    0.117349   0.0129129
2021-05-16T17:40:51.880804+0000     3     256     57748     57492     74.84   73.6484  0.00108458   0.0131509
2021-05-16T17:40:52.880917+0000     4     256     76785     76529   74.7187   74.3633  0.00106744   0.0132662
2021-05-16T17:40:53.881032+0000     5     255     95432     95177   74.3421   72.8438  0.00111979   0.0133688
2021-05-16T17:40:54.881153+0000     6     256    114429    114173   74.3174   74.2031  0.00367339   0.0133371
2021-05-16T17:40:55.881265+0000     7     256    131087    130831   72.9955   65.0703   0.0211008   0.0136355
2021-05-16T17:40:56.881386+0000     8     255    148788    148533   72.5136   69.1484  0.00166887   0.0137469
2021-05-16T17:40:57.881468+0000     9     256    165066    164810   71.5207    63.582  0.00197039   0.0138627
2021-05-16T17:40:58.881578+0000    10     256    181741    181485   70.8816   65.1367  0.00143162   0.0140263
2021-05-16T17:40:59.881663+0000    11     255    197994    197739   70.2094   63.4922   0.0026675   0.0142062
2021-05-16T17:41:00.881740+0000    12     256    216252    215996   70.3012   71.3164   0.0038994    0.014193
2021-05-16T17:41:01.881827+0000    13     256    232953    232697   69.9113   65.2383  0.00177975   0.0142596
2021-05-16T17:41:02.881936+0000    14     256    249384    249128   69.5017   64.1836  0.00198233   0.0143628
2021-05-16T17:41:03.882027+0000    15     255    263937    263682    68.658   56.8516   0.0199396   0.0145495
2021-05-16T17:41:04.882111+0000    16     255    282528    282273   68.9053   72.6211    0.127162   0.0144603
2021-05-16T17:41:05.882191+0000    17     256    299633    299377   68.7819   66.8125   0.0109663   0.0145009
2021-05-16T17:41:06.882297+0000    18     256    317260    317004   68.7856   68.8555    0.108564   0.0145059
2021-05-16T17:41:07.882380+0000    19     256    335108    334852   68.8344   69.7188   0.0181764    0.014514
2021-05-16T17:41:08.882473+0000 min lat: 0.000576794 max lat: 0.206049 avg lat: 0.0145165
2021-05-16T17:41:08.882473+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T17:41:08.882473+0000    20     204    352620    352416   68.8228   68.6094   0.0137136   0.0145165
2021-05-16T17:41:09.882632+0000 Total time run:         20.0368
Total writes made:      352620
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     68.7447
Stddev Bandwidth:       5.00593
Max bandwidth (MB/sec): 76.4453
Min bandwidth (MB/sec): 56.8516
Average IOPS:           17598
Stddev IOPS:            1281.52
Max IOPS:               19570
Min IOPS:               14554
Average Latency(s):     0.0145255
Stddev Latency(s):      0.0303142
Max latency(s):         0.206049
Min latency(s):         0.000576794

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:41:10,440243235-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 42868


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:41:10,444295679-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:41:33,694751069-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:41:33,702255018-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:41:42,454255482-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:41:42,462259551-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:41:50,921207964-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:41:50,928953016-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:41:59,258596731-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:41:59,266026060-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:42:07,862391544-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:42:07,870324559-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:42:07,876200058-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:42:07,879775577-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:42:07,887565202-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=44252
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:42:07,893440191-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:42:07,912505072-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:42:07,916062165-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'da41dd51-d22d-46b4-adb3-15b32f748776', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid da41dd51-d22d-46b4-adb3-15b32f748776 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2t7tgu:/tmp/ceph-asok.2t7tgu -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T17:42:10.234+0000 7f0d01c52d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:42:10.486+0000 7f0d01c52d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:42:10.486+0000 7f0d01c52d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T17:42:10.504963+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T17:42:10.504963+0000     0       0         0         0         0         0           -           0
2021-05-16T17:42:11.505083+0000     1     255     56781     56526   220.758   220.805  0.00100374   0.0044415
2021-05-16T17:42:12.505183+0000     2     255     90847     90592    176.91    133.07  0.00484165  0.00564013
2021-05-16T17:42:13.505262+0000     3     255    139509    139254   181.297   190.086  0.00506117  0.00550646
2021-05-16T17:42:14.505344+0000     4     255    177236    176981   172.813   147.371  0.00248149  0.00577632
2021-05-16T17:42:15.505447+0000     5     255    216403    216148   168.846   152.996   0.0004098  0.00590598
2021-05-16T17:42:16.505518+0000     6     255    257943    257688   167.747   162.266 0.000741233  0.00595176
2021-05-16T17:42:17.505592+0000     7     256    293663    293407   163.715   139.527    0.020012  0.00609464
2021-05-16T17:42:18.505671+0000     8     255    326221    325966   159.147   127.184  0.00441039   0.0062777
2021-05-16T17:42:19.505918+0000 Total time run:       8.57994
Total reads made:     352620
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   160.54
Average IOPS:         41098
Stddev IOPS:          8113.74
Max IOPS:             56526
Min IOPS:             32559
Average Latency(s):   0.00622252
Max latency(s):       0.161891
Min latency(s):       0.000197842

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:42:20,109379724-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 44252


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:42:20,113659645-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:42:43,604327802-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:42:43,612713848-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:42:52,340155501-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:42:52,348033333-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:43:00,843208063-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:43:00,851373836-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:43:09,541833662-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:43:09,550324375-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:43:18,334268283-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 352.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:43:18,342257935-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:43:18,348224716-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T13:43:18,350180911-04:00][RUNNING][ROUND 3/1/40] object_size=4KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:43:18,353536265-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:43:18,362037649-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40247\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.135983\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 2ad29034-418d-402d-af2e-847c191ee04a\nsetting min_mon_release = octopus\nepoch 0\nfsid 2ad29034-418d-402d-af2e-847c191ee04a\nlast_changed 2021-05-16T10:43:43.499531-0700\ncreated 2021-05-16T10:43:43.499531-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40247/0,v1:10.10.1.2:40248/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.135983 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 54c2c1ad-ca69-4632-9ae8-b6b0166ad1cd\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 eb3c203d-2f16-4d7d-ac71-01f104cdcf49\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 4deeeedb-be2c-412e-994e-ccb75080541c\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42247\n  w/ user/pass: admin / 3c338aa1-eac7-4eb3-ad12-39f70a1e921e\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 13:44:00 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40247
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.135983
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 2ad29034-418d-402d-af2e-847c191ee04a
setting min_mon_release = octopus
epoch 0
fsid 2ad29034-418d-402d-af2e-847c191ee04a
last_changed 2021-05-16T10:43:43.499531-0700
created 2021-05-16T10:43:43.499531-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40247/0,v1:10.10.1.2:40248/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.135983 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 54c2c1ad-ca69-4632-9ae8-b6b0166ad1cd
0
start osd.0
add osd1 eb3c203d-2f16-4d7d-ac71-01f104cdcf49
1
start osd.1
add osd2 4deeeedb-be2c-412e-994e-ccb75080541c
2
start osd.2


restful urls: https://10.10.1.2:42247
  w/ user/pass: admin / 3c338aa1-eac7-4eb3-ad12-39f70a1e921e


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T10:43:20.201-0700 7f7a7bb821c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T10:43:20.201-0700 7f7a7bb821c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T10:43:20.217-0700 7fc1af5111c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T10:43:20.217-0700 7fc1af5111c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40247,v1:10.10.1.2:40248] --print /tmp/ceph_monmap.135983 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.135983 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.135983 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42247 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x555a63726000 @  0x7fd8cafda680 0x7fd8caffb824 0x7fd8cb796187 0x7fd8cb79e355 0x7fd8cb796708 0x7fd8cb796877 0x7fd8cb797c24 0x7fd8cb7afec1 0x7fd8cb7225f3 0x7fd8cb783e97 0x7fd8cb78bb1a 0x7fd8cae8ed84 0x7fd8cafaa609 0x7fd8cab7e293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.kxgChDoi16 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 54c2c1ad-ca69-4632-9ae8-b6b0166ad1cd -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDaWaFg79OsARAA6yslWVjBRMcPDXIKTct8Bg== --osd-uuid 54c2c1ad-ca69-4632-9ae8-b6b0166ad1cd 
2021-05-16T10:43:54.661-0700 7fd82cbe8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T10:43:54.681-0700 7fd82cbe8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T10:43:54.681-0700 7fd82cbe8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x559d120c0000 @  0x7fd82d5b1680 0x7fd82d5d2824 0x559d068e4447 0x559d068ec4b5 0x559d068e49c8 0x559d068e4b37 0x559d068e5ee4 0x559d066b6ca1 0x559d0688e423 0x559d066a72a7 0x559d066ac53a 0x7fd82d104d84 0x7fd82d289609 0x7fd82cdf2293
2021-05-16T10:43:54.993-0700 7fd82cbe8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new eb3c203d-2f16-4d7d-ac71-01f104cdcf49 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55e2c05f8000 @  0x7f1a195f8680 0x7f1a19619824 0x55e2b4840447 0x55e2b48484b5 0x55e2b48409c8 0x55e2b4840b37 0x55e2b4841ee4 0x55e2b4612ca1 0x55e2b47ea423 0x55e2b46032a7 0x55e2b460853a 0x7f1a1914bd84 0x7f1a192d0609 0x7f1a18e39293
2021-05-16T10:43:55.661-0700 7f1a18c2ff00 -1 Falling back to public interface
2021-05-16T10:43:55.917-0700 7f1a18c2ff00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDbWaFgjPJ3FBAAQPSVAb4T1L91ZOLjvN8btA== --osd-uuid eb3c203d-2f16-4d7d-ac71-01f104cdcf49 
2021-05-16T10:43:56.041-0700 7f75eff29f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T10:43:56.061-0700 7f75eff29f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T10:43:56.061-0700 7f75eff29f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5638376f4000 @  0x7f75f08f2680 0x7f75f0913824 0x56382cf3b447 0x56382cf434b5 0x56382cf3b9c8 0x56382cf3bb37 0x56382cf3cee4 0x56382cd0dca1 0x56382cee5423 0x56382ccfe2a7 0x56382cd0353a 0x7f75f0445d84 0x7f75f05ca609 0x7f75f0133293
2021-05-16T10:43:56.421-0700 7f75eff29f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4deeeedb-be2c-412e-994e-ccb75080541c -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x563a295de000 @  0x7f98098ce680 0x7f98098ef824 0x563a1df5e447 0x563a1df664b5 0x563a1df5e9c8 0x563a1df5eb37 0x563a1df5fee4 0x563a1dd30ca1 0x563a1df08423 0x563a1dd212a7 0x563a1dd2653a 0x7f9809421d84 0x7f98095a6609 0x7f980910f293
2021-05-16T10:43:57.097-0700 7f9808f05f00 -1 Falling back to public interface
2021-05-16T10:43:57.361-0700 7f9808f05f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDcWaFgcRsCLxAAgxw+mpdp0xsPNK4AykQ3WA== --osd-uuid 4deeeedb-be2c-412e-994e-ccb75080541c 
2021-05-16T10:43:57.457-0700 7f01d4935f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T10:43:57.477-0700 7f01d4935f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T10:43:57.477-0700 7f01d4935f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x559927784000 @  0x7f01d52fe680 0x7f01d531f824 0x55991c995447 0x55991c99d4b5 0x55991c9959c8 0x55991c995b37 0x55991c996ee4 0x55991c767ca1 0x55991c93f423 0x55991c7582a7 0x55991c75d53a 0x7f01d4e51d84 0x7f01d4fd6609 0x7f01d4b3f293
2021-05-16T10:43:57.801-0700 7f01d4935f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x560970080000 @  0x7fce21fb9680 0x7fce21fda824 0x560965506447 0x56096550e4b5 0x5609655069c8 0x560965506b37 0x560965507ee4 0x5609652d8ca1 0x5609654b0423 0x5609652c92a7 0x5609652ce53a 0x7fce21b0cd84 0x7fce21c91609 0x7fce217fa293
2021-05-16T10:43:58.489-0700 7fce215f0f00 -1 Falling back to public interface
2021-05-16T10:43:58.753-0700 7fce215f0f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:00,902947493-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:00,913168618-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:00,954532477-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:00,957682495-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6097cd0d-c819-404d-a0d1-83e503514283', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6097cd0d-c819-404d-a0d1-83e503514283 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:04,246225875-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:04,249752301-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6097cd0d-c819-404d-a0d1-83e503514283', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6097cd0d-c819-404d-a0d1-83e503514283 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:07,550161987-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:07,554015367-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6097cd0d-c819-404d-a0d1-83e503514283', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6097cd0d-c819-404d-a0d1-83e503514283 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:10,830372701-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:10,834001209-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6097cd0d-c819-404d-a0d1-83e503514283', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6097cd0d-c819-404d-a0d1-83e503514283 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:17,339856826-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:17,343710567-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6097cd0d-c819-404d-a0d1-83e503514283', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6097cd0d-c819-404d-a0d1-83e503514283 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:21,757082309-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:21,760954444-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6097cd0d-c819-404d-a0d1-83e503514283', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6097cd0d-c819-404d-a0d1-83e503514283 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:25,082950272-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:25,086455047-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6097cd0d-c819-404d-a0d1-83e503514283', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6097cd0d-c819-404d-a0d1-83e503514283 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:29,416070510-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:29,419922999-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6097cd0d-c819-404d-a0d1-83e503514283', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6097cd0d-c819-404d-a0d1-83e503514283 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:33,762949662-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:33,766354951-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6097cd0d-c819-404d-a0d1-83e503514283', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6097cd0d-c819-404d-a0d1-83e503514283 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:37,252026061-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:37,255780696-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6097cd0d-c819-404d-a0d1-83e503514283', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6097cd0d-c819-404d-a0d1-83e503514283 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:41,477439832-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:41,481445038-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6097cd0d-c819-404d-a0d1-83e503514283', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6097cd0d-c819-404d-a0d1-83e503514283 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:44,779552554-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:44,783046259-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6097cd0d-c819-404d-a0d1-83e503514283', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6097cd0d-c819-404d-a0d1-83e503514283 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:44:48,126629331-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:45:11,420605092-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:45:19,845712675-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:45:28,349135528-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:45:36,650772848-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:45:44,973161812-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:45:53,237167362-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:01,699944681-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:01,707424354-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:10,164072711-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:10,172045310-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:18,616017483-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:18,623720547-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:27,029681142-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:27,037424942-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:35,440247556-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:35,448491616-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:35,454647071-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:35,458474182-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:35,466423809-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=50921
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:35,471702276-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:35,491466852-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:35,494659511-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6097cd0d-c819-404d-a0d1-83e503514283', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6097cd0d-c819-404d-a0d1-83e503514283 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T17:46:37.775+0000 7f671a274d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:46:38.043+0000 7f671a274d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:46:38.043+0000 7f671a274d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T17:46:38.056750+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-16T17:46:38.056760+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T17:46:38.057480+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T17:46:38.057480+0000     0       0         0         0         0         0           -           0
2021-05-16T17:46:39.057589+0000     1     256     19188     18932   73.9442   73.9531  0.00222122   0.0130899
2021-05-16T17:46:40.057659+0000     2     256     37940     37684   73.5946     73.25  0.00103259   0.0133841
2021-05-16T17:46:41.057741+0000     3     256     54353     54097   70.4324   64.1133  0.00231442   0.0140541
2021-05-16T17:46:42.057814+0000     4     256     72744     72488    70.783   71.8398  0.00136574   0.0139391
2021-05-16T17:46:43.057907+0000     5     256     91208     90952     71.05    72.125    0.120599   0.0139131
2021-05-16T17:46:44.058011+0000     6     256    108887    108631   70.7169   69.0586 0.000998577   0.0139942
2021-05-16T17:46:45.058084+0000     7     256    127205    126949   70.8359   71.5547  0.00217958   0.0140013
2021-05-16T17:46:46.058172+0000     8     255    143947    143692   70.1559   65.4023  0.00182329   0.0141287
2021-05-16T17:46:47.058269+0000     9     256    160585    160329   69.5811   64.9883   0.0334324    0.014348
2021-05-16T17:46:48.058338+0000    10     255    179265    179010   69.9197   72.9727   0.0343604   0.0142569
2021-05-16T17:46:49.058418+0000    11     256    197544    197288   70.0536   71.3984 0.000921771   0.0142306
2021-05-16T17:46:50.058493+0000    12     255    213497    213242   69.4088   62.3203  0.00105225   0.0143495
2021-05-16T17:46:51.058588+0000    13     255    229906    229651   68.9998   64.0977  0.00491618   0.0144319
2021-05-16T17:46:52.058659+0000    14     256    247276    247020   68.9171   67.8477 0.000849616   0.0144645
2021-05-16T17:46:53.058737+0000    15     255    262397    262142   68.2604   59.0703  0.00524602   0.0146221
2021-05-16T17:46:54.058812+0000    16     256    279325    279069   68.1264   66.1211  0.00109161   0.0146191
2021-05-16T17:46:55.058907+0000    17     255    295508    295253   67.8373   63.2188 0.000835671   0.0146992
2021-05-16T17:46:56.058990+0000    18     255    311705    311450   67.5833   63.2695  0.00150991   0.0147594
2021-05-16T17:46:57.059067+0000    19     256    326714    326458   67.1115    58.625  0.00332527   0.0148427
2021-05-16T17:46:58.059144+0000 min lat: 0.000549893 max lat: 0.233314 avg lat: 0.0149501
2021-05-16T17:46:58.059144+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T17:46:58.059144+0000    20     229    342161    341932    66.778   60.4453   0.0210733   0.0149501
2021-05-16T17:46:59.059350+0000 Total time run:         20.0592
Total writes made:      342161
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     66.631
Stddev Bandwidth:       4.95829
Max bandwidth (MB/sec): 73.9531
Min bandwidth (MB/sec): 58.625
Average IOPS:           17057
Stddev IOPS:            1269.32
Max IOPS:               18932
Min IOPS:               15008
Average Latency(s):     0.0149771
Stddev Latency(s):      0.0342484
Max latency(s):         0.233314
Min latency(s):         0.000549893

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:59,592216193-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 50921


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:46:59,596485525-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:23,428301181-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.16k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:23,436075087-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:31,794250907-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.16k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:31,802440444-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:40,393478326-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.16k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:40,401514756-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:48,709064921-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.16k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:48,716694847-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:57,279193092-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.16k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:57,286904381-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:57,293123858-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:57,296664019-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:57,304062701-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=52299
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:57,309553818-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:57,327830699-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:47:57,331005885-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6097cd0d-c819-404d-a0d1-83e503514283', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6097cd0d-c819-404d-a0d1-83e503514283 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RuvRir:/tmp/ceph-asok.RuvRir -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T17:47:59.675+0000 7f40c138bd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:47:59.927+0000 7f40c138bd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:47:59.927+0000 7f40c138bd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T17:47:59.946623+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T17:47:59.946623+0000     0       0         0         0         0         0           -           0
2021-05-16T17:48:00.946722+0000     1     255     45683     45428   177.416   177.453 0.000445597  0.00555722
2021-05-16T17:48:01.946830+0000     2     255     99987     99732   194.758   212.125 0.000390504  0.00508442
2021-05-16T17:48:02.946945+0000     3     255    147004    146749   191.052    183.66 0.000409279  0.00520364
2021-05-16T17:48:03.947037+0000     4     255    200207    199952    195.24   207.824  0.00446082  0.00511516
2021-05-16T17:48:04.947119+0000     5     255    244683    244428   190.936   173.734   0.0047585  0.00523125
2021-05-16T17:48:05.947189+0000     6     255    294217    293962    191.36   193.492  0.00307984  0.00521782
2021-05-16T17:48:06.947296+0000     7     255    341916    341661   190.638   186.324  0.00475668  0.00524032
2021-05-16T17:48:07.947434+0000 Total time run:       7.00615
Total reads made:     342161
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   190.77
Average IOPS:         48837
Stddev IOPS:          3756.71
Max IOPS:             54304
Min IOPS:             44476
Average Latency(s):   0.00523803
Max latency(s):       0.159645
Min latency(s):       0.000202791

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:48:08,576794934-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 52299


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:48:08,580963205-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:48:31,905841838-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.16k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:48:31,914371956-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:48:40,377864526-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.16k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:48:40,386437313-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:48:48,932841812-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.16k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:48:48,940110800-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:48:57,497218292-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.16k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:48:57,504860101-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:49:06,209655932-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 342.16k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:49:06,217805865-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:49:06,223668971-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T13:49:06,226160953-04:00][RUNNING][ROUND 4/1/40] object_size=4KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:49:06,230123469-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:49:06,238979598-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40809\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.137076\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f2c6452b-f542-4ed4-96bb-bd7d520405ca\nsetting min_mon_release = octopus\nepoch 0\nfsid f2c6452b-f542-4ed4-96bb-bd7d520405ca\nlast_changed 2021-05-16T10:49:31.281663-0700\ncreated 2021-05-16T10:49:31.281663-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40809/0,v1:10.10.1.2:40810/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.137076 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 9db23f15-5300-4ad9-9138-788f928ea1b0\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 8eb98f85-65f4-4167-b4c1-b59af638a2a7\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 ff551f42-535d-450b-9d31-d1787fbcb171\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42809\n  w/ user/pass: admin / f4d0e03d-3a51-41ab-af5f-42021a360ae5\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:49:48 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40809
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.137076
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f2c6452b-f542-4ed4-96bb-bd7d520405ca
setting min_mon_release = octopus
epoch 0
fsid f2c6452b-f542-4ed4-96bb-bd7d520405ca
last_changed 2021-05-16T10:49:31.281663-0700
created 2021-05-16T10:49:31.281663-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40809/0,v1:10.10.1.2:40810/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.137076 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 9db23f15-5300-4ad9-9138-788f928ea1b0
0
start osd.0
add osd1 8eb98f85-65f4-4167-b4c1-b59af638a2a7
1
start osd.1
add osd2 ff551f42-535d-450b-9d31-d1787fbcb171
2
start osd.2


restful urls: https://10.10.1.2:42809
  w/ user/pass: admin / f4d0e03d-3a51-41ab-af5f-42021a360ae5


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T10:49:08.056-0700 7f4d390d51c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T10:49:08.056-0700 7f4d390d51c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T10:49:08.072-0700 7f869b9c01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T10:49:08.072-0700 7f869b9c01c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40809,v1:10.10.1.2:40810] --print /tmp/ceph_monmap.137076 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.137076 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.137076 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42809 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x562ee077e000 @  0x7f50c7613680 0x7f50c7634824 0x7f50c7dcf187 0x7f50c7dd7355 0x7f50c7dcf708 0x7f50c7dcf877 0x7f50c7dd0c24 0x7f50c7de8ec1 0x7f50c7d5b5f3 0x7f50c7dbce97 0x7f50c7dc4b1a 0x7f50c74c7d84 0x7f50c75e3609 0x7f50c71b7293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.xUPTppx7NQ 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9db23f15-5300-4ad9-9138-788f928ea1b0 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA1W6FgvyogNhAA3BTKMAsTSyCC2qZUD7SUhQ== --osd-uuid 9db23f15-5300-4ad9-9138-788f928ea1b0 
2021-05-16T10:49:42.532-0700 7fdf8fd4ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T10:49:42.552-0700 7fdf8fd4ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T10:49:42.552-0700 7fdf8fd4ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56267b316000 @  0x7fdf90717680 0x7fdf90738824 0x56266fbb2447 0x56266fbba4b5 0x56266fbb29c8 0x56266fbb2b37 0x56266fbb3ee4 0x56266f984ca1 0x56266fb5c423 0x56266f9752a7 0x56266f97a53a 0x7fdf9026ad84 0x7fdf903ef609 0x7fdf8ff58293
2021-05-16T10:49:42.864-0700 7fdf8fd4ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8eb98f85-65f4-4167-b4c1-b59af638a2a7 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x557d1b218000 @  0x7fd13ce2e680 0x7fd13ce4f824 0x557d102f7447 0x557d102ff4b5 0x557d102f79c8 0x557d102f7b37 0x557d102f8ee4 0x557d100c9ca1 0x557d102a1423 0x557d100ba2a7 0x557d100bf53a 0x7fd13c981d84 0x7fd13cb06609 0x7fd13c66f293
2021-05-16T10:49:43.560-0700 7fd13c465f00 -1 Falling back to public interface
2021-05-16T10:49:43.820-0700 7fd13c465f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA3W6FgrX2BDxAAzKnzMI1uWtGenZIO0o578Q== --osd-uuid 8eb98f85-65f4-4167-b4c1-b59af638a2a7 
2021-05-16T10:49:43.940-0700 7f37e2534f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T10:49:43.960-0700 7f37e2534f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T10:49:43.960-0700 7f37e2534f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55c7007f8000 @  0x7f37e2efd680 0x7f37e2f1e824 0x55c6f497e447 0x55c6f49864b5 0x55c6f497e9c8 0x55c6f497eb37 0x55c6f497fee4 0x55c6f4750ca1 0x55c6f4928423 0x55c6f47412a7 0x55c6f474653a 0x7f37e2a50d84 0x7f37e2bd5609 0x7f37e273e293
2021-05-16T10:49:44.276-0700 7f37e2534f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ff551f42-535d-450b-9d31-d1787fbcb171 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55e1ea4e6000 @  0x7f1ed8582680 0x7f1ed85a3824 0x55e1df208447 0x55e1df2104b5 0x55e1df2089c8 0x55e1df208b37 0x55e1df209ee4 0x55e1defdaca1 0x55e1df1b2423 0x55e1defcb2a7 0x55e1defd053a 0x7f1ed80d5d84 0x7f1ed825a609 0x7f1ed7dc3293
2021-05-16T10:49:44.964-0700 7f1ed7bb9f00 -1 Falling back to public interface
2021-05-16T10:49:45.224-0700 7f1ed7bb9f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA4W6FgIzbyJhAAwdTpTk9y4Xqlx0sH4KBGZg== --osd-uuid ff551f42-535d-450b-9d31-d1787fbcb171 
2021-05-16T10:49:45.320-0700 7fb12a396f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T10:49:45.344-0700 7fb12a396f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T10:49:45.344-0700 7fb12a396f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x561461e76000 @  0x7fb12ad5f680 0x7fb12ad80824 0x561457737447 0x56145773f4b5 0x5614577379c8 0x561457737b37 0x561457738ee4 0x561457509ca1 0x5614576e1423 0x5614574fa2a7 0x5614574ff53a 0x7fb12a8b2d84 0x7fb12aa37609 0x7fb12a5a0293
2021-05-16T10:49:45.664-0700 7fb12a396f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5598c6f58000 @  0x7faf2aaf3680 0x7faf2ab14824 0x5598bbb88447 0x5598bbb904b5 0x5598bbb889c8 0x5598bbb88b37 0x5598bbb89ee4 0x5598bb95aca1 0x5598bbb32423 0x5598bb94b2a7 0x5598bb95053a 0x7faf2a646d84 0x7faf2a7cb609 0x7faf2a334293
2021-05-16T10:49:46.328-0700 7faf2a12af00 -1 Falling back to public interface
2021-05-16T10:49:46.596-0700 7faf2a12af00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:49:48,790896607-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:49:48,801719211-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:49:48,843171227-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:49:48,846505852-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55e72218-04da-4a1e-a3bf-8a870933316a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55e72218-04da-4a1e-a3bf-8a870933316a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:49:52,385783361-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:49:52,389843720-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55e72218-04da-4a1e-a3bf-8a870933316a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55e72218-04da-4a1e-a3bf-8a870933316a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:49:55,753364825-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:49:55,757306922-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55e72218-04da-4a1e-a3bf-8a870933316a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55e72218-04da-4a1e-a3bf-8a870933316a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:49:59,059043600-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:49:59,062355924-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55e72218-04da-4a1e-a3bf-8a870933316a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55e72218-04da-4a1e-a3bf-8a870933316a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:05,989397592-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:05,993434687-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55e72218-04da-4a1e-a3bf-8a870933316a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55e72218-04da-4a1e-a3bf-8a870933316a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:10,067140019-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:10,070751264-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55e72218-04da-4a1e-a3bf-8a870933316a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55e72218-04da-4a1e-a3bf-8a870933316a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:14,482301102-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:14,485870648-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55e72218-04da-4a1e-a3bf-8a870933316a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55e72218-04da-4a1e-a3bf-8a870933316a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:17,903230527-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:17,906751923-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55e72218-04da-4a1e-a3bf-8a870933316a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55e72218-04da-4a1e-a3bf-8a870933316a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:22,369377750-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:22,373453518-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55e72218-04da-4a1e-a3bf-8a870933316a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55e72218-04da-4a1e-a3bf-8a870933316a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:25,842745745-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:25,846277361-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55e72218-04da-4a1e-a3bf-8a870933316a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55e72218-04da-4a1e-a3bf-8a870933316a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:30,422024897-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:30,426286715-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55e72218-04da-4a1e-a3bf-8a870933316a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55e72218-04da-4a1e-a3bf-8a870933316a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:33,954983850-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:33,958797486-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55e72218-04da-4a1e-a3bf-8a870933316a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55e72218-04da-4a1e-a3bf-8a870933316a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  161 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  161 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  162 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.02  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:50:37,186251077-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:51:00,455825957-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:51:08,977455212-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:51:17,389980573-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:51:25,971243873-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:51:34,207913320-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:51:42,697114278-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:51:51,278199555-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:51:51,286236115-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:51:59,576899652-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:51:59,583775251-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:52:08,004266739-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:52:08,012284384-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:52:16,425164063-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:52:16,433011378-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:52:24,756831173-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:52:24,765061327-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:52:24,770969328-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:52:24,774285389-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:52:24,782029379-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=58981
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:52:24,787510548-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:52:24,807373559-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:52:24,810717943-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55e72218-04da-4a1e-a3bf-8a870933316a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55e72218-04da-4a1e-a3bf-8a870933316a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T17:52:27.124+0000 7fe49a93ad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:52:27.380+0000 7fe49a93ad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:52:27.380+0000 7fe49a93ad00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T17:52:27.389505+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-16T17:52:27.389515+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T17:52:27.390182+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T17:52:27.390182+0000     0       0         0         0         0         0           -           0
2021-05-16T17:52:28.390294+0000     1     255     19417     19162   74.8429   74.8516    0.118172   0.0126686
2021-05-16T17:52:29.390378+0000     2     255     37027     36772   71.8131   68.7891  0.00165991   0.0135379
2021-05-16T17:52:30.390456+0000     3     256     54898     54642   71.1419   69.8047  0.00278569   0.0138146
2021-05-16T17:52:31.390557+0000     4     255     72904     72649   70.9396   70.3398  0.00164765   0.0139005
2021-05-16T17:52:32.390629+0000     5     256     90837     90581     70.76   70.0469 0.000857471   0.0139744
2021-05-16T17:52:33.390736+0000     6     256    108574    108318    70.513   69.2852  0.00199142   0.0140424
2021-05-16T17:52:34.390808+0000     7     255    125631    125376    69.958   66.6328  0.00084091   0.0142485
2021-05-16T17:52:35.390909+0000     8     256    142484    142228   69.4409   65.8281  0.00201447   0.0143401
2021-05-16T17:52:36.390986+0000     9     255    159305    159050   69.0259   65.7109  0.00083543    0.014438
2021-05-16T17:52:37.391063+0000    10     256    174979    174723   68.2451   61.2227  0.00182352   0.0145839
2021-05-16T17:52:38.391171+0000    11     255    190553    190298   67.5713   60.8398  0.00225685    0.014766
2021-05-16T17:52:39.391270+0000    12     255    205460    205205   66.7924   58.2305  0.00240486   0.0148993
2021-05-16T17:52:40.391341+0000    13     256    221909    221653   66.5965     64.25  0.00249544   0.0149753
2021-05-16T17:52:41.391416+0000    14     255    238389    238134   66.4378   64.3789  0.00101696   0.0150159
2021-05-16T17:52:42.391492+0000    15     256    254020    253764   66.0786   61.0547  0.00111682   0.0151079
2021-05-16T17:52:43.391590+0000    16     255    268782    268527   65.5526    57.668   0.0010863    0.015225
2021-05-16T17:52:44.391686+0000    17     255    285262    285007   65.4829    64.375  0.00133689   0.0152177
2021-05-16T17:52:45.391756+0000    18     256    300665    300409   65.1872   60.1641  0.00286315   0.0153031
2021-05-16T17:52:46.391828+0000    19     256    314978    314722   64.6987   55.9102  0.00131359   0.0154094
2021-05-16T17:52:47.391921+0000 min lat: 0.00055362 max lat: 0.219506 avg lat: 0.0154596
2021-05-16T17:52:47.391921+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T17:52:47.391921+0000    20     228    330698    330470   64.5393   61.5156    0.104588   0.0154596
2021-05-16T17:52:48.392071+0000 Total time run:         20.148
Total writes made:      330698
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     64.1151
Stddev Bandwidth:       4.98828
Max bandwidth (MB/sec): 74.8516
Min bandwidth (MB/sec): 55.9102
Average IOPS:           16413
Stddev IOPS:            1277
Max IOPS:               19162
Min IOPS:               14313
Average Latency(s):     0.0155324
Stddev Latency(s):      0.0342867
Max latency(s):         0.219506
Min latency(s):         0.00055362

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:52:48,913168779-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 58981


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:52:48,917456695-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:12,298194127-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.70k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:12,306398001-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:20,980200142-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.70k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:20,988009566-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:29,409975246-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.70k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:29,418426826-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:38,064212593-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.70k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:38,072336497-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:46,370694743-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.70k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:46,378605898-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:46,384402699-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:46,387932612-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:46,396526860-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=60359
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:46,402156808-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:46,421500985-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:46,425030136-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55e72218-04da-4a1e-a3bf-8a870933316a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55e72218-04da-4a1e-a3bf-8a870933316a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.i46AnV:/tmp/ceph-asok.i46AnV -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T17:53:48.628+0000 7fbcedb38d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:53:49.120+0000 7fbcedb38d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:53:49.120+0000 7fbcedb38d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T17:53:49.137565+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T17:53:49.137565+0000     0       0         0         0         0         0           -           0
2021-05-16T17:53:50.137643+0000     1     255     45221     44966   175.622   175.648 0.000293331   0.0055743
2021-05-16T17:53:51.137758+0000     2     255     82166     81911   159.961   144.316  0.00543012   0.0062153
2021-05-16T17:53:52.137851+0000     3     255    130292    130037   169.299   187.992  0.00518911  0.00589734
2021-05-16T17:53:53.137941+0000     4     255    176576    176321   172.169   180.797  0.00511008  0.00580069
2021-05-16T17:53:54.138019+0000     5     255    222726    222471   173.787   180.273  0.00026082  0.00574196
2021-05-16T17:53:55.138123+0000     6     255    260616    260361   169.488   148.008  0.00503802  0.00589413
2021-05-16T17:53:56.138202+0000     7     255    307561    307306    171.47   183.379  0.00504022  0.00582645
2021-05-16T17:53:57.138364+0000 Total time run:       7.56825
Total reads made:     330698
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   170.685
Average IOPS:         43695
Stddev IOPS:          4536.99
Max IOPS:             48126
Min IOPS:             36945
Average Latency(s):   0.00585505
Max latency(s):       0.16133
Min latency(s):       0.000188704

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:57,707320715-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 60359


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:53:57,711374081-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:54:21,103202765-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.70k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:54:21,111054919-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:54:29,462721825-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.70k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:54:29,471072245-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:54:37,848561903-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.70k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:54:37,856569619-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:54:46,387451425-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.70k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:54:46,394590499-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:54:54,726622322-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 330.70k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:54:54,734103719-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:54:54,740454972-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T13:54:54,742651670-04:00][RUNNING][ROUND 5/1/40] object_size=4KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:54:54,746186001-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:54:54,754886348-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40232\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.138173\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6ebb3e23-4293-4135-806f-5a4f11e79942\nsetting min_mon_release = octopus\nepoch 0\nfsid 6ebb3e23-4293-4135-806f-5a4f11e79942\nlast_changed 2021-05-16T10:55:20.114816-0700\ncreated 2021-05-16T10:55:20.114816-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40232/0,v1:10.10.1.2:40233/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.138173 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 8aa9960a-f99f-434c-b874-a3909f905f76\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 32e7c27c-284f-49fe-88d9-50623dc5f5f8\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 acb07db5-734b-46bb-bf77-f60682300e60\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42232\n  w/ user/pass: admin / 69c58ab1-5cfe-4bad-b864-0018ff6bb02d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:55:39 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40232
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.138173
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6ebb3e23-4293-4135-806f-5a4f11e79942
setting min_mon_release = octopus
epoch 0
fsid 6ebb3e23-4293-4135-806f-5a4f11e79942
last_changed 2021-05-16T10:55:20.114816-0700
created 2021-05-16T10:55:20.114816-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40232/0,v1:10.10.1.2:40233/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.138173 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 8aa9960a-f99f-434c-b874-a3909f905f76
0
start osd.0
add osd1 32e7c27c-284f-49fe-88d9-50623dc5f5f8
1
start osd.1
add osd2 acb07db5-734b-46bb-bf77-f60682300e60
2
start osd.2


restful urls: https://10.10.1.2:42232
  w/ user/pass: admin / 69c58ab1-5cfe-4bad-b864-0018ff6bb02d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T10:54:56.563-0700 7fe342eaf1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T10:54:56.563-0700 7fe342eaf1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T10:54:56.579-0700 7f7e9298e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T10:54:56.579-0700 7f7e9298e1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40232,v1:10.10.1.2:40233] --print /tmp/ceph_monmap.138173 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.138173 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.138173 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42232 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x559b98cd2000 @  0x7fd8456ac680 0x7fd8456cd824 0x7fd845e68187 0x7fd845e70355 0x7fd845e68708 0x7fd845e68877 0x7fd845e69c24 0x7fd845e81ec1 0x7fd845df45f3 0x7fd845e55e97 0x7fd845e5db1a 0x7fd845560d84 0x7fd84567c609 0x7fd845250293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.Ew6kiYxql2 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8aa9960a-f99f-434c-b874-a3909f905f76 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCUXKFgKQt4EBAASQES8mXk4sQ9PUtAaI7J5Q== --osd-uuid 8aa9960a-f99f-434c-b874-a3909f905f76 
2021-05-16T10:55:32.923-0700 7fe83f721f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T10:55:32.943-0700 7fe83f721f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T10:55:32.943-0700 7fe83f721f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56392fcbc000 @  0x7fe8400ea680 0x7fe84010b824 0x56392523a447 0x5639252424b5 0x56392523a9c8 0x56392523ab37 0x56392523bee4 0x56392500cca1 0x5639251e4423 0x563924ffd2a7 0x56392500253a 0x7fe83fc3dd84 0x7fe83fdc2609 0x7fe83f92b293
2021-05-16T10:55:33.263-0700 7fe83f721f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 32e7c27c-284f-49fe-88d9-50623dc5f5f8 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55c4080c4000 @  0x7f128524b680 0x7f128526c824 0x55c3fcf3b447 0x55c3fcf434b5 0x55c3fcf3b9c8 0x55c3fcf3bb37 0x55c3fcf3cee4 0x55c3fcd0dca1 0x55c3fcee5423 0x55c3fccfe2a7 0x55c3fcd0353a 0x7f1284d9ed84 0x7f1284f23609 0x7f1284a8c293
2021-05-16T10:55:33.947-0700 7f1284882f00 -1 Falling back to public interface
2021-05-16T10:55:34.207-0700 7f1284882f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCVXKFgfQ3EJhAAOTf4jysXI+78WfWIQ0umww== --osd-uuid 32e7c27c-284f-49fe-88d9-50623dc5f5f8 
2021-05-16T10:55:34.311-0700 7f53a61b0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T10:55:34.331-0700 7f53a61b0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T10:55:34.331-0700 7f53a61b0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x560e3e88c000 @  0x7f53a6b79680 0x7f53a6b9a824 0x560e33326447 0x560e3332e4b5 0x560e333269c8 0x560e33326b37 0x560e33327ee4 0x560e330f8ca1 0x560e332d0423 0x560e330e92a7 0x560e330ee53a 0x7f53a66ccd84 0x7f53a6851609 0x7f53a63ba293
2021-05-16T10:55:34.659-0700 7f53a61b0f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new acb07db5-734b-46bb-bf77-f60682300e60 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5624ecf3e000 @  0x7f051197f680 0x7f05119a0824 0x5624e20d9447 0x5624e20e14b5 0x5624e20d99c8 0x5624e20d9b37 0x5624e20daee4 0x5624e1eabca1 0x5624e2083423 0x5624e1e9c2a7 0x5624e1ea153a 0x7f05114d2d84 0x7f0511657609 0x7f05111c0293
2021-05-16T10:55:35.279-0700 7f0510fb6f00 -1 Falling back to public interface
2021-05-16T10:55:35.543-0700 7f0510fb6f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCWXKFgOxHHORAAmZLAQ7tYMq1RtedMuuuiIg== --osd-uuid acb07db5-734b-46bb-bf77-f60682300e60 
2021-05-16T10:55:35.639-0700 7f7d4cbb6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T10:55:35.659-0700 7f7d4cbb6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T10:55:35.659-0700 7f7d4cbb6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55863f11a000 @  0x7f7d4d57f680 0x7f7d4d5a0824 0x558635182447 0x55863518a4b5 0x5586351829c8 0x558635182b37 0x558635183ee4 0x558634f54ca1 0x55863512c423 0x558634f452a7 0x558634f4a53a 0x7f7d4d0d2d84 0x7f7d4d257609 0x7f7d4cdc0293
2021-05-16T10:55:35.987-0700 7f7d4cbb6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5598dd5ee000 @  0x7f33b6c45680 0x7f33b6c66824 0x5598d2921447 0x5598d29294b5 0x5598d29219c8 0x5598d2921b37 0x5598d2922ee4 0x5598d26f3ca1 0x5598d28cb423 0x5598d26e42a7 0x5598d26e953a 0x7f33b6798d84 0x7f33b691d609 0x7f33b6486293
2021-05-16T10:55:36.643-0700 7f33b627cf00 -1 Falling back to public interface
2021-05-16T10:55:36.915-0700 7f33b627cf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:55:39,079778534-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:55:39,090566645-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:55:39,133966190-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:55:39,137985221-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:55:42,396211048-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:55:42,399752342-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:55:45,790060513-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:55:45,793658113-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:55:49,130168843-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:55:49,133990113-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:55:55,995893372-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:55:55,999716395-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:00,184878413-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:00,188417643-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:04,543956931-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:04,548078406-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:08,831623672-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:08,835207546-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:13,147970742-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:13,152127703-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:16,603596706-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:16,607163188-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:21,062751976-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:21,066230712-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:24,594373323-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:24,597902624-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  161 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  161 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  0.95   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.02   70      up          osd.2  
                       TOTAL  300 GiB  162 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.02  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:27,968867267-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:51,399204714-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:56:59,697836856-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:57:08,018732533-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:57:16,234989744-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:57:24,528658299-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:57:32,964834071-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:57:32,972561740-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:57:41,436695858-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:57:41,444595982-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:57:49,859852583-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:57:49,868152058-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:57:58,113363947-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:57:58,121750705-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:58:06,379814826-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:58:06,387918603-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:58:06,393783703-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:58:06,397081309-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:58:06,403677132-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=66779
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:58:06,410246225-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:58:06,430451420-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:58:06,433480000-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T17:58:08.729+0000 7f6c3f892d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:58:08.981+0000 7f6c3f892d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:58:08.981+0000 7f6c3f892d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T17:58:08.995358+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-16T17:58:08.995370+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T17:58:08.996117+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T17:58:08.996117+0000     0       0         0         0         0         0           -           0
2021-05-16T17:58:09.996214+0000     1     255     20364     20109   78.5421   78.5508   0.0326833   0.0122935
2021-05-16T17:58:10.996319+0000     2     255     40118     39863    77.849   77.1641  0.00259912   0.0126503
2021-05-16T17:58:11.996396+0000     3     256     60109     59853    77.926   78.0859  0.00106325   0.0126957
2021-05-16T17:58:12.996503+0000     4     256     79728     79472   77.6016   76.6367   0.0474653   0.0127888
2021-05-16T17:58:13.996584+0000     5     256     99411     99155   77.4574   76.8867 0.000826152   0.0128342
2021-05-16T17:58:14.996699+0000     6     256    118983    118727   77.2885   76.4531  0.00173048   0.0128626
2021-05-16T17:58:15.997193+0000     7     255    138724    138469   77.2586   77.1172    0.116732   0.0128463
2021-05-16T17:58:16.997280+0000     8     256    158346    158090    77.181   76.6445     0.11874   0.0128727
2021-05-16T17:58:17.997355+0000     9     256    178099    177843   77.1781   77.1602 0.000814921   0.0128845
2021-05-16T17:58:18.997476+0000    10     256    197598    197342   77.0761    76.168  0.00214651   0.0129062
2021-05-16T17:58:19.997555+0000    11     255    215866    215611   76.5563   71.3633  0.00111766   0.0129997
2021-05-16T17:58:20.997877+0000    12     255    232068    231813   75.4488   63.2891  0.00100753   0.0131983
2021-05-16T17:58:21.997964+0000    13     256    249270    249014   74.8132   67.1914  0.00135398   0.0133069
2021-05-16T17:58:22.998074+0000    14     256    265434    265178    73.979   63.1406  0.00189808   0.0134705
2021-05-16T17:58:23.998149+0000    15     256    283497    283241   73.7506   70.5586 0.000767422   0.0135316
2021-05-16T17:58:24.998224+0000    16     256    301115    300859   73.4422   68.8203  0.00219501   0.0135882
2021-05-16T17:58:25.998304+0000    17     256    317809    317553   72.9577   65.2109   0.0641218   0.0136818
2021-05-16T17:58:26.998408+0000    18     256    336951    336695   73.0581   74.7734  0.00223934   0.0136505
2021-05-16T17:58:27.998489+0000    19     256    351444    351188   72.1924   56.6133 0.000999629   0.0138117
2021-05-16T17:58:28.998564+0000 min lat: 0.000561986 max lat: 0.305409 avg lat: 0.0139052
2021-05-16T17:58:28.998564+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T17:58:28.998564+0000    20     217    367972    367755   71.8183   64.7148  0.00708321   0.0139052
2021-05-16T17:58:29.998715+0000 Total time run:         20.0518
Total writes made:      367972
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     71.684
Stddev Bandwidth:       6.47538
Max bandwidth (MB/sec): 78.5508
Min bandwidth (MB/sec): 56.6133
Average IOPS:           18351
Stddev IOPS:            1657.7
Max IOPS:               20109
Min IOPS:               14493
Average Latency(s):     0.0139242
Stddev Latency(s):      0.030589
Max latency(s):         0.305409
Min latency(s):         0.000561986

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:58:30,530626337-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 66779


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:58:30,534564326-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:58:54,273371055-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.97k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:58:54,281162535-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:02,839931635-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.97k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:02,848309076-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:11,279240079-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.97k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:11,287247885-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:19,701983330-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.97k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:19,710288945-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:28,183718647-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.97k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:28,191806123-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:28,197459626-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:28,201178343-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:28,209423124-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=68153
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:28,215150095-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:28,234875498-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:28,238404559-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 56ae3dd5-a2e7-4d1e-88e1-f9c405ac34df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.yjug6y:/tmp/ceph-asok.yjug6y -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T17:59:30.633+0000 7feef2d9dd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:59:30.949+0000 7feef2d9dd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T17:59:30.949+0000 7feef2d9dd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T17:59:30.968914+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T17:59:30.968914+0000     0     117       117         0         0         0           -           0
2021-05-16T17:59:31.969060+0000     1     255     50711     50456   196.963   197.094  0.00231503  0.00505597
2021-05-16T17:59:32.969167+0000     2     255     97994     97739   190.823   184.699  0.00534727  0.00522907
2021-05-16T17:59:33.969268+0000     3     255    147200    146945   191.279   192.211   0.0052531  0.00521964
2021-05-16T17:59:34.969354+0000     4     255    184375    184120   179.762   145.215  0.00547332   0.0055555
2021-05-16T17:59:35.969437+0000     5     255    223211    222956   174.148   151.703   0.0055502  0.00573555
2021-05-16T17:59:36.969523+0000     6     255    261239    260984    169.88   148.547  0.00555803   0.0058804
2021-05-16T17:59:37.969606+0000     7     255    306131    305876    170.66   175.359  0.00567304  0.00585387
2021-05-16T17:59:38.969688+0000     8     255    349162    348907   170.337    168.09  0.00579759  0.00586529
2021-05-16T17:59:39.969930+0000 Total time run:       8.49112
Total reads made:     367972
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   169.282
Average IOPS:         43336
Stddev IOPS:          5196.31
Max IOPS:             50456
Min IOPS:             37175
Average Latency(s):   0.00590383
Max latency(s):       0.139999
Min latency(s):       0.000204394

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:40,564346899-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 68153


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T13:59:40,568939278-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:00:04,192000086-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.97k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:00:04,200221784-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:00:12,964455829-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.97k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:00:12,972829072-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:00:21,478986076-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.97k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:00:21,486947124-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:00:30,257928282-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.97k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:00:30,266464471-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:00:38,853638839-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 367.97k objects, 1.4 GiB
    usage:   2.8 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:00:38,862269716-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:00:38,868260061-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T14:00:38,872546275-04:00][RUNNING][ROUND 1/2/40] object_size=16KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:00:38,876384968-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:00:38,885083532-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40798\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.139280\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c10049ea-5194-4fe2-9696-da129c1aa674\nsetting min_mon_release = octopus\nepoch 0\nfsid c10049ea-5194-4fe2-9696-da129c1aa674\nlast_changed 2021-05-16T11:00:57.431478-0700\ncreated 2021-05-16T11:00:57.431478-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40798/0,v1:10.10.1.2:40799/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.139280 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 5fd15990-40ae-4f9f-99d2-7c1ed1463fb4\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 9cc1e404-ca58-4a64-8a21-d4cb101e1251\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 d27a74ec-5db3-46e9-adbe-0e0f6e8809d3\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42798\n'
10.10.1.2: b'  w/ user/pass: admin / eaa53afd-8e0c-4489-b7b1-0c0776ad2593\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:01:16 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40798
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.139280
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c10049ea-5194-4fe2-9696-da129c1aa674
setting min_mon_release = octopus
epoch 0
fsid c10049ea-5194-4fe2-9696-da129c1aa674
last_changed 2021-05-16T11:00:57.431478-0700
created 2021-05-16T11:00:57.431478-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40798/0,v1:10.10.1.2:40799/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.139280 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 5fd15990-40ae-4f9f-99d2-7c1ed1463fb4
0
start osd.0
add osd1 9cc1e404-ca58-4a64-8a21-d4cb101e1251
1
start osd.1
add osd2 d27a74ec-5db3-46e9-adbe-0e0f6e8809d3
2
start osd.2


restful urls: https://10.10.1.2:42798
  w/ user/pass: admin / eaa53afd-8e0c-4489-b7b1-0c0776ad2593


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T11:00:41.570-0700 7f760b0bc1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:00:41.570-0700 7f760b0bc1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:00:41.586-0700 7f2b25af11c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:00:41.586-0700 7f2b25af11c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd ceph-mon still alive after 1 seconds
WARNING:  ceph-osd ceph-mon still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40798,v1:10.10.1.2:40799] --print /tmp/ceph_monmap.139280 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.139280 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.139280 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42798 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x563ae311e000 @  0x7f50fb8b0680 0x7f50fb8d1824 0x7f50fc06c187 0x7f50fc074355 0x7f50fc06c708 0x7f50fc06c877 0x7f50fc06dc24 0x7f50fc085ec1 0x7f50fbff85f3 0x7f50fc059e97 0x7f50fc061b1a 0x7f50fb764d84 0x7f50fb880609 0x7f50fb454293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.HgBRoBJwAq 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5fd15990-40ae-4f9f-99d2-7c1ed1463fb4 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDlXaFgKe6fIRAAdEc25Rt4UgCUNwuPF0QrgA== --osd-uuid 5fd15990-40ae-4f9f-99d2-7c1ed1463fb4 
2021-05-16T11:01:10.194-0700 7f826f158f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:01:10.218-0700 7f826f158f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:01:10.218-0700 7f826f158f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x562fdec46000 @  0x7f826fb21680 0x7f826fb42824 0x562fd4c09447 0x562fd4c114b5 0x562fd4c099c8 0x562fd4c09b37 0x562fd4c0aee4 0x562fd49dbca1 0x562fd4bb3423 0x562fd49cc2a7 0x562fd49d153a 0x7f826f674d84 0x7f826f7f9609 0x7f826f362293
2021-05-16T11:01:10.602-0700 7f826f158f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9cc1e404-ca58-4a64-8a21-d4cb101e1251 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55c9baf56000 @  0x7f621b7d9680 0x7f621b7fa824 0x55c9b01e6447 0x55c9b01ee4b5 0x55c9b01e69c8 0x55c9b01e6b37 0x55c9b01e7ee4 0x55c9affb8ca1 0x55c9b0190423 0x55c9affa92a7 0x55c9affae53a 0x7f621b32cd84 0x7f621b4b1609 0x7f621b01a293
2021-05-16T11:01:11.238-0700 7f621ae10f00 -1 Falling back to public interface
2021-05-16T11:01:11.506-0700 7f621ae10f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDmXaFgGb7jNxAAyoOj61rpBEEbhIjDlGaz4A== --osd-uuid 9cc1e404-ca58-4a64-8a21-d4cb101e1251 
2021-05-16T11:01:11.602-0700 7f1f0b1bef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:01:11.622-0700 7f1f0b1bef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:01:11.622-0700 7f1f0b1bef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55cbacece000 @  0x7f1f0bb87680 0x7f1f0bba8824 0x55cba2a72447 0x55cba2a7a4b5 0x55cba2a729c8 0x55cba2a72b37 0x55cba2a73ee4 0x55cba2844ca1 0x55cba2a1c423 0x55cba28352a7 0x55cba283a53a 0x7f1f0b6dad84 0x7f1f0b85f609 0x7f1f0b3c8293
2021-05-16T11:01:11.990-0700 7f1f0b1bef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d27a74ec-5db3-46e9-adbe-0e0f6e8809d3 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x561c71786000 @  0x7f4aa7161680 0x7f4aa7182824 0x561c65f44447 0x561c65f4c4b5 0x561c65f449c8 0x561c65f44b37 0x561c65f45ee4 0x561c65d16ca1 0x561c65eee423 0x561c65d072a7 0x561c65d0c53a 0x7f4aa6cb4d84 0x7f4aa6e39609 0x7f4aa69a2293
2021-05-16T11:01:12.650-0700 7f4aa6798f00 -1 Falling back to public interface
2021-05-16T11:01:12.910-0700 7f4aa6798f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDoXaFgKGhBFBAAIchJ6vgjwBv34KQCVjWjbg== --osd-uuid d27a74ec-5db3-46e9-adbe-0e0f6e8809d3 
2021-05-16T11:01:13.030-0700 7fc6a8b37f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:01:13.050-0700 7fc6a8b37f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:01:13.050-0700 7fc6a8b37f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5583965e2000 @  0x7fc6a9500680 0x7fc6a9521824 0x55838a83b447 0x55838a8434b5 0x55838a83b9c8 0x55838a83bb37 0x55838a83cee4 0x55838a60dca1 0x55838a7e5423 0x55838a5fe2a7 0x55838a60353a 0x7fc6a9053d84 0x7fc6a91d8609 0x7fc6a8d41293
2021-05-16T11:01:13.378-0700 7fc6a8b37f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x557e37e88000 @  0x7f9f81b95680 0x7f9f81bb6824 0x557e2d57d447 0x557e2d5854b5 0x557e2d57d9c8 0x557e2d57db37 0x557e2d57eee4 0x557e2d34fca1 0x557e2d527423 0x557e2d3402a7 0x557e2d34553a 0x7f9f816e8d84 0x7f9f8186d609 0x7f9f813d6293
2021-05-16T11:01:14.030-0700 7f9f811ccf00 -1 Falling back to public interface
2021-05-16T11:01:14.302-0700 7f9f811ccf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:16,530160083-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:16,541297319-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:16,582036306-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:16,585282405-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2428d9f5-a4d1-4533-a579-9678e8757773', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2428d9f5-a4d1-4533-a579-9678e8757773 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:20,076054522-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:20,080053466-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2428d9f5-a4d1-4533-a579-9678e8757773', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2428d9f5-a4d1-4533-a579-9678e8757773 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:23,623870713-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:23,627617644-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2428d9f5-a4d1-4533-a579-9678e8757773', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2428d9f5-a4d1-4533-a579-9678e8757773 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:26,997327611-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:27,001363164-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2428d9f5-a4d1-4533-a579-9678e8757773', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2428d9f5-a4d1-4533-a579-9678e8757773 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:33,826350879-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:33,830069466-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2428d9f5-a4d1-4533-a579-9678e8757773', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2428d9f5-a4d1-4533-a579-9678e8757773 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:37,418302531-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:37,421745019-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2428d9f5-a4d1-4533-a579-9678e8757773', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2428d9f5-a4d1-4533-a579-9678e8757773 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:41,957101699-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:41,960482190-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2428d9f5-a4d1-4533-a579-9678e8757773', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2428d9f5-a4d1-4533-a579-9678e8757773 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:46,302863749-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:46,306834480-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2428d9f5-a4d1-4533-a579-9678e8757773', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2428d9f5-a4d1-4533-a579-9678e8757773 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:50,661989109-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:50,666047795-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2428d9f5-a4d1-4533-a579-9678e8757773', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2428d9f5-a4d1-4533-a579-9678e8757773 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:54,174930576-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:54,178351364-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2428d9f5-a4d1-4533-a579-9678e8757773', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2428d9f5-a4d1-4533-a579-9678e8757773 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:58,495868515-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:01:58,499378380-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2428d9f5-a4d1-4533-a579-9678e8757773', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2428d9f5-a4d1-4533-a579-9678e8757773 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:02:02,013507650-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:02:02,017453585-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2428d9f5-a4d1-4533-a579-9678e8757773', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2428d9f5-a4d1-4533-a579-9678e8757773 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:02:05,335604861-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:02:28,757114854-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:02:37,119949938-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:02:45,599325871-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:02:54,030040764-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:02,386863360-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:10,882761937-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:10,891515554-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:19,254076308-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:19,261784431-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:27,542429130-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:27,550526855-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:35,901025399-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:35,909093208-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:44,252974131-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:44,260997887-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:44,266931115-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:44,270844148-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:44,278292283-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=74572
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:44,284006419-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:44,302649809-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:03:44,305959598-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2428d9f5-a4d1-4533-a579-9678e8757773', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2428d9f5-a4d1-4533-a579-9678e8757773 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:03:46.610+0000 7f0724bc2d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:03:46.874+0000 7f0724bc2d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:03:46.874+0000 7f0724bc2d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:03:46.886944+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-16T18:03:46.886952+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T18:03:46.888351+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:03:46.888351+0000     0       0         0         0         0         0           -           0
2021-05-16T18:03:47.888496+0000     1     255     20439     20184   315.326   315.375  0.00328525   0.0120821
2021-05-16T18:03:48.888590+0000     2     256     40561     40305   314.843   314.391    0.125132   0.0123599
2021-05-16T18:03:49.888665+0000     3     256     60439     60183   313.419   310.594  0.00116271   0.0125156
2021-05-16T18:03:50.888737+0000     4     256     79250     78994    308.54   293.922  0.00112397   0.0127502
2021-05-16T18:03:51.888841+0000     5     255     97757     97502   304.663   289.188    0.142077   0.0129696
2021-05-16T18:03:52.888923+0000     6     256    116430    116174   302.507    291.75  0.00169538   0.0131007
2021-05-16T18:03:53.889019+0000     7     255    134796    134541   300.286   286.984  0.00142922   0.0132089
2021-05-16T18:03:54.889102+0000     8     256    153512    153256     299.3   292.422  0.00191014   0.0132618
2021-05-16T18:03:55.889177+0000     9     256    171954    171698   298.059   288.156    0.111392   0.0133612
2021-05-16T18:03:56.889274+0000    10     256    189991    189735   296.433   281.828  0.00297735   0.0134587
2021-05-16T18:03:57.889310+0000    11     256    208033    207777   295.112   281.906    0.019684   0.0135178
2021-05-16T18:03:58.889347+0000    12     255    224619    224364   292.116   259.172 0.000950436   0.0136046
2021-05-16T18:03:59.889413+0000    13     256    241181    240925   289.549   258.766   0.0025948   0.0137736
2021-05-16T18:04:00.889493+0000    14     256    259836    259580   289.686   291.484  0.00195899   0.0137574
2021-05-16T18:04:01.889564+0000    15     256    277455    277199   288.725   275.297   0.0026766   0.0138314
2021-05-16T18:04:02.889677+0000    16     256    295857    295601   288.649   287.531  0.00116064   0.0138257
2021-05-16T18:04:03.889764+0000    17     255    312968    312713   287.396   267.375 0.000987796   0.0139023
2021-05-16T18:04:04.889833+0000    18     256    329906    329650   286.131   264.641  0.00255274   0.0139478
2021-05-16T18:04:05.889903+0000    19     255    346554    346299   284.762   260.141  0.00098431   0.0140289
2021-05-16T18:04:06.889973+0000 min lat: 0.000597312 max lat: 0.233903 avg lat: 0.01417
2021-05-16T18:04:06.889973+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:04:06.889973+0000    20     241    361294    361053    282.05   230.531   0.0024923     0.01417
2021-05-16T18:04:07.890260+0000 Total time run:         20.0299
Total writes made:      361294
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     281.839
Stddev Bandwidth:       20.9125
Max bandwidth (MB/sec): 315.375
Min bandwidth (MB/sec): 230.531
Average IOPS:           18037
Stddev IOPS:            1338.4
Max IOPS:               20184
Min IOPS:               14754
Average Latency(s):     0.014177
Stddev Latency(s):      0.032308
Max latency(s):         0.233903
Min latency(s):         0.000597312

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:04:08,387802244-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 74572


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:04:08,392119325-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:04:32,172653266-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.30k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:04:32,181572364-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:04:40,539072055-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.30k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:04:40,547195939-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:04:48,926438967-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.30k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:04:48,934326527-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:04:57,351197870-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.30k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:04:57,359741644-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:06,105777688-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.30k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:06,114287448-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:06,120491746-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:06,124417482-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:06,133032249-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=75962
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:06,139185370-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:06,159239541-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:06,162781016-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2428d9f5-a4d1-4533-a579-9678e8757773', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2428d9f5-a4d1-4533-a579-9678e8757773 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.JYp0lG:/tmp/ceph-asok.JYp0lG -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:05:08.451+0000 7f670f783d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:05:08.835+0000 7f670f783d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:05:08.835+0000 7f670f783d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:05:08.852994+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:05:08.852994+0000     0       0         0         0         0         0           -           0
2021-05-16T18:05:09.853088+0000     1     255     49094     48839   762.973   763.109  0.00493822  0.00522054
2021-05-16T18:05:10.853269+0000     2     255     88534     88279   689.556    616.25   0.0041111  0.00578348
2021-05-16T18:05:11.853400+0000     3     255    127143    126888   660.767   603.266  0.00807586  0.00603969
2021-05-16T18:05:12.853489+0000     4     255    162995    162740   635.611   560.188  0.00578314  0.00628264
2021-05-16T18:05:13.853834+0000     5     255    212661    212406   663.646   776.031 0.000969272  0.00600857
2021-05-16T18:05:14.854011+0000     6     255    258135    257880   671.439   710.531  0.00570991  0.00594939
2021-05-16T18:05:15.854133+0000     7     255    302742    302487   675.076   696.984  0.00557386  0.00591789
2021-05-16T18:05:16.854270+0000     8     255    351254    350999   685.428       758  0.00211548  0.00582407
2021-05-16T18:05:17.854495+0000 Total time run:       8.36721
Total reads made:     361294
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   674.684
Average IOPS:         43179
Stddev IOPS:          5271.76
Max IOPS:             49666
Min IOPS:             35852
Average Latency(s):   0.0059236
Max latency(s):       0.189903
Min latency(s):       0.000198213

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:18,483989228-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 75962


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:18,488590574-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:42,155875408-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.30k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:42,164476839-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:50,713876686-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.30k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:50,722360998-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:59,135690356-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.30k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:05:59,143532150-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:06:07,877799613-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.30k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:06:07,885368394-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:06:16,427340363-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.30k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:06:16,435863918-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:06:16,442301735-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T14:06:16,444550550-04:00][RUNNING][ROUND 2/2/40] object_size=16KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:06:16,447872933-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:06:16,456879406-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40412\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.140396\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 242d0264-d0a2-4360-b60a-272c69cde21e\nsetting min_mon_release = octopus\nepoch 0\nfsid 242d0264-d0a2-4360-b60a-272c69cde21e\nlast_changed 2021-05-16T11:06:44.527637-0700\ncreated 2021-05-16T11:06:44.527637-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40412/0,v1:10.10.1.2:40413/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.140396 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 56cd81ea-c242-424a-bfad-85c22ec93da3\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 90810e49-eeb9-4fca-b9bc-012be162a52f\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 5a411b8d-9547-43d8-806e-34ca40c71947\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42412\n  w/ user/pass: admin / a41dff94-fdf3-4b8d-803c-8201b8f96c54\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 14:07:01 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40412
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.140396
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 242d0264-d0a2-4360-b60a-272c69cde21e
setting min_mon_release = octopus
epoch 0
fsid 242d0264-d0a2-4360-b60a-272c69cde21e
last_changed 2021-05-16T11:06:44.527637-0700
created 2021-05-16T11:06:44.527637-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40412/0,v1:10.10.1.2:40413/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.140396 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 56cd81ea-c242-424a-bfad-85c22ec93da3
0
start osd.0
add osd1 90810e49-eeb9-4fca-b9bc-012be162a52f
1
start osd.1
add osd2 5a411b8d-9547-43d8-806e-34ca40c71947
2
start osd.2


restful urls: https://10.10.1.2:42412
  w/ user/pass: admin / a41dff94-fdf3-4b8d-803c-8201b8f96c54


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T11:06:18.841-0700 7fc8c8d131c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:06:18.841-0700 7fc8c8d131c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:06:18.861-0700 7fddaaf641c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:06:18.861-0700 7fddaaf641c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd ceph-mon still alive after 1 seconds
WARNING:  ceph-osd ceph-mon still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40412,v1:10.10.1.2:40413] --print /tmp/ceph_monmap.140396 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.140396 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.140396 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42412 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55b57c184000 @  0x7fc400258680 0x7fc400279824 0x7fc400a14187 0x7fc400a1c355 0x7fc400a14708 0x7fc400a14877 0x7fc400a15c24 0x7fc400a2dec1 0x7fc4009a05f3 0x7fc400a01e97 0x7fc400a09b1a 0x7fc40010cd84 0x7fc400228609 0x7fc3ffdfc293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.eyMWhyT7oT 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 56cd81ea-c242-424a-bfad-85c22ec93da3 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA/X6Fgl2vNChAAL5dmFm3en2S24rYvGbjV8g== --osd-uuid 56cd81ea-c242-424a-bfad-85c22ec93da3 
2021-05-16T11:06:55.814-0700 7fa1ca6e7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:06:55.838-0700 7fa1ca6e7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:06:55.838-0700 7fa1ca6e7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55b0f3854000 @  0x7fa1cb0b0680 0x7fa1cb0d1824 0x55b0e8c47447 0x55b0e8c4f4b5 0x55b0e8c479c8 0x55b0e8c47b37 0x55b0e8c48ee4 0x55b0e8a19ca1 0x55b0e8bf1423 0x55b0e8a0a2a7 0x55b0e8a0f53a 0x7fa1cac03d84 0x7fa1cad88609 0x7fa1ca8f1293
2021-05-16T11:06:56.146-0700 7fa1ca6e7f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 90810e49-eeb9-4fca-b9bc-012be162a52f -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x561233f1a000 @  0x7f5d08669680 0x7f5d0868a824 0x561229e8c447 0x561229e944b5 0x561229e8c9c8 0x561229e8cb37 0x561229e8dee4 0x561229c5eca1 0x561229e36423 0x561229c4f2a7 0x561229c5453a 0x7f5d081bcd84 0x7f5d08341609 0x7f5d07eaa293
2021-05-16T11:06:56.766-0700 7f5d07ca0f00 -1 Falling back to public interface
2021-05-16T11:06:57.034-0700 7f5d07ca0f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBAX6FgdC6dGxAAtxKdJCq8DIfJGARFHaPh1A== --osd-uuid 90810e49-eeb9-4fca-b9bc-012be162a52f 
2021-05-16T11:06:57.134-0700 7f7a67838f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:06:57.158-0700 7f7a67838f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:06:57.158-0700 7f7a67838f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55a02eab4000 @  0x7f7a68201680 0x7f7a68222824 0x55a023091447 0x55a0230994b5 0x55a0230919c8 0x55a023091b37 0x55a023092ee4 0x55a022e63ca1 0x55a02303b423 0x55a022e542a7 0x55a022e5953a 0x7f7a67d54d84 0x7f7a67ed9609 0x7f7a67a42293
2021-05-16T11:06:57.474-0700 7f7a67838f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5a411b8d-9547-43d8-806e-34ca40c71947 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x564df8992000 @  0x7f88d413b680 0x7f88d415c824 0x564ded375447 0x564ded37d4b5 0x564ded3759c8 0x564ded375b37 0x564ded376ee4 0x564ded147ca1 0x564ded31f423 0x564ded1382a7 0x564ded13d53a 0x7f88d3c8ed84 0x7f88d3e13609 0x7f88d397c293
2021-05-16T11:06:58.094-0700 7f88d3772f00 -1 Falling back to public interface
2021-05-16T11:06:58.354-0700 7f88d3772f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBBX6FgIWioLRAA9fn2X+FOGd5ISXapsr8OhA== --osd-uuid 5a411b8d-9547-43d8-806e-34ca40c71947 
2021-05-16T11:06:58.478-0700 7f06579d8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:06:58.498-0700 7f06579d8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:06:58.498-0700 7f06579d8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5594ab2e6000 @  0x7f06583a1680 0x7f06583c2824 0x55949f5fa447 0x55949f6024b5 0x55949f5fa9c8 0x55949f5fab37 0x55949f5fbee4 0x55949f3ccca1 0x55949f5a4423 0x55949f3bd2a7 0x55949f3c253a 0x7f0657ef4d84 0x7f0658079609 0x7f0657be2293
2021-05-16T11:06:58.870-0700 7f06579d8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x559ac962e000 @  0x7f5fb0216680 0x7f5fb0237824 0x559abf251447 0x559abf2594b5 0x559abf2519c8 0x559abf251b37 0x559abf252ee4 0x559abf023ca1 0x559abf1fb423 0x559abf0142a7 0x559abf01953a 0x7f5fafd69d84 0x7f5fafeee609 0x7f5fafa57293
2021-05-16T11:06:59.538-0700 7f5faf84df00 -1 Falling back to public interface
2021-05-16T11:06:59.810-0700 7f5faf84df00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:01,941926893-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:01,952429487-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:01,994321241-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:01,997938527-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a19e3a2b-aa31-40ae-a112-3e6f64d11635', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a19e3a2b-aa31-40ae-a112-3e6f64d11635 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:05,418273412-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:05,421756237-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a19e3a2b-aa31-40ae-a112-3e6f64d11635', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a19e3a2b-aa31-40ae-a112-3e6f64d11635 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:08,976261104-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:08,979975432-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a19e3a2b-aa31-40ae-a112-3e6f64d11635', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a19e3a2b-aa31-40ae-a112-3e6f64d11635 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:12,449548305-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:12,452998729-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a19e3a2b-aa31-40ae-a112-3e6f64d11635', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a19e3a2b-aa31-40ae-a112-3e6f64d11635 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:19,205857768-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:19,209435210-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a19e3a2b-aa31-40ae-a112-3e6f64d11635', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a19e3a2b-aa31-40ae-a112-3e6f64d11635 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:23,219989742-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:23,223596640-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a19e3a2b-aa31-40ae-a112-3e6f64d11635', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a19e3a2b-aa31-40ae-a112-3e6f64d11635 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:27,858127576-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:27,861843478-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a19e3a2b-aa31-40ae-a112-3e6f64d11635', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a19e3a2b-aa31-40ae-a112-3e6f64d11635 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:32,147959063-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:32,152073494-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a19e3a2b-aa31-40ae-a112-3e6f64d11635', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a19e3a2b-aa31-40ae-a112-3e6f64d11635 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:36,387954440-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:36,391705117-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a19e3a2b-aa31-40ae-a112-3e6f64d11635', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a19e3a2b-aa31-40ae-a112-3e6f64d11635 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:40,410695733-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:40,414238530-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a19e3a2b-aa31-40ae-a112-3e6f64d11635', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a19e3a2b-aa31-40ae-a112-3e6f64d11635 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:44,771420770-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:44,775080275-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a19e3a2b-aa31-40ae-a112-3e6f64d11635', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a19e3a2b-aa31-40ae-a112-3e6f64d11635 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:48,315931853-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:48,320034392-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a19e3a2b-aa31-40ae-a112-3e6f64d11635', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a19e3a2b-aa31-40ae-a112-3e6f64d11635 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:07:51,649356553-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:08:14,971340532-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:08:23,556514247-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:08:32,091992517-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:08:40,452691184-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:08:48,908822660-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:08:57,380266639-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:08:57,388327856-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:05,795211932-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:05,803530934-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:14,198387783-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:14,206505095-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:22,557407809-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:22,564865231-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:31,111504401-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:31,120029930-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:31,126301234-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:31,129804757-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:31,137893025-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=82343
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:31,144008716-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:31,163227247-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:31,166789340-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a19e3a2b-aa31-40ae-a112-3e6f64d11635', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a19e3a2b-aa31-40ae-a112-3e6f64d11635 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:09:33.560+0000 7f6055801d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:09:33.824+0000 7f6055801d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:09:33.824+0000 7f6055801d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:09:33.835069+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-16T18:09:33.835079+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T18:09:33.836436+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:09:33.836436+0000     0       0         0         0         0         0           -           0
2021-05-16T18:09:34.836531+0000     1     256     20553     20297   317.107   317.141  0.00154653   0.0122448
2021-05-16T18:09:35.836599+0000     2     256     40123     39867   311.434   305.781 0.000833526   0.0126398
2021-05-16T18:09:36.836658+0000     3     256     59451     59195   308.283       302  0.00179229   0.0127562
2021-05-16T18:09:37.836710+0000     4     255     78610     78355   306.052   299.375    0.075018   0.0129858
2021-05-16T18:09:38.836772+0000     5     255     98158     97903   305.926   305.438   0.0054055   0.0130076
2021-05-16T18:09:39.836829+0000     6     255    118018    117763   306.654   310.312  0.00106358   0.0129895
2021-05-16T18:09:40.836884+0000     7     255    138029    137774   307.511   312.672   0.0432577   0.0129703
2021-05-16T18:09:41.836936+0000     8     255    157126    156871   306.369   298.391  0.00242772   0.0130127
2021-05-16T18:09:42.836993+0000     9     255    175009    174754   303.373   279.422  0.00174836   0.0131401
2021-05-16T18:09:43.837045+0000    10     256    192348    192092   300.125   270.906 0.000967799   0.0132963
2021-05-16T18:09:44.837089+0000    11     256    211793    211537   300.461   303.828  0.00119204   0.0132811
2021-05-16T18:09:45.837173+0000    12     255    230704    230449   300.045     295.5  0.00295031   0.0132835
2021-05-16T18:09:46.837209+0000    13     256    246349    246093   295.767   244.438  0.00208868   0.0134747
2021-05-16T18:09:47.837282+0000    14     255    262925    262670   293.141   259.016  0.00489532    0.013625
2021-05-16T18:09:48.837331+0000    15     256    280393    280137   291.792   272.922   0.0638245   0.0136848
2021-05-16T18:09:49.837400+0000    16     256    297249    296993   290.015   263.375  0.00457594    0.013719
2021-05-16T18:09:50.837449+0000    17     256    314173    313917   288.509   264.438  0.00119734   0.0138254
2021-05-16T18:09:51.837529+0000    18     256    330951    330695   287.044   262.156   0.0544656   0.0138683
2021-05-16T18:09:52.837582+0000    19     256    346552    346296   284.766   243.766   0.0919795   0.0140214
2021-05-16T18:09:53.837655+0000 min lat: 0.000573648 max lat: 0.248884 avg lat: 0.0142091
2021-05-16T18:09:53.837655+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:09:53.837655+0000    20     225    360139    359914   281.166   212.781  0.00282539   0.0142091
2021-05-16T18:09:54.837799+0000 Total time run:         20.0427
Total writes made:      360139
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     280.759
Stddev Bandwidth:       28.2485
Max bandwidth (MB/sec): 317.141
Min bandwidth (MB/sec): 212.781
Average IOPS:           17968
Stddev IOPS:            1807.9
Max IOPS:               20297
Min IOPS:               13618
Average Latency(s):     0.0142272
Stddev Latency(s):      0.0282231
Max latency(s):         0.248884
Min latency(s):         0.000573648

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:55,427159740-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 82343


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:09:55,431290642-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:18,997942753-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 360.14k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:19,006757976-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:27,530043462-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 360.14k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:27,538262837-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:36,000282839-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 360.14k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:36,008582774-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:44,531788447-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 360.14k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:44,539755718-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:53,172720146-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 360.14k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:53,181012887-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:53,187206004-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:53,190546531-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:53,198016457-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=83710
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:53,203607252-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:53,223192783-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:10:53,226622798-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a19e3a2b-aa31-40ae-a112-3e6f64d11635', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a19e3a2b-aa31-40ae-a112-3e6f64d11635 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.BCY0lV:/tmp/ceph-asok.BCY0lV -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:10:55.436+0000 7f505181fd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:10:55.704+0000 7f505181fd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:10:55.704+0000 7f505181fd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:10:55.723117+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:10:55.723117+0000     0       0         0         0         0         0           -           0
2021-05-16T18:10:56.723220+0000     1     256     43393     43137   673.891   674.016  0.00421422  0.00588189
2021-05-16T18:10:57.723354+0000     2     255     81236     80981   632.563   591.312  0.00544315  0.00630842
2021-05-16T18:10:58.723509+0000     3     255    124244    123989   645.674       672  0.00566372  0.00618354
2021-05-16T18:10:59.723655+0000     4     255    165011    164756   643.478   636.984   0.0056077   0.0062065
2021-05-16T18:11:00.723741+0000     5     255    208101    207846   649.427   673.281  0.00574483  0.00615068
2021-05-16T18:11:01.723823+0000     6     255    246304    246049   640.668   596.922  0.00550425   0.0062358
2021-05-16T18:11:02.723950+0000     7     255    287817    287562   641.796   648.641  0.00572299  0.00622521
2021-05-16T18:11:03.724056+0000     8     255    330777    330522   645.468    671.25   0.0225767  0.00618489
2021-05-16T18:11:04.724232+0000 Total time run:       8.66513
Total reads made:     360139
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   649.404
Average IOPS:         41561
Stddev IOPS:          2207.6
Max IOPS:             43137
Min IOPS:             37844
Average Latency(s):   0.0061546
Max latency(s):       0.103117
Min latency(s):       0.000225714

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:11:05,399232074-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 83710


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:11:05,403378594-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:11:28,698282180-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 360.14k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:11:28,706402657-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:11:37,048097661-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 360.14k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:11:37,056178984-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:11:45,416157604-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 360.14k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:11:45,424385873-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:11:54,230506644-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 360.14k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:11:54,238565005-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:12:03,051489352-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 360.14k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:12:03,059650555-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:12:03,066551611-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T14:12:03,069220917-04:00][RUNNING][ROUND 3/2/40] object_size=16KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:12:03,072721494-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:12:03,081069418-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40308\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.141483\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 737239b6-9168-450b-9151-28c13076dcdd\nsetting min_mon_release = octopus\nepoch 0\nfsid 737239b6-9168-450b-9151-28c13076dcdd\nlast_changed 2021-05-16T11:12:32.256084-0700\ncreated 2021-05-16T11:12:32.256084-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40308/0,v1:10.10.1.2:40309/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.141483 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 dc41894b-31cf-4678-bbd1-9cc3b7f10a38\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 8b69ede5-dbc9-4922-8d17-2b75f57e8d55\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 6ba0021a-7b9d-4145-b31c-9582466e317b\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42308\n  w/ user/pass: admin / d55f8634-4db5-49c6-b530-a1e515beb826\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:12:50 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40308
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.141483
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 737239b6-9168-450b-9151-28c13076dcdd
setting min_mon_release = octopus
epoch 0
fsid 737239b6-9168-450b-9151-28c13076dcdd
last_changed 2021-05-16T11:12:32.256084-0700
created 2021-05-16T11:12:32.256084-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40308/0,v1:10.10.1.2:40309/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.141483 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 dc41894b-31cf-4678-bbd1-9cc3b7f10a38
0
start osd.0
add osd1 8b69ede5-dbc9-4922-8d17-2b75f57e8d55
1
start osd.1
add osd2 6ba0021a-7b9d-4145-b31c-9582466e317b
2
start osd.2


restful urls: https://10.10.1.2:42308
  w/ user/pass: admin / d55f8634-4db5-49c6-b530-a1e515beb826


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T11:12:05.912-0700 7f2cd55fa1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:12:05.912-0700 7f2cd55fa1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:12:05.928-0700 7f0aa554b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:12:05.928-0700 7f0aa554b1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd ceph-mon still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40308,v1:10.10.1.2:40309] --print /tmp/ceph_monmap.141483 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.141483 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.141483 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42308 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x562df8cae000 @  0x7f3ba6c0c680 0x7f3ba6c2d824 0x7f3ba73c8187 0x7f3ba73d0355 0x7f3ba73c8708 0x7f3ba73c8877 0x7f3ba73c9c24 0x7f3ba73e1ec1 0x7f3ba73545f3 0x7f3ba73b5e97 0x7f3ba73bdb1a 0x7f3ba6ac0d84 0x7f3ba6bdc609 0x7f3ba67b0293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.skUIl0w4ev 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new dc41894b-31cf-4678-bbd1-9cc3b7f10a38 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCbYKFg4yN3JhAAyhy9c74O8zoEChNT8u3AUA== --osd-uuid dc41894b-31cf-4678-bbd1-9cc3b7f10a38 
2021-05-16T11:12:44.305-0700 7f7865f01f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:12:44.325-0700 7f7865f01f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:12:44.325-0700 7f7865f01f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55c8df30a000 @  0x7f78668ca680 0x7f78668eb824 0x55c8d3cdf447 0x55c8d3ce74b5 0x55c8d3cdf9c8 0x55c8d3cdfb37 0x55c8d3ce0ee4 0x55c8d3ab1ca1 0x55c8d3c89423 0x55c8d3aa22a7 0x55c8d3aa753a 0x7f786641dd84 0x7f78665a2609 0x7f786610b293
2021-05-16T11:12:44.637-0700 7f7865f01f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8b69ede5-dbc9-4922-8d17-2b75f57e8d55 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55bbb3b84000 @  0x7f896e004680 0x7f896e025824 0x55bba957e447 0x55bba95864b5 0x55bba957e9c8 0x55bba957eb37 0x55bba957fee4 0x55bba9350ca1 0x55bba9528423 0x55bba93412a7 0x55bba934653a 0x7f896db57d84 0x7f896dcdc609 0x7f896d845293
2021-05-16T11:12:45.273-0700 7f896d63bf00 -1 Falling back to public interface
2021-05-16T11:12:45.533-0700 7f896d63bf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCcYKFgh8rRORAAnAmFTuK/lkDw0R9oXpSbyA== --osd-uuid 8b69ede5-dbc9-4922-8d17-2b75f57e8d55 
2021-05-16T11:12:45.649-0700 7f5310aa5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:12:45.669-0700 7f5310aa5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:12:45.669-0700 7f5310aa5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56288ce86000 @  0x7f531146e680 0x7f531148f824 0x5628817b2447 0x5628817ba4b5 0x5628817b29c8 0x5628817b2b37 0x5628817b3ee4 0x562881584ca1 0x56288175c423 0x5628815752a7 0x56288157a53a 0x7f5310fc1d84 0x7f5311146609 0x7f5310caf293
2021-05-16T11:12:45.981-0700 7f5310aa5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6ba0021a-7b9d-4145-b31c-9582466e317b -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55f46a20a000 @  0x7fd12a946680 0x7fd12a967824 0x55f45f9b1447 0x55f45f9b94b5 0x55f45f9b19c8 0x55f45f9b1b37 0x55f45f9b2ee4 0x55f45f783ca1 0x55f45f95b423 0x55f45f7742a7 0x55f45f77953a 0x7fd12a499d84 0x7fd12a61e609 0x7fd12a187293
2021-05-16T11:12:46.673-0700 7fd129f7df00 -1 Falling back to public interface
2021-05-16T11:12:46.933-0700 7fd129f7df00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCeYKFg+jK5FRAAtG9q7iFFQCAlJLMBgzfirw== --osd-uuid 6ba0021a-7b9d-4145-b31c-9582466e317b 
2021-05-16T11:12:47.037-0700 7fb803250f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:12:47.057-0700 7fb803250f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:12:47.057-0700 7fb803250f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55b9b220a000 @  0x7fb803c19680 0x7fb803c3a824 0x55b9a6ac6447 0x55b9a6ace4b5 0x55b9a6ac69c8 0x55b9a6ac6b37 0x55b9a6ac7ee4 0x55b9a6898ca1 0x55b9a6a70423 0x55b9a68892a7 0x55b9a688e53a 0x7fb80376cd84 0x7fb8038f1609 0x7fb80345a293
2021-05-16T11:12:47.409-0700 7fb803250f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x561ba6f4c000 @  0x7fec8ed9c680 0x7fec8edbd824 0x561b9cdec447 0x561b9cdf44b5 0x561b9cdec9c8 0x561b9cdecb37 0x561b9cdedee4 0x561b9cbbeca1 0x561b9cd96423 0x561b9cbaf2a7 0x561b9cbb453a 0x7fec8e8efd84 0x7fec8ea74609 0x7fec8e5dd293
2021-05-16T11:12:48.093-0700 7fec8e3d3f00 -1 Falling back to public interface
2021-05-16T11:12:48.365-0700 7fec8e3d3f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:12:50,507162024-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:12:50,518276717-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:12:50,558607662-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:12:50,561899697-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b144c03f-a350-4634-9d38-fb4ceb68688a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b144c03f-a350-4634-9d38-fb4ceb68688a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:12:54,066926253-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:12:54,070844936-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b144c03f-a350-4634-9d38-fb4ceb68688a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b144c03f-a350-4634-9d38-fb4ceb68688a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:12:57,504472394-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:12:57,508033575-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b144c03f-a350-4634-9d38-fb4ceb68688a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b144c03f-a350-4634-9d38-fb4ceb68688a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:00,863057961-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:00,866693171-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b144c03f-a350-4634-9d38-fb4ceb68688a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b144c03f-a350-4634-9d38-fb4ceb68688a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:07,651390178-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:07,654771150-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b144c03f-a350-4634-9d38-fb4ceb68688a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b144c03f-a350-4634-9d38-fb4ceb68688a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:11,236069210-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:11,239023291-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b144c03f-a350-4634-9d38-fb4ceb68688a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b144c03f-a350-4634-9d38-fb4ceb68688a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:15,618048455-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:15,621597052-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b144c03f-a350-4634-9d38-fb4ceb68688a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b144c03f-a350-4634-9d38-fb4ceb68688a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:20,031640895-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:20,035096588-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b144c03f-a350-4634-9d38-fb4ceb68688a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b144c03f-a350-4634-9d38-fb4ceb68688a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:23,590039093-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:23,593715951-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b144c03f-a350-4634-9d38-fb4ceb68688a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b144c03f-a350-4634-9d38-fb4ceb68688a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:28,153775422-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:28,156900544-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b144c03f-a350-4634-9d38-fb4ceb68688a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b144c03f-a350-4634-9d38-fb4ceb68688a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:31,763100093-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:31,766690219-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b144c03f-a350-4634-9d38-fb4ceb68688a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b144c03f-a350-4634-9d38-fb4ceb68688a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:35,309455426-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:35,313550780-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b144c03f-a350-4634-9d38-fb4ceb68688a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b144c03f-a350-4634-9d38-fb4ceb68688a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:13:38,653090309-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:14:02,017779112-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:14:10,598761457-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:14:19,071308222-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:14:27,647736687-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:14:35,954841078-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:14:44,360031560-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:14:44,368445940-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:14:52,790844346-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:14:52,798750160-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:15:01,384162066-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:15:01,392991536-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:15:10,005627155-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:15:10,013789922-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:15:18,588808147-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:15:18,597598252-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:15:18,604116890-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:15:18,607727674-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:15:18,616103590-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=90155
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:15:18,622599675-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:15:18,642584364-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:15:18,645958022-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b144c03f-a350-4634-9d38-fb4ceb68688a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b144c03f-a350-4634-9d38-fb4ceb68688a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:15:21.145+0000 7f176fb5cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:15:21.397+0000 7f176fb5cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:15:21.397+0000 7f176fb5cd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:15:21.412624+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-16T18:15:21.412634+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T18:15:21.414047+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:15:21.414047+0000     0       0         0         0         0         0           -           0
2021-05-16T18:15:22.414168+0000     1     256     20928     20672    322.96       323  0.00301477   0.0120687
2021-05-16T18:15:23.414269+0000     2     256     40929     40673   317.722   312.516  0.00436066    0.012288
2021-05-16T18:15:24.414368+0000     3     255     60081     59826    311.56   299.266  0.00263097   0.0125997
2021-05-16T18:15:25.414688+0000     4     256     77793     77537    302.83   276.734  0.00143396   0.0130268
2021-05-16T18:15:26.414782+0000     5     256     95849     95593   298.684   282.125  0.00165376   0.0132486
2021-05-16T18:15:27.414887+0000     6     256    114408    114152   297.229   289.984  0.00278338   0.0133541
2021-05-16T18:15:28.414976+0000     7     256    133405    133149   297.168   296.828  0.00227928   0.0133661
2021-05-16T18:15:29.415055+0000     8     255    152340    152085   297.003   295.875  0.00152081   0.0134111
2021-05-16T18:15:30.415159+0000     9     256    169707    169451   294.149   271.344   0.0273258   0.0135421
2021-05-16T18:15:31.415254+0000    10     256    185828    185572   289.921   251.891  0.00232917   0.0137333
2021-05-16T18:15:32.415348+0000    11     256    203110    202854   288.111   270.031  0.00158241   0.0137652
2021-05-16T18:15:33.415443+0000    12     256    219949    219693   286.025   263.109  0.00206904   0.0139162
2021-05-16T18:15:34.415535+0000    13     256    237844    237588    285.53   279.609   0.0117197    0.013979
2021-05-16T18:15:35.415629+0000    14     255    254049    253794    283.22   253.219  0.00214076   0.0140938
2021-05-16T18:15:36.415719+0000    15     255    271960    271705   282.994   279.859     0.01274   0.0141098
2021-05-16T18:15:37.415837+0000    16     255    289270    289015    282.21   270.469 0.000889892   0.0141483
2021-05-16T18:15:38.415927+0000    17     256    307475    307219   282.339   284.438   0.0286724    0.014152
2021-05-16T18:15:39.416021+0000    18     255    324994    324739   281.861    273.75  0.00456667   0.0141689
2021-05-16T18:15:40.416115+0000    19     256    343454    343198   282.204   288.422  0.00212205   0.0141587
2021-05-16T18:15:41.416438+0000 min lat: 0.000600228 max lat: 0.228104 avg lat: 0.0141401
2021-05-16T18:15:41.416438+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:15:41.416438+0000    20     213    361805    361592    282.46   287.406  0.00316522   0.0141401
2021-05-16T18:15:42.416626+0000 Total time run:         20.0557
Total writes made:      361805
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     281.875
Stddev Bandwidth:       17.8379
Max bandwidth (MB/sec): 323
Min bandwidth (MB/sec): 251.891
Average IOPS:           18040
Stddev IOPS:            1141.63
Max IOPS:               20672
Min IOPS:               16121
Average Latency(s):     0.0141634
Stddev Latency(s):      0.0300585
Max latency(s):         0.228104
Min latency(s):         0.000600228

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:15:42,984936541-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 90155


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:15:42,989145418-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:06,530381575-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.81k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:06,538787909-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:15,714166852-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.81k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:15,722097643-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:24,080230838-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.81k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:24,088855001-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:32,515752668-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.81k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:32,524110280-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:40,888018692-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.81k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:40,896445736-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:40,902466287-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:40,906075057-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:40,914487092-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=91548
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:40,920734630-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:40,939528431-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:40,942998160-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b144c03f-a350-4634-9d38-fb4ceb68688a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b144c03f-a350-4634-9d38-fb4ceb68688a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UtWIWt:/tmp/ceph-asok.UtWIWt -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:16:43.245+0000 7f03b3cc7d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:16:43.493+0000 7f03b3cc7d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:16:43.493+0000 7f03b3cc7d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:16:43.512070+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:16:43.512070+0000     0       0         0         0         0         0           -           0
2021-05-16T18:16:44.512147+0000     1     255     44661     44406   693.741   693.844  0.00307178  0.00573359
2021-05-16T18:16:45.512256+0000     2     255     98084     97829   764.191   834.734  0.00552085  0.00522035
2021-05-16T18:16:46.512385+0000     3     255    133878    133623   695.864   559.281  0.00551229  0.00573699
2021-05-16T18:16:47.512488+0000     4     255    174454    174199   680.382       634  0.00576022  0.00586919
2021-05-16T18:16:48.512600+0000     5     255    213745    213490   667.076   613.922  0.00592254  0.00598748
2021-05-16T18:16:49.512681+0000     6     255    252091    251836   655.748   599.156  0.00595039   0.0060917
2021-05-16T18:16:50.512761+0000     7     256    293371    293115   654.203   644.984 0.000264277  0.00605635
2021-05-16T18:16:51.512846+0000     8     255    328244    327989   640.536   544.906  0.00425107  0.00623817
2021-05-16T18:16:52.513049+0000 Total time run:       8.63642
Total reads made:     361805
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   654.577
Average IOPS:         41892
Stddev IOPS:          5864.96
Max IOPS:             53423
Min IOPS:             34874
Average Latency(s):   0.00610493
Max latency(s):       0.160285
Min latency(s):       0.000204344

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:53,228234226-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 91548


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:16:53,232993828-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:17:16,593693273-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.81k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:17:16,601649973-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:17:25,201814961-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.81k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:17:25,209728450-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:17:33,707728518-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.81k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:17:33,716163196-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:17:42,288095276-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.81k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:17:42,295716635-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:17:50,822333497-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 361.81k objects, 5.5 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:17:50,830616740-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:17:50,837045258-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T14:17:50,839166564-04:00][RUNNING][ROUND 4/2/40] object_size=16KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:17:50,842614702-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:17:50,850493105-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40621\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.142899\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ff2168fd-42e8-4c85-bf9c-aa553530d838\nsetting min_mon_release = octopus\nepoch 0\nfsid ff2168fd-42e8-4c85-bf9c-aa553530d838\nlast_changed 2021-05-16T11:18:19.588184-0700\ncreated 2021-05-16T11:18:19.588184-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40621/0,v1:10.10.1.2:40622/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.142899 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 f15bae83-81a9-4ec3-b4c7-dd5e0ee192a1\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 7d4f20c1-f34b-4931-860f-0b3be2f97217\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 64349157-d075-4cbc-9637-85861e9ddca8\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42621\n  w/ user/pass: admin / 0d418cbf-9606-4a13-88d5-29fdfaad2c17\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:18:38 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40621
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.142899
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ff2168fd-42e8-4c85-bf9c-aa553530d838
setting min_mon_release = octopus
epoch 0
fsid ff2168fd-42e8-4c85-bf9c-aa553530d838
last_changed 2021-05-16T11:18:19.588184-0700
created 2021-05-16T11:18:19.588184-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40621/0,v1:10.10.1.2:40622/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.142899 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 f15bae83-81a9-4ec3-b4c7-dd5e0ee192a1
0
start osd.0
add osd1 7d4f20c1-f34b-4931-860f-0b3be2f97217
1
start osd.1
add osd2 64349157-d075-4cbc-9637-85861e9ddca8
2
start osd.2


restful urls: https://10.10.1.2:42621
  w/ user/pass: admin / 0d418cbf-9606-4a13-88d5-29fdfaad2c17


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T11:17:53.547-0700 7fa87e9431c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:17:53.547-0700 7fa87e9431c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:17:53.567-0700 7f2f0b1db1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:17:53.567-0700 7f2f0b1db1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40621,v1:10.10.1.2:40622] --print /tmp/ceph_monmap.142899 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.142899 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.142899 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42621 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55f1e9194000 @  0x7fb7b1d9f680 0x7fb7b1dc0824 0x7fb7b255b187 0x7fb7b2563355 0x7fb7b255b708 0x7fb7b255b877 0x7fb7b255cc24 0x7fb7b2574ec1 0x7fb7b24e75f3 0x7fb7b2548e97 0x7fb7b2550b1a 0x7fb7b1c53d84 0x7fb7b1d6f609 0x7fb7b1943293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.vS0xUEtoDj 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f15bae83-81a9-4ec3-b4c7-dd5e0ee192a1 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD3YaFgOkfRDxAApadLYgh+mW3Dfz2hLvGWfw== --osd-uuid f15bae83-81a9-4ec3-b4c7-dd5e0ee192a1 
2021-05-16T11:18:31.928-0700 7f0de77a6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:18:31.948-0700 7f0de77a6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:18:31.948-0700 7f0de77a6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x560476c1c000 @  0x7f0de816f680 0x7f0de8190824 0x56046cb3b447 0x56046cb434b5 0x56046cb3b9c8 0x56046cb3bb37 0x56046cb3cee4 0x56046c90dca1 0x56046cae5423 0x56046c8fe2a7 0x56046c90353a 0x7f0de7cc2d84 0x7f0de7e47609 0x7f0de79b0293
2021-05-16T11:18:32.268-0700 7f0de77a6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7d4f20c1-f34b-4931-860f-0b3be2f97217 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55c2923be000 @  0x7f9db9a0c680 0x7f9db9a2d824 0x55c287eb1447 0x55c287eb94b5 0x55c287eb19c8 0x55c287eb1b37 0x55c287eb2ee4 0x55c287c83ca1 0x55c287e5b423 0x55c287c742a7 0x55c287c7953a 0x7f9db955fd84 0x7f9db96e4609 0x7f9db924d293
2021-05-16T11:18:32.944-0700 7f9db9043f00 -1 Falling back to public interface
2021-05-16T11:18:33.204-0700 7f9db9043f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD4YaFge38LJhAAavy9GezJSZwgwZDtQJGJxA== --osd-uuid 7d4f20c1-f34b-4931-860f-0b3be2f97217 
2021-05-16T11:18:33.292-0700 7ffa7ce27f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:18:33.312-0700 7ffa7ce27f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:18:33.312-0700 7ffa7ce27f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55b7e4d62000 @  0x7ffa7d7f0680 0x7ffa7d811824 0x55b7d98a5447 0x55b7d98ad4b5 0x55b7d98a59c8 0x55b7d98a5b37 0x55b7d98a6ee4 0x55b7d9677ca1 0x55b7d984f423 0x55b7d96682a7 0x55b7d966d53a 0x7ffa7d343d84 0x7ffa7d4c8609 0x7ffa7d031293
2021-05-16T11:18:33.632-0700 7ffa7ce27f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 64349157-d075-4cbc-9637-85861e9ddca8 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x56331a24a000 @  0x7fbba6e8f680 0x7fbba6eb0824 0x56330eb33447 0x56330eb3b4b5 0x56330eb339c8 0x56330eb33b37 0x56330eb34ee4 0x56330e905ca1 0x56330eadd423 0x56330e8f62a7 0x56330e8fb53a 0x7fbba69e2d84 0x7fbba6b67609 0x7fbba66d0293
2021-05-16T11:18:34.284-0700 7fbba64c6f00 -1 Falling back to public interface
2021-05-16T11:18:34.548-0700 7fbba64c6f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD5YaFgdCdTOhAAZ/DtWzE144XnpFktg9ka6g== --osd-uuid 64349157-d075-4cbc-9637-85861e9ddca8 
2021-05-16T11:18:34.712-0700 7fb80fb8bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:18:34.732-0700 7fb80fb8bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:18:34.732-0700 7fb80fb8bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55fe768ce000 @  0x7fb810554680 0x7fb810575824 0x55fe6b05e447 0x55fe6b0664b5 0x55fe6b05e9c8 0x55fe6b05eb37 0x55fe6b05fee4 0x55fe6ae30ca1 0x55fe6b008423 0x55fe6ae212a7 0x55fe6ae2653a 0x7fb8100a7d84 0x7fb81022c609 0x7fb80fd95293
2021-05-16T11:18:35.152-0700 7fb80fb8bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55ad65614000 @  0x7efc0302c680 0x7efc0304d824 0x55ad5a80c447 0x55ad5a8144b5 0x55ad5a80c9c8 0x55ad5a80cb37 0x55ad5a80dee4 0x55ad5a5deca1 0x55ad5a7b6423 0x55ad5a5cf2a7 0x55ad5a5d453a 0x7efc02b7fd84 0x7efc02d04609 0x7efc0286d293
2021-05-16T11:18:35.876-0700 7efc02663f00 -1 Falling back to public interface
2021-05-16T11:18:36.148-0700 7efc02663f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:18:38,199485710-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:18:38,210698147-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:18:38,254291933-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:18:38,258010870-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:18:41,677809247-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:18:41,681407378-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:18:45,243503652-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:18:45,247228741-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:18:48,615755367-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:18:48,619267826-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:18:55,612185652-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:18:55,616078075-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:00,345449324-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:00,349427749-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:04,144129960-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:04,147933676-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:08,466345222-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:08,470241633-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:12,417760234-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:12,421960125-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:16,283359533-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:16,286971309-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:20,826786222-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:20,830299724-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:24,242287331-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:24,246182830-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  161 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  161 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  0.95   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.02   70      up          osd.2  
                       TOTAL  300 GiB  162 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.02  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:27,734448233-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:51,297169730-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:19:59,900675600-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:20:08,360330359-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:20:16,831709077-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:20:25,347412278-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:20:33,925286663-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   238 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:20:33,933932187-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:20:42,522248333-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:20:42,530541585-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:20:51,013862392-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:20:51,022241776-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:20:59,611786968-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:20:59,620103894-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:21:08,114308477-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:21:08,122392646-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:21:08,128743228-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:21:08,132301062-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:21:08,140986712-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=98027
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:21:08,146041269-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:21:08,165438634-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:21:08,168807032-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:21:10.534+0000 7fee0a255d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:21:10.810+0000 7fee0a255d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:21:10.810+0000 7fee0a255d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:21:10.822118+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-16T18:21:10.822128+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T18:21:10.823474+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:21:10.823474+0000     0       0         0         0         0         0           -           0
2021-05-16T18:21:11.823575+0000     1     256     21224     20968   327.586   327.625  0.00108276   0.0116796
2021-05-16T18:21:12.823636+0000     2     256     42440     42184   329.533     331.5   0.0913997   0.0119274
2021-05-16T18:21:13.823689+0000     3     256     63124     62868   327.412   323.188 0.000787259    0.012067
2021-05-16T18:21:14.823765+0000     4     255     83148     82893   323.776   312.891  0.00307668   0.0122204
2021-05-16T18:21:15.823845+0000     5     256    103101    102845   321.366    311.75  0.00266735   0.0123543
2021-05-16T18:21:16.823904+0000     6     256    123030    122774     319.7   311.391  0.00204525   0.0124542
2021-05-16T18:21:17.823969+0000     7     256    142869    142613   318.309   309.984   0.0020161   0.0124922
2021-05-16T18:21:18.824028+0000     8     256    161555    161299   315.015   291.969  0.00105817   0.0125941
2021-05-16T18:21:19.824116+0000     9     256    180255    179999   312.475   292.188  0.00118094   0.0127102
2021-05-16T18:21:20.824179+0000    10     255    197630    197375   308.376     271.5    0.128662   0.0129083
2021-05-16T18:21:21.824238+0000    11     256    215625    215369     305.9   281.156  0.00148046   0.0130344
2021-05-16T18:21:22.824293+0000    12     256    232324    232068   302.151   260.922    0.119489   0.0131806
2021-05-16T18:21:23.824347+0000    13     256    246890    246634   296.415   227.594 0.000678945   0.0134299
2021-05-16T18:21:24.824411+0000    14     255    261493    261238    291.54   228.188   0.0466667   0.0136962
2021-05-16T18:21:25.824467+0000    15     255    279684    279429   291.052   284.234  0.00112804   0.0137202
2021-05-16T18:21:26.824521+0000    16     256    297564    297308   290.321   279.359  0.00292378    0.013744
2021-05-16T18:21:27.824590+0000    17     256    314865    314609   289.143   270.328   0.0843276   0.0137744
2021-05-16T18:21:28.824665+0000    18     256    330553    330297   286.697   245.125   0.0008994   0.0139309
2021-05-16T18:21:29.824744+0000    19     256    347714    347458   285.719   268.141  0.00143575   0.0139579
2021-05-16T18:21:30.824862+0000 min lat: 0.000588296 max lat: 0.219725 avg lat: 0.0140123
2021-05-16T18:21:30.824862+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:21:30.824862+0000    20     256    365149    364893   285.053   272.422   0.0199538   0.0140123
2021-05-16T18:21:31.825028+0000 Total time run:         20.1181
Total writes made:      365149
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     283.598
Stddev Bandwidth:       30.7209
Max bandwidth (MB/sec): 331.5
Min bandwidth (MB/sec): 227.594
Average IOPS:           18150
Stddev IOPS:            1966.14
Max IOPS:               21216
Min IOPS:               14566
Average Latency(s):     0.0140779
Stddev Latency(s):      0.0319091
Max latency(s):         0.219725
Min latency(s):         0.000588296

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:21:32,398761573-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 98027


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:21:32,403071270-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:21:55,842630410-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 365.15k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:21:55,850653795-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:04,346915521-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 365.15k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:04,355860979-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:13,163497850-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 365.15k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:13,172303515-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:21,710923523-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 365.15k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:21,719659737-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:30,141926035-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 365.15k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:30,150636390-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:30,156504386-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:30,160173891-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:30,168392081-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=99421
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:30,174709361-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:30,194572250-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:30,198084018-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8bdb3a9-d58b-42fa-8f4c-0d5ed30ee42f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.IbvajJ:/tmp/ceph-asok.IbvajJ -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:22:32.662+0000 7f2a0b292d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:22:32.914+0000 7f2a0b292d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:22:32.914+0000 7f2a0b292d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:22:32.931671+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:22:32.931671+0000     0       0         0         0         0         0           -           0
2021-05-16T18:22:33.932063+0000     1     255     61355     61100   954.238   954.688  0.00395123  0.00417642
2021-05-16T18:22:34.932208+0000     2     255    102396    102141   797.731   641.266  0.00565851  0.00500132
2021-05-16T18:22:35.932369+0000     3     256    143529    143273    746.02   642.688 0.000240843  0.00522564
2021-05-16T18:22:36.932523+0000     4     255    186620    186365   727.819   673.312  0.00601762  0.00548653
2021-05-16T18:22:37.932665+0000     5     255    225069    224814   702.393   600.766  0.00590853  0.00568644
2021-05-16T18:22:38.932793+0000     6     255    263453    263198   685.274    599.75  0.00584866  0.00582932
2021-05-16T18:22:39.932921+0000     7     255    304358    304103   678.672   639.141  0.00601168  0.00588648
2021-05-16T18:22:40.933047+0000     8     255    342325    342070   667.984   593.234  0.00603501  0.00598114
2021-05-16T18:22:41.933342+0000 Total time run:       8.5603
Total reads made:     365149
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   666.501
Average IOPS:         42656
Stddev IOPS:          7617.83
Max IOPS:             61100
Min IOPS:             37967
Average Latency(s):   0.00599632
Max latency(s):       0.133933
Min latency(s):       0.000217028

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:42,542720133-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 99421


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:22:42,547335715-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:23:06,262663081-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 365.15k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:23:06,271020353-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:23:15,040798555-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 365.15k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:23:15,048248091-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:23:23,623610884-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 365.15k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:23:23,631592080-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:23:31,904878615-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 365.15k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:23:31,913160595-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:23:40,463246142-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 365.15k objects, 5.6 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:23:40,471529655-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:23:40,478289907-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T14:23:40,480640013-04:00][RUNNING][ROUND 5/2/40] object_size=16KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:23:40,484470600-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:23:40,493908102-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40050\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.144035\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 331781c3-6a12-4f50-b62a-2b8a11339564\nsetting min_mon_release = octopus\nepoch 0\nfsid 331781c3-6a12-4f50-b62a-2b8a11339564\nlast_changed 2021-05-16T11:24:08.686302-0700\ncreated 2021-05-16T11:24:08.686302-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40050/0,v1:10.10.1.2:40051/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.144035 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 692c6140-ec8d-4ab3-bc25-e6de1f1b3798\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 b463b8ea-5272-4da8-a83d-7ffe133ff180\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 afe55689-8f8f-40e9-8ba5-9dc6a224241c\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42050\n  w/ user/pass: admin / f5c85d30-d4ba-49e4-b8dc-96e50043d7e6\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:24:27 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40050
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.144035
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 331781c3-6a12-4f50-b62a-2b8a11339564
setting min_mon_release = octopus
epoch 0
fsid 331781c3-6a12-4f50-b62a-2b8a11339564
last_changed 2021-05-16T11:24:08.686302-0700
created 2021-05-16T11:24:08.686302-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40050/0,v1:10.10.1.2:40051/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.144035 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 692c6140-ec8d-4ab3-bc25-e6de1f1b3798
0
start osd.0
add osd1 b463b8ea-5272-4da8-a83d-7ffe133ff180
1
start osd.1
add osd2 afe55689-8f8f-40e9-8ba5-9dc6a224241c
2
start osd.2


restful urls: https://10.10.1.2:42050
  w/ user/pass: admin / f5c85d30-d4ba-49e4-b8dc-96e50043d7e6


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T11:23:42.887-0700 7fef968a31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:23:42.887-0700 7fef968a31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:23:42.903-0700 7fee1ceac1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:23:42.903-0700 7fee1ceac1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40050,v1:10.10.1.2:40051] --print /tmp/ceph_monmap.144035 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.144035 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.144035 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42050 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x56192edec000 @  0x7f793df9a680 0x7f793dfbb824 0x7f793e756187 0x7f793e75e355 0x7f793e756708 0x7f793e756877 0x7f793e757c24 0x7f793e76fec1 0x7f793e6e25f3 0x7f793e743e97 0x7f793e74bb1a 0x7f793de4ed84 0x7f793df6a609 0x7f793db3e293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.7zw9Z1iRdY 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 692c6140-ec8d-4ab3-bc25-e6de1f1b3798 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBUY6Fg9l0lDhAAG2xnLSQ38EUtB0zVJQMU8Q== --osd-uuid 692c6140-ec8d-4ab3-bc25-e6de1f1b3798 
2021-05-16T11:24:20.871-0700 7fee8ab28f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:24:20.891-0700 7fee8ab28f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:24:20.891-0700 7fee8ab28f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55c8f78d8000 @  0x7fee8b4f1680 0x7fee8b512824 0x55c8ed5f8447 0x55c8ed6004b5 0x55c8ed5f89c8 0x55c8ed5f8b37 0x55c8ed5f9ee4 0x55c8ed3caca1 0x55c8ed5a2423 0x55c8ed3bb2a7 0x55c8ed3c053a 0x7fee8b044d84 0x7fee8b1c9609 0x7fee8ad32293
2021-05-16T11:24:21.215-0700 7fee8ab28f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b463b8ea-5272-4da8-a83d-7ffe133ff180 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55e346ecc000 @  0x7f8a294ef680 0x7f8a29510824 0x55e33c877447 0x55e33c87f4b5 0x55e33c8779c8 0x55e33c877b37 0x55e33c878ee4 0x55e33c649ca1 0x55e33c821423 0x55e33c63a2a7 0x55e33c63f53a 0x7f8a29042d84 0x7f8a291c7609 0x7f8a28d30293
2021-05-16T11:24:21.859-0700 7f8a28b26f00 -1 Falling back to public interface
2021-05-16T11:24:22.115-0700 7f8a28b26f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBVY6FgPofeIBAAVZOdRI//mINsYmpoUFZbgA== --osd-uuid b463b8ea-5272-4da8-a83d-7ffe133ff180 
2021-05-16T11:24:22.227-0700 7f0e14f02f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:24:22.247-0700 7f0e14f02f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:24:22.247-0700 7f0e14f02f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5601d1332000 @  0x7f0e158cb680 0x7f0e158ec824 0x5601c6df7447 0x5601c6dff4b5 0x5601c6df79c8 0x5601c6df7b37 0x5601c6df8ee4 0x5601c6bc9ca1 0x5601c6da1423 0x5601c6bba2a7 0x5601c6bbf53a 0x7f0e1541ed84 0x7f0e155a3609 0x7f0e1510c293
2021-05-16T11:24:22.575-0700 7f0e14f02f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new afe55689-8f8f-40e9-8ba5-9dc6a224241c -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5647b103c000 @  0x7fd49f46b680 0x7fd49f48c824 0x5647a5b43447 0x5647a5b4b4b5 0x5647a5b439c8 0x5647a5b43b37 0x5647a5b44ee4 0x5647a5915ca1 0x5647a5aed423 0x5647a59062a7 0x5647a590b53a 0x7fd49efbed84 0x7fd49f143609 0x7fd49ecac293
2021-05-16T11:24:23.191-0700 7fd49eaa2f00 -1 Falling back to public interface
2021-05-16T11:24:23.451-0700 7fd49eaa2f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBWY6FgPxQTNBAAJ2i1TbfN4JvTtJa0Ddgw1g== --osd-uuid afe55689-8f8f-40e9-8ba5-9dc6a224241c 
2021-05-16T11:24:23.591-0700 7f49e8332f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:24:23.611-0700 7f49e8332f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:24:23.611-0700 7f49e8332f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x556f9db72000 @  0x7f49e8cfb680 0x7f49e8d1c824 0x556f91f47447 0x556f91f4f4b5 0x556f91f479c8 0x556f91f47b37 0x556f91f48ee4 0x556f91d19ca1 0x556f91ef1423 0x556f91d0a2a7 0x556f91d0f53a 0x7f49e884ed84 0x7f49e89d3609 0x7f49e853c293
2021-05-16T11:24:23.935-0700 7f49e8332f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55d91feae000 @  0x7f57eee22680 0x7f57eee43824 0x55d915d47447 0x55d915d4f4b5 0x55d915d479c8 0x55d915d47b37 0x55d915d48ee4 0x55d915b19ca1 0x55d915cf1423 0x55d915b0a2a7 0x55d915b0f53a 0x7f57ee975d84 0x7f57eeafa609 0x7f57ee663293
2021-05-16T11:24:24.543-0700 7f57ee459f00 -1 Falling back to public interface
2021-05-16T11:24:24.811-0700 7f57ee459f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:27,090740457-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:27,102034457-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:27,146116592-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:27,149857471-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e82706b9-06a7-45a0-8515-3a43b072654f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e82706b9-06a7-45a0-8515-3a43b072654f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:30,621345053-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:30,625269326-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e82706b9-06a7-45a0-8515-3a43b072654f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e82706b9-06a7-45a0-8515-3a43b072654f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:34,263361085-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:34,266890667-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e82706b9-06a7-45a0-8515-3a43b072654f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e82706b9-06a7-45a0-8515-3a43b072654f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:37,815537161-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:37,819022569-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e82706b9-06a7-45a0-8515-3a43b072654f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e82706b9-06a7-45a0-8515-3a43b072654f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:44,662633935-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:44,666072876-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e82706b9-06a7-45a0-8515-3a43b072654f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e82706b9-06a7-45a0-8515-3a43b072654f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:48,517811522-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:48,521414662-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e82706b9-06a7-45a0-8515-3a43b072654f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e82706b9-06a7-45a0-8515-3a43b072654f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:52,107390856-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:52,111190124-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e82706b9-06a7-45a0-8515-3a43b072654f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e82706b9-06a7-45a0-8515-3a43b072654f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:56,597138118-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:24:56,600970128-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e82706b9-06a7-45a0-8515-3a43b072654f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e82706b9-06a7-45a0-8515-3a43b072654f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:25:00,278096513-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:25:00,281851067-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e82706b9-06a7-45a0-8515-3a43b072654f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e82706b9-06a7-45a0-8515-3a43b072654f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:25:04,822908126-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:25:04,826759191-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e82706b9-06a7-45a0-8515-3a43b072654f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e82706b9-06a7-45a0-8515-3a43b072654f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:25:09,503994512-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:25:09,507737555-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e82706b9-06a7-45a0-8515-3a43b072654f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e82706b9-06a7-45a0-8515-3a43b072654f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:25:13,074753668-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:25:13,078709170-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e82706b9-06a7-45a0-8515-3a43b072654f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e82706b9-06a7-45a0-8515-3a43b072654f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:25:16,548034224-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:25:39,891387648-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:25:48,355065637-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:25:56,785404529-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:05,315615411-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:13,895988404-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:22,321796490-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:22,329924011-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:30,836399154-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:30,845190852-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:39,225124102-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:39,233846360-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:47,697422046-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:47,705152771-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:56,145501128-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:56,154117156-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:56,160164679-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:56,163663082-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:56,171725430-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=105823
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:56,177882769-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:56,195790876-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:26:56,199110463-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e82706b9-06a7-45a0-8515-3a43b072654f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e82706b9-06a7-45a0-8515-3a43b072654f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:26:58.591+0000 7f97132b7d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:26:58.839+0000 7f97132b7d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:26:58.839+0000 7f97132b7d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:26:58.852264+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-16T18:26:58.852273+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T18:26:58.853651+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:26:58.853651+0000     0       0         0         0         0         0           -           0
2021-05-16T18:26:59.853732+0000     1     256     19750     19494   304.582   304.594  0.00131226   0.0127022
2021-05-16T18:27:00.853825+0000     2     255     38341     38086   297.527     290.5  0.00131638    0.013166
2021-05-16T18:27:01.853908+0000     3     255     57371     57116   297.458   297.344   0.0111782   0.0132783
2021-05-16T18:27:02.853992+0000     4     255     76694     76439   298.567   301.922  0.00158299   0.0132708
2021-05-16T18:27:03.854077+0000     5     256     95905     95649    298.88   300.156    0.117063   0.0132512
2021-05-16T18:27:04.854161+0000     6     256    114818    114562   298.315   295.516  0.00181499   0.0132941
2021-05-16T18:27:05.854233+0000     7     255    129482    129227   288.431   229.141  0.00227301    0.013714
2021-05-16T18:27:06.854311+0000     8     256    148440    148184   289.399   296.203  0.00109118    0.013766
2021-05-16T18:27:07.854390+0000     9     256    167163    166907   289.747   292.547  0.00167346   0.0137734
2021-05-16T18:27:08.854479+0000    10     256    182562    182306   284.831   240.609  0.00541305   0.0138934
2021-05-16T18:27:09.854555+0000    11     256    198144    197888   281.069   243.469  0.00308514   0.0141548
2021-05-16T18:27:10.854636+0000    12     256    216812    216556   281.952   291.688  0.00321901   0.0141473
2021-05-16T18:27:11.854713+0000    13     256    233795    233539   280.674   265.359   0.0300371   0.0142281
2021-05-16T18:27:12.854784+0000    14     256    249628    249372   278.295   247.391  0.00299508   0.0143274
2021-05-16T18:27:13.854854+0000    15     255    267804    267549   278.675   284.016  0.00229695   0.0143293
2021-05-16T18:27:14.854970+0000    16     255    283130    282875   276.223   239.469  0.00401043   0.0144539
2021-05-16T18:27:15.855048+0000    17     255    298779    298524   274.357   244.516  0.00199973   0.0145502
2021-05-16T18:27:16.855125+0000    18     256    313555    313299   271.939   230.859 0.000677673    0.014626
2021-05-16T18:27:17.855195+0000    19     256    328766    328510   270.135   237.672  0.00261758   0.0147753
2021-05-16T18:27:18.855278+0000 min lat: 0.00061206 max lat: 0.271347 avg lat: 0.0148849
2021-05-16T18:27:18.855278+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:27:18.855278+0000    20     227    343834    343607   268.422   235.891     0.02654   0.0148849
2021-05-16T18:27:19.855495+0000 Total time run:         20.0449
Total writes made:      343834
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     268.018
Stddev Bandwidth:       28.8822
Max bandwidth (MB/sec): 304.594
Min bandwidth (MB/sec): 229.141
Average IOPS:           17153
Stddev IOPS:            1848.46
Max IOPS:               19494
Min IOPS:               14665
Average Latency(s):     0.0149001
Stddev Latency(s):      0.0305157
Max latency(s):         0.271347
Min latency(s):         0.00061206

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:27:20,472119109-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 105823


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:27:20,476715316-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:27:43,900401902-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:27:43,908767661-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:27:52,371858636-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:27:52,380788443-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:00,690978138-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:00,699837384-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:09,262344735-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:09,271024263-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:17,684289975-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:17,693140034-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:17,699986258-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:17,703429227-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:17,711542140-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=107192
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:17,717844412-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:17,737752839-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:17,741410532-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e82706b9-06a7-45a0-8515-3a43b072654f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e82706b9-06a7-45a0-8515-3a43b072654f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Oz5qFx:/tmp/ceph-asok.Oz5qFx -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:28:20.004+0000 7f976f6b0d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:28:20.432+0000 7f976f6b0d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:28:20.432+0000 7f976f6b0d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:28:20.448644+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:28:20.448644+0000     0       0         0         0         0         0           -           0
2021-05-16T18:28:21.448742+0000     1     255     49493     49238   769.185   769.344  0.00410349  0.00517999
2021-05-16T18:28:22.448817+0000     2     255    110495    110240   861.129   953.156  0.00405214   0.0046343
2021-05-16T18:28:23.448926+0000     3     255    153632    153377   798.735   674.016  0.00793316  0.00499893
2021-05-16T18:28:24.449004+0000     4     255    210357    210102   820.615   886.328  0.00561642  0.00486619
2021-05-16T18:28:25.449073+0000     5     255    251890    251635   786.275   648.953  0.00568895  0.00507967
2021-05-16T18:28:26.449151+0000     6     256    294706    294450   766.718   668.984 0.000247686  0.00520276
2021-05-16T18:28:27.449251+0000     7     255    333782    333527   744.404   610.578  0.00652696  0.00536675
2021-05-16T18:28:28.449539+0000 Total time run:       7.42553
Total reads made:     343834
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   723.505
Average IOPS:         46304
Stddev IOPS:          8344.77
Max IOPS:             61002
Min IOPS:             39077
Average Latency(s):   0.00552379
Max latency(s):       0.170968
Min latency(s):       0.000200447

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:29,053978061-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 107192


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:29,058609112-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:52,679429130-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:28:52,688197826-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:29:01,352543254-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:29:01,361675112-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:29:09,700289809-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:29:09,708879398-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:29:18,122887326-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:29:18,131371928-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:29:26,809717004-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.83k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:29:26,818395881-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:29:26,824744620-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T14:29:26,828526947-04:00][RUNNING][ROUND 1/3/40] object_size=64KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:29:26,832165343-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:29:26,841459767-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40879\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.145137\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid befd1035-949a-44ca-9ce2-32831d348ee9\nsetting min_mon_release = octopus\nepoch 0\nfsid befd1035-949a-44ca-9ce2-32831d348ee9\nlast_changed 2021-05-16T11:29:54.232523-0700\ncreated 2021-05-16T11:29:54.232523-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40879/0,v1:10.10.1.2:40880/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.145137 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 43e90e7e-f29a-43ac-bca1-cf7ad49551ac\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 69bdf036-9b17-47b4-a0b5-e92cfd30804b\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 a1eb7a24-4009-4347-a47f-d95a3a615330\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42879\n  w/ user/pass: admin / 5213702b-2b14-4a2e-906c-23bb9afd76bb\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:30:11 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40879
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.145137
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid befd1035-949a-44ca-9ce2-32831d348ee9
setting min_mon_release = octopus
epoch 0
fsid befd1035-949a-44ca-9ce2-32831d348ee9
last_changed 2021-05-16T11:29:54.232523-0700
created 2021-05-16T11:29:54.232523-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40879/0,v1:10.10.1.2:40880/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.145137 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 43e90e7e-f29a-43ac-bca1-cf7ad49551ac
0
start osd.0
add osd1 69bdf036-9b17-47b4-a0b5-e92cfd30804b
1
start osd.1
add osd2 a1eb7a24-4009-4347-a47f-d95a3a615330
2
start osd.2


restful urls: https://10.10.1.2:42879
  w/ user/pass: admin / 5213702b-2b14-4a2e-906c-23bb9afd76bb


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T11:29:28.966-0700 7ff5fcd731c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:29:28.966-0700 7ff5fcd731c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:29:28.986-0700 7f9877d7b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:29:28.986-0700 7f9877d7b1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40879,v1:10.10.1.2:40880] --print /tmp/ceph_monmap.145137 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.145137 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.145137 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42879 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55b108fc8000 @  0x7f1e1fbfb680 0x7f1e1fc1c824 0x7f1e203b7187 0x7f1e203bf355 0x7f1e203b7708 0x7f1e203b7877 0x7f1e203b8c24 0x7f1e203d0ec1 0x7f1e203435f3 0x7f1e203a4e97 0x7f1e203acb1a 0x7f1e1faafd84 0x7f1e1fbcb609 0x7f1e1f79f293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.RuRpe8Msra 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 43e90e7e-f29a-43ac-bca1-cf7ad49551ac -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCsZKFgxDVqMhAAmSO3KubPCLbXeoT8AO9iag== --osd-uuid 43e90e7e-f29a-43ac-bca1-cf7ad49551ac 
2021-05-16T11:30:05.490-0700 7f629df21f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:30:05.510-0700 7f629df21f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:30:05.510-0700 7f629df21f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5632c8c8a000 @  0x7f629e8ea680 0x7f629e90b824 0x5632bdb75447 0x5632bdb7d4b5 0x5632bdb759c8 0x5632bdb75b37 0x5632bdb76ee4 0x5632bd947ca1 0x5632bdb1f423 0x5632bd9382a7 0x5632bd93d53a 0x7f629e43dd84 0x7f629e5c2609 0x7f629e12b293
2021-05-16T11:30:05.830-0700 7f629df21f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 69bdf036-9b17-47b4-a0b5-e92cfd30804b -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55ab66b18000 @  0x7fc3d7154680 0x7fc3d7175824 0x55ab5b662447 0x55ab5b66a4b5 0x55ab5b6629c8 0x55ab5b662b37 0x55ab5b663ee4 0x55ab5b434ca1 0x55ab5b60c423 0x55ab5b4252a7 0x55ab5b42a53a 0x7fc3d6ca7d84 0x7fc3d6e2c609 0x7fc3d6995293
2021-05-16T11:30:06.494-0700 7fc3d678bf00 -1 Falling back to public interface
2021-05-16T11:30:06.754-0700 7fc3d678bf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCuZKFgv0ghCxAAP7pww8vpfNZw6VqfaIApwQ== --osd-uuid 69bdf036-9b17-47b4-a0b5-e92cfd30804b 
2021-05-16T11:30:06.846-0700 7f5a7453bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:30:06.866-0700 7f5a7453bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:30:06.866-0700 7f5a7453bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x561bb38fa000 @  0x7f5a74f04680 0x7f5a74f25824 0x561ba94bf447 0x561ba94c74b5 0x561ba94bf9c8 0x561ba94bfb37 0x561ba94c0ee4 0x561ba9291ca1 0x561ba9469423 0x561ba92822a7 0x561ba928753a 0x7f5a74a57d84 0x7f5a74bdc609 0x7f5a74745293
2021-05-16T11:30:07.214-0700 7f5a7453bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a1eb7a24-4009-4347-a47f-d95a3a615330 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55ee3d74e000 @  0x7f23cc233680 0x7f23cc254824 0x55ee32a6a447 0x55ee32a724b5 0x55ee32a6a9c8 0x55ee32a6ab37 0x55ee32a6bee4 0x55ee3283cca1 0x55ee32a14423 0x55ee3282d2a7 0x55ee3283253a 0x7f23cbd86d84 0x7f23cbf0b609 0x7f23cba74293
2021-05-16T11:30:07.838-0700 7f23cb86af00 -1 Falling back to public interface
2021-05-16T11:30:08.098-0700 7f23cb86af00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCvZKFgZprKHxAAW3BUGX+cH1FIcvQvT54EKA== --osd-uuid a1eb7a24-4009-4347-a47f-d95a3a615330 
2021-05-16T11:30:08.290-0700 7f2e89d1cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:30:08.310-0700 7f2e89d1cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:30:08.310-0700 7f2e89d1cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55df9d0f4000 @  0x7f2e8a6e5680 0x7f2e8a706824 0x55df91aeb447 0x55df91af34b5 0x55df91aeb9c8 0x55df91aebb37 0x55df91aecee4 0x55df918bdca1 0x55df91a95423 0x55df918ae2a7 0x55df918b353a 0x7f2e8a238d84 0x7f2e8a3bd609 0x7f2e89f26293
2021-05-16T11:30:08.630-0700 7f2e89d1cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55de84c5e000 @  0x7f46c7f47680 0x7f46c7f68824 0x55de79776447 0x55de7977e4b5 0x55de797769c8 0x55de79776b37 0x55de79777ee4 0x55de79548ca1 0x55de79720423 0x55de795392a7 0x55de7953e53a 0x7f46c7a9ad84 0x7f46c7c1f609 0x7f46c7788293
2021-05-16T11:30:09.286-0700 7f46c757ef00 -1 Falling back to public interface
2021-05-16T11:30:09.558-0700 7f46c757ef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:11,790871114-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:11,801816510-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:11,844467229-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:11,847795813-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6e35b54b-81e2-4a0b-9821-44208a6dec1e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6e35b54b-81e2-4a0b-9821-44208a6dec1e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:15,254644789-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:15,257830544-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6e35b54b-81e2-4a0b-9821-44208a6dec1e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6e35b54b-81e2-4a0b-9821-44208a6dec1e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:18,840851530-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:18,844354272-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6e35b54b-81e2-4a0b-9821-44208a6dec1e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6e35b54b-81e2-4a0b-9821-44208a6dec1e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:22,169842444-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:22,173633518-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6e35b54b-81e2-4a0b-9821-44208a6dec1e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6e35b54b-81e2-4a0b-9821-44208a6dec1e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:29,050025169-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:29,053992674-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6e35b54b-81e2-4a0b-9821-44208a6dec1e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6e35b54b-81e2-4a0b-9821-44208a6dec1e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:33,005746300-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:33,009768618-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6e35b54b-81e2-4a0b-9821-44208a6dec1e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6e35b54b-81e2-4a0b-9821-44208a6dec1e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:37,537005190-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:37,540646612-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6e35b54b-81e2-4a0b-9821-44208a6dec1e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6e35b54b-81e2-4a0b-9821-44208a6dec1e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:41,937926672-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:41,942031335-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6e35b54b-81e2-4a0b-9821-44208a6dec1e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6e35b54b-81e2-4a0b-9821-44208a6dec1e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:46,334047865-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:46,337945559-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6e35b54b-81e2-4a0b-9821-44208a6dec1e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6e35b54b-81e2-4a0b-9821-44208a6dec1e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:50,056816010-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:50,060040328-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6e35b54b-81e2-4a0b-9821-44208a6dec1e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6e35b54b-81e2-4a0b-9821-44208a6dec1e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:54,638305452-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:54,642342257-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6e35b54b-81e2-4a0b-9821-44208a6dec1e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6e35b54b-81e2-4a0b-9821-44208a6dec1e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:58,045946318-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:30:58,049417070-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6e35b54b-81e2-4a0b-9821-44208a6dec1e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6e35b54b-81e2-4a0b-9821-44208a6dec1e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:31:01,411033498-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:31:24,705571070-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:31:33,404745663-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:31:41,867873296-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:31:50,348673576-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:31:58,802773078-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:07,234675975-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:07,242784341-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:15,530884239-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:15,539316443-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:23,896300584-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:23,904263716-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:32,387090139-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:32,395389734-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:40,863921580-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:40,872206827-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:40,879113344-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:40,883013753-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:40,891339456-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=113614
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:40,897689838-04:00] INFO: > Run rados bench[0m
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.1
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:40,917946850-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:32:40,921695544-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6e35b54b-81e2-4a0b-9821-44208a6dec1e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6e35b54b-81e2-4a0b-9821-44208a6dec1e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:32:43.349+0000 7fc5a76e5d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:32:43.601+0000 7fc5a76e5d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:32:43.601+0000 7fc5a76e5d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:32:43.615162+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-16T18:32:43.615171+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T18:32:43.619344+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:32:43.619344+0000     0       0         0         0         0         0           -           0
2021-05-16T18:32:44.619460+0000     1     256       398       142   8.87451     8.875   0.0011224   0.0107525
2021-05-16T18:32:45.619589+0000     2     256       877       621   19.4045   29.9375 0.000769766    0.434544
2021-05-16T18:32:46.619801+0000     3     256       877       621   12.9358         0           -    0.434544
2021-05-16T18:32:47.619913+0000     4     256      1000       744   11.6235   3.84375      2.1482     0.86203
2021-05-16T18:32:48.619980+0000     5     256      1364      1108   13.8484     22.75 0.000809862    0.925617
2021-05-16T18:32:49.620087+0000     6     256      1364      1108   11.5404         0           -    0.925617
2021-05-16T18:32:50.620153+0000     7     256      1395      1139   10.1686   0.96875   0.0015936    0.946734
2021-05-16T18:32:51.620222+0000     8     256      1395      1139   8.89753         0           -    0.946734
2021-05-16T18:32:52.620287+0000     9     256      1435      1179    8.1867      1.25  0.00135998     1.00849
2021-05-16T18:32:53.620354+0000    10     256      1435      1179   7.36805         0           -     1.00849
2021-05-16T18:32:54.620453+0000    11     256      2025      1769   10.0502   18.4375 0.000739369     1.38286
2021-05-16T18:32:55.620522+0000    12     256      2025      1769   9.21268         0           -     1.38286
2021-05-16T18:32:56.620624+0000    13     256      3154      2898   13.9314   35.2812  0.00078765     1.11573
2021-05-16T18:32:57.620731+0000    14     256      3154      2898   12.9363         0           -     1.11573
2021-05-16T18:32:58.620801+0000    15     256      3666      3410    14.207        16  0.00108328      1.1063
2021-05-16T18:32:59.620950+0000    16     256      3666      3410    13.319         0           -      1.1063
2021-05-16T18:33:00.621180+0000    17     256      3977      3721   13.6787   9.71875  0.00107041     1.07383
2021-05-16T18:33:01.621271+0000    18     256      3977      3721   12.9188         0           -     1.07383
2021-05-16T18:33:02.621336+0000    19     256      3977      3721   12.2389         0           -     1.07383
2021-05-16T18:33:03.621550+0000 min lat: 0.000739369 max lat: 8.58294 avg lat: 0.999697
2021-05-16T18:33:03.621550+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:33:03.621550+0000    20     256      5139      4883   15.2577   24.2083 0.000784845    0.999697
2021-05-16T18:33:04.621631+0000    21     256      5139      4883   14.5312         0           -    0.999697
2021-05-16T18:33:05.621748+0000 Total time run:         21.1772
Total writes made:      5139
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     15.1667
Stddev Bandwidth:       11.4765
Max bandwidth (MB/sec): 35.2812
Min bandwidth (MB/sec): 0
Average IOPS:           242
Stddev IOPS:            183.564
Max IOPS:               564
Min IOPS:               0
Average Latency(s):     1.05432
Stddev Latency(s):      1.70635
Max latency(s):         8.58294
Min latency(s):         0.000739369

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:33:06,180148715-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 113614


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:33:06,184751814-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:33:29,626611217-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.14k objects, 321 MiB
    usage:   643 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:33:29,635092643-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:33:37,961791329-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.14k objects, 321 MiB
    usage:   643 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:33:37,970406296-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:33:46,311963742-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.14k objects, 321 MiB
    usage:   643 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:33:46,320844378-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:33:54,848582691-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.14k objects, 321 MiB
    usage:   643 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:33:54,856983476-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:34:03,233870347-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.14k objects, 321 MiB
    usage:   643 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:34:03,242135767-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:34:03,248653654-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:34:03,252790587-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:34:03,261505021-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=114984
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:34:03,267465961-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:34:03,286062603-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:34:03,289445971-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6e35b54b-81e2-4a0b-9821-44208a6dec1e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6e35b54b-81e2-4a0b-9821-44208a6dec1e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bwGHOy:/tmp/ceph-asok.bwGHOy -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:34:05.541+0000 7f8c15fd7d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:34:05.813+0000 7f8c15fd7d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:34:05.813+0000 7f8c15fd7d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:34:05.832257+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:34:05.832257+0000     0       0         0         0         0         0           -           0
2021-05-16T18:34:06.832370+0000     1     256      1020       764   47.7399     47.75 0.000249609  0.00409558
2021-05-16T18:34:07.832450+0000     2     256      1537      1281   40.0254   32.3125 0.000315393    0.233705
2021-05-16T18:34:08.832526+0000     3     256      1537      1281   26.6842         0           -    0.233705
2021-05-16T18:34:09.832602+0000     4     256      1761      1505    23.513         7 0.000523514    0.321026
2021-05-16T18:34:10.832679+0000     5     256      1761      1505   18.8105         0           -    0.321026
2021-05-16T18:34:11.832756+0000     6     256      1907      1651   17.1962    4.5625     4.28584    0.653913
2021-05-16T18:34:12.832976+0000     7     256      1907      1651   14.7394         0           -    0.653913
2021-05-16T18:34:13.833191+0000     8     256      2156      1900   14.8418   7.78125 0.000524125    0.668857
2021-05-16T18:34:14.833266+0000     9     256      2156      1900   13.1928         0           -    0.668857
2021-05-16T18:34:15.833487+0000    10     256      2780      2524   15.7729      19.5  0.00039953    0.933362
2021-05-16T18:34:16.833583+0000    11     256      2780      2524   14.3391         0           -    0.933362
2021-05-16T18:34:17.833805+0000    12     256      2988      2732   14.2272       6.5  0.00031935    0.969938
2021-05-16T18:34:18.833881+0000    13     256      2988      2732   13.1329         0           -    0.969938
2021-05-16T18:34:19.833958+0000    14     256      2988      2732   12.1949         0           -    0.969938
2021-05-16T18:34:20.834033+0000    15     256      3402      3146   13.1067     8.625 0.000499008     1.14343
2021-05-16T18:34:21.834109+0000    16     256      3402      3146   12.2876         0           -     1.14343
2021-05-16T18:34:22.834184+0000    17     256      3917      3661    13.458   16.0938 0.000361429      1.0882
2021-05-16T18:34:23.834262+0000    18     256      3917      3661   12.7103         0           -      1.0882
2021-05-16T18:34:24.834494+0000    19     256      4330      4074   13.3997   12.9062  0.00056463     1.15283
2021-05-16T18:34:25.834573+0000 min lat: 0.000239641 max lat: 8.57588 avg lat: 1.15283
2021-05-16T18:34:25.834573+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:34:25.834573+0000    20     256      4330      4074   12.7297         0           -     1.15283
2021-05-16T18:34:26.834806+0000    21     256      4663      4407   13.1144   10.4062 0.000282861     1.15681
2021-05-16T18:34:27.835029+0000    22     256      4663      4407   12.5183         0           -     1.15681
2021-05-16T18:34:28.835263+0000    23     256      4888      4632   12.5853   7.03125     2.14481     1.23629
2021-05-16T18:34:29.835487+0000    24     256      4888      4632   12.0608         0           -     1.23629
2021-05-16T18:34:30.835563+0000    25     251      5139      4888   12.2183         8 0.000488528     1.26205
2021-05-16T18:34:31.835642+0000    26     251      5139      4888   11.7484         0           -     1.26205
2021-05-16T18:34:32.835867+0000    27      89      5139      5050   11.6882    5.0625     2.14913      1.3253
2021-05-16T18:34:33.835944+0000    28      89      5139      5050   11.2708         0           -      1.3253
2021-05-16T18:34:34.836021+0000    29      89      5139      5050   10.8822         0           -      1.3253
2021-05-16T18:34:35.836098+0000    30      49      5139      5090   10.6028  0.833333     4.28933      1.3486
2021-05-16T18:34:36.836178+0000    31      49      5139      5090   10.2608         0           -      1.3486
2021-05-16T18:34:37.836254+0000    32       3      5139      5136     10.03    1.4375     6.43485     1.39417
2021-05-16T18:34:38.836330+0000    33       3      5139      5136   9.72604         0           -     1.39417
2021-05-16T18:34:39.836501+0000 Total time run:       33.3721
Total reads made:     5139
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   9.62442
Average IOPS:         153
Stddev IOPS:          166.47
Max IOPS:             764
Min IOPS:             0
Average Latency(s):   1.39836
Max latency(s):       8.58107
Min latency(s):       0.000239641

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:34:40,342542237-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 114984


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:34:40,347234734-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:35:03,863605037-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.14k objects, 321 MiB
    usage:   643 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:35:03,872014588-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:35:12,316445240-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.14k objects, 321 MiB
    usage:   643 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:35:12,324495305-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:35:20,781637914-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.14k objects, 321 MiB
    usage:   643 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:35:20,790350595-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:35:29,238809468-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.14k objects, 321 MiB
    usage:   643 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:35:29,247109644-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:35:37,666259939-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.14k objects, 321 MiB
    usage:   643 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:35:37,674883462-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:35:37,681025603-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T14:35:37,683436383-04:00][RUNNING][ROUND 2/3/40] object_size=64KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:35:37,687562405-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:35:37,696704533-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40594\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.146231\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c8f0d434-b76b-431c-84f9-8eb07c223b89\nsetting min_mon_release = octopus\nepoch 0\nfsid c8f0d434-b76b-431c-84f9-8eb07c223b89\nlast_changed 2021-05-16T11:35:47.576049-0700\ncreated 2021-05-16T11:35:47.576049-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40594/0,v1:10.10.1.2:40595/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.146231 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 564a8e2d-9d63-4b51-aaf6-531ca52d9d30\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 9fee85a5-724f-4e51-b45c-06ce3f2cd624\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 3e58eb3b-da67-4f0a-b89c-b389935d0a5e\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42594\n  w/ user/pass: admin / dda1c5bf-f64c-4f6a-8de5-de3823c772a8\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:36:06 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40594
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.146231
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c8f0d434-b76b-431c-84f9-8eb07c223b89
setting min_mon_release = octopus
epoch 0
fsid c8f0d434-b76b-431c-84f9-8eb07c223b89
last_changed 2021-05-16T11:35:47.576049-0700
created 2021-05-16T11:35:47.576049-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40594/0,v1:10.10.1.2:40595/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.146231 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 564a8e2d-9d63-4b51-aaf6-531ca52d9d30
0
start osd.0
add osd1 9fee85a5-724f-4e51-b45c-06ce3f2cd624
1
start osd.1
add osd2 3e58eb3b-da67-4f0a-b89c-b389935d0a5e
2
start osd.2


restful urls: https://10.10.1.2:42594
  w/ user/pass: admin / dda1c5bf-f64c-4f6a-8de5-de3823c772a8


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T11:35:39.513-0700 7ff52c3681c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:35:39.513-0700 7ff52c3681c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:35:39.529-0700 7f1ebb65e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:35:39.529-0700 7f1ebb65e1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40594,v1:10.10.1.2:40595] --print /tmp/ceph_monmap.146231 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.146231 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.146231 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42594 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5634253fa000 @  0x7fd19e38d680 0x7fd19e3ae824 0x7fd19eb49187 0x7fd19eb51355 0x7fd19eb49708 0x7fd19eb49877 0x7fd19eb4ac24 0x7fd19eb62ec1 0x7fd19ead55f3 0x7fd19eb36e97 0x7fd19eb3eb1a 0x7fd19e241d84 0x7fd19e35d609 0x7fd19df31293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.ZCG5bxZ481 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 564a8e2d-9d63-4b51-aaf6-531ca52d9d30 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAPZqFgbjaRChAA8fkO6OoYjO+eeLSZyL5hWQ== --osd-uuid 564a8e2d-9d63-4b51-aaf6-531ca52d9d30 
2021-05-16T11:35:59.809-0700 7fedefc88f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:35:59.829-0700 7fedefc88f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:35:59.829-0700 7fedefc88f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55f1ea404000 @  0x7fedf0651680 0x7fedf0672824 0x55f1dfe75447 0x55f1dfe7d4b5 0x55f1dfe759c8 0x55f1dfe75b37 0x55f1dfe76ee4 0x55f1dfc47ca1 0x55f1dfe1f423 0x55f1dfc382a7 0x55f1dfc3d53a 0x7fedf01a4d84 0x7fedf0329609 0x7fedefe92293
2021-05-16T11:36:00.141-0700 7fedefc88f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9fee85a5-724f-4e51-b45c-06ce3f2cd624 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55f36583c000 @  0x7f6829c0d680 0x7f6829c2e824 0x55f35a7d2447 0x55f35a7da4b5 0x55f35a7d29c8 0x55f35a7d2b37 0x55f35a7d3ee4 0x55f35a5a4ca1 0x55f35a77c423 0x55f35a5952a7 0x55f35a59a53a 0x7f6829760d84 0x7f68298e5609 0x7f682944e293
2021-05-16T11:36:00.773-0700 7f6829244f00 -1 Falling back to public interface
2021-05-16T11:36:01.029-0700 7f6829244f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAQZqFgKPusGxAA3WtJi01/kO6RTC7nGPKDPw== --osd-uuid 9fee85a5-724f-4e51-b45c-06ce3f2cd624 
2021-05-16T11:36:01.149-0700 7fc2f841cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:36:01.169-0700 7fc2f841cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:36:01.169-0700 7fc2f841cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x559b83778000 @  0x7fc2f8de5680 0x7fc2f8e06824 0x559b78bc5447 0x559b78bcd4b5 0x559b78bc59c8 0x559b78bc5b37 0x559b78bc6ee4 0x559b78997ca1 0x559b78b6f423 0x559b789882a7 0x559b7898d53a 0x7fc2f8938d84 0x7fc2f8abd609 0x7fc2f8626293
2021-05-16T11:36:01.477-0700 7fc2f841cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3e58eb3b-da67-4f0a-b89c-b389935d0a5e -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55f473496000 @  0x7fd3a002b680 0x7fd3a004c824 0x55f46794b447 0x55f4679534b5 0x55f46794b9c8 0x55f46794bb37 0x55f46794cee4 0x55f46771dca1 0x55f4678f5423 0x55f46770e2a7 0x55f46771353a 0x7fd39fb7ed84 0x7fd39fd03609 0x7fd39f86c293
2021-05-16T11:36:02.121-0700 7fd39f662f00 -1 Falling back to public interface
2021-05-16T11:36:02.381-0700 7fd39f662f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQARZqFgnmMSMBAAtkgzOipT6aEOl4E56FKqNw== --osd-uuid 3e58eb3b-da67-4f0a-b89c-b389935d0a5e 
2021-05-16T11:36:02.501-0700 7fa0a1047f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:36:02.521-0700 7fa0a1047f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:36:02.521-0700 7fa0a1047f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55639693e000 @  0x7fa0a1a10680 0x7fa0a1a31824 0x55638b8aa447 0x55638b8b24b5 0x55638b8aa9c8 0x55638b8aab37 0x55638b8abee4 0x55638b67cca1 0x55638b854423 0x55638b66d2a7 0x55638b67253a 0x7fa0a1563d84 0x7fa0a16e8609 0x7fa0a1251293
2021-05-16T11:36:02.841-0700 7fa0a1047f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55db97b50000 @  0x7f3ed1a24680 0x7f3ed1a45824 0x55db8cae7447 0x55db8caef4b5 0x55db8cae79c8 0x55db8cae7b37 0x55db8cae8ee4 0x55db8c8b9ca1 0x55db8ca91423 0x55db8c8aa2a7 0x55db8c8af53a 0x7f3ed1577d84 0x7f3ed16fc609 0x7f3ed1265293
2021-05-16T11:36:03.509-0700 7f3ed105bf00 -1 Falling back to public interface
2021-05-16T11:36:03.777-0700 7f3ed105bf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:06,078865964-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:06,090391911-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:06,131142099-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:06,134435327-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd3c434e1-60cf-4baa-9d80-020a39c00519', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d3c434e1-60cf-4baa-9d80-020a39c00519 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:09,467124359-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:09,470773356-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd3c434e1-60cf-4baa-9d80-020a39c00519', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d3c434e1-60cf-4baa-9d80-020a39c00519 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:12,796625883-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:12,800149544-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd3c434e1-60cf-4baa-9d80-020a39c00519', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d3c434e1-60cf-4baa-9d80-020a39c00519 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:16,227197062-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:16,231256149-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd3c434e1-60cf-4baa-9d80-020a39c00519', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d3c434e1-60cf-4baa-9d80-020a39c00519 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:23,030892872-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:23,034445558-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd3c434e1-60cf-4baa-9d80-020a39c00519', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d3c434e1-60cf-4baa-9d80-020a39c00519 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:27,637058014-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:27,640965116-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd3c434e1-60cf-4baa-9d80-020a39c00519', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d3c434e1-60cf-4baa-9d80-020a39c00519 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:31,259363004-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:31,263047968-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd3c434e1-60cf-4baa-9d80-020a39c00519', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d3c434e1-60cf-4baa-9d80-020a39c00519 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:35,615717182-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:35,619411724-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd3c434e1-60cf-4baa-9d80-020a39c00519', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d3c434e1-60cf-4baa-9d80-020a39c00519 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:39,128153605-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:39,131242799-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd3c434e1-60cf-4baa-9d80-020a39c00519', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d3c434e1-60cf-4baa-9d80-020a39c00519 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:43,601855866-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:43,605816878-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd3c434e1-60cf-4baa-9d80-020a39c00519', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d3c434e1-60cf-4baa-9d80-020a39c00519 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:47,139353407-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:47,142898849-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd3c434e1-60cf-4baa-9d80-020a39c00519', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d3c434e1-60cf-4baa-9d80-020a39c00519 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:50,563036316-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:50,566510443-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd3c434e1-60cf-4baa-9d80-020a39c00519', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d3c434e1-60cf-4baa-9d80-020a39c00519 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:36:53,990914256-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:37:17,514036577-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:37:26,001179552-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:37:34,484602618-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:37:42,918129804-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:37:51,442091393-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:37:59,917781804-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:37:59,926047304-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:08,481464680-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:08,490255267-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:16,874012250-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:16,881619264-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:25,182845537-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:25,191288130-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:33,669735295-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:33,678189109-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:33,684449593-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:33,688164913-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:33,696388745-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=121401
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:33,703386424-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:33,723378919-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:33,726648422-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd3c434e1-60cf-4baa-9d80-020a39c00519', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d3c434e1-60cf-4baa-9d80-020a39c00519 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:38:36.062+0000 7f53858d3d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:38:36.318+0000 7f53858d3d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:38:36.318+0000 7f53858d3d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:38:36.331140+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-16T18:38:36.331149+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T18:38:36.335276+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:38:36.335276+0000     0       0         0         0         0         0           -           0
2021-05-16T18:38:37.335543+0000     1     256       511       255   15.9341   15.9375 0.000945216   0.0112401
2021-05-16T18:38:38.335647+0000     2     256       911       655   20.4655        25  0.00099512    0.292605
2021-05-16T18:38:39.335797+0000     3     256       911       655   13.6437         0           -    0.292605
2021-05-16T18:38:40.335900+0000     4     256      3290      3034   47.3995   74.3438 0.000781128    0.301731
2021-05-16T18:38:41.335970+0000     5     256      3290      3034   37.9201         0           -    0.301731
2021-05-16T18:38:42.336079+0000     6     256      4213      3957   41.2136   28.8438  0.00081412    0.364203
2021-05-16T18:38:43.336162+0000     7     256      4213      3957   35.3262         0           -    0.364203
2021-05-16T18:38:44.336232+0000     8     256      5401      5145   40.1908    37.125 0.000816114    0.387758
2021-05-16T18:38:45.336345+0000     9     256      5401      5145   35.7251         0           -    0.387758
2021-05-16T18:38:46.336439+0000    10     256      6164      5908   36.9209   23.8438 0.000996462    0.430389
2021-05-16T18:38:47.336520+0000    11     256      6164      5908   33.5645         0           -    0.430389
2021-05-16T18:38:48.336588+0000    12     256      6164      5908   30.7676         0           -    0.430389
2021-05-16T18:38:49.336666+0000    13     256      6592      6336   30.4584   8.91667 0.000796136    0.449807
2021-05-16T18:38:50.336771+0000    14     256      6592      6336   28.2828         0           -    0.449807
2021-05-16T18:38:51.336957+0000    15     256      7082      6826   28.4386   15.3125 0.000809741    0.532713
2021-05-16T18:38:52.337060+0000    16     256      7082      6826   26.6612         0           -    0.532713
2021-05-16T18:38:53.337188+0000    17     256      7382      7126   26.1957     9.375 0.000944595    0.541324
2021-05-16T18:38:54.337260+0000    18     256      7382      7126   24.7404         0           -    0.541324
2021-05-16T18:38:55.337350+0000    19     256      7849      7593   24.9743   14.5938 0.000781277    0.623632
2021-05-16T18:38:56.337421+0000 min lat: 0.00074516 max lat: 4.29383 avg lat: 0.623632
2021-05-16T18:38:56.337421+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:38:56.337421+0000    20     256      7849      7593   23.7256         0           -    0.623632
2021-05-16T18:38:57.337646+0000    21      66      7849      7783   23.1611    5.9375     2.12134    0.660415
2021-05-16T18:38:58.337715+0000    22      66      7849      7783   22.1084         0           -    0.660415
2021-05-16T18:38:59.337879+0000 Total time run:         22.7703
Total writes made:      7849
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     21.5439
Stddev Bandwidth:       17.9686
Max bandwidth (MB/sec): 74.3438
Min bandwidth (MB/sec): 0
Average IOPS:           344
Stddev IOPS:            287.378
Max IOPS:               1189
Min IOPS:               0
Average Latency(s):     0.690732
Stddev Latency(s):      1.18353
Max latency(s):         4.29383
Min latency(s):         0.00074516

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:59,911770152-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 121401


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:38:59,916156133-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:23,314713416-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.85k objects, 491 MiB
    usage:   981 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:23,323376804-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:31,678130723-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.85k objects, 491 MiB
    usage:   981 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:31,686385743-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:40,141419183-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.85k objects, 491 MiB
    usage:   981 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:40,149361327-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:48,604334484-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.85k objects, 491 MiB
    usage:   981 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:48,613239647-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:57,182785414-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.85k objects, 491 MiB
    usage:   981 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:57,191252152-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:57,198370718-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:57,202511298-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:57,210988867-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=122785
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:57,217693134-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:57,237985152-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:39:57,241765545-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd3c434e1-60cf-4baa-9d80-020a39c00519', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d3c434e1-60cf-4baa-9d80-020a39c00519 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9p4XCm:/tmp/ceph-asok.9p4XCm -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:39:59.402+0000 7f183f523d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:39:59.670+0000 7f183f523d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:39:59.670+0000 7f183f523d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:39:59.687660+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:39:59.687660+0000     0       0         0         0         0         0           -           0
2021-05-16T18:40:00.687835+0000     1     256      1126       870   54.3613    54.375 0.000329309  0.00370167
2021-05-16T18:40:01.687874+0000     2     256      1589      1333   41.6502   28.9375  0.00044179    0.316477
2021-05-16T18:40:02.687908+0000     3     256      1589      1333   27.7678         0           -    0.316477
2021-05-16T18:40:03.687948+0000     4     256      1935      1679    26.232   10.8125  0.00028712      0.4122
2021-05-16T18:40:04.687982+0000     5     256      1935      1679   20.9858         0           -      0.4122
2021-05-16T18:40:05.688034+0000     6     256      2816      2560   26.6647   27.5312 0.000235473    0.594976
2021-05-16T18:40:06.688069+0000     7     256      2816      2560   22.8556         0           -    0.594976
2021-05-16T18:40:07.688112+0000     8     256      2816      2560   19.9987         0           -    0.594976
2021-05-16T18:40:08.688145+0000     9     256      3061      2805    19.478   5.10417 0.000341491    0.656767
2021-05-16T18:40:09.688177+0000    10     256      3061      2805   17.5302         0           -    0.656767
2021-05-16T18:40:10.688210+0000    11     256      3302      3046   17.3058   7.53125 0.000269246    0.654854
2021-05-16T18:40:11.688252+0000    12     256      3302      3046   15.8637         0           -    0.654854
2021-05-16T18:40:12.688284+0000    13     256      3590      3334    16.028         9 0.000331744    0.783255
2021-05-16T18:40:13.688318+0000    14     256      3590      3334   14.8831         0           -    0.783255
2021-05-16T18:40:14.688350+0000    15     256      3990      3734   15.5575      12.5  0.00034071    0.996279
2021-05-16T18:40:15.688405+0000    16     256      3990      3734   14.5852         0           -    0.996279
2021-05-16T18:40:16.688436+0000    17     256      4338      4082   15.0066    10.875 0.000389181    0.973368
2021-05-16T18:40:17.688471+0000    18     256      4338      4082   14.1729         0           -    0.973368
2021-05-16T18:40:18.688504+0000    19     256      4574      4318   14.2033     7.375 0.000248476    0.997742
2021-05-16T18:40:19.688547+0000 min lat: 0.000229141 max lat: 8.56257 avg lat: 0.997742
2021-05-16T18:40:19.688547+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:40:19.688547+0000    20     256      4574      4318   13.4931         0           -    0.997742
2021-05-16T18:40:20.688589+0000    21     256      5463      5207   15.4963   27.7812 0.000273825     1.03151
2021-05-16T18:40:21.688623+0000    22     256      5463      5207   14.7919         0           -     1.03151
2021-05-16T18:40:22.688656+0000    23     256      5463      5207   14.1488         0           -     1.03151
2021-05-16T18:40:23.688696+0000    24     256      6092      5836   15.1972   13.1042 0.000317256     1.01443
2021-05-16T18:40:24.688733+0000    25     256      6092      5836   14.5893         0           -     1.01443
2021-05-16T18:40:25.688777+0000    26     256      6268      6012   14.4513       5.5 0.000494439     1.00603
2021-05-16T18:40:26.688965+0000    27     256      6268      6012    13.916         0           -     1.00603
2021-05-16T18:40:27.689179+0000    28     256      6499      6243   13.9345   7.21875 0.000355237      1.0767
2021-05-16T18:40:28.689228+0000    29     256      6499      6243    13.454         0           -      1.0767
2021-05-16T18:40:29.689276+0000    30     256      6873      6617   13.7846   11.6875 0.000287601     1.09303
2021-05-16T18:40:30.689479+0000    31     256      6873      6617   13.3399         0           -     1.09303
2021-05-16T18:40:31.689568+0000    32     256      7050      6794   13.2687   5.53125 0.000288523     1.15241
2021-05-16T18:40:32.689602+0000    33     256      7050      6794   12.8666         0           -     1.15241
2021-05-16T18:40:33.689790+0000    34     256      7246      6990   12.8484     6.125 0.000487085     1.13946
2021-05-16T18:40:34.689979+0000    35     256      7246      6990   12.4813         0           -     1.13946
2021-05-16T18:40:35.690031+0000    36     255      7288      7033   12.2092   1.34375     6.44144     1.16821
2021-05-16T18:40:36.690064+0000    37     256      7518      7262   12.2661   14.3125  0.00036212     1.18993
2021-05-16T18:40:37.690097+0000    38     256      7518      7262   11.9433         0           -     1.18993
2021-05-16T18:40:38.690130+0000    39     256      7661      7405   11.8662   4.46875 0.000341261     1.25538
2021-05-16T18:40:39.690173+0000 min lat: 0.000229141 max lat: 8.58317 avg lat: 1.25538
2021-05-16T18:40:39.690173+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:40:39.690173+0000    40     256      7661      7405   11.5696         0           -     1.25538
2021-05-16T18:40:40.690309+0000    41      55      7849      7794   11.8803   12.1562  0.00566881     1.32369
2021-05-16T18:40:41.690477+0000    42      55      7849      7794   11.5974         0           -     1.32369
2021-05-16T18:40:42.690550+0000    43      21      7849      7828   11.3771    1.0625      2.1442     1.32726
2021-05-16T18:40:43.690593+0000    44      21      7849      7828   11.1186         0           -     1.32726
2021-05-16T18:40:44.690625+0000    45      18      7849      7831   10.8757   0.09375     4.29115      1.3284
2021-05-16T18:40:45.690657+0000    46      18      7849      7831   10.6392         0           -      1.3284
2021-05-16T18:40:46.690689+0000    47      15      7849      7834   10.4169   0.09375     6.43892     1.33035
2021-05-16T18:40:47.690731+0000    48      15      7849      7834   10.1999         0           -     1.33035
2021-05-16T18:40:48.690833+0000 Total time run:       48.8853
Total reads made:     7849
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   10.035
Average IOPS:         160
Stddev IOPS:          166.318
Max IOPS:             870
Min IOPS:             0
Average Latency(s):   1.34422
Max latency(s):       8.58648
Min latency(s):       0.000229141

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:40:49,239455577-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 122785


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:40:49,243788338-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:41:12,594790467-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.85k objects, 491 MiB
    usage:   981 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:41:12,603516212-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:41:21,071145903-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.85k objects, 491 MiB
    usage:   981 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:41:21,079445237-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:41:29,450930307-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.85k objects, 491 MiB
    usage:   981 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:41:29,459446449-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:41:37,933990864-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.85k objects, 491 MiB
    usage:   981 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:41:37,942063653-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:41:46,485742841-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.85k objects, 491 MiB
    usage:   981 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:41:46,494198429-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:41:46,500747544-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T14:41:46,502883067-04:00][RUNNING][ROUND 3/3/40] object_size=64KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:41:46,506335554-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:41:46,514373537-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40336\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.147308\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 906f24c9-a85d-4d31-8b44-9b4d5c247942\nsetting min_mon_release = octopus\nepoch 0\nfsid 906f24c9-a85d-4d31-8b44-9b4d5c247942\nlast_changed 2021-05-16T11:41:56.892832-0700\ncreated 2021-05-16T11:41:56.892832-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40336/0,v1:10.10.1.2:40337/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.147308 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 4f609756-3f2e-4cde-973b-8f02ae0faf42\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 5c07faa4-7340-46e4-908f-bcf039271be7\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 d466771e-9a17-43f8-b8fa-9f32c583ef3c\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42336\n  w/ user/pass: admin / 7633373c-d981-47c1-a749-a2364a464c8d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:42:14 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40336
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.147308
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 906f24c9-a85d-4d31-8b44-9b4d5c247942
setting min_mon_release = octopus
epoch 0
fsid 906f24c9-a85d-4d31-8b44-9b4d5c247942
last_changed 2021-05-16T11:41:56.892832-0700
created 2021-05-16T11:41:56.892832-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40336/0,v1:10.10.1.2:40337/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.147308 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 4f609756-3f2e-4cde-973b-8f02ae0faf42
0
start osd.0
add osd1 5c07faa4-7340-46e4-908f-bcf039271be7
1
start osd.1
add osd2 d466771e-9a17-43f8-b8fa-9f32c583ef3c
2
start osd.2


restful urls: https://10.10.1.2:42336
  w/ user/pass: admin / 7633373c-d981-47c1-a749-a2364a464c8d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T11:41:48.384-0700 7f423b5561c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:41:48.384-0700 7f423b5561c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:41:48.400-0700 7fbae4dbc1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:41:48.400-0700 7fbae4dbc1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40336,v1:10.10.1.2:40337] --print /tmp/ceph_monmap.147308 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.147308 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.147308 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42336 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55c015a20000 @  0x7f5ae8fcd680 0x7f5ae8fee824 0x7f5ae9789187 0x7f5ae9791355 0x7f5ae9789708 0x7f5ae9789877 0x7f5ae978ac24 0x7f5ae97a2ec1 0x7f5ae97155f3 0x7f5ae9776e97 0x7f5ae977eb1a 0x7f5ae8e81d84 0x7f5ae8f9d609 0x7f5ae8b71293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.noQJ0ZI8YK 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4f609756-3f2e-4cde-973b-8f02ae0faf42 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB/Z6Fg4pGqIBAAEq6MgNOTzo3ZwUrrfYEzlg== --osd-uuid 4f609756-3f2e-4cde-973b-8f02ae0faf42 
2021-05-16T11:42:08.205-0700 7fad26a6af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:42:08.229-0700 7fad26a6af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:42:08.229-0700 7fad26a6af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55da3bf70000 @  0x7fad27433680 0x7fad27454824 0x55da31e18447 0x55da31e204b5 0x55da31e189c8 0x55da31e18b37 0x55da31e19ee4 0x55da31beaca1 0x55da31dc2423 0x55da31bdb2a7 0x55da31be053a 0x7fad26f86d84 0x7fad2710b609 0x7fad26c74293
2021-05-16T11:42:08.545-0700 7fad26a6af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5c07faa4-7340-46e4-908f-bcf039271be7 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x562c56d72000 @  0x7fce4c48f680 0x7fce4c4b0824 0x562c4cd10447 0x562c4cd184b5 0x562c4cd109c8 0x562c4cd10b37 0x562c4cd11ee4 0x562c4cae2ca1 0x562c4ccba423 0x562c4cad32a7 0x562c4cad853a 0x7fce4bfe2d84 0x7fce4c167609 0x7fce4bcd0293
2021-05-16T11:42:09.185-0700 7fce4bac6f00 -1 Falling back to public interface
2021-05-16T11:42:09.453-0700 7fce4bac6f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCAZ6FgIpteNBAAFcUWWru0b4wi2mgxLddF/g== --osd-uuid 5c07faa4-7340-46e4-908f-bcf039271be7 
2021-05-16T11:42:09.577-0700 7f5987636f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:42:09.597-0700 7f5987636f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:42:09.597-0700 7f5987636f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x564d5b2b8000 @  0x7f5987fff680 0x7f5988020824 0x564d4f63a447 0x564d4f6424b5 0x564d4f63a9c8 0x564d4f63ab37 0x564d4f63bee4 0x564d4f40cca1 0x564d4f5e4423 0x564d4f3fd2a7 0x564d4f40253a 0x7f5987b52d84 0x7f5987cd7609 0x7f5987840293
2021-05-16T11:42:09.905-0700 7f5987636f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d466771e-9a17-43f8-b8fa-9f32c583ef3c -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55b5a21b8000 @  0x7fab5a712680 0x7fab5a733824 0x55b5969b9447 0x55b5969c14b5 0x55b5969b99c8 0x55b5969b9b37 0x55b5969baee4 0x55b59678bca1 0x55b596963423 0x55b59677c2a7 0x55b59678153a 0x7fab5a265d84 0x7fab5a3ea609 0x7fab59f53293
2021-05-16T11:42:10.537-0700 7fab59d49f00 -1 Falling back to public interface
2021-05-16T11:42:10.793-0700 7fab59d49f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCCZ6FgU5Z6DRAArvskpu7g1IzXOBZOkiqvIA== --osd-uuid d466771e-9a17-43f8-b8fa-9f32c583ef3c 
2021-05-16T11:42:10.901-0700 7fdadddb0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:42:10.921-0700 7fdadddb0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:42:10.921-0700 7fdadddb0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x562909a8e000 @  0x7fdade779680 0x7fdade79a824 0x5628ff655447 0x5628ff65d4b5 0x5628ff6559c8 0x5628ff655b37 0x5628ff656ee4 0x5628ff427ca1 0x5628ff5ff423 0x5628ff4182a7 0x5628ff41d53a 0x7fdade2ccd84 0x7fdade451609 0x7fdaddfba293
2021-05-16T11:42:11.245-0700 7fdadddb0f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x562d105ba000 @  0x7fefdaadf680 0x7fefdab00824 0x562d0629b447 0x562d062a34b5 0x562d0629b9c8 0x562d0629bb37 0x562d0629cee4 0x562d0606dca1 0x562d06245423 0x562d0605e2a7 0x562d0606353a 0x7fefda632d84 0x7fefda7b7609 0x7fefda320293
2021-05-16T11:42:11.873-0700 7fefda116f00 -1 Falling back to public interface
2021-05-16T11:42:12.149-0700 7fefda116f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:14,367274079-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:14,378724183-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:14,422780664-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:14,426536311-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e89c03f-7bef-4c67-b2ef-9aee80894ed1', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e89c03f-7bef-4c67-b2ef-9aee80894ed1 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:17,954002942-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:17,957688197-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e89c03f-7bef-4c67-b2ef-9aee80894ed1', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e89c03f-7bef-4c67-b2ef-9aee80894ed1 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:21,363714557-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:21,366920871-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e89c03f-7bef-4c67-b2ef-9aee80894ed1', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e89c03f-7bef-4c67-b2ef-9aee80894ed1 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:24,679057255-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:24,682407841-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e89c03f-7bef-4c67-b2ef-9aee80894ed1', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e89c03f-7bef-4c67-b2ef-9aee80894ed1 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:31,639657331-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:31,643436382-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e89c03f-7bef-4c67-b2ef-9aee80894ed1', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e89c03f-7bef-4c67-b2ef-9aee80894ed1 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:35,710535655-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:35,714251246-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e89c03f-7bef-4c67-b2ef-9aee80894ed1', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e89c03f-7bef-4c67-b2ef-9aee80894ed1 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:40,139759226-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:40,143605433-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e89c03f-7bef-4c67-b2ef-9aee80894ed1', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e89c03f-7bef-4c67-b2ef-9aee80894ed1 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:44,647815492-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:44,651910006-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e89c03f-7bef-4c67-b2ef-9aee80894ed1', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e89c03f-7bef-4c67-b2ef-9aee80894ed1 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:48,847968709-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:48,851725959-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e89c03f-7bef-4c67-b2ef-9aee80894ed1', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e89c03f-7bef-4c67-b2ef-9aee80894ed1 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:52,677845841-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:52,681413084-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e89c03f-7bef-4c67-b2ef-9aee80894ed1', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e89c03f-7bef-4c67-b2ef-9aee80894ed1 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:57,210837370-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:42:57,214012726-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e89c03f-7bef-4c67-b2ef-9aee80894ed1', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e89c03f-7bef-4c67-b2ef-9aee80894ed1 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:43:00,684463339-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:43:00,688005495-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e89c03f-7bef-4c67-b2ef-9aee80894ed1', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e89c03f-7bef-4c67-b2ef-9aee80894ed1 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:43:04,165940611-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:43:27,513725060-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:43:35,982479160-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:43:44,463949115-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:43:52,906656537-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:01,422797639-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:09,750061729-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:09,758403062-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:18,181552198-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:18,190460496-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:26,761022606-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:26,769665155-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:35,321098735-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:35,329777742-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:44,030308732-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:44,039095022-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:44,045920146-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:44,049912728-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:44,058842857-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=129201
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:44,065435575-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:44,084185034-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:44:44,088124466-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e89c03f-7bef-4c67-b2ef-9aee80894ed1', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e89c03f-7bef-4c67-b2ef-9aee80894ed1 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:44:46.431+0000 7f2618287d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:44:46.703+0000 7f2618287d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:44:46.703+0000 7f2618287d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:44:46.715436+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-16T18:44:46.715446+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T18:44:46.719536+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:44:46.719536+0000     0       0         0         0         0         0           -           0
2021-05-16T18:44:47.719645+0000     1     256       462       206   12.8743    12.875 0.000880073   0.0122959
2021-05-16T18:44:48.719883+0000     2     256       899       643   20.0908   27.3125 0.000826874    0.301184
2021-05-16T18:44:49.719957+0000     3     256       899       643   13.3942         0           -    0.301184
2021-05-16T18:44:50.720066+0000     4     256      1286      1030   16.0918   12.0938  0.00918383    0.875062
2021-05-16T18:44:51.720154+0000     5     256      1286      1030   12.8736         0           -    0.875062
2021-05-16T18:44:52.720223+0000     6     256      1430      1174   12.2279       4.5  0.00112594     0.95042
2021-05-16T18:44:53.720290+0000     7     256      1430      1174   10.4811         0           -     0.95042
2021-05-16T18:44:54.720394+0000     8     256      1865      1609   12.5691   13.5938 0.000832524     1.01269
2021-05-16T18:44:55.720464+0000     9     256      1865      1609   11.1725         0           -     1.01269
2021-05-16T18:44:56.720542+0000    10     256      2702      2446    15.286   26.1562   0.0228282      1.0457
2021-05-16T18:44:57.720633+0000    11     256      2723      2467   14.0157    1.3125   0.0199677     1.03698
2021-05-16T18:44:58.720739+0000    12     256      2723      2467   12.8477         0           -     1.03698
2021-05-16T18:44:59.721053+0000    13     256      3021      2765   13.2918    9.3125 0.000898308     1.07665
2021-05-16T18:45:00.721269+0000    14     256      3021      2765   12.3423         0           -     1.07665
2021-05-16T18:45:01.721336+0000    15     256      3523      3267   13.6109   15.6875 0.000991724     1.05787
2021-05-16T18:45:02.721572+0000    16     256      3523      3267   12.7601         0           -     1.05787
2021-05-16T18:45:03.721799+0000    17     256      4834      4578   16.8287   40.9688 0.000807818    0.920746
2021-05-16T18:45:04.721897+0000    18     256      4834      4578   15.8938         0           -    0.920746
2021-05-16T18:45:05.721963+0000    19     256      5230      4974   16.3598    12.375 0.000858874    0.910575
2021-05-16T18:45:06.722068+0000 min lat: 0.000777951 max lat: 6.43708 avg lat: 0.910575
2021-05-16T18:45:06.722068+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:45:06.722068+0000    20     256      5230      4974   15.5418         0           -    0.910575
2021-05-16T18:45:07.722223+0000 Total time run:         20.7015
Total writes made:      5230
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     15.7899
Stddev Bandwidth:       11.6104
Max bandwidth (MB/sec): 40.9688
Min bandwidth (MB/sec): 0
Average IOPS:           252
Stddev IOPS:            185.635
Max IOPS:               655
Min IOPS:               0
Average Latency(s):     1.01299
Stddev Latency(s):      1.48616
Max latency(s):         6.43708
Min latency(s):         0.000777951

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:45:08,263903603-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 129201


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:45:08,268666171-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:45:31,709710564-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.23k objects, 327 MiB
    usage:   654 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:45:31,718050795-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:45:40,190543766-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.23k objects, 327 MiB
    usage:   654 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:45:40,198948218-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:45:48,764597958-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.23k objects, 327 MiB
    usage:   654 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:45:48,773646901-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:45:57,278114505-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.23k objects, 327 MiB
    usage:   654 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:45:57,286729763-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:46:05,740844989-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.23k objects, 327 MiB
    usage:   654 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:46:05,750074762-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:46:05,756487782-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:46:05,760481456-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:46:05,769492818-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=130594
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:46:05,775663383-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:46:05,795345635-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:46:05,798714125-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e89c03f-7bef-4c67-b2ef-9aee80894ed1', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e89c03f-7bef-4c67-b2ef-9aee80894ed1 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kgCGnV:/tmp/ceph-asok.kgCGnV -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:46:08.040+0000 7f13fb18cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:46:08.292+0000 7f13fb18cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:46:08.292+0000 7f13fb18cd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:46:08.308237+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:46:08.308237+0000     0       0         0         0         0         0           -           0
2021-05-16T18:46:09.308329+0000     1     256       667       411   25.6834   25.6875 0.000270678  0.00500217
2021-05-16T18:46:10.308445+0000     2     256       799       543   16.9664      8.25 0.000324109    0.209031
2021-05-16T18:46:11.308526+0000     3     256       799       543   11.3112         0           -    0.209031
2021-05-16T18:46:12.308630+0000     4     256       957       701   10.9519    4.9375     3.45369    0.891195
2021-05-16T18:46:13.308698+0000     5     256       957       701   8.76157         0           -    0.891195
2021-05-16T18:46:14.308765+0000     6     256      1132       876   9.12409   5.46875 0.000257954     1.19355
2021-05-16T18:46:15.308844+0000     7     256      1132       876   7.82067         0           -     1.19355
2021-05-16T18:46:16.308979+0000     8     256      1518      1262   9.85838   12.0625 0.000321063     1.17443
2021-05-16T18:46:17.309046+0000     9     256      1518      1262   8.76303         0           -     1.17443
2021-05-16T18:46:18.309112+0000    10     256      1653      1397   8.73043   4.21875   0.0002765     1.48365
2021-05-16T18:46:19.309217+0000    11     256      1653      1397   7.93674         0           -     1.48365
2021-05-16T18:46:20.309429+0000    12     256      1653      1397   7.27528         0           -     1.48365
2021-05-16T18:46:21.309494+0000    13     256      1716      1460   7.01852    1.3125 0.000690187      1.4799
2021-05-16T18:46:22.309582+0000    14     256      1716      1460    6.5172         0           -      1.4799
2021-05-16T18:46:23.309665+0000    15     256      2476      2220   9.24908     23.75  0.00032498     1.63847
2021-05-16T18:46:24.309876+0000    16     256      2476      2220   8.67095         0           -     1.63847
2021-05-16T18:46:25.309961+0000    17     256      3095      2839   10.4364   19.3438 0.000341532     1.47488
2021-05-16T18:46:26.310049+0000    18     256      3095      2839   9.85661         0           -     1.47488
2021-05-16T18:46:27.310273+0000    19     256      3352      3096   10.1831   8.03125 0.000375596      1.4091
2021-05-16T18:46:28.310341+0000 min lat: 0.000229812 max lat: 8.59031 avg lat: 1.4091
2021-05-16T18:46:28.310341+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:46:28.310341+0000    20     256      3352      3096   9.67395         0           -      1.4091
2021-05-16T18:46:29.310427+0000    21     256      3715      3459   10.2935   11.3438  0.00031405     1.52307
2021-05-16T18:46:30.310650+0000    22     256      3715      3459    9.8256         0           -     1.52307
2021-05-16T18:46:31.310728+0000    23     256      4077      3821    10.382   11.3125  0.00039363     1.45016
2021-05-16T18:46:32.310951+0000    24     256      4077      3821   9.94937         0           -     1.45016
2021-05-16T18:46:33.311032+0000    25     256      4875      4619   11.5462   24.9375 0.000272402     1.38268
2021-05-16T18:46:34.311122+0000    26     256      4875      4619   11.1021         0           -     1.38268
2021-05-16T18:46:35.311328+0000    27     256      4875      4619   10.6909         0           -     1.38268
2021-05-16T18:46:36.311483+0000    28     133      5230      5097   11.3759   9.95833  0.00442348     1.36072
2021-05-16T18:46:37.311572+0000    29     133      5230      5097   10.9836         0           -     1.36072
2021-05-16T18:46:38.311639+0000    30      99      5230      5131   10.6883    1.0625     2.14233      1.3659
2021-05-16T18:46:39.311871+0000    31      99      5230      5131   10.3435         0           -      1.3659
2021-05-16T18:46:40.311940+0000    32      64      5230      5166   10.0887   1.09375     4.28857     1.38571
2021-05-16T18:46:41.312009+0000    33      64      5230      5166   9.78295         0           -     1.38571
2021-05-16T18:46:42.312133+0000 Total time run:       33.5282
Total reads made:     5230
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   9.74924
Average IOPS:         155
Stddev IOPS:          127.252
Max IOPS:             411
Min IOPS:             0
Average Latency(s):   1.4475
Max latency(s):       8.59031
Min latency(s):       0.000229812

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:46:42,811279406-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 130594


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:46:42,815658975-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:47:06,307755367-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.23k objects, 327 MiB
    usage:   654 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:47:06,317014175-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:47:14,840428789-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.23k objects, 327 MiB
    usage:   654 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:47:14,849072600-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:47:23,336266527-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.23k objects, 327 MiB
    usage:   654 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:47:23,344193311-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:47:31,826152331-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.23k objects, 327 MiB
    usage:   654 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:47:31,834936797-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:47:40,122695389-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 5.23k objects, 327 MiB
    usage:   654 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:47:40,131743039-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:47:40,138511708-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T14:47:40,141127713-04:00][RUNNING][ROUND 4/3/40] object_size=64KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:47:40,144963160-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:47:40,153293543-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40737\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.148394\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ebb2079d-029b-44c6-b179-4666c6b4c22d\nsetting min_mon_release = octopus\nepoch 0\nfsid ebb2079d-029b-44c6-b179-4666c6b4c22d\nlast_changed 2021-05-16T11:47:50.052841-0700\ncreated 2021-05-16T11:47:50.052841-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40737/0,v1:10.10.1.2:40738/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.148394 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 389a1355-7366-46c7-bf3e-c8807a3d09d4\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 eb3d6d51-8186-408b-ab3f-2b0964529ad8\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 aab7df9c-4971-4e5c-9df0-01f2ae1cff91\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42737\n  w/ user/pass: admin / e9d9f2d5-0ba2-4efb-9739-7c4c17a5e852\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 14:48:07 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40737
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.148394
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ebb2079d-029b-44c6-b179-4666c6b4c22d
setting min_mon_release = octopus
epoch 0
fsid ebb2079d-029b-44c6-b179-4666c6b4c22d
last_changed 2021-05-16T11:47:50.052841-0700
created 2021-05-16T11:47:50.052841-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40737/0,v1:10.10.1.2:40738/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.148394 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 389a1355-7366-46c7-bf3e-c8807a3d09d4
0
start osd.0
add osd1 eb3d6d51-8186-408b-ab3f-2b0964529ad8
1
start osd.1
add osd2 aab7df9c-4971-4e5c-9df0-01f2ae1cff91
2
start osd.2


restful urls: https://10.10.1.2:42737
  w/ user/pass: admin / e9d9f2d5-0ba2-4efb-9739-7c4c17a5e852


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T11:47:42.024-0700 7f0228ab31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:47:42.028-0700 7f0228ab31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:47:42.044-0700 7f40487d61c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:47:42.044-0700 7f40487d61c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40737,v1:10.10.1.2:40738] --print /tmp/ceph_monmap.148394 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.148394 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.148394 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42737 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55f0b0d18000 @  0x7f650aa51680 0x7f650aa72824 0x7f650b20d187 0x7f650b215355 0x7f650b20d708 0x7f650b20d877 0x7f650b20ec24 0x7f650b226ec1 0x7f650b1995f3 0x7f650b1fae97 0x7f650b202b1a 0x7f650a905d84 0x7f650aa21609 0x7f650a5f5293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.MuPotu8F1H 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 389a1355-7366-46c7-bf3e-c8807a3d09d4 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDgaKFgw82GORAAgJYOJHSIn+lvBzsvUCUQ1w== --osd-uuid 389a1355-7366-46c7-bf3e-c8807a3d09d4 
2021-05-16T11:48:01.624-0700 7f77063ddf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:48:01.644-0700 7f77063ddf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:48:01.644-0700 7f77063ddf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55dfdec48000 @  0x7f7706da6680 0x7f7706dc7824 0x55dfd40a7447 0x55dfd40af4b5 0x55dfd40a79c8 0x55dfd40a7b37 0x55dfd40a8ee4 0x55dfd3e79ca1 0x55dfd4051423 0x55dfd3e6a2a7 0x55dfd3e6f53a 0x7f77068f9d84 0x7f7706a7e609 0x7f77065e7293
2021-05-16T11:48:01.960-0700 7f77063ddf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new eb3d6d51-8186-408b-ab3f-2b0964529ad8 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x557d4c192000 @  0x7fadcac69680 0x7fadcac8a824 0x557d40b16447 0x557d40b1e4b5 0x557d40b169c8 0x557d40b16b37 0x557d40b17ee4 0x557d408e8ca1 0x557d40ac0423 0x557d408d92a7 0x557d408de53a 0x7fadca7bcd84 0x7fadca941609 0x7fadca4aa293
2021-05-16T11:48:02.604-0700 7fadca2a0f00 -1 Falling back to public interface
2021-05-16T11:48:02.868-0700 7fadca2a0f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDiaKFgzny8ERAAgRj1q4Lr4KW0REWMdi6xQw== --osd-uuid eb3d6d51-8186-408b-ab3f-2b0964529ad8 
2021-05-16T11:48:02.972-0700 7f5819978f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:48:02.992-0700 7f5819978f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:48:02.992-0700 7f5819978f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x558751270000 @  0x7f581a341680 0x7f581a362824 0x5587454b7447 0x5587454bf4b5 0x5587454b79c8 0x5587454b7b37 0x5587454b8ee4 0x558745289ca1 0x558745461423 0x55874527a2a7 0x55874527f53a 0x7f5819e94d84 0x7f581a019609 0x7f5819b82293
2021-05-16T11:48:03.304-0700 7f5819978f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new aab7df9c-4971-4e5c-9df0-01f2ae1cff91 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5613b0e1e000 @  0x7f40673f5680 0x7f4067416824 0x5613a5b7c447 0x5613a5b844b5 0x5613a5b7c9c8 0x5613a5b7cb37 0x5613a5b7dee4 0x5613a594eca1 0x5613a5b26423 0x5613a593f2a7 0x5613a594453a 0x7f4066f48d84 0x7f40670cd609 0x7f4066c36293
2021-05-16T11:48:03.996-0700 7f4066a2cf00 -1 Falling back to public interface
2021-05-16T11:48:04.256-0700 7f4066a2cf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDjaKFggA85JxAAhIY/pthUDTIudvxHjuf1Hw== --osd-uuid aab7df9c-4971-4e5c-9df0-01f2ae1cff91 
2021-05-16T11:48:04.340-0700 7fa48f8cff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:48:04.364-0700 7fa48f8cff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:48:04.364-0700 7fa48f8cff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5612def14000 @  0x7fa490298680 0x7fa4902b9824 0x5612d3e27447 0x5612d3e2f4b5 0x5612d3e279c8 0x5612d3e27b37 0x5612d3e28ee4 0x5612d3bf9ca1 0x5612d3dd1423 0x5612d3bea2a7 0x5612d3bef53a 0x7fa48fdebd84 0x7fa48ff70609 0x7fa48fad9293
2021-05-16T11:48:04.760-0700 7fa48f8cff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55ce67250000 @  0x7efda3c48680 0x7efda3c69824 0x55ce5c615447 0x55ce5c61d4b5 0x55ce5c6159c8 0x55ce5c615b37 0x55ce5c616ee4 0x55ce5c3e7ca1 0x55ce5c5bf423 0x55ce5c3d82a7 0x55ce5c3dd53a 0x7efda379bd84 0x7efda3920609 0x7efda3489293
2021-05-16T11:48:05.412-0700 7efda327ff00 -1 Falling back to public interface
2021-05-16T11:48:05.680-0700 7efda327ff00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:07,803306911-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:07,813988372-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:07,856089268-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:07,859578324-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88cf14e2-67e1-41b2-ab82-2dead7837383', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88cf14e2-67e1-41b2-ab82-2dead7837383 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:11,237053661-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:11,240958007-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88cf14e2-67e1-41b2-ab82-2dead7837383', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88cf14e2-67e1-41b2-ab82-2dead7837383 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:14,810075636-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:14,813981395-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88cf14e2-67e1-41b2-ab82-2dead7837383', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88cf14e2-67e1-41b2-ab82-2dead7837383 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:18,213983579-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:18,217749424-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88cf14e2-67e1-41b2-ab82-2dead7837383', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88cf14e2-67e1-41b2-ab82-2dead7837383 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:24,994736903-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:24,998330366-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88cf14e2-67e1-41b2-ab82-2dead7837383', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88cf14e2-67e1-41b2-ab82-2dead7837383 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:28,782505124-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:28,786454705-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88cf14e2-67e1-41b2-ab82-2dead7837383', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88cf14e2-67e1-41b2-ab82-2dead7837383 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:32,705031203-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:32,709344788-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88cf14e2-67e1-41b2-ab82-2dead7837383', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88cf14e2-67e1-41b2-ab82-2dead7837383 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:36,948615415-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:36,952711742-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88cf14e2-67e1-41b2-ab82-2dead7837383', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88cf14e2-67e1-41b2-ab82-2dead7837383 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:41,307906409-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:41,311938175-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88cf14e2-67e1-41b2-ab82-2dead7837383', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88cf14e2-67e1-41b2-ab82-2dead7837383 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:44,770469905-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:44,774081161-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88cf14e2-67e1-41b2-ab82-2dead7837383', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88cf14e2-67e1-41b2-ab82-2dead7837383 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:49,004020085-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:49,007271405-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88cf14e2-67e1-41b2-ab82-2dead7837383', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88cf14e2-67e1-41b2-ab82-2dead7837383 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:52,413493316-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:52,416779100-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88cf14e2-67e1-41b2-ab82-2dead7837383', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88cf14e2-67e1-41b2-ab82-2dead7837383 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:48:55,964706764-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:49:19,404130073-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:49:27,897962995-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:49:36,456742390-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:49:44,906845345-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:49:53,404579268-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:01,901049538-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:01,909772748-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:10,589491510-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:10,597356107-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:19,043952265-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:19,052969098-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:27,417020095-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:27,425562546-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:35,944896459-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:35,953587339-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:35,960053509-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:35,963887914-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:35,972042846-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=137010
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:35,978039645-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:35,997327226-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:50:36,000234207-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88cf14e2-67e1-41b2-ab82-2dead7837383', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88cf14e2-67e1-41b2-ab82-2dead7837383 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:50:38.325+0000 7f4ae2e07d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:50:38.597+0000 7f4ae2e07d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:50:38.597+0000 7f4ae2e07d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:50:38.611775+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-16T18:50:38.611789+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T18:50:38.616022+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:50:38.616022+0000     0       0         0         0         0         0           -           0
2021-05-16T18:50:39.616127+0000     1     256       426       170   10.6245    10.625   0.0012779   0.0136131
2021-05-16T18:50:40.616218+0000     2     256      1077       821   25.6545   40.6875 0.000816053    0.452976
2021-05-16T18:50:41.616301+0000     3     256      1077       821   17.1029         0           -    0.452976
2021-05-16T18:50:42.616373+0000     4     256      2079      1823   28.4823   31.3125 0.000741563    0.538435
2021-05-16T18:50:43.616475+0000     5     256      2079      1823   22.7857         0           -    0.538435
2021-05-16T18:50:44.616548+0000     6     256      2726      2470   25.7272   20.2188   0.0155868     0.62049
2021-05-16T18:50:45.616638+0000     7     256      2805      2549   22.7571    4.9375 0.000770097     0.60147
2021-05-16T18:50:46.616704+0000     8     256      2805      2549   19.9125         0           -     0.60147
2021-05-16T18:50:47.616949+0000     9     256      3364      3108   21.5813   17.4688 0.000709884    0.631783
2021-05-16T18:50:48.617023+0000    10     256      3364      3108   19.4232         0           -    0.631783
2021-05-16T18:50:49.617100+0000    11     256      4542      4286     24.35   36.8125 0.000799873    0.614638
2021-05-16T18:50:50.617275+0000    12     256      4542      4286   22.3207         0           -    0.614638
2021-05-16T18:50:51.617366+0000    13     256      5595      5339   25.6657   32.9062 0.000796376    0.596321
2021-05-16T18:50:52.617560+0000    14     256      5595      5339   23.8323         0           -    0.596321
2021-05-16T18:50:53.617636+0000    15     256      7985      7729   32.2008   74.6875   0.0121141    0.486749
2021-05-16T18:50:54.617882+0000    16     256      7985      7729    30.188         0           -    0.486749
2021-05-16T18:50:55.617959+0000    17     256      8186      7930   29.1512   6.28125 0.000936239    0.508026
2021-05-16T18:50:56.618025+0000    18     256      8186      7930   27.5317         0           -    0.508026
2021-05-16T18:50:57.618094+0000    19     256      8630      8374   27.5431    13.875  0.00119105    0.536074
2021-05-16T18:50:58.618169+0000 min lat: 0.000709884 max lat: 4.25147 avg lat: 0.536074
2021-05-16T18:50:58.618169+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:50:58.618169+0000    20     256      8630      8374    26.166         0           -    0.536074
2021-05-16T18:50:59.618329+0000 Total time run:         20.9966
Total writes made:      8630
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     25.6886
Stddev Bandwidth:       19.9153
Max bandwidth (MB/sec): 74.6875
Min bandwidth (MB/sec): 0
Average IOPS:           411
Stddev IOPS:            318.62
Max IOPS:               1195
Min IOPS:               0
Average Latency(s):     0.62251
Stddev Latency(s):      1.12111
Max latency(s):         6.28619
Min latency(s):         0.000709884

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:00,185130442-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 137010


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:00,189702022-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:23,594598312-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.63k objects, 539 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:23,603085059-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:32,067949798-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.63k objects, 539 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:32,076196163-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:40,672587271-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.63k objects, 539 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:40,680789833-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:49,183980007-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.63k objects, 539 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:49,192696856-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:57,850452106-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.63k objects, 539 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:57,859226453-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:57,865741465-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:57,869746580-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:57,878280446-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=138396
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:57,885449706-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:57,904003098-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:51:57,907435327-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88cf14e2-67e1-41b2-ab82-2dead7837383', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88cf14e2-67e1-41b2-ab82-2dead7837383 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.m3WP2T:/tmp/ceph-asok.m3WP2T -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:52:00.169+0000 7f666325ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:52:00.421+0000 7f666325ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:52:00.421+0000 7f666325ed00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:52:00.438554+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:52:00.438554+0000     0       0         0         0         0         0           -           0
2021-05-16T18:52:01.438627+0000     1     256       865       609   38.0572   38.0625 0.000247344  0.00349883
2021-05-16T18:52:02.438845+0000     2     256      1304      1048   32.7441   27.4375 0.000327675    0.337211
2021-05-16T18:52:03.438921+0000     3     256      1304      1048   21.8302         0           -    0.337211
2021-05-16T18:52:04.439181+0000     4     256      1734      1478   23.0897   13.4375 0.000272583    0.451058
2021-05-16T18:52:05.439250+0000     5     256      1734      1478   18.4722         0           -    0.451058
2021-05-16T18:52:06.439472+0000     6     256      2422      2166   22.5588      21.5 0.000240482    0.670907
2021-05-16T18:52:07.439568+0000     7     256      2422      2166   19.3363         0           -    0.670907
2021-05-16T18:52:08.439633+0000     8     256      2876      2620   20.4658   14.1875 0.000348595    0.764395
2021-05-16T18:52:09.439847+0000     9     256      2876      2620   18.1917         0           -    0.764395
2021-05-16T18:52:10.439913+0000    10     256      2947      2691   16.8164   2.21875 0.000337645    0.780863
2021-05-16T18:52:11.439987+0000    11     256      2947      2691   15.2877         0           -    0.780863
2021-05-16T18:52:12.440053+0000    12     256      2947      2691   14.0138         0           -    0.780863
2021-05-16T18:52:13.440119+0000    13     256      3528      3272   15.7288   12.1042 0.000295035    0.890298
2021-05-16T18:52:14.440189+0000    14     256      3528      3272   14.6054         0           -    0.890298
2021-05-16T18:52:15.440267+0000    15     256      3672      3416   14.2316       4.5     2.14699    0.991572
2021-05-16T18:52:16.440335+0000    16     256      3672      3416   13.3422         0           -    0.991572
2021-05-16T18:52:17.440423+0000    17     256      3965      3709   13.6345   9.15625  0.00037824    0.964237
2021-05-16T18:52:18.440490+0000    18     256      3965      3709    12.877         0           -    0.964237
2021-05-16T18:52:19.440589+0000    19     256      4289      4033    13.265    10.125     2.15212     1.16087
2021-05-16T18:52:20.440689+0000 min lat: 0.000220394 max lat: 6.43721 avg lat: 1.16087
2021-05-16T18:52:20.440689+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:52:20.440689+0000    20     256      4289      4033   12.6017         0           -     1.16087
2021-05-16T18:52:21.440764+0000    21     256      4612      4356   12.9629   10.0938 0.000452721     1.19113
2021-05-16T18:52:22.440834+0000    22     256      4612      4356   12.3737         0           -     1.19113
2021-05-16T18:52:23.440917+0000    23     256      4900      4644   12.6182         9 0.000343606     1.16166
2021-05-16T18:52:24.440985+0000    24     256      4900      4644   12.0925         0           -     1.16166
2021-05-16T18:52:25.441054+0000    25     256      5066      4810   12.0238    5.1875     4.29051      1.2724
2021-05-16T18:52:26.441119+0000    26     256      5066      4810   11.5613         0           -      1.2724
2021-05-16T18:52:27.441288+0000    27     256      5066      4810   11.1331         0           -      1.2724
2021-05-16T18:52:28.441499+0000    28     256      5187      4931   11.0055   2.52083 0.000401585     1.27774
2021-05-16T18:52:29.441710+0000    29     256      5187      4931    10.626         0           -     1.27774
2021-05-16T18:52:30.441801+0000    30     256      5314      5058   10.5363   3.96875 0.000539754     1.27071
2021-05-16T18:52:31.441879+0000    31     256      5314      5058   10.1965         0           -     1.27071
2021-05-16T18:52:32.441944+0000    32     256      6160      5904     11.53   26.4375 0.000353384     1.36369
2021-05-16T18:52:33.442011+0000    33     256      6160      5904   11.1806         0           -     1.36369
2021-05-16T18:52:34.442077+0000    34     256      6408      6152   11.3076      7.75 0.000311134     1.33523
2021-05-16T18:52:35.442153+0000    35     256      6408      6152   10.9846         0           -     1.33523
2021-05-16T18:52:36.442218+0000    36     256      6653      6397   11.1048   7.65625 0.000302379     1.38652
2021-05-16T18:52:37.442286+0000    37     256      6653      6397   10.8046         0           -     1.38652
2021-05-16T18:52:38.442353+0000    38     256      6915      6659   10.9512    8.1875 0.000603855     1.35584
2021-05-16T18:52:39.442427+0000    39     256      6915      6659   10.6704         0           -     1.35584
2021-05-16T18:52:40.442493+0000 min lat: 0.000220394 max lat: 10.7287 avg lat: 1.35584
2021-05-16T18:52:40.442493+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:52:40.442493+0000    40     256      6915      6659   10.4036         0           -     1.35584
2021-05-16T18:52:41.442571+0000    41     256      7148      6892   10.5051   4.85417     4.29526     1.44392
2021-05-16T18:52:42.442636+0000    42     256      7148      6892   10.2549         0           -     1.44392
2021-05-16T18:52:43.442709+0000    43     256      7251      6995   10.1662   3.21875 0.000381356     1.44351
2021-05-16T18:52:44.442777+0000    44     256      7251      6995   9.93511         0           -     1.44351
2021-05-16T18:52:45.442968+0000    45     256      7424      7168   9.95456   5.40625  0.00046304     1.42814
2021-05-16T18:52:46.443035+0000    46     256      7424      7168   9.73817         0           -     1.42814
2021-05-16T18:52:47.443255+0000    47     256      7583      7327   9.74236   4.96875 0.000355989     1.47452
2021-05-16T18:52:48.443322+0000    48     256      7583      7327    9.5394         0           -     1.47452
2021-05-16T18:52:49.443536+0000    49     256      7705      7449    9.5003    3.8125 0.000564721     1.50657
2021-05-16T18:52:50.443606+0000    50     256      7705      7449    9.3103         0           -     1.50657
2021-05-16T18:52:51.443682+0000    51     256      8132      7876   9.65098   13.3438 0.000631487     1.57599
2021-05-16T18:52:52.443862+0000    52     256      8132      7876   9.46537         0           -     1.57599
2021-05-16T18:52:53.443940+0000    53     256      8262      8006   9.44007    4.0625 0.000323638     1.62282
2021-05-16T18:52:54.444007+0000    54     256      8262      8006   9.26526         0           -     1.62282
2021-05-16T18:52:55.444081+0000    55     256      8262      8006    9.0968         0           -     1.62282
2021-05-16T18:52:56.444147+0000    56     233      8630      8397    9.3707   8.14583 0.000800885     1.62426
2021-05-16T18:52:57.444213+0000    57     233      8630      8397   9.20631         0           -     1.62426
2021-05-16T18:52:58.444278+0000    58      46      8630      8584   9.24908   5.84375     2.14903     1.68891
2021-05-16T18:52:59.444383+0000    59      46      8630      8584   9.09231         0           -     1.68891
2021-05-16T18:53:00.444451+0000 min lat: 0.000220394 max lat: 12.8775 avg lat: 1.70067
2021-05-16T18:53:00.444451+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:53:00.444451+0000    60       7      8630      8623    8.9814   1.21875     4.28783     1.70067
2021-05-16T18:53:01.444527+0000    61       7      8630      8623   8.83417         0           -     1.70067
2021-05-16T18:53:02.444652+0000 Total time run:       61.502
Total reads made:     8630
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   8.77003
Average IOPS:         140
Stddev IOPS:          123.141
Max IOPS:             609
Min IOPS:             0
Average Latency(s):   1.70451
Max latency(s):       12.8775
Min latency(s):       0.000220394

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:53:03,009800495-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 138396


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:53:03,014782816-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:53:26,569841365-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.63k objects, 539 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:53:26,579136179-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:53:34,942186471-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.63k objects, 539 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:53:34,950342075-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:53:43,371506582-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.63k objects, 539 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:53:43,380334109-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:53:51,801096121-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.63k objects, 539 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:53:51,809359618-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:00,247941964-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.63k objects, 539 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:00,255748563-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:00,262573397-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T14:54:00,265159726-04:00][RUNNING][ROUND 5/3/40] object_size=64KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:00,268844149-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:00,277353649-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40044\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.149479\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid d01d5929-48de-45f8-ac1c-05b0d5754133\nsetting min_mon_release = octopus\nepoch 0\nfsid d01d5929-48de-45f8-ac1c-05b0d5754133\nlast_changed 2021-05-16T11:54:10.343971-0700\ncreated 2021-05-16T11:54:10.343971-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40044/0,v1:10.10.1.2:40045/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.149479 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 b905fd4e-a75c-4b59-b977-a0d1bd4b53f5\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 d36ad854-e550-41e9-8539-7ceadfe53ec3\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 b6e2cc4e-f695-4770-8988-6b3f687700a7\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42044\n  w/ user/pass: admin / 00846139-de83-4cc4-b499-6357bc36942d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:54:28 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40044
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.149479
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid d01d5929-48de-45f8-ac1c-05b0d5754133
setting min_mon_release = octopus
epoch 0
fsid d01d5929-48de-45f8-ac1c-05b0d5754133
last_changed 2021-05-16T11:54:10.343971-0700
created 2021-05-16T11:54:10.343971-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40044/0,v1:10.10.1.2:40045/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.149479 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 b905fd4e-a75c-4b59-b977-a0d1bd4b53f5
0
start osd.0
add osd1 d36ad854-e550-41e9-8539-7ceadfe53ec3
1
start osd.1
add osd2 b6e2cc4e-f695-4770-8988-6b3f687700a7
2
start osd.2


restful urls: https://10.10.1.2:42044
  w/ user/pass: admin / 00846139-de83-4cc4-b499-6357bc36942d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T11:54:02.115-0700 7f411c7c11c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:54:02.115-0700 7f411c7c11c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:54:02.131-0700 7f7988f2a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T11:54:02.131-0700 7f7988f2a1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40044,v1:10.10.1.2:40045] --print /tmp/ceph_monmap.149479 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.149479 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.149479 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42044 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55c8014b6000 @  0x7f6f30dfb680 0x7f6f30e1c824 0x7f6f315b7187 0x7f6f315bf355 0x7f6f315b7708 0x7f6f315b7877 0x7f6f315b8c24 0x7f6f315d0ec1 0x7f6f315435f3 0x7f6f315a4e97 0x7f6f315acb1a 0x7f6f30cafd84 0x7f6f30dcb609 0x7f6f3099f293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.yTtt7HZqcc 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b905fd4e-a75c-4b59-b977-a0d1bd4b53f5 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBdaqFgo/JODhAAYPeN3YKsjIR3RvJsaRVZMA== --osd-uuid b905fd4e-a75c-4b59-b977-a0d1bd4b53f5 
2021-05-16T11:54:21.887-0700 7fd65c71bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:54:21.911-0700 7fd65c71bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T11:54:21.911-0700 7fd65c71bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55ad5499a000 @  0x7fd65d0e4680 0x7fd65d105824 0x55ad4a972447 0x55ad4a97a4b5 0x55ad4a9729c8 0x55ad4a972b37 0x55ad4a973ee4 0x55ad4a744ca1 0x55ad4a91c423 0x55ad4a7352a7 0x55ad4a73a53a 0x7fd65cc37d84 0x7fd65cdbc609 0x7fd65c925293
2021-05-16T11:54:22.235-0700 7fd65c71bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d36ad854-e550-41e9-8539-7ceadfe53ec3 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x557441f74000 @  0x7f7518ec5680 0x7f7518ee6824 0x557436582447 0x55743658a4b5 0x5574365829c8 0x557436582b37 0x557436583ee4 0x557436354ca1 0x55743652c423 0x5574363452a7 0x55743634a53a 0x7f7518a18d84 0x7f7518b9d609 0x7f7518706293
2021-05-16T11:54:22.859-0700 7f75184fcf00 -1 Falling back to public interface
2021-05-16T11:54:23.119-0700 7f75184fcf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBeaqFgduv6IBAAAPkgQoxDQyM7FGdL8FCFvA== --osd-uuid d36ad854-e550-41e9-8539-7ceadfe53ec3 
2021-05-16T11:54:23.243-0700 7f02c256cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:54:23.267-0700 7f02c256cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T11:54:23.267-0700 7f02c256cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56359b222000 @  0x7f02c2f35680 0x7f02c2f56824 0x56358ff3f447 0x56358ff474b5 0x56358ff3f9c8 0x56358ff3fb37 0x56358ff40ee4 0x56358fd11ca1 0x56358fee9423 0x56358fd022a7 0x56358fd0753a 0x7f02c2a88d84 0x7f02c2c0d609 0x7f02c2776293
2021-05-16T11:54:23.587-0700 7f02c256cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b6e2cc4e-f695-4770-8988-6b3f687700a7 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5616f3b58000 @  0x7fbef8aaa680 0x7fbef8acb824 0x5616e7de4447 0x5616e7dec4b5 0x5616e7de49c8 0x5616e7de4b37 0x5616e7de5ee4 0x5616e7bb6ca1 0x5616e7d8e423 0x5616e7ba72a7 0x5616e7bac53a 0x7fbef85fdd84 0x7fbef8782609 0x7fbef82eb293
2021-05-16T11:54:24.267-0700 7fbef80e1f00 -1 Falling back to public interface
2021-05-16T11:54:24.527-0700 7fbef80e1f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBfaqFgc4I/ORAAy4Ab/kv/myKsd1SKYM4uJQ== --osd-uuid b6e2cc4e-f695-4770-8988-6b3f687700a7 
2021-05-16T11:54:24.627-0700 7ff077dfdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:54:24.647-0700 7ff077dfdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T11:54:24.647-0700 7ff077dfdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55dc97ab8000 @  0x7ff0787c6680 0x7ff0787e7824 0x55dc8cd95447 0x55dc8cd9d4b5 0x55dc8cd959c8 0x55dc8cd95b37 0x55dc8cd96ee4 0x55dc8cb67ca1 0x55dc8cd3f423 0x55dc8cb582a7 0x55dc8cb5d53a 0x7ff078319d84 0x7ff07849e609 0x7ff078007293
2021-05-16T11:54:24.959-0700 7ff077dfdf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x559036f7a000 @  0x7ffbfb3ce680 0x7ffbfb3ef824 0x55902b0a1447 0x55902b0a94b5 0x55902b0a19c8 0x55902b0a1b37 0x55902b0a2ee4 0x55902ae73ca1 0x55902b04b423 0x55902ae642a7 0x55902ae6953a 0x7ffbfaf21d84 0x7ffbfb0a6609 0x7ffbfac0f293
2021-05-16T11:54:25.647-0700 7ffbfaa05f00 -1 Falling back to public interface
2021-05-16T11:54:25.923-0700 7ffbfaa05f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:28,131071189-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:28,142394175-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:28,185069491-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:28,188268482-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '27006f2b-f078-4f71-96a1-143a5695da0f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 27006f2b-f078-4f71-96a1-143a5695da0f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:31,638129211-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:31,641737882-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '27006f2b-f078-4f71-96a1-143a5695da0f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 27006f2b-f078-4f71-96a1-143a5695da0f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:35,211678770-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:35,215069150-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '27006f2b-f078-4f71-96a1-143a5695da0f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 27006f2b-f078-4f71-96a1-143a5695da0f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:38,670526535-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:38,674164561-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '27006f2b-f078-4f71-96a1-143a5695da0f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 27006f2b-f078-4f71-96a1-143a5695da0f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:45,683447624-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:45,687013223-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '27006f2b-f078-4f71-96a1-143a5695da0f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 27006f2b-f078-4f71-96a1-143a5695da0f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:50,301509760-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:50,305571783-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '27006f2b-f078-4f71-96a1-143a5695da0f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 27006f2b-f078-4f71-96a1-143a5695da0f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:54,755473317-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:54,758670374-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '27006f2b-f078-4f71-96a1-143a5695da0f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 27006f2b-f078-4f71-96a1-143a5695da0f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:59,257981750-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:54:59,261515129-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '27006f2b-f078-4f71-96a1-143a5695da0f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 27006f2b-f078-4f71-96a1-143a5695da0f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:55:03,584098865-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:55:03,588092809-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '27006f2b-f078-4f71-96a1-143a5695da0f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 27006f2b-f078-4f71-96a1-143a5695da0f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:55:07,114773884-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:55:07,118644697-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '27006f2b-f078-4f71-96a1-143a5695da0f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 27006f2b-f078-4f71-96a1-143a5695da0f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:55:11,513577417-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:55:11,517618169-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '27006f2b-f078-4f71-96a1-143a5695da0f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 27006f2b-f078-4f71-96a1-143a5695da0f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:55:14,846006864-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:55:14,849898927-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '27006f2b-f078-4f71-96a1-143a5695da0f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 27006f2b-f078-4f71-96a1-143a5695da0f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  161 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  161 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  0.95   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.02   70      up          osd.2  
                       TOTAL  300 GiB  162 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.02  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:55:18,318015129-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:55:41,874116014-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:55:50,496267332-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:55:59,060515923-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:07,552179998-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:16,059334391-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:24,486516126-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:24,494623871-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:32,984348634-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:32,993536057-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:41,616911079-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:41,626230570-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:50,094862803-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:50,102964666-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:58,604840833-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:58,613241408-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:58,619800923-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:58,623593209-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:58,632166368-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=144818
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:58,638292058-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:58,657948252-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:56:58,661199180-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '27006f2b-f078-4f71-96a1-143a5695da0f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 27006f2b-f078-4f71-96a1-143a5695da0f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:57:00.982+0000 7f7fce686d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:57:01.254+0000 7f7fce686d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:57:01.254+0000 7f7fce686d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:57:01.266710+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-16T18:57:01.266719+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T18:57:01.270929+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:57:01.270929+0000     0       0         0         0         0         0           -           0
2021-05-16T18:57:02.271184+0000     1     256       354        98   6.12378     6.125   0.0137131   0.0111877
2021-05-16T18:57:03.271447+0000     2     256       513       257   8.02939    9.9375  0.00123194    0.495349
2021-05-16T18:57:04.271524+0000     3     256       513       257    5.3532         0           -    0.495349
2021-05-16T18:57:05.271601+0000     4     256       939       683   10.6702   13.3125 0.000819049    0.801899
2021-05-16T18:57:06.271696+0000     5     256       939       683   8.53629         0           -    0.801899
2021-05-16T18:57:07.271885+0000     6     256      1461      1205   12.5502   16.3125 0.000942691     1.18306
2021-05-16T18:57:08.271992+0000     7     256      1461      1205   10.7574         0           -     1.18306
2021-05-16T18:57:09.272061+0000     8     256      1939      1683   13.1467   14.9375 0.000806124     1.05061
2021-05-16T18:57:10.272133+0000     9     256      1939      1683    11.686         0           -     1.05061
2021-05-16T18:57:11.272236+0000    10     256      3096      2840   17.7478   36.1562  0.00080902    0.891303
2021-05-16T18:57:12.272347+0000    11     256      3096      2840   16.1344         0           -    0.891303
2021-05-16T18:57:13.272443+0000    12     256      3316      3060   15.9356     6.875     2.08369    0.978845
2021-05-16T18:57:14.272519+0000    13     256      4413      4157   19.9832   68.5625 0.000807528    0.741545
2021-05-16T18:57:15.272632+0000    14     256      4413      4157   18.5559         0           -    0.741545
2021-05-16T18:57:16.272710+0000    15     256      5244      4988   20.7809   25.9688 0.000811084    0.727283
2021-05-16T18:57:17.272964+0000    16     256      5244      4988    19.482         0           -    0.727283
2021-05-16T18:57:18.273191+0000    17     256      5846      5590   20.5488   18.8125 0.000826813    0.728518
2021-05-16T18:57:19.273470+0000    18     256      5846      5590    19.407         0           -    0.728518
2021-05-16T18:57:20.273554+0000    19     256      6908      6652   21.8786   33.1875 0.000844938     0.71107
2021-05-16T18:57:21.273821+0000 min lat: 0.000740361 max lat: 5.56673 avg lat: 0.71107
2021-05-16T18:57:21.273821+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:57:21.273821+0000    20     256      6908      6652   20.7846         0           -     0.71107
2021-05-16T18:57:22.273987+0000 Total time run:         20.5992
Total writes made:      6908
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     20.9596
Stddev Bandwidth:       17.559
Max bandwidth (MB/sec): 68.5625
Min bandwidth (MB/sec): 0
Average IOPS:           335
Stddev IOPS:            280.889
Max IOPS:               1097
Min IOPS:               0
Average Latency(s):     0.76305
Stddev Latency(s):      1.22355
Max latency(s):         5.56673
Min latency(s):         0.000740361

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:57:22,809868004-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 144818


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:57:22,814617488-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:57:46,207286961-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 6.91k objects, 432 MiB
    usage:   864 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:57:46,215840112-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:57:54,662852415-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 6.91k objects, 432 MiB
    usage:   864 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:57:54,670838220-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:58:03,236841536-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 6.91k objects, 432 MiB
    usage:   864 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:58:03,245806822-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:58:11,791517901-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 6.91k objects, 432 MiB
    usage:   864 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:58:11,800019575-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:58:20,239411190-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 6.91k objects, 432 MiB
    usage:   864 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:58:20,247021129-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:58:20,253619798-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:58:20,257545675-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:58:20,265851111-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=146201
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:58:20,272708185-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:58:20,290698549-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:58:20,294104008-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '27006f2b-f078-4f71-96a1-143a5695da0f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 27006f2b-f078-4f71-96a1-143a5695da0f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.bTMiid:/tmp/ceph-asok.bTMiid -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T18:58:22.630+0000 7fe67200bd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:58:22.898+0000 7fe67200bd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T18:58:22.898+0000 7fe67200bd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T18:58:22.914145+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:58:22.914145+0000     0       0         0         0         0         0           -           0
2021-05-16T18:58:23.914216+0000     1     256      1127       871   54.4297   54.4375 0.000252345  0.00403274
2021-05-16T18:58:24.914308+0000     2     256      1865      1609   50.2754    46.125 0.000439495    0.184648
2021-05-16T18:58:25.914393+0000     3     256      1865      1609   33.5173         0           -    0.184648
2021-05-16T18:58:26.914497+0000     4     256      1976      1720   26.8722   3.46875     2.14379    0.308708
2021-05-16T18:58:27.914590+0000     5     256      1976      1720   21.4978         0           -    0.308708
2021-05-16T18:58:28.914661+0000     6     256      2421      2165   22.5499   13.9062 0.000362431    0.604832
2021-05-16T18:58:29.914733+0000     7     256      2421      2165   19.3285         0           -    0.604832
2021-05-16T18:58:30.914840+0000     8     256      2639      2383   18.6154    6.8125 0.000442401    0.609693
2021-05-16T18:58:31.914905+0000     9     256      2639      2383   16.5471         0           -    0.609693
2021-05-16T18:58:32.914971+0000    10     256      2933      2677   16.7297    9.1875  0.00044173    0.744946
2021-05-16T18:58:33.915187+0000    11     256      2933      2677   15.2087         0           -    0.744946
2021-05-16T18:58:34.915291+0000    12     256      3301      3045   15.8578      11.5 0.000451839    0.879806
2021-05-16T18:58:35.915359+0000    13     256      3301      3045    14.638         0           -    0.879806
2021-05-16T18:58:36.915556+0000    14     256      3301      3045   13.5923         0           -    0.879806
2021-05-16T18:58:37.915772+0000    15     256      3689      3433   14.3025   8.08333 0.000458281     1.04705
2021-05-16T18:58:38.915891+0000    16     256      3689      3433   13.4086         0           -     1.04705
2021-05-16T18:58:39.915985+0000    17     256      3957      3701   13.6051     8.375 0.000492766     1.08022
2021-05-16T18:58:40.916094+0000    18     256      3957      3701   12.8493         0           -     1.08022
2021-05-16T18:58:41.916159+0000    19     256      4493      4237    13.936     16.75 0.000363293     1.05714
2021-05-16T18:58:42.916268+0000 min lat: 0.000238839 max lat: 6.44191 avg lat: 1.05714
2021-05-16T18:58:42.916268+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T18:58:42.916268+0000    20     256      4493      4237   13.2392         0           -     1.05714
2021-05-16T18:58:43.916454+0000    21     256      5208      4952   14.7364   22.3438 0.000456027     1.05915
2021-05-16T18:58:44.916543+0000    22     256      5208      4952   14.0666         0           -     1.05915
2021-05-16T18:58:45.916614+0000    23     256      5959      5703   15.4956   23.4688 0.000459954     1.01607
2021-05-16T18:58:46.916736+0000    24     256      5959      5703   14.8499         0           -     1.01607
2021-05-16T18:58:47.916808+0000    25     256      6398      6142   15.3533   13.7188 0.000302849     1.03282
2021-05-16T18:58:48.916964+0000    26     256      6398      6142   14.7628         0           -     1.03282
2021-05-16T18:58:49.917030+0000    27     256      6616      6360   14.7206    6.8125 0.000312707     1.04731
2021-05-16T18:58:50.917139+0000    28     256      6616      6360   14.1949         0           -     1.04731
2021-05-16T18:58:51.917275+0000    29     256      6616      6360   13.7054         0           -     1.04731
2021-05-16T18:58:52.917491+0000    30     256      6850      6594   13.7359     4.875 0.000354617     1.04467
2021-05-16T18:58:53.917567+0000    31     256      6850      6594   13.2929         0           -     1.04467
2021-05-16T18:58:54.917691+0000    32      98      6908      6810   13.2993      6.75      2.1512     1.15302
2021-05-16T18:58:55.917759+0000    33      98      6908      6810   12.8963         0           -     1.15302
2021-05-16T18:58:56.917828+0000    34      53      6908      6855   12.5997   1.40625     4.29146     1.16705
2021-05-16T18:58:57.917894+0000    35      53      6908      6855   12.2397         0           -     1.16705
2021-05-16T18:58:58.917999+0000    36      13      6908      6895   11.9692      1.25     6.43513     1.19763
2021-05-16T18:58:59.918065+0000    37      13      6908      6895   11.6457         0           -     1.19763
2021-05-16T18:59:00.918195+0000 Total time run:       37.6576
Total reads made:     6908
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   11.4652
Average IOPS:         183
Stddev IOPS:          198.015
Max IOPS:             871
Min IOPS:             0
Average Latency(s):   1.2078
Max latency(s):       8.58043
Min latency(s):       0.000232056

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:59:01,440213530-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 146201


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:59:01,444876411-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:59:25,002375346-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 6.91k objects, 432 MiB
    usage:   864 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:59:25,011161584-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:59:33,484566396-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 6.91k objects, 432 MiB
    usage:   864 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:59:33,493591173-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:59:41,973075628-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 6.91k objects, 432 MiB
    usage:   864 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:59:41,981909527-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:59:50,555799891-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 6.91k objects, 432 MiB
    usage:   864 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:59:50,564654097-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:59:59,093061320-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 6.91k objects, 432 MiB
    usage:   864 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:59:59,100808977-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:59:59,107034786-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T14:59:59,110976943-04:00][RUNNING][ROUND 1/4/40] object_size=256KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:59:59,114887981-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T14:59:59,123324854-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40524\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.150564\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 72a8a670-11d1-4cea-bb19-6db0b0062726\nsetting min_mon_release = octopus\nepoch 0\nfsid 72a8a670-11d1-4cea-bb19-6db0b0062726\nlast_changed 2021-05-16T12:00:09.207954-0700\ncreated 2021-05-16T12:00:09.207954-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40524/0,v1:10.10.1.2:40525/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.150564 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 bafd4f67-8d70-4bfe-b7e5-3178499ed17d\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 55a4feab-20b7-4cc6-b8ab-fd3e8197b696\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 d1473b9e-8130-4772-9e11-0d7ab0e362e3\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42524\n  w/ user/pass: admin / 167a3e1c-322d-4210-990f-c9fcd96e07d0\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:00:26 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40524
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.150564
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 72a8a670-11d1-4cea-bb19-6db0b0062726
setting min_mon_release = octopus
epoch 0
fsid 72a8a670-11d1-4cea-bb19-6db0b0062726
last_changed 2021-05-16T12:00:09.207954-0700
created 2021-05-16T12:00:09.207954-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40524/0,v1:10.10.1.2:40525/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.150564 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 bafd4f67-8d70-4bfe-b7e5-3178499ed17d
0
start osd.0
add osd1 55a4feab-20b7-4cc6-b8ab-fd3e8197b696
1
start osd.1
add osd2 d1473b9e-8130-4772-9e11-0d7ab0e362e3
2
start osd.2


restful urls: https://10.10.1.2:42524
  w/ user/pass: admin / 167a3e1c-322d-4210-990f-c9fcd96e07d0


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T12:00:00.978-0700 7f14e78d41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:00:00.978-0700 7f14e78d41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:00:00.994-0700 7f34dc4681c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:00:00.994-0700 7f34dc4681c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40524,v1:10.10.1.2:40525] --print /tmp/ceph_monmap.150564 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.150564 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.150564 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42524 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55df5916a000 @  0x7f365e534680 0x7f365e555824 0x7f365ecf0187 0x7f365ecf8355 0x7f365ecf0708 0x7f365ecf0877 0x7f365ecf1c24 0x7f365ed09ec1 0x7f365ec7c5f3 0x7f365ecdde97 0x7f365ece5b1a 0x7f365e3e8d84 0x7f365e504609 0x7f365e0d8293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.JQhVTJt9wD 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new bafd4f67-8d70-4bfe-b7e5-3178499ed17d -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDEa6FgoIDsCBAAoaerTNX/z2tR63UtkRc57w== --osd-uuid bafd4f67-8d70-4bfe-b7e5-3178499ed17d 
2021-05-16T12:00:20.806-0700 7ff17d785f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:00:20.830-0700 7ff17d785f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:00:20.830-0700 7ff17d785f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x556c71f44000 @  0x7ff17e14e680 0x7ff17e16f824 0x556c67654447 0x556c6765c4b5 0x556c676549c8 0x556c67654b37 0x556c67655ee4 0x556c67426ca1 0x556c675fe423 0x556c674172a7 0x556c6741c53a 0x7ff17dca1d84 0x7ff17de26609 0x7ff17d98f293
2021-05-16T12:00:21.146-0700 7ff17d785f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 55a4feab-20b7-4cc6-b8ab-fd3e8197b696 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x556958022000 @  0x7fe28eab1680 0x7fe28ead2824 0x55694de61447 0x55694de694b5 0x55694de619c8 0x55694de61b37 0x55694de62ee4 0x55694dc33ca1 0x55694de0b423 0x55694dc242a7 0x55694dc2953a 0x7fe28e604d84 0x7fe28e789609 0x7fe28e2f2293
2021-05-16T12:00:21.762-0700 7fe28e0e8f00 -1 Falling back to public interface
2021-05-16T12:00:22.026-0700 7fe28e0e8f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDFa6Fg5lkPGxAAmpXMDcgcoratR3Bi5gKhjQ== --osd-uuid 55a4feab-20b7-4cc6-b8ab-fd3e8197b696 
2021-05-16T12:00:22.138-0700 7fd65c81bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:00:22.158-0700 7fd65c81bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:00:22.158-0700 7fd65c81bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55a0becea000 @  0x7fd65d1e4680 0x7fd65d205824 0x55a0b4830447 0x55a0b48384b5 0x55a0b48309c8 0x55a0b4830b37 0x55a0b4831ee4 0x55a0b4602ca1 0x55a0b47da423 0x55a0b45f32a7 0x55a0b45f853a 0x7fd65cd37d84 0x7fd65cebc609 0x7fd65ca25293
2021-05-16T12:00:22.474-0700 7fd65c81bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d1473b9e-8130-4772-9e11-0d7ab0e362e3 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x561da868e000 @  0x7f9722012680 0x7f9722033824 0x561d9cd3b447 0x561d9cd434b5 0x561d9cd3b9c8 0x561d9cd3bb37 0x561d9cd3cee4 0x561d9cb0dca1 0x561d9cce5423 0x561d9cafe2a7 0x561d9cb0353a 0x7f9721b65d84 0x7f9721cea609 0x7f9721853293
2021-05-16T12:00:23.110-0700 7f9721649f00 -1 Falling back to public interface
2021-05-16T12:00:23.366-0700 7f9721649f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDGa6Fg/aS6LxAAk7dgNgU6pcScH6zbDUfIBA== --osd-uuid d1473b9e-8130-4772-9e11-0d7ab0e362e3 
2021-05-16T12:00:23.470-0700 7f0c2c41df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:00:23.490-0700 7f0c2c41df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:00:23.490-0700 7f0c2c41df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x555a388d6000 @  0x7f0c2cde6680 0x7f0c2ce07824 0x555a2d622447 0x555a2d62a4b5 0x555a2d6229c8 0x555a2d622b37 0x555a2d623ee4 0x555a2d3f4ca1 0x555a2d5cc423 0x555a2d3e52a7 0x555a2d3ea53a 0x7f0c2c939d84 0x7f0c2cabe609 0x7f0c2c627293
2021-05-16T12:00:23.810-0700 7f0c2c41df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55de7522e000 @  0x7fd016790680 0x7fd0167b1824 0x55de6a65a447 0x55de6a6624b5 0x55de6a65a9c8 0x55de6a65ab37 0x55de6a65bee4 0x55de6a42cca1 0x55de6a604423 0x55de6a41d2a7 0x55de6a42253a 0x7fd0162e3d84 0x7fd016468609 0x7fd015fd1293
2021-05-16T12:00:24.494-0700 7fd015dc7f00 -1 Falling back to public interface
2021-05-16T12:00:24.766-0700 7fd015dc7f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:26,954404371-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:26,965480292-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:27,007531686-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:27,010904253-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5aead285-2aa9-447f-a92f-cb7aebe240be', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5aead285-2aa9-447f-a92f-cb7aebe240be --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:30,507437720-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:30,511021704-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5aead285-2aa9-447f-a92f-cb7aebe240be', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5aead285-2aa9-447f-a92f-cb7aebe240be --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:34,114598915-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:34,118460150-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5aead285-2aa9-447f-a92f-cb7aebe240be', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5aead285-2aa9-447f-a92f-cb7aebe240be --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:37,539098871-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:37,542435440-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5aead285-2aa9-447f-a92f-cb7aebe240be', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5aead285-2aa9-447f-a92f-cb7aebe240be --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:44,631582307-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:44,634548180-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5aead285-2aa9-447f-a92f-cb7aebe240be', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5aead285-2aa9-447f-a92f-cb7aebe240be --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:49,404930357-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:49,408365232-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5aead285-2aa9-447f-a92f-cb7aebe240be', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5aead285-2aa9-447f-a92f-cb7aebe240be --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:54,002332317-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:54,006072485-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5aead285-2aa9-447f-a92f-cb7aebe240be', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5aead285-2aa9-447f-a92f-cb7aebe240be --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:58,199415161-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:00:58,203229738-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5aead285-2aa9-447f-a92f-cb7aebe240be', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5aead285-2aa9-447f-a92f-cb7aebe240be --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:01:02,539091467-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:01:02,542813771-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5aead285-2aa9-447f-a92f-cb7aebe240be', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5aead285-2aa9-447f-a92f-cb7aebe240be --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:01:07,262628464-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:01:07,266459732-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5aead285-2aa9-447f-a92f-cb7aebe240be', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5aead285-2aa9-447f-a92f-cb7aebe240be --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:01:11,593392273-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:01:11,596924240-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5aead285-2aa9-447f-a92f-cb7aebe240be', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5aead285-2aa9-447f-a92f-cb7aebe240be --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:01:15,106823621-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:01:15,110629363-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5aead285-2aa9-447f-a92f-cb7aebe240be', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5aead285-2aa9-447f-a92f-cb7aebe240be --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:01:18,661149080-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:01:42,157549947-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:01:50,827361033-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:01:59,218653765-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:07,768065753-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:16,402581251-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:25,089728083-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   245 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:25,098545962-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:33,590771188-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:33,599842022-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:42,016762301-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:42,025625345-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:50,504919915-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:50,513638889-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:59,115957271-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:59,124795628-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:59,131331058-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:59,134850030-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:59,143433087-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=152663
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:59,149896763-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:59,169304420-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:02:59,172458827-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5aead285-2aa9-447f-a92f-cb7aebe240be', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5aead285-2aa9-447f-a92f-cb7aebe240be --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:03:01.571+0000 7f29a705bd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:03:01.847+0000 7f29a705bd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:03:01.847+0000 7f29a705bd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:03:01.858876+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-16T19:03:01.858886+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T19:03:01.873546+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:03:01.873546+0000     0       0         0         0         0         0           -           0
2021-05-16T19:03:02.873769+0000     1     214       214         0         0         0           -           0
2021-05-16T19:03:03.873904+0000     2     255       404       149   18.6222    18.625     1.43167     1.43452
2021-05-16T19:03:04.873994+0000     3     255       404       149   12.4151         0           -     1.43452
2021-05-16T19:03:05.874069+0000     4     255       668       413   25.8095        33   0.0349346     1.98098
2021-05-16T19:03:06.874331+0000     5     255       668       413    20.647         0           -     1.98098
2021-05-16T19:03:07.874404+0000     6     255       824       569   23.7052      19.5     2.12804     2.10147
2021-05-16T19:03:08.874501+0000     7     255       824       569   20.3188         0           -     2.10147
2021-05-16T19:03:09.874575+0000     8     255       935       680   21.2474    13.875      2.1605     2.29096
2021-05-16T19:03:10.874698+0000     9     255       935       680   18.8866         0           -     2.29096
2021-05-16T19:03:11.874762+0000    10     255       935       680    16.998         0           -     2.29096
2021-05-16T19:03:12.874835+0000    11     255      1242       987   22.4293   25.5833     4.34063     2.53408
2021-05-16T19:03:13.874981+0000    12     255      1242       987   20.5601         0           -     2.53408
2021-05-16T19:03:14.875080+0000    13     255      1363      1108   21.3053    15.125     2.12013     2.50094
2021-05-16T19:03:15.875289+0000    14     255      1363      1108   19.7833         0           -     2.50094
2021-05-16T19:03:16.875528+0000    15     255      1601      1346   22.4305     29.75   0.0372928     2.61954
2021-05-16T19:03:17.875673+0000    16     255      1601      1346   21.0285         0           -     2.61954
2021-05-16T19:03:18.875772+0000    17     255      1736      1481   21.7766    16.875     2.13552     2.60501
2021-05-16T19:03:19.875868+0000    18     255      1736      1481   20.5669         0           -     2.60501
2021-05-16T19:03:20.875943+0000    19     255      1986      1731   22.7735     31.25     4.31174     2.67322
2021-05-16T19:03:21.876040+0000 min lat: 0.0259622 max lat: 6.48384 avg lat: 2.67322
2021-05-16T19:03:21.876040+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:03:21.876040+0000    20     255      1986      1731   21.6349         0           -     2.67322
2021-05-16T19:03:22.876145+0000    21      66      1987      1921   22.8663     23.75     2.09649     2.66059
2021-05-16T19:03:23.876210+0000    22      66      1987      1921    21.827         0           -     2.66059
2021-05-16T19:03:24.876353+0000 Total time run:         22.9019
Total writes made:      1987
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     21.6903
Stddev Bandwidth:       12.4466
Max bandwidth (MB/sec): 33
Min bandwidth (MB/sec): 0
Average IOPS:           86
Stddev IOPS:            49.723
Max IOPS:               132
Min IOPS:               0
Average Latency(s):     2.71355
Stddev Latency(s):      1.38395
Max latency(s):         8.55558
Min latency(s):         0.0259622

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:03:25,424832605-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 152663


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:03:25,429772347-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:03:49,110337078-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 497 MiB
    usage:   994 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:03:49,119157853-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:03:57,583289427-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 497 MiB
    usage:   994 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:03:57,592230738-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:04:06,160833873-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 497 MiB
    usage:   994 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:04:06,169579215-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:04:14,761134449-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 497 MiB
    usage:   994 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:04:14,769817263-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:04:23,430267771-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 497 MiB
    usage:   994 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:04:23,438858553-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:04:23,445498068-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:04:23,449753505-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:04:23,457997265-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=154047
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:04:23,464467011-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:04:23,483629367-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:04:23,487317878-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5aead285-2aa9-447f-a92f-cb7aebe240be', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5aead285-2aa9-447f-a92f-cb7aebe240be --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.c757ZL:/tmp/ceph-asok.c757ZL -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:04:25.852+0000 7f8382828d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:04:26.108+0000 7f8382828d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:04:26.108+0000 7f8382828d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:04:26.123412+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:04:26.123412+0000     0       0         0         0         0         0           -           0
2021-05-16T19:04:27.123552+0000     1     256       487       231   57.7363     57.75   0.0106749    0.110003
2021-05-16T19:04:28.123622+0000     2     256       502       246   30.7453      3.75     1.94145    0.218144
2021-05-16T19:04:29.123689+0000     3     256       502       246   20.4974         0           -    0.218144
2021-05-16T19:04:30.123796+0000     4     256       502       246   15.3731         0           -    0.218144
2021-05-16T19:04:31.123881+0000     5     256       516       260   12.9985   1.16667     4.08825    0.419838
2021-05-16T19:04:32.123948+0000     6     256       516       260   10.8322         0           -    0.419838
2021-05-16T19:04:33.124015+0000     7     256       607       351   12.5345    11.375 0.000683083     1.31149
2021-05-16T19:04:34.124083+0000     8     256       607       351   10.9677         0           -     1.31149
2021-05-16T19:04:35.124169+0000     9     256       699       443   12.3044      11.5     8.16862     2.72743
2021-05-16T19:04:36.124236+0000    10     256       699       443    11.074         0           -     2.72743
2021-05-16T19:04:37.124306+0000    11     256       752       496   11.2717     6.625 0.000751943      2.6585
2021-05-16T19:04:38.124374+0000    12     256       752       496   10.3324         0           -      2.6585
2021-05-16T19:04:39.124458+0000    13     256       897       641   12.3258    18.125 0.000514747     2.97351
2021-05-16T19:04:40.124527+0000    14     256       897       641   11.4454         0           -     2.97351
2021-05-16T19:04:41.124593+0000    15     256       910       654   10.8991     1.625 0.000707239      2.9988
2021-05-16T19:04:42.124662+0000    16     256       910       654   10.2179         0           -      2.9988
2021-05-16T19:04:43.124748+0000    17     256      1001       745    10.955    11.375     16.7568     4.22229
2021-05-16T19:04:44.124977+0000    18     256      1001       745   10.3463         0           -     4.22229
2021-05-16T19:04:45.125047+0000    19     256      1001       745   9.80174         0           -     4.22229
2021-05-16T19:04:46.125280+0000 min lat: 0.000514747 max lat: 16.9685 avg lat: 4.14625
2021-05-16T19:04:46.125280+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:04:46.125280+0000    20     256      1025       769   9.61156         2  0.00147597     4.14625
2021-05-16T19:04:47.125525+0000    21     256      1025       769    9.1538         0           -     4.14625
2021-05-16T19:04:48.125601+0000    22     256      1185       929   10.5557        20 0.000501502     4.19064
2021-05-16T19:04:49.125817+0000    23     256      1185       929   10.0967         0           -     4.19064
2021-05-16T19:04:50.125886+0000    24     256      1253       997   10.3843       8.5  0.00058535     4.40231
2021-05-16T19:04:51.125975+0000    25     256      1253       997   9.96894         0           -     4.40231
2021-05-16T19:04:52.126041+0000    26     256      1464      1208   11.6142    26.375     2.16211     5.30858
2021-05-16T19:04:53.126109+0000    27     256      1464      1208    11.184         0           -     5.30858
2021-05-16T19:04:54.126176+0000    28     256      1642      1386   12.3737     22.25 0.000492064     4.85926
2021-05-16T19:04:55.126263+0000    29     256      1642      1386   11.9471         0           -     4.85926
2021-05-16T19:04:56.126330+0000    30     256      1685      1429   11.9071     5.375  0.00114336     4.81507
2021-05-16T19:04:57.126397+0000    31     256      1685      1429    11.523         0           -     4.81507
2021-05-16T19:04:58.126559+0000    32     256      1685      1429   11.1629         0           -     4.81507
2021-05-16T19:04:59.126644+0000    33     256      1774      1518   11.4988   7.41667     2.15209     4.81684
2021-05-16T19:05:00.126711+0000    34     256      1774      1518   11.1606         0           -     4.81684
2021-05-16T19:05:01.127021+0000    35     256      1860      1604   11.4559     10.75 0.000667174     4.66976
2021-05-16T19:05:02.127235+0000    36     256      1860      1604   11.1377         0           -     4.66976
2021-05-16T19:05:03.127364+0000    37     256      1940      1684   11.3771        10  0.00043673     4.54482
2021-05-16T19:05:04.127578+0000    38     256      1940      1684   11.0777         0           -     4.54482
2021-05-16T19:05:05.127645+0000    39     242      1987      1745   11.1847     7.625  0.00165894     4.41794
2021-05-16T19:05:06.127727+0000 min lat: 0.00043673 max lat: 19.3274 avg lat: 4.41794
2021-05-16T19:05:06.127727+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:05:06.127727+0000    40     242      1987      1745    10.905         0           -     4.41794
2021-05-16T19:05:07.127822+0000    41     231      1987      1756   10.7061     1.375     15.0324     4.49297
2021-05-16T19:05:08.127890+0000    42     231      1987      1756   10.4512         0           -     4.49297
2021-05-16T19:05:09.127957+0000    43      67      1987      1920   11.1616      20.5      10.743     5.40538
2021-05-16T19:05:10.128026+0000    44      67      1987      1920   10.9079         0           -     5.40538
2021-05-16T19:05:11.128112+0000    45      15      1987      1972   10.9544       6.5     8.58782     5.52741
2021-05-16T19:05:12.128179+0000    46      15      1987      1972   10.7163         0           -     5.52741
2021-05-16T19:05:13.128248+0000    47      15      1987      1972   10.4883         0           -     5.52741
2021-05-16T19:05:14.128315+0000    48       5      1987      1982   10.3218  0.833333     8.58731     5.54609
2021-05-16T19:05:15.128403+0000    49       5      1987      1982   10.1112         0           -     5.54609
2021-05-16T19:05:16.128524+0000 Total time run:       49.1854
Total reads made:     1987
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   10.0996
Average IOPS:         40
Stddev IOPS:          41.1476
Max IOPS:             231
Min IOPS:             0
Average Latency(s):   5.55914
Max latency(s):       19.3274
Min latency(s):       0.00043673

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:05:16,643614537-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 154047


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:05:16,648322444-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:05:40,097475814-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 497 MiB
    usage:   994 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:05:40,106560835-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:05:48,631159734-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 497 MiB
    usage:   994 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:05:48,639753431-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:05:57,224829316-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 497 MiB
    usage:   994 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:05:57,233389250-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:05,529991285-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 497 MiB
    usage:   994 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:05,538090994-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:14,084159957-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 497 MiB
    usage:   994 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:14,093060180-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:14,100073087-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T15:06:14,102615014-04:00][RUNNING][ROUND 2/4/40] object_size=256KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:14,106505483-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:14,115602156-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40900\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.152311\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b394a1f8-9097-44f2-ae7c-fcf4140eb7f5\nsetting min_mon_release = octopus\nepoch 0\nfsid b394a1f8-9097-44f2-ae7c-fcf4140eb7f5\nlast_changed 2021-05-16T12:06:24.085746-0700\ncreated 2021-05-16T12:06:24.085746-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40900/0,v1:10.10.1.2:40901/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.152311 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 da4bfc81-9d2d-4d91-9379-208a4f423fba\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 068aa308-bc2b-426f-ae78-27e224883da8\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 625e4ec4-6599-474e-a5ab-fff549553772\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42900\n  w/ user/pass: admin / 86335f8d-9392-4f4a-a22b-1db197478a07\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:06:41 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40900
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.152311
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b394a1f8-9097-44f2-ae7c-fcf4140eb7f5
setting min_mon_release = octopus
epoch 0
fsid b394a1f8-9097-44f2-ae7c-fcf4140eb7f5
last_changed 2021-05-16T12:06:24.085746-0700
created 2021-05-16T12:06:24.085746-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40900/0,v1:10.10.1.2:40901/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.152311 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 da4bfc81-9d2d-4d91-9379-208a4f423fba
0
start osd.0
add osd1 068aa308-bc2b-426f-ae78-27e224883da8
1
start osd.1
add osd2 625e4ec4-6599-474e-a5ab-fff549553772
2
start osd.2


restful urls: https://10.10.1.2:42900
  w/ user/pass: admin / 86335f8d-9392-4f4a-a22b-1db197478a07


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T12:06:15.950-0700 7f2e2232d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:06:15.950-0700 7f2e2232d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:06:15.966-0700 7f12d5ede1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:06:15.966-0700 7f12d5ede1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40900,v1:10.10.1.2:40901] --print /tmp/ceph_monmap.152311 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.152311 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.152311 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42900 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x56130bc7e000 @  0x7f55ce7f5680 0x7f55ce816824 0x7f55cefb1187 0x7f55cefb9355 0x7f55cefb1708 0x7f55cefb1877 0x7f55cefb2c24 0x7f55cefcaec1 0x7f55cef3d5f3 0x7f55cef9ee97 0x7f55cefa6b1a 0x7f55ce6a9d84 0x7f55ce7c5609 0x7f55ce399293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.LxkFN1H0xS 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new da4bfc81-9d2d-4d91-9379-208a4f423fba -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA6baFgH9AcMhAAffmnA41eHNQicGT1tDwZew== --osd-uuid da4bfc81-9d2d-4d91-9379-208a4f423fba 
2021-05-16T12:06:35.498-0700 7fa626dbcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:06:35.518-0700 7fa626dbcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:06:35.518-0700 7fa626dbcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55ea30608000 @  0x7fa627785680 0x7fa6277a6824 0x55ea253ec447 0x55ea253f44b5 0x55ea253ec9c8 0x55ea253ecb37 0x55ea253edee4 0x55ea251beca1 0x55ea25396423 0x55ea251af2a7 0x55ea251b453a 0x7fa6272d8d84 0x7fa62745d609 0x7fa626fc6293
2021-05-16T12:06:35.830-0700 7fa626dbcf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 068aa308-bc2b-426f-ae78-27e224883da8 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55963dabc000 @  0x7f4efc49c680 0x7f4efc4bd824 0x55963286f447 0x5596328774b5 0x55963286f9c8 0x55963286fb37 0x559632870ee4 0x559632641ca1 0x559632819423 0x5596326322a7 0x55963263753a 0x7f4efbfefd84 0x7f4efc174609 0x7f4efbcdd293
2021-05-16T12:06:36.462-0700 7f4efbad3f00 -1 Falling back to public interface
2021-05-16T12:06:36.738-0700 7f4efbad3f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA8baFgo2lICRAAbkkWWFLjmDLp/zMirdhq7Q== --osd-uuid 068aa308-bc2b-426f-ae78-27e224883da8 
2021-05-16T12:06:36.838-0700 7f840495ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:06:36.858-0700 7f840495ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:06:36.858-0700 7f840495ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55f87a3fe000 @  0x7f8405327680 0x7f8405348824 0x55f86f334447 0x55f86f33c4b5 0x55f86f3349c8 0x55f86f334b37 0x55f86f335ee4 0x55f86f106ca1 0x55f86f2de423 0x55f86f0f72a7 0x55f86f0fc53a 0x7f8404e7ad84 0x7f8404fff609 0x7f8404b68293
2021-05-16T12:06:37.182-0700 7f840495ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 625e4ec4-6599-474e-a5ab-fff549553772 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x559c87f3e000 @  0x7f62db8af680 0x7f62db8d0824 0x559c7cea7447 0x559c7ceaf4b5 0x559c7cea79c8 0x559c7cea7b37 0x559c7cea8ee4 0x559c7cc79ca1 0x559c7ce51423 0x559c7cc6a2a7 0x559c7cc6f53a 0x7f62db402d84 0x7f62db587609 0x7f62db0f0293
2021-05-16T12:06:37.870-0700 7f62daee6f00 -1 Falling back to public interface
2021-05-16T12:06:38.126-0700 7f62daee6f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA9baFglwDqHxAARqAMSOfVthh0srHqQOmipA== --osd-uuid 625e4ec4-6599-474e-a5ab-fff549553772 
2021-05-16T12:06:38.294-0700 7fec502eff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:06:38.314-0700 7fec502eff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:06:38.314-0700 7fec502eff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56079a7a8000 @  0x7fec50cb8680 0x7fec50cd9824 0x5607905e0447 0x5607905e84b5 0x5607905e09c8 0x5607905e0b37 0x5607905e1ee4 0x5607903b2ca1 0x56079058a423 0x5607903a32a7 0x5607903a853a 0x7fec5080bd84 0x7fec50990609 0x7fec504f9293
2021-05-16T12:06:38.646-0700 7fec502eff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x564ed6ac6000 @  0x7f26d672b680 0x7f26d674c824 0x564ecbbf3447 0x564ecbbfb4b5 0x564ecbbf39c8 0x564ecbbf3b37 0x564ecbbf4ee4 0x564ecb9c5ca1 0x564ecbb9d423 0x564ecb9b62a7 0x564ecb9bb53a 0x7f26d627ed84 0x7f26d6403609 0x7f26d5f6c293
2021-05-16T12:06:39.306-0700 7f26d5d62f00 -1 Falling back to public interface
2021-05-16T12:06:39.578-0700 7f26d5d62f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:41,843035530-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:41,854668208-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:41,897851018-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:41,901502860-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8f50ebc8-b1f7-49d9-b283-73d07b206ce6', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8f50ebc8-b1f7-49d9-b283-73d07b206ce6 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:45,466938272-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:45,470857958-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8f50ebc8-b1f7-49d9-b283-73d07b206ce6', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8f50ebc8-b1f7-49d9-b283-73d07b206ce6 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:49,123015360-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:49,126221084-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8f50ebc8-b1f7-49d9-b283-73d07b206ce6', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8f50ebc8-b1f7-49d9-b283-73d07b206ce6 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:52,579992922-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:52,583439678-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8f50ebc8-b1f7-49d9-b283-73d07b206ce6', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8f50ebc8-b1f7-49d9-b283-73d07b206ce6 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:59,728108340-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:06:59,731791270-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8f50ebc8-b1f7-49d9-b283-73d07b206ce6', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8f50ebc8-b1f7-49d9-b283-73d07b206ce6 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:04,318597202-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:04,322084545-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8f50ebc8-b1f7-49d9-b283-73d07b206ce6', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8f50ebc8-b1f7-49d9-b283-73d07b206ce6 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:08,847039460-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:08,850927125-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8f50ebc8-b1f7-49d9-b283-73d07b206ce6', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8f50ebc8-b1f7-49d9-b283-73d07b206ce6 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:12,884689682-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:12,888488210-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8f50ebc8-b1f7-49d9-b283-73d07b206ce6', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8f50ebc8-b1f7-49d9-b283-73d07b206ce6 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:17,176825421-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:17,180298898-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8f50ebc8-b1f7-49d9-b283-73d07b206ce6', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8f50ebc8-b1f7-49d9-b283-73d07b206ce6 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:20,849088714-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:20,853294517-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8f50ebc8-b1f7-49d9-b283-73d07b206ce6', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8f50ebc8-b1f7-49d9-b283-73d07b206ce6 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:25,323787829-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:25,327757468-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8f50ebc8-b1f7-49d9-b283-73d07b206ce6', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8f50ebc8-b1f7-49d9-b283-73d07b206ce6 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:28,871829049-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:28,875945564-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8f50ebc8-b1f7-49d9-b283-73d07b206ce6', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8f50ebc8-b1f7-49d9-b283-73d07b206ce6 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  161 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  161 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  0.95   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.02   70      up          osd.2  
                       TOTAL  300 GiB  162 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.02  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:32,479339462-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:07:55,925588859-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:08:04,580866200-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:08:13,120388438-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:08:21,550170856-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:08:30,075737844-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:08:38,622445114-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     1.042% pgs not active
             190 active+clean
             2   peering
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:08:38,631123951-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:08:47,276545756-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:08:47,285581995-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:08:55,855189366-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:08:55,863670181-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:09:04,397997110-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:09:04,406735770-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:09:12,929089232-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:09:12,937325197-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:09:12,943859836-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:09:12,947575167-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:09:12,956522589-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=160507
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:09:12,962863513-04:00] INFO: > Run rados bench[0m
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.2
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:09:12,982129274-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:09:12,985433432-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8f50ebc8-b1f7-49d9-b283-73d07b206ce6', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8f50ebc8-b1f7-49d9-b283-73d07b206ce6 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:09:15.389+0000 7f65ea19fd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:09:15.649+0000 7f65ea19fd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:09:15.649+0000 7f65ea19fd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:09:15.663382+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-16T19:09:15.663393+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T19:09:15.678086+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:09:15.678086+0000     0       0         0         0         0         0           -           0
2021-05-16T19:09:16.678189+0000     1     238       238         0         0         0           -           0
2021-05-16T19:09:17.678281+0000     2     255       337        82   10.2493     10.25     1.30082      1.2949
2021-05-16T19:09:18.678386+0000     3     255       337        82   6.83278         0           -      1.2949
2021-05-16T19:09:19.678477+0000     4     255       656       401   25.0604    39.875   0.0503563     2.08013
2021-05-16T19:09:20.678545+0000     5     255       656       401   20.0484         0           -     2.08013
2021-05-16T19:09:21.678611+0000     6     255       888       633    26.373        29     2.18239      2.0825
2021-05-16T19:09:22.678716+0000     7     255       888       633   22.6053         0           -      2.0825
2021-05-16T19:09:23.678801+0000     8     255      1019       764    23.873    16.375     2.12145     2.21528
2021-05-16T19:09:24.678973+0000     9     255      1019       764   21.2203         0           -     2.21528
2021-05-16T19:09:25.679190+0000    10     255      1214       959   23.9725    24.375   0.0459907     2.34584
2021-05-16T19:09:26.679296+0000    11     255      1214       959   21.7932         0           -     2.34584
2021-05-16T19:09:27.679395+0000    12     255      1214       959   19.9771         0           -     2.34584
2021-05-16T19:09:28.679463+0000    13     255      1464      1209   23.2476   20.8333   0.0311578     2.49281
2021-05-16T19:09:29.679560+0000    14     255      1464      1209   21.5871         0           -     2.49281
2021-05-16T19:09:30.679830+0000    15     255      1474      1219   20.3144      1.25     2.10973     2.49846
2021-05-16T19:09:31.679927+0000    16     255      1474      1219   19.0448         0           -     2.49846
2021-05-16T19:09:32.679993+0000    17     255      1737      1482   21.7917    32.875   0.0514129     2.67492
2021-05-16T19:09:33.680074+0000    18     255      1737      1482   20.5811         0           -     2.67492
2021-05-16T19:09:34.680180+0000    19     255      1966      1711   22.5107    28.625     2.13823     2.71935
2021-05-16T19:09:35.680248+0000 min lat: 0.0262736 max lat: 8.50937 avg lat: 2.71935
2021-05-16T19:09:35.680248+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:09:35.680248+0000    20     255      1966      1711   21.3853         0           -     2.71935
2021-05-16T19:09:36.680328+0000    21      73      1967      1894   22.5453    22.875     2.10307     2.69182
2021-05-16T19:09:37.680396+0000    22      73      1967      1894   21.5205         0           -     2.69182
2021-05-16T19:09:38.680560+0000 Total time run:         22.7639
Total writes made:      1967
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     21.6022
Stddev Bandwidth:       13.6781
Max bandwidth (MB/sec): 39.875
Min bandwidth (MB/sec): 0
Average IOPS:           86
Stddev IOPS:            54.5204
Max IOPS:               159
Min IOPS:               0
Average Latency(s):     2.75015
Stddev Latency(s):      1.50616
Max latency(s):         8.50937
Min latency(s):         0.0262736

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:09:39,282997443-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 160507


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:09:39,287488402-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:02,697355848-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.97k objects, 492 MiB
    usage:   983 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:02,705688094-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:11,193208839-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.97k objects, 492 MiB
    usage:   983 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:11,201355065-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:19,784205835-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.97k objects, 492 MiB
    usage:   983 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:19,792553621-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:28,288560782-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.97k objects, 492 MiB
    usage:   983 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:28,297161122-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:36,712815471-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.97k objects, 492 MiB
    usage:   983 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:36,721380826-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:36,727793024-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:36,731872169-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:36,740950858-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=161897
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:36,747743130-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:36,767118737-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:10:36,770716768-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8f50ebc8-b1f7-49d9-b283-73d07b206ce6', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8f50ebc8-b1f7-49d9-b283-73d07b206ce6 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.go30Uo:/tmp/ceph-asok.go30Uo -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:10:39.177+0000 7ff3de2f9d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:10:39.453+0000 7ff3de2f9d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:10:39.453+0000 7ff3de2f9d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:10:39.469826+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:10:39.469826+0000     0       0         0         0         0         0           -           0
2021-05-16T19:10:40.469908+0000     1     256       354        98   24.4963      24.5  0.00837487  0.00574425
2021-05-16T19:10:41.470034+0000     2     256       569       313   39.1196     53.75 0.000507454    0.518539
2021-05-16T19:10:42.470111+0000     3     256       569       313   26.0803         0           -    0.518539
2021-05-16T19:10:43.470214+0000     4     256       682       426    26.622    14.125 0.000449615    0.773598
2021-05-16T19:10:44.470342+0000     5     256       682       426   21.2975         0           -    0.773598
2021-05-16T19:10:45.470442+0000     6     256       729       473   19.7061     5.875     5.48263     1.01664
2021-05-16T19:10:46.470540+0000     7     256       729       473    16.891         0           -     1.01664
2021-05-16T19:10:47.470611+0000     8     256       783       527    16.467      6.75 0.000687081     1.06724
2021-05-16T19:10:48.470716+0000     9     256       783       527   14.6373         0           -     1.06724
2021-05-16T19:10:49.470829+0000    10     256       906       650   16.2483    15.375 0.000650422     1.28951
2021-05-16T19:10:50.470990+0000    11     256       906       650   14.7711         0           -     1.28951
2021-05-16T19:10:51.471161+0000    12     256       985       729   15.1857     9.875     10.7423     2.11406
2021-05-16T19:10:52.471285+0000    13     256       985       729   14.0176         0           -     2.11406
2021-05-16T19:10:53.471393+0000    14     256       985       729   13.0163         0           -     2.11406
2021-05-16T19:10:54.471576+0000    15     256      1066       810   13.4984      6.75 0.000439125     2.00338
2021-05-16T19:10:55.471648+0000    16     256      1066       810   12.6548         0           -     2.00338
2021-05-16T19:10:56.471864+0000    17     256      1187       931   13.6895    15.125 0.000465054      2.2997
2021-05-16T19:10:57.471968+0000    18     256      1187       931    12.929         0           -      2.2997
2021-05-16T19:10:58.472072+0000    19     256      1472      1216   15.9981    35.625     4.31075     3.77274
2021-05-16T19:10:59.472159+0000 min lat: 0.000363603 max lat: 18.3616 avg lat: 3.77274
2021-05-16T19:10:59.472159+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:10:59.472159+0000    20     256      1472      1216   15.1982         0           -     3.77274
2021-05-16T19:11:00.472276+0000    21     256      1507      1251   14.8911     4.375 0.000537219     3.70122
2021-05-16T19:11:01.472386+0000    22     256      1507      1251   14.2142         0           -     3.70122
2021-05-16T19:11:02.472488+0000    23     256      1528      1272   13.8244     2.625     4.29351     3.72788
2021-05-16T19:11:03.472560+0000    24     256      1528      1272   13.2485         0           -     3.72788
2021-05-16T19:11:04.472663+0000    25     256      1596      1340   13.3984       8.5  0.00140553     3.81429
2021-05-16T19:11:05.472764+0000    26     256      1596      1340   12.8831         0           -     3.81429
2021-05-16T19:11:06.472973+0000    27     256      1631      1375     12.73     4.375     8.59163     3.92173
2021-05-16T19:11:07.473191+0000    28     256      1631      1375   12.2753         0           -     3.92173
2021-05-16T19:11:08.473297+0000    29     256      1631      1375    11.852         0           -     3.92173
2021-05-16T19:11:09.473535+0000    30     256      1666      1410   11.7485   2.91667 0.000478629     3.96138
2021-05-16T19:11:10.473787+0000    31     256      1666      1410   11.3695         0           -     3.96138
2021-05-16T19:11:11.473881+0000    32     256      1916      1660   12.9671     31.25 0.000661322     4.25899
2021-05-16T19:11:12.473976+0000    33     256      1916      1660   12.5742         0           -     4.25899
2021-05-16T19:11:13.474042+0000    34     256      1943      1687   12.4028     3.375     15.0178     4.40451
2021-05-16T19:11:14.474148+0000    35     256      1943      1687   12.0485         0           -     4.40451
2021-05-16T19:11:15.474220+0000    36     256      1962      1706   11.8458     2.375 0.000684666     4.41205
2021-05-16T19:11:16.474316+0000    37     256      1962      1706   11.5256         0           -     4.41205
2021-05-16T19:11:17.474383+0000    38     199      1967      1768   11.6302      7.75     12.8892     4.70897
2021-05-16T19:11:18.474452+0000    39     199      1967      1768    11.332         0           -     4.70897
2021-05-16T19:11:19.474526+0000 min lat: 0.000363603 max lat: 19.3102 avg lat: 4.80821
2021-05-16T19:11:19.474526+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:11:19.474526+0000    40     157      1967      1810   11.3112      5.25     6.44479     4.80821
2021-05-16T19:11:20.474606+0000    41     157      1967      1810   11.0353         0           -     4.80821
2021-05-16T19:11:21.474674+0000    42     145      1967      1822    10.844       1.5     4.29532     4.82368
2021-05-16T19:11:22.474864+0000    43     145      1967      1822   10.5918         0           -     4.82368
2021-05-16T19:11:23.475033+0000    44     145      1967      1822    10.351         0           -     4.82368
2021-05-16T19:11:24.475206+0000    45      40      1967      1927   10.7043      8.75      12.877     5.33862
2021-05-16T19:11:25.475273+0000    46      40      1967      1927   10.4716         0           -     5.33862
2021-05-16T19:11:26.475377+0000    47       2      1967      1965   10.4509      4.75     10.7386     5.49206
2021-05-16T19:11:27.475566+0000    48       2      1967      1965   10.2331         0           -     5.49206
2021-05-16T19:11:28.475841+0000 Total time run:       48.4281
Total reads made:     1967
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   10.1542
Average IOPS:         40
Stddev IOPS:          42.7229
Max IOPS:             215
Min IOPS:             0
Average Latency(s):   5.49849
Max latency(s):       19.3248
Min latency(s):       0.000363603

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:11:29,067643835-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 161897


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:11:29,072791979-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:11:52,509120392-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.97k objects, 492 MiB
    usage:   983 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:11:52,518359954-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:01,128809657-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.97k objects, 492 MiB
    usage:   983 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:01,137575208-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:09,672337586-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.97k objects, 492 MiB
    usage:   983 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:09,680784408-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:18,275162995-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.97k objects, 492 MiB
    usage:   983 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:18,283761622-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:26,848546082-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.97k objects, 492 MiB
    usage:   983 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:26,857444131-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:26,864305314-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T15:12:26,866865234-04:00][RUNNING][ROUND 3/4/40] object_size=256KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:26,870726459-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:26,880316018-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40231\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.157471\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fe2e7f3d-6c76-466b-b6cd-8032fe270447\nsetting min_mon_release = octopus\nepoch 0\nfsid fe2e7f3d-6c76-466b-b6cd-8032fe270447\nlast_changed 2021-05-16T12:12:36.874250-0700\ncreated 2021-05-16T12:12:36.874250-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40231/0,v1:10.10.1.2:40232/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.157471 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 f1eb3352-ab4d-4466-b09c-850e9885823f\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 fcc69aa9-3fee-4d87-b1e2-ba26aeba348c\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 0a2f8eda-ebaa-4f34-9f66-2584406269cc\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42231\n  w/ user/pass: admin / e87e17c1-4b9c-4035-a9f5-fe5b167d9031\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:12:54 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40231
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.157471
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fe2e7f3d-6c76-466b-b6cd-8032fe270447
setting min_mon_release = octopus
epoch 0
fsid fe2e7f3d-6c76-466b-b6cd-8032fe270447
last_changed 2021-05-16T12:12:36.874250-0700
created 2021-05-16T12:12:36.874250-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40231/0,v1:10.10.1.2:40232/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.157471 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 f1eb3352-ab4d-4466-b09c-850e9885823f
0
start osd.0
add osd1 fcc69aa9-3fee-4d87-b1e2-ba26aeba348c
1
start osd.1
add osd2 0a2f8eda-ebaa-4f34-9f66-2584406269cc
2
start osd.2


restful urls: https://10.10.1.2:42231
  w/ user/pass: admin / e87e17c1-4b9c-4035-a9f5-fe5b167d9031


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T12:12:28.737-0700 7f413d02f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:12:28.737-0700 7f413d02f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:12:28.757-0700 7fbeedc081c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:12:28.757-0700 7fbeedc081c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40231,v1:10.10.1.2:40232] --print /tmp/ceph_monmap.157471 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.157471 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.157471 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42231 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55ee8c23e000 @  0x7f3df81bf680 0x7f3df81e0824 0x7f3df897b187 0x7f3df8983355 0x7f3df897b708 0x7f3df897b877 0x7f3df897cc24 0x7f3df8994ec1 0x7f3df89075f3 0x7f3df8968e97 0x7f3df8970b1a 0x7f3df8073d84 0x7f3df818f609 0x7f3df7d63293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.S3eZPsLEgF 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f1eb3352-ab4d-4466-b09c-850e9885823f -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCvbqFgUX3JIhAAeSxVjgDshovtcMhyVABbpA== --osd-uuid f1eb3352-ab4d-4466-b09c-850e9885823f 
2021-05-16T12:12:48.225-0700 7f47162edf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:12:48.245-0700 7f47162edf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:12:48.245-0700 7f47162edf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x563b88296000 @  0x7f4716cb6680 0x7f4716cd7824 0x563b7d054447 0x563b7d05c4b5 0x563b7d0549c8 0x563b7d054b37 0x563b7d055ee4 0x563b7ce26ca1 0x563b7cffe423 0x563b7ce172a7 0x563b7ce1c53a 0x7f4716809d84 0x7f471698e609 0x7f47164f7293
2021-05-16T12:12:48.561-0700 7f47162edf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new fcc69aa9-3fee-4d87-b1e2-ba26aeba348c -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55ef8861a000 @  0x7fefc7818680 0x7fefc7839824 0x55ef7d8d3447 0x55ef7d8db4b5 0x55ef7d8d39c8 0x55ef7d8d3b37 0x55ef7d8d4ee4 0x55ef7d6a5ca1 0x55ef7d87d423 0x55ef7d6962a7 0x55ef7d69b53a 0x7fefc736bd84 0x7fefc74f0609 0x7fefc7059293
2021-05-16T12:12:49.217-0700 7fefc6e4ff00 -1 Falling back to public interface
2021-05-16T12:12:49.477-0700 7fefc6e4ff00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCwbqFgHroVNhAAF8KVg4GMiAjNvbS+DBx7qw== --osd-uuid fcc69aa9-3fee-4d87-b1e2-ba26aeba348c 
2021-05-16T12:12:49.581-0700 7efdb3ddbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:12:49.601-0700 7efdb3ddbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:12:49.601-0700 7efdb3ddbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5593b1dc4000 @  0x7efdb47a4680 0x7efdb47c5824 0x5593a61f1447 0x5593a61f94b5 0x5593a61f19c8 0x5593a61f1b37 0x5593a61f2ee4 0x5593a5fc3ca1 0x5593a619b423 0x5593a5fb42a7 0x5593a5fb953a 0x7efdb42f7d84 0x7efdb447c609 0x7efdb3fe5293
2021-05-16T12:12:49.925-0700 7efdb3ddbf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0a2f8eda-ebaa-4f34-9f66-2584406269cc -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55f487974000 @  0x7f11cd930680 0x7f11cd951824 0x55f47d18d447 0x55f47d1954b5 0x55f47d18d9c8 0x55f47d18db37 0x55f47d18eee4 0x55f47cf5fca1 0x55f47d137423 0x55f47cf502a7 0x55f47cf5553a 0x7f11cd483d84 0x7f11cd608609 0x7f11cd171293
2021-05-16T12:12:50.581-0700 7f11ccf67f00 -1 Falling back to public interface
2021-05-16T12:12:50.845-0700 7f11ccf67f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCybqFgzPE6EBAAZJrxlveIOA4kk5l6OnLhOg== --osd-uuid 0a2f8eda-ebaa-4f34-9f66-2584406269cc 
2021-05-16T12:12:50.937-0700 7f8dffdb3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:12:50.957-0700 7f8dffdb3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:12:50.957-0700 7f8dffdb3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55dc1b0b2000 @  0x7f8e0077c680 0x7f8e0079d824 0x55dc0f924447 0x55dc0f92c4b5 0x55dc0f9249c8 0x55dc0f924b37 0x55dc0f925ee4 0x55dc0f6f6ca1 0x55dc0f8ce423 0x55dc0f6e72a7 0x55dc0f6ec53a 0x7f8e002cfd84 0x7f8e00454609 0x7f8dfffbd293
2021-05-16T12:12:51.281-0700 7f8dffdb3f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55c8268a0000 @  0x7ff61098d680 0x7ff6109ae824 0x55c81aac4447 0x55c81aacc4b5 0x55c81aac49c8 0x55c81aac4b37 0x55c81aac5ee4 0x55c81a896ca1 0x55c81aa6e423 0x55c81a8872a7 0x55c81a88c53a 0x7ff6104e0d84 0x7ff610665609 0x7ff6101ce293
2021-05-16T12:12:51.961-0700 7ff60ffc4f00 -1 Falling back to public interface
2021-05-16T12:12:52.237-0700 7ff60ffc4f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:54,502654640-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:54,513711526-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:54,554915097-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:54,558627383-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af0e0302-e73b-472c-9774-f09e969406e4', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af0e0302-e73b-472c-9774-f09e969406e4 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:58,143900850-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:12:58,148135046-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af0e0302-e73b-472c-9774-f09e969406e4', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af0e0302-e73b-472c-9774-f09e969406e4 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:01,881481502-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:01,885560416-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af0e0302-e73b-472c-9774-f09e969406e4', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af0e0302-e73b-472c-9774-f09e969406e4 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:05,365836053-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:05,369217536-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af0e0302-e73b-472c-9774-f09e969406e4', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af0e0302-e73b-472c-9774-f09e969406e4 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:12,551564233-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:12,555517932-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af0e0302-e73b-472c-9774-f09e969406e4', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af0e0302-e73b-472c-9774-f09e969406e4 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:17,267539977-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:17,271258495-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af0e0302-e73b-472c-9774-f09e969406e4', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af0e0302-e73b-472c-9774-f09e969406e4 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:21,866600017-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:21,870181767-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af0e0302-e73b-472c-9774-f09e969406e4', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af0e0302-e73b-472c-9774-f09e969406e4 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:25,878984285-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:25,882805415-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af0e0302-e73b-472c-9774-f09e969406e4', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af0e0302-e73b-472c-9774-f09e969406e4 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:29,707465910-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:29,711338116-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af0e0302-e73b-472c-9774-f09e969406e4', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af0e0302-e73b-472c-9774-f09e969406e4 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:34,020531828-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:34,024057292-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af0e0302-e73b-472c-9774-f09e969406e4', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af0e0302-e73b-472c-9774-f09e969406e4 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:37,679290625-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:37,682733885-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af0e0302-e73b-472c-9774-f09e969406e4', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af0e0302-e73b-472c-9774-f09e969406e4 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:41,341788919-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:41,345293023-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af0e0302-e73b-472c-9774-f09e969406e4', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af0e0302-e73b-472c-9774-f09e969406e4 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:13:44,880771396-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:14:08,318740053-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:14:16,807990010-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:14:25,388233154-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:14:33,856504634-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:14:42,388515260-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:14:50,892279782-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   238 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:14:50,901505327-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:14:59,439043180-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:14:59,448209353-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:15:07,892403944-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:15:07,901583863-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:15:16,415008768-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:15:16,424517335-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:15:24,878658577-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:15:24,888075371-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:15:24,894638503-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:15:24,898420670-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:15:24,908199685-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=168350
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:15:24,915579511-04:00] INFO: > Run rados bench[0m
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.3
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:15:24,936187884-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:15:24,939682761-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af0e0302-e73b-472c-9774-f09e969406e4', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af0e0302-e73b-472c-9774-f09e969406e4 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:15:27.266+0000 7f58e4862d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:15:27.518+0000 7f58e4862d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:15:27.518+0000 7f58e4862d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:15:27.532089+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-16T19:15:27.532100+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T19:15:27.546819+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:15:27.546819+0000     0       0         0         0         0         0           -           0
2021-05-16T19:15:28.546956+0000     1     211       211         0         0         0           -           0
2021-05-16T19:15:29.547022+0000     2     219       219         0         0         0           -           0
2021-05-16T19:15:30.547251+0000     3     255       463       208   17.3312   17.3333   0.0461994     1.50954
2021-05-16T19:15:31.547337+0000     4     255       463       208   12.9985         0           -     1.50954
2021-05-16T19:15:32.547554+0000     5     255       688       433   21.6471    28.125     2.15423      2.0913
2021-05-16T19:15:33.547635+0000     6     255       688       433   18.0394         0           -      2.0913
2021-05-16T19:15:34.547702+0000     7     255       822       567   20.2476     16.75     2.14205     2.23187
2021-05-16T19:15:35.547788+0000     8     255       822       567   17.7168         0           -     2.23187
2021-05-16T19:15:36.547853+0000     9     255       940       685   19.0257     14.75     2.15705     2.42315
2021-05-16T19:15:37.547920+0000    10     255       940       685   17.1232         0           -     2.42315
2021-05-16T19:15:38.547985+0000    11     255      1259      1004   22.8159    39.875   0.0436131     2.62313
2021-05-16T19:15:39.548071+0000    12     255      1259      1004   20.9146         0           -     2.62313
2021-05-16T19:15:40.548137+0000    13     255      1534      1279   24.5938    34.375   0.0462565     2.41244
2021-05-16T19:15:41.548203+0000    14     255      1534      1279   22.8371         0           -     2.41244
2021-05-16T19:15:42.548273+0000    15     255      1928      1673   27.8808     49.25   0.0416063     2.23327
2021-05-16T19:15:43.548372+0000    16     255      1928      1673   26.1382         0           -     2.23327
2021-05-16T19:15:44.548442+0000    17     255      1928      1673   24.6007         0           -     2.23327
2021-05-16T19:15:45.548510+0000    18     255      2136      1881   26.1226   17.3333      2.1393     2.22075
2021-05-16T19:15:46.548578+0000    19     255      2136      1881   24.7478         0           -     2.22075
2021-05-16T19:15:47.548662+0000 min lat: 0.0267752 max lat: 6.44216 avg lat: 2.25802
2021-05-16T19:15:47.548662+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:15:47.548662+0000    20     255      2226      1971   24.6353     11.25     4.24517     2.25802
2021-05-16T19:15:48.548741+0000    21     255      2226      1971   23.4622         0           -     2.25802
2021-05-16T19:15:49.548951+0000    22      56      2227      2171   24.6681        25     2.14179     2.38924
2021-05-16T19:15:50.549018+0000    23      56      2227      2171   23.5956         0           -     2.38924
2021-05-16T19:15:51.549248+0000    24       2      2227      2225   23.1748      6.75     4.28922     2.46293
2021-05-16T19:15:52.549315+0000    25       2      2227      2225   22.2478         0           -     2.46293
2021-05-16T19:15:53.549446+0000 Total time run:         25.6063
Total writes made:      2227
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     21.7427
Stddev Bandwidth:       14.6684
Max bandwidth (MB/sec): 49.25
Min bandwidth (MB/sec): 0
Average IOPS:           86
Stddev IOPS:            58.5597
Max IOPS:               197
Min IOPS:               0
Average Latency(s):     2.46648
Stddev Latency(s):      1.50874
Max latency(s):         6.44216
Min latency(s):         0.0267752

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:15:54,050046602-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 168350


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:15:54,054741012-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:17,610371154-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.23k objects, 557 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:17,618203270-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:26,106274570-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.23k objects, 557 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:26,115317612-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:34,678737121-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.23k objects, 557 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:34,687055701-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:43,309776598-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.23k objects, 557 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:43,318834258-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:51,980028096-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.23k objects, 557 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:51,988299798-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:51,995145631-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:51,998546752-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:52,007164325-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=169744
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:52,014024815-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:52,033773463-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:16:52,037430585-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af0e0302-e73b-472c-9774-f09e969406e4', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af0e0302-e73b-472c-9774-f09e969406e4 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sIhdlx:/tmp/ceph-asok.sIhdlx -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:16:54.479+0000 7fae1e9bbd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:16:54.731+0000 7fae1e9bbd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:16:54.731+0000 7fae1e9bbd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:16:54.748040+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:16:54.748040+0000     0       0         0         0         0         0           -           0
2021-05-16T19:16:55.748141+0000     1     256       372       116    28.995        29   0.0122692   0.0068819
2021-05-16T19:16:56.748210+0000     2     256       441       185   23.1222     17.25      1.7225    0.645663
2021-05-16T19:16:57.748278+0000     3     256       441       185   15.4151         0           -    0.645663
2021-05-16T19:16:58.748382+0000     4     256       579       323   20.1854     17.25  0.00131627     1.42445
2021-05-16T19:16:59.748450+0000     5     256       579       323   16.1484         0           -     1.42445
2021-05-16T19:17:00.748535+0000     6     256       579       323   13.4571         0           -     1.42445
2021-05-16T19:17:01.748619+0000     7     256       618       362   12.9274      3.25 0.000512513     1.66959
2021-05-16T19:17:02.748722+0000     8     256       618       362   11.3114         0           -     1.66959
2021-05-16T19:17:03.748787+0000     9     256       688       432   11.9989      8.75 0.000470444       2.249
2021-05-16T19:17:04.749031+0000    10     256       688       432   10.7989         0           -       2.249
2021-05-16T19:17:05.749196+0000    11     256       710       454    10.317      2.75 0.000832484      2.5937
2021-05-16T19:17:06.749299+0000    12     256       710       454   9.45728         0           -      2.5937
2021-05-16T19:17:07.749367+0000    13     256       774       518   9.96047         8     8.59103     3.61107
2021-05-16T19:17:08.749514+0000    14     256       774       518   9.24898         0           -     3.61107
2021-05-16T19:17:09.749629+0000    15     256       829       573   9.54894     6.875      10.728     4.29845
2021-05-16T19:17:10.749852+0000    16     256       829       573   8.95207         0           -     4.29845
2021-05-16T19:17:11.749935+0000    17     256       884       628   9.23423     6.875 0.000485052     4.11688
2021-05-16T19:17:12.750002+0000    18     256       884       628   8.72124         0           -     4.11688
2021-05-16T19:17:13.750067+0000    19     256       940       684   8.99901         7     15.0314     4.80018
2021-05-16T19:17:14.750170+0000 min lat: 0.000402717 max lat: 17.1756 avg lat: 4.80018
2021-05-16T19:17:14.750170+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:17:14.750170+0000    20     256       940       684   8.54906         0           -     4.80018
2021-05-16T19:17:15.750264+0000    21     256       940       684   8.14197         0           -     4.80018
2021-05-16T19:17:16.750331+0000    22     256       962       706   8.02187   1.83333 0.000491363     4.68402
2021-05-16T19:17:17.750499+0000    23     256       962       706   7.67307         0           -     4.68402
2021-05-16T19:17:18.750604+0000    24     256      1167       911   9.48854    25.625  0.00111092     5.04053
2021-05-16T19:17:19.750673+0000    25     256      1167       911   9.10901         0           -     5.04053
2021-05-16T19:17:20.750758+0000    26     256      1250       994   9.55667    10.375 0.000556877     4.93035
2021-05-16T19:17:21.750983+0000    27     256      1250       994   9.20268         0           -     4.93035
2021-05-16T19:17:22.751085+0000    28     256      1621      1365   12.1861    46.375 0.000507744     5.14156
2021-05-16T19:17:23.751278+0000    29     256      1621      1365   11.7659         0           -     5.14156
2021-05-16T19:17:24.751510+0000    30     256      1640      1384    11.532     2.375 0.000950336     5.11283
2021-05-16T19:17:25.751596+0000    31     256      1640      1384     11.16         0           -     5.11283
2021-05-16T19:17:26.751699+0000    32     256      1767      1511   11.8033    15.875      2.1569      5.0363
2021-05-16T19:17:27.751858+0000    33     256      1767      1511   11.4456         0           -      5.0363
2021-05-16T19:17:28.751954+0000    34     256      1849      1593   11.7119     10.25 0.000675379     4.90108
2021-05-16T19:17:29.752022+0000    35     256      1849      1593   11.3773         0           -     4.90108
2021-05-16T19:17:30.752124+0000    36     256      1849      1593   11.0612         0           -     4.90108
2021-05-16T19:17:31.752209+0000    37     256      1879      1623    10.965       2.5 0.000515449     4.89248
2021-05-16T19:17:32.752316+0000    38     256      1879      1623   10.6764         0           -     4.89248
2021-05-16T19:17:33.752381+0000    39     256      1939      1683   10.7872       7.5     4.30065     5.00756
2021-05-16T19:17:34.752483+0000 min lat: 0.000402717 max lat: 21.4735 avg lat: 5.00756
2021-05-16T19:17:34.752483+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:17:34.752483+0000    40     256      1939      1683   10.5176         0           -     5.00756
2021-05-16T19:17:35.752580+0000    41     256      1966      1710   10.4257     3.375 0.000517212     4.95233
2021-05-16T19:17:36.752648+0000    42     256      1966      1710   10.1774         0           -     4.95233
2021-05-16T19:17:37.752722+0000    43     256      1980      1724   10.0221      1.75 0.000692571     4.98059
2021-05-16T19:17:38.752963+0000    44     256      1980      1724   9.79434         0           -     4.98059
2021-05-16T19:17:39.753041+0000    45     256      2078      1822   10.1211     12.25  0.00575492     5.08634
2021-05-16T19:17:40.753175+0000    46     256      2078      1822   9.90105         0           -     5.08634
2021-05-16T19:17:41.753261+0000    47     256      2123      1867   9.92973     5.625 0.000456819     4.99821
2021-05-16T19:17:42.753475+0000    48     256      2123      1867   9.72284         0           -     4.99821
2021-05-16T19:17:43.753599+0000    49     249      2227      1978   10.0907    13.875  0.00140098     5.18975
2021-05-16T19:17:44.753814+0000    50     249      2227      1978   9.88884         0           -     5.18975
2021-05-16T19:17:45.753898+0000    51     249      2227      1978   9.69495         0           -     5.18975
2021-05-16T19:17:46.753999+0000    52     225      2227      2002   9.62388         2     2.13886     5.17464
2021-05-16T19:17:47.754066+0000    53     225      2227      2002   9.44231         0           -     5.17464
2021-05-16T19:17:48.754151+0000    54      21      2227      2206   10.2118      25.5     4.30502       6.103
2021-05-16T19:17:49.754276+0000    55      21      2227      2206   10.0261         0           -       6.103
2021-05-16T19:17:50.754382+0000    56      11      2227      2216   9.89172      1.25     6.43578     6.10451
2021-05-16T19:17:51.754474+0000    57      11      2227      2216   9.71819         0           -     6.10451
2021-05-16T19:17:52.754650+0000 Total time run:       57.5514
Total reads made:     2227
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   9.67395
Average IOPS:         38
Stddev IOPS:          36.1562
Max IOPS:             185
Min IOPS:             0
Average Latency(s):   6.11674
Max latency(s):       25.7438
Min latency(s):       0.000402717

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:17:53,324593990-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 169744


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:17:53,329158917-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:18:16,886739381-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.23k objects, 557 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:18:16,895568892-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:18:25,409832383-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.23k objects, 557 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:18:25,418718890-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:18:34,016396567-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.23k objects, 557 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:18:34,025959415-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:18:42,420024765-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.23k objects, 557 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:18:42,429058710-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:18:51,019412776-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.23k objects, 557 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:18:51,028278054-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:18:51,035248822-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T15:18:51,038002296-04:00][RUNNING][ROUND 4/4/40] object_size=256KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:18:51,042025155-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:18:51,050336242-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40825\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.158561\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 79fb948d-5347-42d6-8d0b-3d2c61c4a7f8\nsetting min_mon_release = octopus\nepoch 0\nfsid 79fb948d-5347-42d6-8d0b-3d2c61c4a7f8\nlast_changed 2021-05-16T12:19:01.056117-0700\ncreated 2021-05-16T12:19:01.056117-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40825/0,v1:10.10.1.2:40826/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.158561 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 af5ed1e0-34d3-4d1a-9955-43ed474254d6\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 8fceece3-5e3b-465c-9097-ff72632eefa5\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 62349b21-8cc0-4a6c-90f6-9b53cd2d967c\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42825\n  w/ user/pass: admin / deec21b2-d4a9-4f93-8ca2-e0196ae4eea2\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 15:19:19 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40825
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.158561
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 79fb948d-5347-42d6-8d0b-3d2c61c4a7f8
setting min_mon_release = octopus
epoch 0
fsid 79fb948d-5347-42d6-8d0b-3d2c61c4a7f8
last_changed 2021-05-16T12:19:01.056117-0700
created 2021-05-16T12:19:01.056117-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40825/0,v1:10.10.1.2:40826/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.158561 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 af5ed1e0-34d3-4d1a-9955-43ed474254d6
0
start osd.0
add osd1 8fceece3-5e3b-465c-9097-ff72632eefa5
1
start osd.1
add osd2 62349b21-8cc0-4a6c-90f6-9b53cd2d967c
2
start osd.2


restful urls: https://10.10.1.2:42825
  w/ user/pass: admin / deec21b2-d4a9-4f93-8ca2-e0196ae4eea2


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T12:18:52.905-0700 7f64467911c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:18:52.905-0700 7f64467911c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:18:52.925-0700 7f07a1c1f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:18:52.925-0700 7f07a1c1f1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40825,v1:10.10.1.2:40826] --print /tmp/ceph_monmap.158561 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.158561 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.158561 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42825 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x564683192000 @  0x7fed67c7d680 0x7fed67c9e824 0x7fed68439187 0x7fed68441355 0x7fed68439708 0x7fed68439877 0x7fed6843ac24 0x7fed68452ec1 0x7fed683c55f3 0x7fed68426e97 0x7fed6842eb1a 0x7fed67b31d84 0x7fed67c4d609 0x7fed67821293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.yT1A3Ul3jV 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new af5ed1e0-34d3-4d1a-9955-43ed474254d6 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAwcKFgYGy6IhAA10K9tL72av1RP5mzk65O9A== --osd-uuid af5ed1e0-34d3-4d1a-9955-43ed474254d6 
2021-05-16T12:19:13.225-0700 7faa26563f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:19:13.245-0700 7faa26563f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:19:13.245-0700 7faa26563f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x562523e1a000 @  0x7faa26f2c680 0x7faa26f4d824 0x562518948447 0x5625189504b5 0x5625189489c8 0x562518948b37 0x562518949ee4 0x56251871aca1 0x5625188f2423 0x56251870b2a7 0x56251871053a 0x7faa26a7fd84 0x7faa26c04609 0x7faa2676d293
2021-05-16T12:19:13.561-0700 7faa26563f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8fceece3-5e3b-465c-9097-ff72632eefa5 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x557bf3790000 @  0x7f19441ec680 0x7f194420d824 0x557be7943447 0x557be794b4b5 0x557be79439c8 0x557be7943b37 0x557be7944ee4 0x557be7715ca1 0x557be78ed423 0x557be77062a7 0x557be770b53a 0x7f1943d3fd84 0x7f1943ec4609 0x7f1943a2d293
2021-05-16T12:19:14.189-0700 7f1943823f00 -1 Falling back to public interface
2021-05-16T12:19:14.449-0700 7f1943823f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAxcKFg29LoNBAA7AoOkWhykv4Fan0bRG3ang== --osd-uuid 8fceece3-5e3b-465c-9097-ff72632eefa5 
2021-05-16T12:19:14.549-0700 7fd1eee18f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:19:14.573-0700 7fd1eee18f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:19:14.573-0700 7fd1eee18f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x563c8e90a000 @  0x7fd1ef7e1680 0x7fd1ef802824 0x563c84018447 0x563c840204b5 0x563c840189c8 0x563c84018b37 0x563c84019ee4 0x563c83deaca1 0x563c83fc2423 0x563c83ddb2a7 0x563c83de053a 0x7fd1ef334d84 0x7fd1ef4b9609 0x7fd1ef022293
2021-05-16T12:19:14.881-0700 7fd1eee18f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 62349b21-8cc0-4a6c-90f6-9b53cd2d967c -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x56546715a000 @  0x7ff1de110680 0x7ff1de131824 0x56545b4d9447 0x56545b4e14b5 0x56545b4d99c8 0x56545b4d9b37 0x56545b4daee4 0x56545b2abca1 0x56545b483423 0x56545b29c2a7 0x56545b2a153a 0x7ff1ddc63d84 0x7ff1ddde8609 0x7ff1dd951293
2021-05-16T12:19:15.517-0700 7ff1dd747f00 -1 Falling back to public interface
2021-05-16T12:19:15.777-0700 7ff1dd747f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAzcKFgNvCDDBAA8tiJXl7IXItJyQ42jf8ukg== --osd-uuid 62349b21-8cc0-4a6c-90f6-9b53cd2d967c 
2021-05-16T12:19:15.905-0700 7fea016c8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:19:15.929-0700 7fea016c8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:19:15.929-0700 7fea016c8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5600abdbe000 @  0x7fea02091680 0x7fea020b2824 0x5600a0915447 0x5600a091d4b5 0x5600a09159c8 0x5600a0915b37 0x5600a0916ee4 0x5600a06e7ca1 0x5600a08bf423 0x5600a06d82a7 0x5600a06dd53a 0x7fea01be4d84 0x7fea01d69609 0x7fea018d2293
2021-05-16T12:19:16.253-0700 7fea016c8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x565496714000 @  0x7f584ab64680 0x7f584ab85824 0x56548c724447 0x56548c72c4b5 0x56548c7249c8 0x56548c724b37 0x56548c725ee4 0x56548c4f6ca1 0x56548c6ce423 0x56548c4e72a7 0x56548c4ec53a 0x7f584a6b7d84 0x7f584a83c609 0x7f584a3a5293
2021-05-16T12:19:16.909-0700 7f584a19bf00 -1 Falling back to public interface
2021-05-16T12:19:17.181-0700 7f584a19bf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:19,374846570-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:19,385756089-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:19,429904894-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:19,433717999-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dee8bd8f-9c77-4f44-8a2b-b90cc6073081', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dee8bd8f-9c77-4f44-8a2b-b90cc6073081 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:22,978877020-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:22,982906812-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dee8bd8f-9c77-4f44-8a2b-b90cc6073081', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dee8bd8f-9c77-4f44-8a2b-b90cc6073081 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:26,596757384-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:26,600801533-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dee8bd8f-9c77-4f44-8a2b-b90cc6073081', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dee8bd8f-9c77-4f44-8a2b-b90cc6073081 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:30,204059051-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:30,207571762-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dee8bd8f-9c77-4f44-8a2b-b90cc6073081', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dee8bd8f-9c77-4f44-8a2b-b90cc6073081 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:37,320023303-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:37,323956453-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dee8bd8f-9c77-4f44-8a2b-b90cc6073081', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dee8bd8f-9c77-4f44-8a2b-b90cc6073081 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:40,941889527-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:40,945684117-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dee8bd8f-9c77-4f44-8a2b-b90cc6073081', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dee8bd8f-9c77-4f44-8a2b-b90cc6073081 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:45,425814609-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:45,429800839-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dee8bd8f-9c77-4f44-8a2b-b90cc6073081', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dee8bd8f-9c77-4f44-8a2b-b90cc6073081 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:49,832423384-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:49,835911308-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dee8bd8f-9c77-4f44-8a2b-b90cc6073081', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dee8bd8f-9c77-4f44-8a2b-b90cc6073081 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:54,347719232-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:54,351274412-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dee8bd8f-9c77-4f44-8a2b-b90cc6073081', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dee8bd8f-9c77-4f44-8a2b-b90cc6073081 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:59,015313128-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:19:59,019410336-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dee8bd8f-9c77-4f44-8a2b-b90cc6073081', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dee8bd8f-9c77-4f44-8a2b-b90cc6073081 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:20:02,519546651-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:20:02,523207860-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dee8bd8f-9c77-4f44-8a2b-b90cc6073081', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dee8bd8f-9c77-4f44-8a2b-b90cc6073081 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:20:06,137521403-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:20:06,141099176-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dee8bd8f-9c77-4f44-8a2b-b90cc6073081', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dee8bd8f-9c77-4f44-8a2b-b90cc6073081 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:20:09,569565592-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:20:33,258707596-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:20:41,849912853-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:20:50,353958361-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:20:59,094785244-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:07,674358357-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:16,147660451-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:16,157173987-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:24,699685484-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:24,708970140-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:33,353681808-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:33,362553788-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:41,950857759-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:41,960105616-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:50,289724567-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:50,298887874-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:50,305825340-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:50,309777385-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:50,318566169-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=176198
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:50,325618821-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:50,345841118-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:21:50,349532243-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dee8bd8f-9c77-4f44-8a2b-b90cc6073081', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dee8bd8f-9c77-4f44-8a2b-b90cc6073081 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0 -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:21:52.700+0000 7f35beb8fd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:21:52.960+0000 7f35beb8fd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:21:52.960+0000 7f35beb8fd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:21:52.971237+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-16T19:21:52.971248+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T19:21:52.985768+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:21:52.985768+0000     0       0         0         0         0         0           -           0
2021-05-16T19:21:53.985953+0000     1     141       141         0         0         0           -           0
2021-05-16T19:21:54.986051+0000     2     141       141         0         0         0           -           0
2021-05-16T19:21:55.986118+0000     3     205       205         0         0         0           -           0
2021-05-16T19:21:56.986199+0000     4     205       205         0         0         0           -           0
2021-05-16T19:21:57.986266+0000     5     255       356       101   5.04954      5.05     4.21415     4.20552
2021-05-16T19:21:58.986364+0000     6     255       356       101   4.20795         0           -     4.20552
2021-05-16T19:21:59.986432+0000     7     255       495       240   8.57067    17.375     2.15704     3.77697
2021-05-16T19:22:00.986515+0000     8     255       495       240   7.49934         0           -     3.77697
2021-05-16T19:22:01.986581+0000     9     255       611       356   9.88804      14.5     8.50024     4.18439
2021-05-16T19:22:02.986850+0000    10     255       611       356   8.89908         0           -     4.18439
2021-05-16T19:22:03.986916+0000    11     255       765       510   11.5897     19.25      2.1637     4.18711
2021-05-16T19:22:04.987000+0000    12     255       765       510   10.6239         0           -     4.18711
2021-05-16T19:22:05.987197+0000    13     255       961       706   13.5755      24.5     4.33427     3.88873
2021-05-16T19:22:06.987298+0000    14     255       961       706   12.6058         0           -     3.88873
2021-05-16T19:22:07.987510+0000    15     255      1092       837   13.9484    16.375     2.13448     3.78577
2021-05-16T19:22:08.987591+0000    16     255      1092       837   13.0767         0           -     3.78577
2021-05-16T19:22:09.987804+0000    17     255      1092       837   12.3074         0           -     3.78577
2021-05-16T19:22:10.987902+0000    18     255      1163       908   12.6096   5.91667     2.16116     3.79514
2021-05-16T19:22:11.987971+0000    19     255      1163       908    11.946         0           -     3.79514
2021-05-16T19:22:12.988053+0000 min lat: 0.0304789 max lat: 10.6064 avg lat: 3.78352
2021-05-16T19:22:12.988053+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:22:12.988053+0000    20     255      1421      1166   14.5734     32.25   0.0376138     3.78352
2021-05-16T19:22:13.988139+0000    21     255      1421      1166   13.8794         0           -     3.78352
2021-05-16T19:22:14.988245+0000    22     105      1422      1317   14.9643    18.875     2.06868     3.72617
2021-05-16T19:22:15.988318+0000    23     105      1422      1317   14.3137         0           -     3.72617
2021-05-16T19:22:16.988420+0000    24      29      1422      1393   14.5088       9.5     4.23421     3.75462
2021-05-16T19:22:17.988498+0000    25      29      1422      1393   13.9285         0           -     3.75462
2021-05-16T19:22:18.988651+0000 Total time run:         25.654
Total writes made:      1422
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     13.8575
Stddev Bandwidth:       9.63434
Max bandwidth (MB/sec): 32.25
Min bandwidth (MB/sec): 0
Average IOPS:           55
Stddev IOPS:            38.4696
Max IOPS:               129
Min IOPS:               0
Average Latency(s):     3.80798
Stddev Latency(s):      1.81301
Max latency(s):         10.6064
Min latency(s):         0.0304789

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:22:19,495207352-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 176198


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:22:19,499919927-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:22:42,872044723-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.42k objects, 356 MiB
    usage:   711 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:22:42,880960385-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:22:51,415660782-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.42k objects, 356 MiB
    usage:   711 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:22:51,424943123-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:22:59,955847418-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.42k objects, 356 MiB
    usage:   711 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:22:59,964794890-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:23:08,452532477-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.42k objects, 356 MiB
    usage:   711 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:23:08,461370172-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:23:17,018914554-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.42k objects, 356 MiB
    usage:   711 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:23:17,028295561-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:23:17,035817785-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:23:17,040014641-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:23:17,049580976-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=177575
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:23:17,056206284-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:23:17,075262059-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:23:17,078751616-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dee8bd8f-9c77-4f44-8a2b-b90cc6073081', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dee8bd8f-9c77-4f44-8a2b-b90cc6073081 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3sAut0:/tmp/ceph-asok.3sAut0 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:23:19.516+0000 7f37336cad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:23:19.784+0000 7f37336cad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:23:19.784+0000 7f37336cad00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:23:19.803285+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:23:19.803285+0000     0       0         0         0         0         0           -           0
2021-05-16T19:23:20.803525+0000     1     256       283        27   6.74792      6.75   0.0039707  0.00316736
2021-05-16T19:23:21.803754+0000     2     256       283        27   3.37409         0           -  0.00316736
2021-05-16T19:23:22.803854+0000     3     256       451       195   16.2466        21 0.000444745     1.29677
2021-05-16T19:23:23.803922+0000     4     256       451       195   12.1854         0           -     1.29677
2021-05-16T19:23:24.803990+0000     5     256       567       311   15.5476      14.5      4.2926     2.39164
2021-05-16T19:23:25.804056+0000     6     256       567       311   12.9565         0           -     2.39164
2021-05-16T19:23:26.804156+0000     7     256       581       325   11.6056      1.75     6.42844     2.48653
2021-05-16T19:23:27.804223+0000     8     256       581       325    10.155         0           -     2.48653
2021-05-16T19:23:28.804289+0000     9     256       637       381   10.5821         7 0.000507764     2.50408
2021-05-16T19:23:29.804379+0000    10     256       637       381   9.52389         0           -     2.50408
2021-05-16T19:23:30.804481+0000    11     256       784       528   11.9986    18.375      2.1584     3.96664
2021-05-16T19:23:31.804548+0000    12     256       784       528   10.9988         0           -     3.96664
2021-05-16T19:23:32.804614+0000    13     256       820       564    10.845       4.5     2.14702     4.06741
2021-05-16T19:23:33.804682+0000    14     256       820       564   10.0704         0           -     4.06741
2021-05-16T19:23:34.804780+0000    15     256       820       564   9.39902         0           -     4.06741
2021-05-16T19:23:35.804994+0000    16     256       928       672   10.4988         9     4.30264     4.89319
2021-05-16T19:23:36.805208+0000    17     256       928       672    9.8812         0           -     4.89319
2021-05-16T19:23:37.805274+0000    18     256      1073       817   11.3459    18.125 0.000510449     4.51635
2021-05-16T19:23:38.805522+0000    19     256      1073       817   10.7487         0           -     4.51635
2021-05-16T19:23:39.805588+0000 min lat: 0.000444745 max lat: 12.8781 avg lat: 4.54298
2021-05-16T19:23:39.805588+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:23:39.805588+0000    20     256      1089       833   10.4113         2 0.000689716     4.54298
2021-05-16T19:23:40.805670+0000    21     256      1089       833   9.91551         0           -     4.54298
2021-05-16T19:23:41.805875+0000    22     256      1133       877    9.9647       5.5 0.000451048     4.60877
2021-05-16T19:23:42.805981+0000    23     256      1133       877   9.53146         0           -     4.60877
2021-05-16T19:23:43.806061+0000    24     256      1162       906   9.43638     3.625     8.59062      4.8119
2021-05-16T19:23:44.806281+0000    25     256      1162       906   9.05889         0           -      4.8119
2021-05-16T19:23:45.806348+0000    26     256      1185       929   8.93162     2.875 0.000549192     4.71819
2021-05-16T19:23:46.806579+0000    27     256      1185       929   8.60078         0           -     4.71819
2021-05-16T19:23:47.806790+0000    28     256      1361      1105   9.86481        22  0.00101806     5.06098
2021-05-16T19:23:48.806862+0000    29     256      1361      1105   9.52466         0           -     5.06098
2021-05-16T19:23:49.806949+0000    30     256      1361      1105   9.20719         0           -     5.06098
2021-05-16T19:23:50.807066+0000    31     256      1394      1138   9.17628      2.75 0.000662394     5.10664
2021-05-16T19:23:51.807183+0000    32     256      1394      1138   8.88952         0           -     5.10664
2021-05-16T19:23:52.807255+0000    33     187      1422      1235   9.35492    12.125  0.00613058     5.30714
2021-05-16T19:23:53.807323+0000    34     187      1422      1235   9.07979         0           -     5.30714
2021-05-16T19:23:54.807450+0000    35     139      1422      1283   9.16318         6     17.1799     5.72777
2021-05-16T19:23:55.807517+0000    36     139      1422      1283   8.90866         0           -     5.72777
2021-05-16T19:23:56.807581+0000    37      34      1422      1388   9.37727    13.125     8.58654     6.28618
2021-05-16T19:23:57.807649+0000    38      34      1422      1388   9.13051         0           -     6.28618
2021-05-16T19:23:58.807749+0000    39       5      1422      1417   9.08228     3.625     6.44198     6.34081
2021-05-16T19:23:59.807853+0000 min lat: 0.000409901 max lat: 21.4652 avg lat: 6.34081
2021-05-16T19:23:59.807853+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:23:59.807853+0000    40       5      1422      1417   8.85522         0           -     6.34081
2021-05-16T19:24:00.808009+0000 Total time run:       40.7903
Total reads made:     1422
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   8.7153
Average IOPS:         34
Stddev IOPS:          26.0912
Max IOPS:             88
Min IOPS:             0
Average Latency(s):   6.34871
Max latency(s):       21.4652
Min latency(s):       0.000409901

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:24:01,411393710-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 177575


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:24:01,416238163-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:24:24,987628316-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.42k objects, 356 MiB
    usage:   711 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:24:24,997059467-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:24:33,330729342-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.42k objects, 356 MiB
    usage:   711 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:24:33,339843728-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:24:41,843369897-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.42k objects, 356 MiB
    usage:   711 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:24:41,852698546-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:24:50,326735668-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.42k objects, 356 MiB
    usage:   711 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:24:50,336065979-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:24:58,906030115-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.42k objects, 356 MiB
    usage:   711 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:24:58,914952320-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:24:58,921639815-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T15:24:58,924368533-04:00][RUNNING][ROUND 5/4/40] object_size=256KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:24:58,928086188-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:24:58,936647404-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40717\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.159674\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 2a428941-f6c9-46ae-a78d-2bf051708a49\nsetting min_mon_release = octopus\nepoch 0\nfsid 2a428941-f6c9-46ae-a78d-2bf051708a49\nlast_changed 2021-05-16T12:25:08.846534-0700\ncreated 2021-05-16T12:25:08.846534-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40717/0,v1:10.10.1.2:40718/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.159674 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 19998d28-9b03-4ec1-9136-91e94edb29e8\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 b8757808-3178-4981-a82e-f6e5d2ffe7da\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 ff3abb36-6460-47b0-8744-58f74915ef13\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42717\n  w/ user/pass: admin / 683e8cf4-5b04-462b-ad1b-78218258d871\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 15:25:26 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40717
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.159674
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 2a428941-f6c9-46ae-a78d-2bf051708a49
setting min_mon_release = octopus
epoch 0
fsid 2a428941-f6c9-46ae-a78d-2bf051708a49
last_changed 2021-05-16T12:25:08.846534-0700
created 2021-05-16T12:25:08.846534-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40717/0,v1:10.10.1.2:40718/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.159674 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 19998d28-9b03-4ec1-9136-91e94edb29e8
0
start osd.0
add osd1 b8757808-3178-4981-a82e-f6e5d2ffe7da
1
start osd.1
add osd2 ff3abb36-6460-47b0-8744-58f74915ef13
2
start osd.2


restful urls: https://10.10.1.2:42717
  w/ user/pass: admin / 683e8cf4-5b04-462b-ad1b-78218258d871


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T12:25:00.764-0700 7f4b0b5b31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:25:00.764-0700 7f4b0b5b31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:25:00.780-0700 7f2ab568a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:25:00.780-0700 7f2ab568a1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40717,v1:10.10.1.2:40718] --print /tmp/ceph_monmap.159674 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.159674 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.159674 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42717 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5608c69cc000 @  0x7f6642137680 0x7f6642158824 0x7f66428f3187 0x7f66428fb355 0x7f66428f3708 0x7f66428f3877 0x7f66428f4c24 0x7f664290cec1 0x7f664287f5f3 0x7f66428e0e97 0x7f66428e8b1a 0x7f6641febd84 0x7f6642107609 0x7f6641cdb293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.a3GL0mDACE 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 19998d28-9b03-4ec1-9136-91e94edb29e8 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCfcaFgK0+KGhAAqqXuf2Cagg0E5BhbMPRXBQ== --osd-uuid 19998d28-9b03-4ec1-9136-91e94edb29e8 
2021-05-16T12:25:20.084-0700 7f2b6cbcdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:25:20.104-0700 7f2b6cbcdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:25:20.104-0700 7f2b6cbcdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56277cdba000 @  0x7f2b6d596680 0x7f2b6d5b7824 0x562772cee447 0x562772cf64b5 0x562772cee9c8 0x562772ceeb37 0x562772cefee4 0x562772ac0ca1 0x562772c98423 0x562772ab12a7 0x562772ab653a 0x7f2b6d0e9d84 0x7f2b6d26e609 0x7f2b6cdd7293
2021-05-16T12:25:20.488-0700 7f2b6cbcdf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b8757808-3178-4981-a82e-f6e5d2ffe7da -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55570f0ac000 @  0x7f871cb81680 0x7f871cba2824 0x555704720447 0x5557047284b5 0x5557047209c8 0x555704720b37 0x555704721ee4 0x5557044f2ca1 0x5557046ca423 0x5557044e32a7 0x5557044e853a 0x7f871c6d4d84 0x7f871c859609 0x7f871c3c2293
2021-05-16T12:25:21.140-0700 7f871c1b8f00 -1 Falling back to public interface
2021-05-16T12:25:21.400-0700 7f871c1b8f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCgcaFghHe6MRAAjq2NOrA9KrmTWIZ3laeR6Q== --osd-uuid b8757808-3178-4981-a82e-f6e5d2ffe7da 
2021-05-16T12:25:21.532-0700 7fa9e0c86f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:25:21.552-0700 7fa9e0c86f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:25:21.552-0700 7fa9e0c86f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x561bdebd8000 @  0x7fa9e164f680 0x7fa9e1670824 0x561bd2ead447 0x561bd2eb54b5 0x561bd2ead9c8 0x561bd2eadb37 0x561bd2eaeee4 0x561bd2c7fca1 0x561bd2e57423 0x561bd2c702a7 0x561bd2c7553a 0x7fa9e11a2d84 0x7fa9e1327609 0x7fa9e0e90293
2021-05-16T12:25:21.884-0700 7fa9e0c86f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ff3abb36-6460-47b0-8744-58f74915ef13 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55e3df838000 @  0x7faddda68680 0x7faddda89824 0x55e3d41ef447 0x55e3d41f74b5 0x55e3d41ef9c8 0x55e3d41efb37 0x55e3d41f0ee4 0x55e3d3fc1ca1 0x55e3d4199423 0x55e3d3fb22a7 0x55e3d3fb753a 0x7faddd5bbd84 0x7faddd740609 0x7faddd2a9293
2021-05-16T12:25:22.544-0700 7faddd09ff00 -1 Falling back to public interface
2021-05-16T12:25:22.800-0700 7faddd09ff00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCicaFg3Nc2DhAAxpXf8h0ARUkHF0I9xbkeCQ== --osd-uuid ff3abb36-6460-47b0-8744-58f74915ef13 
2021-05-16T12:25:22.904-0700 7f983d5f9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:25:22.924-0700 7f983d5f9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:25:22.924-0700 7f983d5f9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55559603a000 @  0x7f983dfc2680 0x7f983dfe3824 0x55558bc30447 0x55558bc384b5 0x55558bc309c8 0x55558bc30b37 0x55558bc31ee4 0x55558ba02ca1 0x55558bbda423 0x55558b9f32a7 0x55558b9f853a 0x7f983db15d84 0x7f983dc9a609 0x7f983d803293
2021-05-16T12:25:23.260-0700 7f983d5f9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5632eae38000 @  0x7f622af6b680 0x7f622af8c824 0x5632e0544447 0x5632e054c4b5 0x5632e05449c8 0x5632e0544b37 0x5632e0545ee4 0x5632e0316ca1 0x5632e04ee423 0x5632e03072a7 0x5632e030c53a 0x7f622aabed84 0x7f622ac43609 0x7f622a7ac293
2021-05-16T12:25:23.952-0700 7f622a5a2f00 -1 Falling back to public interface
2021-05-16T12:25:24.228-0700 7f622a5a2f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:26,402842653-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:26,413872638-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:26,455193248-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:26,458731246-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '063d57dc-46d1-46b1-b9dd-87e71e98ea25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 063d57dc-46d1-46b1-b9dd-87e71e98ea25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:30,046956086-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:30,050611564-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '063d57dc-46d1-46b1-b9dd-87e71e98ea25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 063d57dc-46d1-46b1-b9dd-87e71e98ea25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:33,540325645-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:33,544841160-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '063d57dc-46d1-46b1-b9dd-87e71e98ea25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 063d57dc-46d1-46b1-b9dd-87e71e98ea25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:36,941992088-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:36,946236403-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '063d57dc-46d1-46b1-b9dd-87e71e98ea25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 063d57dc-46d1-46b1-b9dd-87e71e98ea25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:44,037814753-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:44,041065381-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '063d57dc-46d1-46b1-b9dd-87e71e98ea25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 063d57dc-46d1-46b1-b9dd-87e71e98ea25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:48,785897789-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:48,789301315-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '063d57dc-46d1-46b1-b9dd-87e71e98ea25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 063d57dc-46d1-46b1-b9dd-87e71e98ea25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:53,498937802-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:53,502437418-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '063d57dc-46d1-46b1-b9dd-87e71e98ea25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 063d57dc-46d1-46b1-b9dd-87e71e98ea25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:57,747511067-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:25:57,751074412-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '063d57dc-46d1-46b1-b9dd-87e71e98ea25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 063d57dc-46d1-46b1-b9dd-87e71e98ea25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:26:01,662090667-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:26:01,666305968-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '063d57dc-46d1-46b1-b9dd-87e71e98ea25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 063d57dc-46d1-46b1-b9dd-87e71e98ea25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:26:05,990915682-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:26:05,994316042-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '063d57dc-46d1-46b1-b9dd-87e71e98ea25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 063d57dc-46d1-46b1-b9dd-87e71e98ea25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:26:09,622499794-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:26:09,625853776-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '063d57dc-46d1-46b1-b9dd-87e71e98ea25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 063d57dc-46d1-46b1-b9dd-87e71e98ea25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:26:13,084493070-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:26:13,087732186-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '063d57dc-46d1-46b1-b9dd-87e71e98ea25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 063d57dc-46d1-46b1-b9dd-87e71e98ea25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:26:16,542543006-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:26:40,022487924-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:26:48,493688775-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:26:56,999008365-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:05,377699084-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:13,959195669-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:22,431503272-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   238 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:22,439941917-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:30,943539675-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:30,952534726-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:39,523192587-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:39,532220110-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:48,078972815-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:48,088338743-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:56,785669831-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:56,794114067-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:56,800815900-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:56,804324802-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:56,812818041-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=184028
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:56,819440525-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:56,837833184-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:27:56,841253560-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '063d57dc-46d1-46b1-b9dd-87e71e98ea25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 063d57dc-46d1-46b1-b9dd-87e71e98ea25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:27:59.009+0000 7f038832ad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:27:59.297+0000 7f038832ad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:27:59.297+0000 7f038832ad00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:27:59.310306+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-16T19:27:59.310320+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T19:27:59.324890+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:27:59.324890+0000     0       0         0         0         0         0           -           0
2021-05-16T19:28:00.325008+0000     1     214       214         0         0         0           -           0
2021-05-16T19:28:01.325227+0000     2     255       403       148   18.4973      18.5     1.86591     1.86497
2021-05-16T19:28:02.325336+0000     3     255       403       148   12.3317         0           -     1.86497
2021-05-16T19:28:03.325495+0000     4     255       403       148   9.24871         0           -     1.86497
2021-05-16T19:28:04.325590+0000     5     255       620       365   18.2476   18.0833     2.17596     2.34733
2021-05-16T19:28:05.325792+0000     6     255       620       365   15.2062         0           -     2.34733
2021-05-16T19:28:06.325878+0000     7     255       750       495   17.6762     16.25     2.13055     2.48884
2021-05-16T19:28:07.325970+0000     8     255       750       495   15.4668         0           -     2.48884
2021-05-16T19:28:08.326057+0000     9     255      1142       887   24.6358        49   0.0494406     2.28452
2021-05-16T19:28:09.326136+0000    10     255      1142       887   22.1723         0           -     2.28452
2021-05-16T19:28:10.326242+0000    11     255      1380      1125   25.5652     29.75     2.15672     2.24785
2021-05-16T19:28:11.326502+0000    12     255      1380      1125   23.4344         0           -     2.24785
2021-05-16T19:28:12.326607+0000    13     255      1520      1265   24.3238      17.5     2.12082     2.29469
2021-05-16T19:28:13.326829+0000    14     255      1520      1265   22.5862         0           -     2.29469
2021-05-16T19:28:14.326947+0000    15     255      1783      1528   25.4633    32.875     4.29108     2.35108
2021-05-16T19:28:15.327199+0000    16     255      1783      1528   23.8716         0           -     2.35108
2021-05-16T19:28:16.327304+0000    17     255      2037      1782   26.2022     31.75   0.0482838      2.3156
2021-05-16T19:28:17.327384+0000    18     255      2037      1782   24.7466         0           -      2.3156
2021-05-16T19:28:18.327490+0000    19     255      2037      1782   23.4442         0           -      2.3156
2021-05-16T19:28:19.327732+0000 min lat: 0.0335913 max lat: 6.42318 avg lat: 2.31957
2021-05-16T19:28:19.327732+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:28:19.327732+0000    20     255      2269      2014   25.1715   19.3333     4.32452     2.31957
2021-05-16T19:28:20.327890+0000    21     255      2269      2014   23.9728         0           -     2.31957
2021-05-16T19:28:21.327968+0000    22     122      2270      2148   24.4057     16.75     2.09951      2.3669
2021-05-16T19:28:22.328075+0000    23     122      2270      2148   23.3446         0           -      2.3669
2021-05-16T19:28:23.328167+0000    24     115      2270      2155   22.4449     0.875     4.24566       2.373
2021-05-16T19:28:24.328247+0000    25     115      2270      2155   21.5471         0           -       2.373
2021-05-16T19:28:25.328351+0000    26      60      2270      2210   21.2472     6.875     6.37451     2.47301
2021-05-16T19:28:26.328458+0000    27      60      2270      2210   20.4603         0           -     2.47301
2021-05-16T19:28:27.328620+0000 Total time run:         27.6254
Total writes made:      2270
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     20.5427
Stddev Bandwidth:       13.6723
Max bandwidth (MB/sec): 49
Min bandwidth (MB/sec): 0
Average IOPS:           82
Stddev IOPS:            54.6555
Max IOPS:               196
Min IOPS:               0
Average Latency(s):     2.63355
Stddev Latency(s):      1.75356
Max latency(s):         8.56761
Min latency(s):         0.0335913

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:28:27,886927252-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 184028


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:28:27,891257027-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:28:51,257112160-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 568 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:28:51,266106279-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:28:59,711959042-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 568 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:28:59,720989049-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:29:08,243015529-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 568 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:29:08,252060334-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:29:16,794463222-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 568 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:29:16,802915494-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:29:25,286627734-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 568 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:29:25,295603780-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:29:25,302326772-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:29:25,306269631-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:29:25,314792335-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=185414
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:29:25,321888358-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:29:25,341544622-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:29:25,345295389-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '063d57dc-46d1-46b1-b9dd-87e71e98ea25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 063d57dc-46d1-46b1-b9dd-87e71e98ea25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RtooYi:/tmp/ceph-asok.RtooYi -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:29:27.705+0000 7f060171ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:29:27.981+0000 7f060171ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:29:27.981+0000 7f060171ed00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:29:27.998559+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:29:27.998559+0000     0       0         0         0         0         0           -           0
2021-05-16T19:29:28.998678+0000     1     256       318        62   15.4971      15.5  0.00928258  0.00626166
2021-05-16T19:29:29.998950+0000     2     256       517       261   32.6175     49.75   0.0091001    0.700738
2021-05-16T19:29:30.999149+0000     3     256       517       261   21.7452         0           -    0.700738
2021-05-16T19:29:31.999247+0000     4     256       549       293    18.309         4 0.000588977    0.777095
2021-05-16T19:29:32.999344+0000     5     256       549       293   14.6475         0           -    0.777095
2021-05-16T19:29:33.999563+0000     6     256       574       318   13.2476     3.125 0.000684696     0.99039
2021-05-16T19:29:34.999665+0000     7     256       574       318   11.3552         0           -     0.99039
2021-05-16T19:29:35.999762+0000     8     256       726       470   14.6852        19  0.00109072     1.84428
2021-05-16T19:29:36.999878+0000     9     256       726       470   13.0535         0           -     1.84428
2021-05-16T19:29:37.999972+0000    10     256       832       576   14.3979     13.25      8.5948     3.08378
2021-05-16T19:29:39.000074+0000    11     256       832       576    13.089         0           -     3.08378
2021-05-16T19:29:40.000168+0000    12     256      1033       777   16.1852    25.125  0.00617105     3.61586
2021-05-16T19:29:41.000265+0000    13     256      1033       777   14.9403         0           -     3.61586
2021-05-16T19:29:42.000378+0000    14     256      1033       777   13.8731         0           -     3.61586
2021-05-16T19:29:43.000480+0000    15     256      1044       788   13.1316  0.916667       2.137     3.60613
2021-05-16T19:29:44.000585+0000    16     256      1044       788   12.3109         0           -     3.60613
2021-05-16T19:29:45.000684+0000    17     256      1103       847   12.4543     7.375     6.43828     3.82364
2021-05-16T19:29:46.000947+0000    18     256      1103       847   11.7623         0           -     3.82364
2021-05-16T19:29:47.001047+0000    19     256      1143       887   11.6695         5     8.58728     3.98763
2021-05-16T19:29:48.001256+0000 min lat: 0.000588977 max lat: 10.7342 avg lat: 3.98763
2021-05-16T19:29:48.001256+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:29:48.001256+0000    20     256      1143       887    11.086         0           -     3.98763
2021-05-16T19:29:49.001508+0000    21     256      1198       942   11.2127     6.875  0.00089396      3.9805
2021-05-16T19:29:50.001604+0000    22     256      1198       942    10.703         0           -      3.9805
2021-05-16T19:29:51.001705+0000    23     256      1234       978   10.6289       4.5 0.000695848     4.14126
2021-05-16T19:29:52.001842+0000    24     256      1234       978   10.1861         0           -     4.14126
2021-05-16T19:29:53.001940+0000    25     256      1389      1133   11.3284    19.375  0.00916131     4.76321
2021-05-16T19:29:54.002036+0000    26     256      1389      1133   10.8927         0           -     4.76321
2021-05-16T19:29:55.002197+0000    27     256      1465      1209   11.1929       9.5     8.59359      5.2699
2021-05-16T19:29:56.002293+0000    28     256      1465      1209   10.7932         0           -      5.2699
2021-05-16T19:29:57.002495+0000    29     256      1465      1209    10.421         0           -      5.2699
2021-05-16T19:29:58.002592+0000    30     256      1526      1270   10.5819   5.08333     4.29396     5.33129
2021-05-16T19:29:59.002694+0000    31     256      1526      1270   10.2405         0           -     5.33129
2021-05-16T19:30:00.002951+0000    32     256      1597      1341   10.4751     8.875     4.29771     5.36919
2021-05-16T19:30:01.003049+0000    33     256      1597      1341   10.1577         0           -     5.36919
2021-05-16T19:30:02.003165+0000    34     256      1761      1505   11.0647      20.5 0.000832735     5.21377
2021-05-16T19:30:03.003269+0000    35     256      1761      1505   10.7485         0           -     5.21377
2021-05-16T19:30:04.003517+0000    36     256      1828      1572   10.9151     8.375     6.44405     5.32616
2021-05-16T19:30:05.003615+0000    37     256      1828      1572   10.6202         0           -     5.32616
2021-05-16T19:30:06.003857+0000    38     256      1855      1599   10.5183     3.375 0.000602432     5.26038
2021-05-16T19:30:07.003959+0000    39     256      1855      1599   10.2486         0           -     5.26038
2021-05-16T19:30:08.004053+0000 min lat: 0.00039945 max lat: 15.0245 avg lat: 5.10558
2021-05-16T19:30:08.004053+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:30:08.004053+0000    40     256      2071      1815   11.3422        27 0.000854786     5.10558
2021-05-16T19:30:09.004163+0000    41     256      2071      1815   11.0655         0           -     5.10558
2021-05-16T19:30:10.004258+0000    42     256      2101      1845   10.9806      3.75 0.000743357     5.09702
2021-05-16T19:30:11.004369+0000    43     256      2101      1845   10.7253         0           -     5.09702
2021-05-16T19:30:12.004471+0000    44     256      2101      1845   10.4815         0           -     5.09702
2021-05-16T19:30:13.004579+0000    45     256      2179      1923   10.6819       6.5     2.15298     5.08218
2021-05-16T19:30:14.004683+0000    46     256      2179      1923   10.4497         0           -     5.08218
2021-05-16T19:30:15.004946+0000    47     256      2230      1974   10.4986     6.375  0.00132695     4.99545
2021-05-16T19:30:16.005191+0000    48     256      2230      1974   10.2798         0           -     4.99545
2021-05-16T19:30:17.005289+0000    49     243      2270      2027   10.3404     6.625   0.0014514     4.90297
2021-05-16T19:30:18.005385+0000    50     243      2270      2027   10.1336         0           -     4.90297
2021-05-16T19:30:19.005634+0000    51     140      2270      2130   10.4397    12.875     12.8907     5.52382
2021-05-16T19:30:20.005879+0000    52     140      2270      2130   10.2389         0           -     5.52382
2021-05-16T19:30:21.005977+0000    53      98      2270      2172   10.2438      5.25     12.8794     5.67106
2021-05-16T19:30:22.006072+0000    54      98      2270      2172   10.0541         0           -     5.67106
2021-05-16T19:30:23.006173+0000    55      55      2270      2215   10.0668     5.375     10.7394     5.83809
2021-05-16T19:30:24.006268+0000    56      55      2270      2215   9.88702         0           -     5.83809
2021-05-16T19:30:25.006406+0000 Total time run:       56.9996
Total reads made:     2270
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   9.95621
Average IOPS:         39
Stddev IOPS:          36.4542
Max IOPS:             199
Min IOPS:             0
Average Latency(s):   5.96623
Max latency(s):       21.4722
Min latency(s):       0.00039945

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:30:25,622027376-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 185414


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:30:25,626806656-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:30:49,295302484-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 568 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:30:49,304795601-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:30:57,888572877-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 568 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:30:57,897857603-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:06,461461936-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 568 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:06,470255709-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:14,948763198-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 568 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:14,957962954-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:23,559914831-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 568 MiB
    usage:   1.1 GiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:23,568947934-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:23,576430673-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T15:31:23,580515339-04:00][RUNNING][ROUND 1/5/40] object_size=1MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:23,584540903-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:23,593900940-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40360\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.160816\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fbe56982-7064-4ad4-90f6-8ee0d3e771bb\nsetting min_mon_release = octopus\nepoch 0\nfsid fbe56982-7064-4ad4-90f6-8ee0d3e771bb\nlast_changed 2021-05-16T12:31:33.624344-0700\ncreated 2021-05-16T12:31:33.624344-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40360/0,v1:10.10.1.2:40361/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.160816 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 575789e6-314e-46f9-bbe2-4259b2caf2be\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 05f2e89b-857e-4605-a297-b8a3b8ee8f4e\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 2d73008e-1321-4870-9976-820043012181\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42360\n  w/ user/pass: admin / 5c5a7758-9dcb-4587-b0d0-ba37570f6587\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 15:31:51 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40360
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.160816
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fbe56982-7064-4ad4-90f6-8ee0d3e771bb
setting min_mon_release = octopus
epoch 0
fsid fbe56982-7064-4ad4-90f6-8ee0d3e771bb
last_changed 2021-05-16T12:31:33.624344-0700
created 2021-05-16T12:31:33.624344-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40360/0,v1:10.10.1.2:40361/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.160816 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 575789e6-314e-46f9-bbe2-4259b2caf2be
0
start osd.0
add osd1 05f2e89b-857e-4605-a297-b8a3b8ee8f4e
1
start osd.1
add osd2 2d73008e-1321-4870-9976-820043012181
2
start osd.2


restful urls: https://10.10.1.2:42360
  w/ user/pass: admin / 5c5a7758-9dcb-4587-b0d0-ba37570f6587


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T12:31:25.427-0700 7f7b614ad1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:31:25.427-0700 7f7b614ad1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:31:25.443-0700 7f658636f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:31:25.443-0700 7f658636f1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40360,v1:10.10.1.2:40361] --print /tmp/ceph_monmap.160816 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.160816 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.160816 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42360 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55b955424000 @  0x7f55e3dcc680 0x7f55e3ded824 0x7f55e4588187 0x7f55e4590355 0x7f55e4588708 0x7f55e4588877 0x7f55e4589c24 0x7f55e45a1ec1 0x7f55e45145f3 0x7f55e4575e97 0x7f55e457db1a 0x7f55e3c80d84 0x7f55e3d9c609 0x7f55e3970293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.If3oODLyFl 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 575789e6-314e-46f9-bbe2-4259b2caf2be -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAgc6FgSz3fGBAAjN9sZ7ZNA1m2+y88pXb6Fg== --osd-uuid 575789e6-314e-46f9-bbe2-4259b2caf2be 
2021-05-16T12:31:45.048-0700 7fd6491f3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:31:45.072-0700 7fd6491f3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:31:45.072-0700 7fd6491f3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55e5737f6000 @  0x7fd649bbc680 0x7fd649bdd824 0x55e5683e5447 0x55e5683ed4b5 0x55e5683e59c8 0x55e5683e5b37 0x55e5683e6ee4 0x55e5681b7ca1 0x55e56838f423 0x55e5681a82a7 0x55e5681ad53a 0x7fd64970fd84 0x7fd649894609 0x7fd6493fd293
2021-05-16T12:31:45.396-0700 7fd6491f3f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 05f2e89b-857e-4605-a297-b8a3b8ee8f4e -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x56051c3aa000 @  0x7f9a2ab55680 0x7f9a2ab76824 0x560511b2b447 0x560511b334b5 0x560511b2b9c8 0x560511b2bb37 0x560511b2cee4 0x5605118fdca1 0x560511ad5423 0x5605118ee2a7 0x5605118f353a 0x7f9a2a6a8d84 0x7f9a2a82d609 0x7f9a2a396293
2021-05-16T12:31:46.048-0700 7f9a2a18cf00 -1 Falling back to public interface
2021-05-16T12:31:46.308-0700 7f9a2a18cf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAhc6FgeOhILBAA7MIhLk8n6/IMSlRM24bQGw== --osd-uuid 05f2e89b-857e-4605-a297-b8a3b8ee8f4e 
2021-05-16T12:31:46.416-0700 7f3d4ce5af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:31:46.436-0700 7f3d4ce5af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:31:46.436-0700 7f3d4ce5af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55db20486000 @  0x7f3d4d823680 0x7f3d4d844824 0x55db16198447 0x55db161a04b5 0x55db161989c8 0x55db16198b37 0x55db16199ee4 0x55db15f6aca1 0x55db16142423 0x55db15f5b2a7 0x55db15f6053a 0x7f3d4d376d84 0x7f3d4d4fb609 0x7f3d4d064293
2021-05-16T12:31:46.760-0700 7f3d4ce5af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 2d73008e-1321-4870-9976-820043012181 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x56428a27c000 @  0x7fdb656e4680 0x7fdb65705824 0x56427fba0447 0x56427fba84b5 0x56427fba09c8 0x56427fba0b37 0x56427fba1ee4 0x56427f972ca1 0x56427fb4a423 0x56427f9632a7 0x56427f96853a 0x7fdb65237d84 0x7fdb653bc609 0x7fdb64f25293
2021-05-16T12:31:47.472-0700 7fdb64d1bf00 -1 Falling back to public interface
2021-05-16T12:31:47.728-0700 7fdb64d1bf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAjc6FgsS2hCRAA8ToUfyf9ZrMB2lZj01yqCA== --osd-uuid 2d73008e-1321-4870-9976-820043012181 
2021-05-16T12:31:47.824-0700 7f50bd138f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:31:47.848-0700 7f50bd138f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:31:47.848-0700 7f50bd138f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55b063458000 @  0x7f50bdb01680 0x7f50bdb22824 0x55b057f26447 0x55b057f2e4b5 0x55b057f269c8 0x55b057f26b37 0x55b057f27ee4 0x55b057cf8ca1 0x55b057ed0423 0x55b057ce92a7 0x55b057cee53a 0x7f50bd654d84 0x7f50bd7d9609 0x7f50bd342293
2021-05-16T12:31:48.196-0700 7f50bd138f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55de09e9a000 @  0x7f2c89e28680 0x7f2c89e49824 0x55ddff3cc447 0x55ddff3d44b5 0x55ddff3cc9c8 0x55ddff3ccb37 0x55ddff3cdee4 0x55ddff19eca1 0x55ddff376423 0x55ddff18f2a7 0x55ddff19453a 0x7f2c8997bd84 0x7f2c89b00609 0x7f2c89669293
2021-05-16T12:31:48.852-0700 7f2c8945ff00 -1 Falling back to public interface
2021-05-16T12:31:49.128-0700 7f2c8945ff00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:51,322734566-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:51,334057001-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:51,373321608-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:51,376243648-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0112c4b4-823f-454d-be3c-71e377adb5cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0112c4b4-823f-454d-be3c-71e377adb5cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:55,035577381-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:55,039257025-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0112c4b4-823f-454d-be3c-71e377adb5cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0112c4b4-823f-454d-be3c-71e377adb5cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:58,595207784-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:31:58,599197541-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0112c4b4-823f-454d-be3c-71e377adb5cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0112c4b4-823f-454d-be3c-71e377adb5cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:02,143394395-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:02,146965645-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0112c4b4-823f-454d-be3c-71e377adb5cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0112c4b4-823f-454d-be3c-71e377adb5cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:09,185739349-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:09,190000876-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0112c4b4-823f-454d-be3c-71e377adb5cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0112c4b4-823f-454d-be3c-71e377adb5cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:13,049286674-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:13,053048442-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0112c4b4-823f-454d-be3c-71e377adb5cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0112c4b4-823f-454d-be3c-71e377adb5cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:17,507888399-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:17,511561129-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0112c4b4-823f-454d-be3c-71e377adb5cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0112c4b4-823f-454d-be3c-71e377adb5cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:21,357931219-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:21,362120090-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0112c4b4-823f-454d-be3c-71e377adb5cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0112c4b4-823f-454d-be3c-71e377adb5cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:25,728544316-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:25,732578947-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0112c4b4-823f-454d-be3c-71e377adb5cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0112c4b4-823f-454d-be3c-71e377adb5cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:29,503293302-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:29,506554399-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0112c4b4-823f-454d-be3c-71e377adb5cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0112c4b4-823f-454d-be3c-71e377adb5cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:34,027275064-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:34,030978593-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0112c4b4-823f-454d-be3c-71e377adb5cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0112c4b4-823f-454d-be3c-71e377adb5cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:37,524464775-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:37,528047636-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0112c4b4-823f-454d-be3c-71e377adb5cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0112c4b4-823f-454d-be3c-71e377adb5cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:32:40,942152623-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:33:04,513812894-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:33:13,011757532-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:33:21,476212525-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:33:30,056686004-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:33:38,530019410-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:33:47,050410607-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   238 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:33:47,059659055-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:33:55,624901156-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:33:55,634324372-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:34:04,040916354-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:34:04,049870369-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:34:12,623811045-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:34:12,632685961-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:34:21,169490921-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:34:21,178680107-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:34:21,186082426-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:34:21,189722867-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:34:21,198658677-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=191842
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:34:21,206142919-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:34:21,226476045-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:34:21,229636744-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0112c4b4-823f-454d-be3c-71e377adb5cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0112c4b4-823f-454d-be3c-71e377adb5cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40 -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:34:23.554+0000 7f8823996d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:34:23.826+0000 7f8823996d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:34:23.826+0000 7f8823996d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:34:23.841603+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-16T19:34:23.841615+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T19:34:23.896308+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:34:23.896308+0000     0       0         0         0         0         0           -           0
2021-05-16T19:34:24.896436+0000     1      50        50         0         0         0           -           0
2021-05-16T19:34:25.896542+0000     2     101       101         0         0         0           -           0
2021-05-16T19:34:26.896675+0000     3     101       101         0         0         0           -           0
2021-05-16T19:34:27.896756+0000     4     125       125         0         0         0           -           0
2021-05-16T19:34:28.896851+0000     5     125       125         0         0         0           -           0
2021-05-16T19:34:29.896940+0000     6     157       157         0         0         0           -           0
2021-05-16T19:34:30.897223+0000     7     201       201         0         0         0           -           0
2021-05-16T19:34:31.897473+0000     8     201       201         0         0         0           -           0
2021-05-16T19:34:32.897685+0000     9     255       302        47   5.22146   5.22222     8.19287     8.17402
2021-05-16T19:34:33.897954+0000    10     255       302        47   4.69926         0           -     8.17402
2021-05-16T19:34:34.898192+0000    11     255       348        93   8.45315        23     8.60885     8.45846
2021-05-16T19:34:35.898287+0000    12     255       348        93   7.74877         0           -     8.45846
2021-05-16T19:34:36.898371+0000    13     255       392       137   10.5368        22     6.45854     8.47029
2021-05-16T19:34:37.898578+0000    14     255       392       137   9.78418         0           -     8.47029
2021-05-16T19:34:38.898688+0000    15     255       406       151   10.0651         7     8.57698     8.47906
2021-05-16T19:34:39.898764+0000    16     255       406       151   9.43609         0           -     8.47906
2021-05-16T19:34:40.898843+0000    17     255       441       186   10.9396      17.5     10.7152     8.90104
2021-05-16T19:34:41.898962+0000    18     255       441       186   10.3318         0           -     8.90104
2021-05-16T19:34:42.899187+0000    19     255       461       206   10.8405        10     10.7317     9.24126
2021-05-16T19:34:43.899285+0000 min lat: 6.44685 max lat: 12.8331 avg lat: 9.24126
2021-05-16T19:34:43.899285+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:34:43.899285+0000    20     255       461       206   10.2985         0           -     9.24126
2021-05-16T19:34:44.899473+0000    21      31       462       431   20.5208     112.5     2.12423     9.73478
2021-05-16T19:34:45.899545+0000    22      31       462       431   19.5881         0           -     9.73478
2021-05-16T19:34:46.899765+0000    23      31       462       431   18.7364         0           -     9.73478
2021-05-16T19:34:47.899842+0000    24      15       462       447   18.6223   5.33333     4.28825     9.58271
2021-05-16T19:34:48.899913+0000    25      15       462       447   17.8775         0           -     9.58271
2021-05-16T19:34:49.900048+0000 Total time run:         25.295
Total writes made:      462
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     18.2645
Stddev Bandwidth:       22.8574
Max bandwidth (MB/sec): 112.5
Min bandwidth (MB/sec): 0
Average IOPS:           18
Stddev IOPS:            22.7568
Max IOPS:               112
Min IOPS:               0
Average Latency(s):     9.51747
Stddev Latency(s):      2.54132
Max latency(s):         12.8504
Min latency(s):         2.12423

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:34:50,432272201-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 191842


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:34:50,437753721-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:13,829551770-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 463 objects, 462 MiB
    usage:   924 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:13,838539377-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:22,371246682-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 463 objects, 462 MiB
    usage:   924 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:22,380135504-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:30,857967209-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 463 objects, 462 MiB
    usage:   924 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:30,867999359-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:39,387920791-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 463 objects, 462 MiB
    usage:   924 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:39,396295567-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:48,021595913-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 463 objects, 462 MiB
    usage:   924 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:48,030850211-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:48,037877476-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:48,042158189-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:48,050478813-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=193252
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:48,056889709-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:48,074892326-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:35:48,078477432-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0112c4b4-823f-454d-be3c-71e377adb5cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0112c4b4-823f-454d-be3c-71e377adb5cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8JiN40:/tmp/ceph-asok.8JiN40 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:35:50.463+0000 7fd3e1ebdd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:35:50.735+0000 7fd3e1ebdd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:35:50.735+0000 7fd3e1ebdd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:35:50.752969+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:35:50.752969+0000     0       0         0         0         0         0           -           0
2021-05-16T19:35:51.753200+0000     1     141       141         0         0         0           -           0
2021-05-16T19:35:52.753293+0000     2     177       177         0         0         0           -           0
2021-05-16T19:35:53.753386+0000     3     185       185         0         0         0           -           0
2021-05-16T19:35:54.753477+0000     4     205       205         0         0         0           -           0
2021-05-16T19:35:55.753568+0000     5     223       223         0         0         0           -           0
2021-05-16T19:35:56.753797+0000     6     223       223         0         0         0           -           0
2021-05-16T19:35:57.753900+0000     7     249       249         0         0         0           -           0
2021-05-16T19:35:58.753992+0000     8     255       261         6  0.749895      0.75     7.10668     7.10566
2021-05-16T19:35:59.754085+0000     9     255       271        16   1.77754        10     8.17993     7.77581
2021-05-16T19:36:00.754178+0000    10     255       276        21   2.09973         5     9.25132       8.127
2021-05-16T19:36:01.754271+0000    11     255       281        26   2.36334         5     10.3257     8.54958
2021-05-16T19:36:02.754367+0000    12     255       301        46   3.83286        20     11.4042     9.78907
2021-05-16T19:36:03.754461+0000    13     255       321        66    5.0763        20     12.4797      10.603
2021-05-16T19:36:04.754558+0000    14     255       336        81   5.78502        15     13.5495     11.1482
2021-05-16T19:36:05.754653+0000    15     255       358       103   6.86585        22     14.6247     11.8901
2021-05-16T19:36:06.754751+0000    16     255       370       115   7.18666        12     15.4901     12.2727
2021-05-16T19:36:07.754972+0000    17     255       382       127   7.46967        12     16.5583     12.6777
2021-05-16T19:36:08.755214+0000    18     255       398       143   7.94341        16     16.8381      13.226
2021-05-16T19:36:09.755309+0000    19     255       411       156   8.20947        13     17.1843     13.5929
2021-05-16T19:36:10.755405+0000 min lat: 7.10499 max lat: 18.2531 avg lat: 14.1223
2021-05-16T19:36:10.755405+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:36:10.755405+0000    20     255       431       176   8.79888        20     18.2531     14.1223
2021-05-16T19:36:11.755511+0000    21     255       431       176    8.3799         0           -     14.1223
2021-05-16T19:36:12.755605+0000    22     255       439       184    8.3626         4     18.2561     14.3135
2021-05-16T19:36:13.755849+0000    23     255       450       195   8.47716        11     18.2549     14.5468
2021-05-16T19:36:14.755954+0000    24      98       462       364   15.1647       169     1.08717     13.6012
2021-05-16T19:36:15.756053+0000    25      92       462       370   14.7981         6     12.8819     13.5895
2021-05-16T19:36:16.756148+0000    26      90       462       372   14.3059         2     3.21809     13.5338
2021-05-16T19:36:17.756256+0000    27      79       462       383   14.1834        11     12.8886     13.5487
2021-05-16T19:36:18.756351+0000    28      63       462       399   14.2482        16      10.739     13.4091
2021-05-16T19:36:19.756459+0000    29      57       462       405   13.9638         6     13.9558     13.4252
2021-05-16T19:36:20.756556+0000    30      46       462       416    13.865        11     8.59119     13.3412
2021-05-16T19:36:21.756652+0000    31      40       462       422   13.6112         6     13.9587     13.3627
2021-05-16T19:36:22.756747+0000    32      36       462       426   13.3109         4     9.66142     13.3304
2021-05-16T19:36:23.756949+0000    33      34       462       428   12.9681         2     16.1033     13.3434
2021-05-16T19:36:24.757044+0000    34      31       462       431   12.6749         3     10.7353     13.3252
2021-05-16T19:36:25.757284+0000    35      31       462       431   12.3127         0           -     13.3252
2021-05-16T19:36:26.757381+0000    36      22       462       440   12.2207       4.5     16.1083     13.4016
2021-05-16T19:36:27.757491+0000    37      22       462       440   11.8904         0           -     13.4016
2021-05-16T19:36:28.757729+0000    38      18       462       444   11.6827         2     18.2498     13.4453
2021-05-16T19:36:29.757822+0000    39      18       462       444   11.3832         0           -     13.4453
2021-05-16T19:36:30.757916+0000 min lat: 1.08717 max lat: 20.3956 avg lat: 13.5261
2021-05-16T19:36:30.757916+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:36:30.757916+0000    40      12       462       450   11.2486         3     19.3232     13.5261
2021-05-16T19:36:31.758179+0000    41      11       462       451   10.9986         1     17.1769     13.5342
2021-05-16T19:36:32.758275+0000    42       2       462       460    10.951         9     18.2559     13.6522
2021-05-16T19:36:33.758370+0000    43       2       462       460   10.6963         0           -     13.6522
2021-05-16T19:36:34.758523+0000 Total time run:       43.6109
Total reads made:     462
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   10.5937
Average IOPS:         10
Stddev IOPS:          25.6894
Max IOPS:             169
Min IOPS:             0
Average Latency(s):   13.6814
Max latency(s):       21.4677
Min latency(s):       1.08717

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:36:35,298503644-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 193252


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:36:35,302976990-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:36:58,869014338-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 463 objects, 462 MiB
    usage:   924 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:36:58,878480444-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:37:07,241679745-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 463 objects, 462 MiB
    usage:   924 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:37:07,251185596-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:37:15,614175674-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 463 objects, 462 MiB
    usage:   924 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:37:15,623847286-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:37:24,085099396-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 463 objects, 462 MiB
    usage:   924 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:37:24,094546417-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:37:32,577120822-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 463 objects, 462 MiB
    usage:   924 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:37:32,585973085-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:37:32,593013313-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T15:37:32,595692367-04:00][RUNNING][ROUND 2/5/40] object_size=1MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:37:32,599798843-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:37:32,609915812-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40995\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.161898\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid eb15718e-0eb4-43f5-be8b-c097b2b8ee50\nsetting min_mon_release = octopus\nepoch 0\nfsid eb15718e-0eb4-43f5-be8b-c097b2b8ee50\nlast_changed 2021-05-16T12:37:42.572152-0700\ncreated 2021-05-16T12:37:42.572152-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40995/0,v1:10.10.1.2:40996/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.161898 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 37fd62c4-7ef9-4d08-855d-1a3004756c98\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 f49258cd-3393-466c-938b-e96b20077173\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 aa5abf23-89a2-4168-9337-bedd8a3d9dfc\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42995\n  w/ user/pass: admin / cc16ef5d-5842-46f5-918d-374b92dc2460\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 15:38:00 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40995
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.161898
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid eb15718e-0eb4-43f5-be8b-c097b2b8ee50
setting min_mon_release = octopus
epoch 0
fsid eb15718e-0eb4-43f5-be8b-c097b2b8ee50
last_changed 2021-05-16T12:37:42.572152-0700
created 2021-05-16T12:37:42.572152-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40995/0,v1:10.10.1.2:40996/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.161898 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 37fd62c4-7ef9-4d08-855d-1a3004756c98
0
start osd.0
add osd1 f49258cd-3393-466c-938b-e96b20077173
1
start osd.1
add osd2 aa5abf23-89a2-4168-9337-bedd8a3d9dfc
2
start osd.2


restful urls: https://10.10.1.2:42995
  w/ user/pass: admin / cc16ef5d-5842-46f5-918d-374b92dc2460


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T12:37:34.459-0700 7f4ab67521c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:37:34.459-0700 7f4ab67521c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:37:34.475-0700 7f6e339af1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:37:34.475-0700 7f6e339af1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40995,v1:10.10.1.2:40996] --print /tmp/ceph_monmap.161898 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.161898 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.161898 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42995 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x560c0da2c000 @  0x7fd813a1e680 0x7fd813a3f824 0x7fd8141da187 0x7fd8141e2355 0x7fd8141da708 0x7fd8141da877 0x7fd8141dbc24 0x7fd8141f3ec1 0x7fd8141665f3 0x7fd8141c7e97 0x7fd8141cfb1a 0x7fd8138d2d84 0x7fd8139ee609 0x7fd8135c2293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.G4TTUOIhIs 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 37fd62c4-7ef9-4d08-855d-1a3004756c98 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCRdKFgGiUlExAAnhcnblXqISE4qmUbcnQ0GA== --osd-uuid 37fd62c4-7ef9-4d08-855d-1a3004756c98 
2021-05-16T12:37:53.955-0700 7fcc643d2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:37:53.975-0700 7fcc643d2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:37:53.975-0700 7fcc643d2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x558423ae2000 @  0x7fcc64d9b680 0x7fcc64dbc824 0x558419808447 0x5584198104b5 0x5584198089c8 0x558419808b37 0x558419809ee4 0x5584195daca1 0x5584197b2423 0x5584195cb2a7 0x5584195d053a 0x7fcc648eed84 0x7fcc64a73609 0x7fcc645dc293
2021-05-16T12:37:54.295-0700 7fcc643d2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f49258cd-3393-466c-938b-e96b20077173 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x562bc665a000 @  0x7fddf85fe680 0x7fddf861f824 0x562bbc653447 0x562bbc65b4b5 0x562bbc6539c8 0x562bbc653b37 0x562bbc654ee4 0x562bbc425ca1 0x562bbc5fd423 0x562bbc4162a7 0x562bbc41b53a 0x7fddf8151d84 0x7fddf82d6609 0x7fddf7e3f293
2021-05-16T12:37:54.955-0700 7fddf7c35f00 -1 Falling back to public interface
2021-05-16T12:37:55.223-0700 7fddf7c35f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCSdKFgZ+NhJhAAZjSDlQ9dXKh+oH0KN0QWEA== --osd-uuid f49258cd-3393-466c-938b-e96b20077173 
2021-05-16T12:37:55.327-0700 7f0f87a31f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:37:55.347-0700 7f0f87a31f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:37:55.347-0700 7f0f87a31f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56158eb68000 @  0x7f0f883fa680 0x7f0f8841b824 0x561583950447 0x5615839584b5 0x5615839509c8 0x561583950b37 0x561583951ee4 0x561583722ca1 0x5615838fa423 0x5615837132a7 0x56158371853a 0x7f0f87f4dd84 0x7f0f880d2609 0x7f0f87c3b293
2021-05-16T12:37:55.671-0700 7f0f87a31f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new aa5abf23-89a2-4168-9337-bedd8a3d9dfc -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55c1c9bba000 @  0x7f4fcfb71680 0x7f4fcfb92824 0x55c1bec32447 0x55c1bec3a4b5 0x55c1bec329c8 0x55c1bec32b37 0x55c1bec33ee4 0x55c1bea04ca1 0x55c1bebdc423 0x55c1be9f52a7 0x55c1be9fa53a 0x7f4fcf6c4d84 0x7f4fcf849609 0x7f4fcf3b2293
2021-05-16T12:37:56.447-0700 7f4fcf1a8f00 -1 Falling back to public interface
2021-05-16T12:37:56.711-0700 7f4fcf1a8f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCUdKFgeB0HBRAAGuly38JS27O+CvJyFLOCEQ== --osd-uuid aa5abf23-89a2-4168-9337-bedd8a3d9dfc 
2021-05-16T12:37:56.795-0700 7f41dda61f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:37:56.815-0700 7f41dda61f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:37:56.815-0700 7f41dda61f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x555677eca000 @  0x7f41de42a680 0x7f41de44b824 0x55566d0db447 0x55566d0e34b5 0x55566d0db9c8 0x55566d0dbb37 0x55566d0dcee4 0x55566ceadca1 0x55566d085423 0x55566ce9e2a7 0x55566cea353a 0x7f41ddf7dd84 0x7f41de102609 0x7f41ddc6b293
2021-05-16T12:37:57.151-0700 7f41dda61f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5614bda3e000 @  0x7feabe638680 0x7feabe659824 0x5614b1c81447 0x5614b1c894b5 0x5614b1c819c8 0x5614b1c81b37 0x5614b1c82ee4 0x5614b1a53ca1 0x5614b1c2b423 0x5614b1a442a7 0x5614b1a4953a 0x7feabe18bd84 0x7feabe310609 0x7feabde79293
2021-05-16T12:37:57.815-0700 7feabdc6ff00 -1 Falling back to public interface
2021-05-16T12:37:58.095-0700 7feabdc6ff00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:00,278806165-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:00,290565390-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:00,330443590-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:00,333887051-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5dbed5cc-02c7-4155-9226-3d0addf67835', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5dbed5cc-02c7-4155-9226-3d0addf67835 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:03,763201883-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:03,766813358-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5dbed5cc-02c7-4155-9226-3d0addf67835', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5dbed5cc-02c7-4155-9226-3d0addf67835 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:07,427862824-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:07,431890542-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5dbed5cc-02c7-4155-9226-3d0addf67835', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5dbed5cc-02c7-4155-9226-3d0addf67835 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:11,006553829-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:11,009905477-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5dbed5cc-02c7-4155-9226-3d0addf67835', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5dbed5cc-02c7-4155-9226-3d0addf67835 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:17,943669477-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:17,947581808-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5dbed5cc-02c7-4155-9226-3d0addf67835', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5dbed5cc-02c7-4155-9226-3d0addf67835 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:22,569567057-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:22,573316902-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5dbed5cc-02c7-4155-9226-3d0addf67835', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5dbed5cc-02c7-4155-9226-3d0addf67835 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:27,018168742-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:27,022043342-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5dbed5cc-02c7-4155-9226-3d0addf67835', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5dbed5cc-02c7-4155-9226-3d0addf67835 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:31,472170484-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:31,476261190-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5dbed5cc-02c7-4155-9226-3d0addf67835', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5dbed5cc-02c7-4155-9226-3d0addf67835 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:35,834462847-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:35,837734113-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5dbed5cc-02c7-4155-9226-3d0addf67835', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5dbed5cc-02c7-4155-9226-3d0addf67835 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:40,518532681-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:40,522246760-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5dbed5cc-02c7-4155-9226-3d0addf67835', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5dbed5cc-02c7-4155-9226-3d0addf67835 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:44,796200657-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:44,799984618-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5dbed5cc-02c7-4155-9226-3d0addf67835', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5dbed5cc-02c7-4155-9226-3d0addf67835 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:48,290948808-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:48,294040407-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5dbed5cc-02c7-4155-9226-3d0addf67835', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5dbed5cc-02c7-4155-9226-3d0addf67835 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:38:51,666260621-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:39:15,230003811-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:39:23,750807250-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:39:32,175379212-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:39:40,695958481-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:39:49,177008736-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:39:57,757562526-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   238 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:39:57,766943343-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:06,312505635-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:06,321982652-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:14,823625141-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:14,833097790-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:23,372223259-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:23,380992456-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:31,903829991-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:31,912812478-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:31,919873747-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:31,924202410-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:31,933556045-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=199673
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:31,940288605-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:31,959424371-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:31,963151424-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5dbed5cc-02c7-4155-9226-3d0addf67835', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5dbed5cc-02c7-4155-9226-3d0addf67835 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:40:34.344+0000 7f979a531d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:40:34.596+0000 7f979a531d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:40:34.596+0000 7f979a531d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:40:34.607273+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-16T19:40:34.607290+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T19:40:34.662752+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:40:34.662752+0000     0       0         0         0         0         0           -           0
2021-05-16T19:40:35.662989+0000     1      49        49         0         0         0           -           0
2021-05-16T19:40:36.663083+0000     2      57        57         0         0         0           -           0
2021-05-16T19:40:37.663273+0000     3      57        57         0         0         0           -           0
2021-05-16T19:40:38.663401+0000     4      87        87         0         0         0           -           0
2021-05-16T19:40:39.663567+0000     5      87        87         0         0         0           -           0
2021-05-16T19:40:40.663653+0000     6     141       141         0         0         0           -           0
2021-05-16T19:40:41.663743+0000     7     141       141         0         0         0           -           0
2021-05-16T19:40:42.663818+0000     8     193       193         0         0         0           -           0
2021-05-16T19:40:43.663915+0000     9     193       193         0         0         0           -           0
2021-05-16T19:40:44.664007+0000    10     244       244         0         0         0           -           0
2021-05-16T19:40:45.664110+0000    11     244       244         0         0         0           -           0
2021-05-16T19:40:46.664176+0000    12     244       244         0         0         0           -           0
2021-05-16T19:40:47.664256+0000    13     255       300        45   3.46115   3.46154     12.1102     12.1012
2021-05-16T19:40:48.664355+0000    14     255       300        45   3.21393         0           -     12.1012
2021-05-16T19:40:49.664446+0000    15     255       376       121   8.06579        38     8.64185     10.9629
2021-05-16T19:40:50.664543+0000    16     255       376       121   7.56168         0           -     10.9629
2021-05-16T19:40:51.664629+0000    17     255       415       160   9.41076      19.5         8.6     10.6626
2021-05-16T19:40:52.664723+0000    18     255       415       160   8.88795         0           -     10.6626
2021-05-16T19:40:53.664965+0000    19     255       468       213   11.2093      26.5     8.60853     10.4882
2021-05-16T19:40:54.665217+0000 min lat: 8.58781 max lat: 14.2288 avg lat: 10.4882
2021-05-16T19:40:54.665217+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:40:54.665217+0000    20     255       468       213   10.6487         0           -     10.4882
2021-05-16T19:40:55.665301+0000    21      12       469       457   21.7593       122     2.11757     8.30642
2021-05-16T19:40:56.665516+0000    22      12       469       457   20.7702         0           -     8.30642
2021-05-16T19:40:57.665645+0000 Total time run:         22.8199
Total writes made:      469
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     20.5522
Stddev Bandwidth:       27.1049
Max bandwidth (MB/sec): 122
Min bandwidth (MB/sec): 0
Average IOPS:           20
Stddev IOPS:            27.0867
Max IOPS:               122
Min IOPS:               0
Average Latency(s):     8.20305
Stddev Latency(s):      3.04967
Max latency(s):         14.2288
Min latency(s):         2.10393

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:58,264582758-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 199673


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:40:58,269600947-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:21,759932609-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 470 objects, 469 MiB
    usage:   938 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:21,769516918-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:30,140653144-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 470 objects, 469 MiB
    usage:   938 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:30,149567824-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:38,612350511-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 470 objects, 469 MiB
    usage:   938 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:38,621541651-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:47,168123606-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 470 objects, 469 MiB
    usage:   938 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:47,177552893-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:55,547869610-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 470 objects, 469 MiB
    usage:   938 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:55,557396009-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:55,564733537-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:55,568647722-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:55,578031143-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=201049
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:55,585130462-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:55,604206425-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:41:55,607517647-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5dbed5cc-02c7-4155-9226-3d0addf67835', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5dbed5cc-02c7-4155-9226-3d0addf67835 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Hto4mR:/tmp/ceph-asok.Hto4mR -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:41:58.124+0000 7fb3f5e3ad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:41:58.400+0000 7fb3f5e3ad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:41:58.400+0000 7fb3f5e3ad00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:41:58.420644+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:41:58.420644+0000     0      17        17         0         0         0           -           0
2021-05-16T19:41:59.420751+0000     1     123       123         0         0         0           -           0
2021-05-16T19:42:00.420841+0000     2     131       131         0         0         0           -           0
2021-05-16T19:42:01.420950+0000     3     135       135         0         0         0           -           0
2021-05-16T19:42:02.421192+0000     4     187       187         0         0         0           -           0
2021-05-16T19:42:03.421276+0000     5     205       205         0         0         0           -           0
2021-05-16T19:42:04.421501+0000     6     223       223         0         0         0           -           0
2021-05-16T19:42:05.421569+0000     7     226       226         0         0         0           -           0
2021-05-16T19:42:06.421634+0000     8     235       235         0         0         0           -           0
2021-05-16T19:42:07.421720+0000     9     255       290        35   3.88831   3.88889     8.82341       8.817
2021-05-16T19:42:08.421884+0000    10     255       299        44   4.39934         9     9.87987     9.03415
2021-05-16T19:42:09.421956+0000    11     255       307        52   4.72659         8     10.9529     9.32915
2021-05-16T19:42:10.422036+0000    12     255       307        52   4.33273         0           -     9.32915
2021-05-16T19:42:11.422110+0000    13     255       319        64   4.92242         6     12.0279     9.83484
2021-05-16T19:42:12.422183+0000    14     255       333        78   5.57071        14     13.1042      10.421
2021-05-16T19:42:13.422384+0000    15     255       346        91   6.06586        13     14.1761     10.9572
2021-05-16T19:42:14.422452+0000    16     255       350        95   5.93673         4     15.2463     11.1378
2021-05-16T19:42:15.422532+0000    17     255       353        98   5.76398         3     16.3202     11.2964
2021-05-16T19:42:16.422602+0000    18     255       362       107   5.94371         9     17.3937     11.8092
2021-05-16T19:42:17.422672+0000    19     255       368       113   5.94665         6     18.4642     12.1626
2021-05-16T19:42:18.422744+0000 min lat: 8.81119 max lat: 19.5354 avg lat: 12.6931
2021-05-16T19:42:18.422744+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:42:18.422744+0000    20     255       377       122   6.09928         9     19.3315     12.6931
2021-05-16T19:42:19.422964+0000    21     255       381       126   5.99926         4     19.3278     12.9208
2021-05-16T19:42:20.423032+0000    22     255       391       136   6.18107        10     19.3299     13.4382
2021-05-16T19:42:21.423100+0000    23     255       454       199   8.65115        63     18.2775     15.2448
2021-05-16T19:42:22.423294+0000    24      79       469       390    16.248       191     0.01615     12.7755
2021-05-16T19:42:23.423546+0000    25      79       469       390    15.598         0           -     12.7755
2021-05-16T19:42:24.423611+0000    26      73       469       396   15.2289         3     2.14502     12.6144
2021-05-16T19:42:25.423714+0000    27      73       469       396   14.6649         0           -     12.6144
2021-05-16T19:42:26.423858+0000    28      73       469       396   14.1411         0           -     12.6144
2021-05-16T19:42:27.423932+0000    29      57       469       412   14.2052   5.33333     15.0365     12.7811
2021-05-16T19:42:28.424000+0000    30      57       469       412   13.7317         0           -     12.7811
2021-05-16T19:42:29.424067+0000    31      39       469       430   13.8693         9     12.8909     12.9153
2021-05-16T19:42:30.424135+0000    32      39       469       430   13.4359         0           -     12.9153
2021-05-16T19:42:31.424210+0000    33       3       469       466   14.1196        18     8.60244     12.7955
2021-05-16T19:42:32.424276+0000    34       3       469       466   13.7043         0           -     12.7955
2021-05-16T19:42:33.424343+0000    35       1       469       468   13.3699         1      10.734     12.7867
2021-05-16T19:42:34.424424+0000    36       1       469       468   12.9985         0           -     12.7867
2021-05-16T19:42:35.424561+0000 Total time run:       36.7206
Total reads made:     469
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   12.7721
Average IOPS:         12
Stddev IOPS:          32.7773
Max IOPS:             191
Min IOPS:             0
Average Latency(s):   12.7869
Max latency(s):       20.4023
Min latency(s):       0.01615

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:42:35,989255154-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 201049


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:42:35,994125726-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:42:59,547054432-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 470 objects, 469 MiB
    usage:   938 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:42:59,556342775-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:43:08,055429131-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 470 objects, 469 MiB
    usage:   938 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:43:08,064220560-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:43:16,544881756-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 470 objects, 469 MiB
    usage:   938 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:43:16,554180608-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:43:24,926982209-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 470 objects, 469 MiB
    usage:   938 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:43:24,936279148-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:43:33,550791959-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 470 objects, 469 MiB
    usage:   938 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:43:33,560313910-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:43:33,567710859-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T15:43:33,570341692-04:00][RUNNING][ROUND 3/5/40] object_size=1MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:43:33,574297475-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:43:33,582935496-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40037\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.162978\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ab2e7fb9-d2ce-40f2-b7dd-f4ae2c4172db\nsetting min_mon_release = octopus\nepoch 0\nfsid ab2e7fb9-d2ce-40f2-b7dd-f4ae2c4172db\nlast_changed 2021-05-16T12:43:43.575678-0700\ncreated 2021-05-16T12:43:43.575678-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40037/0,v1:10.10.1.2:40038/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.162978 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 e999f2f3-1114-4c55-ad2a-4ab14f55fdaf\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 41dcb6bc-a618-4d47-afc2-73120ec23d5c\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 4001c952-2ee5-457f-9b8a-93c67c34c5d0\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42037\n  w/ user/pass: admin / 8ce6a633-ccfa-4c42-8ec1-4fa8c039966a\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:44:01 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40037
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.162978
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ab2e7fb9-d2ce-40f2-b7dd-f4ae2c4172db
setting min_mon_release = octopus
epoch 0
fsid ab2e7fb9-d2ce-40f2-b7dd-f4ae2c4172db
last_changed 2021-05-16T12:43:43.575678-0700
created 2021-05-16T12:43:43.575678-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40037/0,v1:10.10.1.2:40038/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.162978 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 e999f2f3-1114-4c55-ad2a-4ab14f55fdaf
0
start osd.0
add osd1 41dcb6bc-a618-4d47-afc2-73120ec23d5c
1
start osd.1
add osd2 4001c952-2ee5-457f-9b8a-93c67c34c5d0
2
start osd.2


restful urls: https://10.10.1.2:42037
  w/ user/pass: admin / 8ce6a633-ccfa-4c42-8ec1-4fa8c039966a


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T12:43:35.410-0700 7f4f5866c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:43:35.410-0700 7f4f5866c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:43:35.430-0700 7f21b43531c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:43:35.430-0700 7f21b43531c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40037,v1:10.10.1.2:40038] --print /tmp/ceph_monmap.162978 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.162978 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.162978 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42037 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x564f8b6f6000 @  0x7f671a423680 0x7f671a444824 0x7f671abdf187 0x7f671abe7355 0x7f671abdf708 0x7f671abdf877 0x7f671abe0c24 0x7f671abf8ec1 0x7f671ab6b5f3 0x7f671abcce97 0x7f671abd4b1a 0x7f671a2d7d84 0x7f671a3f3609 0x7f6719fc7293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.vCCwYECmT1 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e999f2f3-1114-4c55-ad2a-4ab14f55fdaf -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD6daFg+GW9DxAA9AFMfkknxpTr5UbniVFZsg== --osd-uuid e999f2f3-1114-4c55-ad2a-4ab14f55fdaf 
2021-05-16T12:43:54.898-0700 7f938a190f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:43:54.918-0700 7f938a190f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:43:54.918-0700 7f938a190f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x561828c58000 @  0x7f938ab59680 0x7f938ab7a824 0x56181d891447 0x56181d8994b5 0x56181d8919c8 0x56181d891b37 0x56181d892ee4 0x56181d663ca1 0x56181d83b423 0x56181d6542a7 0x56181d65953a 0x7f938a6acd84 0x7f938a831609 0x7f938a39a293
2021-05-16T12:43:55.234-0700 7f938a190f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 41dcb6bc-a618-4d47-afc2-73120ec23d5c -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x556594460000 @  0x7f878f0d5680 0x7f878f0f6824 0x55658867d447 0x5565886854b5 0x55658867d9c8 0x55658867db37 0x55658867eee4 0x55658844fca1 0x556588627423 0x5565884402a7 0x55658844553a 0x7f878ec28d84 0x7f878edad609 0x7f878e916293
2021-05-16T12:43:55.902-0700 7f878e70cf00 -1 Falling back to public interface
2021-05-16T12:43:56.162-0700 7f878e70cf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD7daFgXxB7IxAAVTbnepGZkKkMMbe36S4k1A== --osd-uuid 41dcb6bc-a618-4d47-afc2-73120ec23d5c 
2021-05-16T12:43:56.282-0700 7f610d66bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:43:56.306-0700 7f610d66bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:43:56.306-0700 7f610d66bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55b231fb4000 @  0x7f610e034680 0x7f610e055824 0x55b2273fe447 0x55b2274064b5 0x55b2273fe9c8 0x55b2273feb37 0x55b2273ffee4 0x55b2271d0ca1 0x55b2273a8423 0x55b2271c12a7 0x55b2271c653a 0x7f610db87d84 0x7f610dd0c609 0x7f610d875293
2021-05-16T12:43:56.622-0700 7f610d66bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4001c952-2ee5-457f-9b8a-93c67c34c5d0 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55e006524000 @  0x7f4f11082680 0x7f4f110a3824 0x55dffb306447 0x55dffb30e4b5 0x55dffb3069c8 0x55dffb306b37 0x55dffb307ee4 0x55dffb0d8ca1 0x55dffb2b0423 0x55dffb0c92a7 0x55dffb0ce53a 0x7f4f10bd5d84 0x7f4f10d5a609 0x7f4f108c3293
2021-05-16T12:43:57.330-0700 7f4f106b9f00 -1 Falling back to public interface
2021-05-16T12:43:57.590-0700 7f4f106b9f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD9daFgf3DrABAAYI7I3XJoWV5Kn6nRaDKieA== --osd-uuid 4001c952-2ee5-457f-9b8a-93c67c34c5d0 
2021-05-16T12:43:57.682-0700 7f8abc84cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:43:57.702-0700 7f8abc84cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:43:57.702-0700 7f8abc84cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5558c1388000 @  0x7f8abd215680 0x7f8abd236824 0x5558b6a7e447 0x5558b6a864b5 0x5558b6a7e9c8 0x5558b6a7eb37 0x5558b6a7fee4 0x5558b6850ca1 0x5558b6a28423 0x5558b68412a7 0x5558b684653a 0x7f8abcd68d84 0x7f8abceed609 0x7f8abca56293
2021-05-16T12:43:58.034-0700 7f8abc84cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5600d9e3e000 @  0x7ff31b91a680 0x7ff31b93b824 0x5600ce84c447 0x5600ce8544b5 0x5600ce84c9c8 0x5600ce84cb37 0x5600ce84dee4 0x5600ce61eca1 0x5600ce7f6423 0x5600ce60f2a7 0x5600ce61453a 0x7ff31b46dd84 0x7ff31b5f2609 0x7ff31b15b293
2021-05-16T12:43:58.726-0700 7ff31af51f00 -1 Falling back to public interface
2021-05-16T12:43:58.998-0700 7ff31af51f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:01,163177336-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:01,174027724-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:01,215763534-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:01,219068484-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c4a2186-232c-4f33-9ec7-7157364115eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c4a2186-232c-4f33-9ec7-7157364115eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:04,672421913-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:04,676352618-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c4a2186-232c-4f33-9ec7-7157364115eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c4a2186-232c-4f33-9ec7-7157364115eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:08,286080490-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:08,289856405-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c4a2186-232c-4f33-9ec7-7157364115eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c4a2186-232c-4f33-9ec7-7157364115eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:11,910650210-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:11,914194550-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c4a2186-232c-4f33-9ec7-7157364115eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c4a2186-232c-4f33-9ec7-7157364115eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:19,119257681-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:19,123176715-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c4a2186-232c-4f33-9ec7-7157364115eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c4a2186-232c-4f33-9ec7-7157364115eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:23,575873025-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:23,580150142-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c4a2186-232c-4f33-9ec7-7157364115eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c4a2186-232c-4f33-9ec7-7157364115eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:28,268171743-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:28,271778449-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c4a2186-232c-4f33-9ec7-7157364115eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c4a2186-232c-4f33-9ec7-7157364115eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:32,650027915-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:32,653376748-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c4a2186-232c-4f33-9ec7-7157364115eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c4a2186-232c-4f33-9ec7-7157364115eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:36,449788796-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:36,453514697-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c4a2186-232c-4f33-9ec7-7157364115eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c4a2186-232c-4f33-9ec7-7157364115eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:40,796178259-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:40,799809181-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c4a2186-232c-4f33-9ec7-7157364115eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c4a2186-232c-4f33-9ec7-7157364115eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:44,495878057-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:44,499931834-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c4a2186-232c-4f33-9ec7-7157364115eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c4a2186-232c-4f33-9ec7-7157364115eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:47,967191993-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:47,970860126-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c4a2186-232c-4f33-9ec7-7157364115eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c4a2186-232c-4f33-9ec7-7157364115eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:44:51,504324802-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:45:14,974258223-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:45:23,440139926-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:45:31,962552920-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:45:40,441334382-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:45:48,925178548-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:45:57,513735717-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   238 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:45:57,522659424-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:06,112676156-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:06,122683610-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:14,717168168-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:14,726326556-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:23,209089882-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:23,218640318-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:31,830551680-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:31,839535571-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:31,846412713-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:31,850291711-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:31,859840483-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=207480
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:31,867158624-04:00] INFO: > Run rados bench[0m
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.3
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:31,887520313-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:31,891300245-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c4a2186-232c-4f33-9ec7-7157364115eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c4a2186-232c-4f33-9ec7-7157364115eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:46:34.309+0000 7f012737fd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:46:34.565+0000 7f012737fd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:46:34.565+0000 7f012737fd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:46:34.581522+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-16T19:46:34.581532+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T19:46:34.636854+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:46:34.636854+0000     0       0         0         0         0         0           -           0
2021-05-16T19:46:35.636957+0000     1      54        54         0         0         0           -           0
2021-05-16T19:46:36.637077+0000     2      54        54         0         0         0           -           0
2021-05-16T19:46:37.637249+0000     3      97        97         0         0         0           -           0
2021-05-16T19:46:38.637322+0000     4      97        97         0         0         0           -           0
2021-05-16T19:46:39.637552+0000     5     197       197         0         0         0           -           0
2021-05-16T19:46:40.637763+0000     6     197       197         0         0         0           -           0
2021-05-16T19:46:41.637882+0000     7     255       262         7  0.999859         1     6.51136     6.51024
2021-05-16T19:46:42.637948+0000     8     255       262         7  0.874885         0           -     6.51024
2021-05-16T19:46:43.638022+0000     9     255       268        13   1.44426         3     8.59222     7.46725
2021-05-16T19:46:44.638089+0000    10     255       268        13   1.29985         0           -     7.46725
2021-05-16T19:46:45.638156+0000    11     255       312        57   5.18123        22     8.64797     9.92174
2021-05-16T19:46:46.638222+0000    12     255       312        57   4.74948         0           -     9.92174
2021-05-16T19:46:47.638297+0000    13     255       379       124   9.53743      33.5      8.6243     9.92479
2021-05-16T19:46:48.638401+0000    14     255       379       124   8.85619         0           -     9.92479
2021-05-16T19:46:49.638467+0000    15     255       379       124    8.2658         0           -     9.92479
2021-05-16T19:46:50.638532+0000    16     255       388       133   8.31165         3     10.6822     9.97646
2021-05-16T19:46:51.638606+0000    17     255       388       133   7.82274         0           -     9.97646
2021-05-16T19:46:52.638673+0000    18     255       438       183   10.1657        25     12.8357       10.76
2021-05-16T19:46:53.638788+0000    19     255       438       183   9.63062         0           -       10.76
2021-05-16T19:46:54.639002+0000 min lat: 6.50982 max lat: 14.9497 avg lat: 11.4487
2021-05-16T19:46:54.639002+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:46:54.639002+0000    20     255       504       249   12.4487        33     12.9002     11.4487
2021-05-16T19:46:55.639238+0000    21     255       504       249   11.8558         0           -     11.4487
2021-05-16T19:46:56.639303+0000    22      14       505       491   22.3157       121     2.09269      9.2962
2021-05-16T19:46:57.639518+0000    23      14       505       491   21.3454         0           -      9.2962
2021-05-16T19:46:58.639788+0000 Total time run:         23.6321
Total writes made:      505
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     21.3692
Stddev Bandwidth:       26.4957
Max bandwidth (MB/sec): 121
Min bandwidth (MB/sec): 0
Average IOPS:           21
Stddev IOPS:            26.4762
Max IOPS:               121
Min IOPS:               0
Average Latency(s):     9.15646
Stddev Latency(s):      3.74038
Max latency(s):         14.9841
Min latency(s):         2.08798

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:59,295531317-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 207480


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:46:59,300611392-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:22,892665064-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 506 objects, 505 MiB
    usage:   1010 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:22,901795760-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:31,449638553-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 506 objects, 505 MiB
    usage:   1010 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:31,458833099-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:40,004985056-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 506 objects, 505 MiB
    usage:   1010 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:40,013936395-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:48,700888582-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 506 objects, 505 MiB
    usage:   1010 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:48,709792963-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:57,350466985-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 506 objects, 505 MiB
    usage:   1010 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:57,359725171-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:57,366737597-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:57,370565800-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:57,380077352-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=208905
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:57,386783844-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:57,406179558-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:47:57,410007110-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c4a2186-232c-4f33-9ec7-7157364115eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c4a2186-232c-4f33-9ec7-7157364115eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.nraorh:/tmp/ceph-asok.nraorh -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:48:00.014+0000 7fe9bd297d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:48:00.290+0000 7fe9bd297d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:48:00.290+0000 7fe9bd297d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:48:00.308010+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:48:00.308010+0000     0       0         0         0         0         0           -           0
2021-05-16T19:48:01.308094+0000     1     123       123         0         0         0           -           0
2021-05-16T19:48:02.308159+0000     2     170       170         0         0         0           -           0
2021-05-16T19:48:03.308244+0000     3     170       170         0         0         0           -           0
2021-05-16T19:48:04.308310+0000     4     211       211         0         0         0           -           0
2021-05-16T19:48:05.308379+0000     5     211       211         0         0         0           -           0
2021-05-16T19:48:06.308445+0000     6     229       229         0         0         0           -           0
2021-05-16T19:48:07.308521+0000     7     229       229         0         0         0           -           0
2021-05-16T19:48:08.308585+0000     8     255       270        15   1.87485     1.875     7.70908     7.70679
2021-05-16T19:48:09.308651+0000     9     255       270        15   1.66654         0           -     7.70679
2021-05-16T19:48:10.308720+0000    10     255       287        32   3.19975       8.5     9.84791     8.84283
2021-05-16T19:48:11.308798+0000    11     255       287        32   2.90886         0           -     8.84283
2021-05-16T19:48:12.308864+0000    12     255       314        59   4.91629      13.5          12     10.2851
2021-05-16T19:48:13.308932+0000    13     255       321        66   5.07654         7     12.0038     10.4672
2021-05-16T19:48:14.308999+0000    14     255       321        66   4.71393         0           -     10.4672
2021-05-16T19:48:15.309077+0000    15     255       348        93   6.19953      13.5     14.1448     11.5336
2021-05-16T19:48:16.309287+0000    16     255       348        93   5.81201         0           -     11.5336
2021-05-16T19:48:17.309354+0000    17     255       355       100   5.88186       3.5     16.2874     11.8662
2021-05-16T19:48:18.309420+0000    18     255       355       100    5.5551         0           -     11.8662
2021-05-16T19:48:19.309496+0000    19     255       357       102   5.36798         1     18.4314      11.995
2021-05-16T19:48:20.309563+0000 min lat: 7.70454 max lat: 18.4314 avg lat: 11.995
2021-05-16T19:48:20.309563+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:48:20.309563+0000    20     255       357       102   5.09959         0           -      11.995
2021-05-16T19:48:21.309802+0000    21     255       377       122   5.80901        10     20.5787      13.402
2021-05-16T19:48:22.309882+0000    22     255       377       122   5.54497         0           -      13.402
2021-05-16T19:48:23.309959+0000    23     255       417       162   7.04286        20      21.444     15.4048
2021-05-16T19:48:24.310029+0000    24     255       417       162   6.74941         0           -     15.4048
2021-05-16T19:48:25.310094+0000    25     255       420       165   6.59943       1.5     23.5764     15.5534
2021-05-16T19:48:26.310161+0000    26     255       420       165   6.34561         0           -     15.5534
2021-05-16T19:48:27.310348+0000    27     255       420       165   6.11057         0           -     15.5534
2021-05-16T19:48:28.310415+0000    28     255       459       204   7.28507        13     23.6261     17.1582
2021-05-16T19:48:29.310482+0000    29     255       459       204   7.03387         0           -     17.1582
2021-05-16T19:48:30.310548+0000    30      89       505       416   13.8655       106   0.0175357     16.0616
2021-05-16T19:48:31.310772+0000    31      89       505       416   13.4181         0           -     16.0616
2021-05-16T19:48:32.310948+0000    32      58       505       447   13.9674      15.5     2.14652     15.1877
2021-05-16T19:48:33.311016+0000    33      58       505       447   13.5442         0           -     15.1877
2021-05-16T19:48:34.311232+0000    34      56       505       449   13.2046         1     4.28177     15.1391
2021-05-16T19:48:35.311313+0000    35      56       505       449   12.8273         0           -     15.1391
2021-05-16T19:48:36.311522+0000    36      33       505       472   13.1098      11.5     12.8895     15.0929
2021-05-16T19:48:37.311590+0000    37      33       505       472   12.7555         0           -     15.0929
2021-05-16T19:48:38.311658+0000    38      12       505       493   12.9724      10.5     8.59516     14.9596
2021-05-16T19:48:39.311736+0000    39      12       505       493   12.6398         0           -     14.9596
2021-05-16T19:48:40.311808+0000 min lat: 0.0175357 max lat: 25.7594 avg lat: 14.9087
2021-05-16T19:48:40.311808+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:48:40.311808+0000    40       6       505       499   12.4738         3     10.7264     14.9087
2021-05-16T19:48:41.311886+0000    41       6       505       499   12.1696         0           -     14.9087
2021-05-16T19:48:42.311954+0000    42       6       505       499   11.8798         0           -     14.9087
2021-05-16T19:48:43.312059+0000    43       2       505       503   11.6966   1.33333     12.8691     14.8925
2021-05-16T19:48:44.312127+0000    44       2       505       503   11.4307         0           -     14.8925
2021-05-16T19:48:45.312259+0000 Total time run:       44.2018
Total reads made:     505
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   11.4249
Average IOPS:         11
Stddev IOPS:          16.388
Max IOPS:             106
Min IOPS:             0
Average Latency(s):   14.893
Max latency(s):       25.7594
Min latency(s):       0.0175357

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:48:45,900422066-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 208905


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:48:45,905223779-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:49:09,533194433-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 506 objects, 505 MiB
    usage:   1010 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:49:09,541679266-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:49:18,104711364-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 506 objects, 505 MiB
    usage:   1010 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:49:18,113646423-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:49:26,668950322-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 506 objects, 505 MiB
    usage:   1010 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:49:26,678285231-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:49:35,122776626-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 506 objects, 505 MiB
    usage:   1010 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:49:35,131581500-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:49:43,656003583-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 506 objects, 505 MiB
    usage:   1010 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:49:43,665032027-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:49:43,672578517-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T15:49:43,675466513-04:00][RUNNING][ROUND 4/5/40] object_size=1MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:49:43,679764379-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:49:43,689708143-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40286\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.164059\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 12d6d371-173d-454c-b038-c2ff47355b33\nsetting min_mon_release = octopus\nepoch 0\nfsid 12d6d371-173d-454c-b038-c2ff47355b33\nlast_changed 2021-05-16T12:49:53.703475-0700\ncreated 2021-05-16T12:49:53.703475-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40286/0,v1:10.10.1.2:40287/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.164059 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 6aff3f0e-4892-446d-8fc9-8a8b1d866ea1\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 1731f1a1-f92f-4c05-8af9-a0906b9ca87b\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 78b6f90e-28a1-4ed8-a100-32e94f2efd99\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42286\n  w/ user/pass: admin / 3bcfd02a-df63-4ea1-a294-1b007059a6a9\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:50:11 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40286
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.164059
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 12d6d371-173d-454c-b038-c2ff47355b33
setting min_mon_release = octopus
epoch 0
fsid 12d6d371-173d-454c-b038-c2ff47355b33
last_changed 2021-05-16T12:49:53.703475-0700
created 2021-05-16T12:49:53.703475-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40286/0,v1:10.10.1.2:40287/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.164059 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 6aff3f0e-4892-446d-8fc9-8a8b1d866ea1
0
start osd.0
add osd1 1731f1a1-f92f-4c05-8af9-a0906b9ca87b
1
start osd.1
add osd2 78b6f90e-28a1-4ed8-a100-32e94f2efd99
2
start osd.2


restful urls: https://10.10.1.2:42286
  w/ user/pass: admin / 3bcfd02a-df63-4ea1-a294-1b007059a6a9


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T12:49:45.529-0700 7fd35ee871c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:49:45.529-0700 7fd35ee871c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:49:45.549-0700 7fbe9d9ee1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:49:45.549-0700 7fbe9d9ee1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40286,v1:10.10.1.2:40287] --print /tmp/ceph_monmap.164059 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.164059 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.164059 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42286 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55bbd5aa6000 @  0x7fc844721680 0x7fc844742824 0x7fc844edd187 0x7fc844ee5355 0x7fc844edd708 0x7fc844edd877 0x7fc844edec24 0x7fc844ef6ec1 0x7fc844e695f3 0x7fc844ecae97 0x7fc844ed2b1a 0x7fc8445d5d84 0x7fc8446f1609 0x7fc8442c5293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.xIg2UPhwQf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6aff3f0e-4892-446d-8fc9-8a8b1d866ea1 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBsd6FgJKNuGxAARbfDg/dvPHCGKReXfEwilQ== --osd-uuid 6aff3f0e-4892-446d-8fc9-8a8b1d866ea1 
2021-05-16T12:50:05.130-0700 7f1dbdd1bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:50:05.150-0700 7f1dbdd1bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:50:05.150-0700 7f1dbdd1bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56189750a000 @  0x7f1dbe6e4680 0x7f1dbe705824 0x56188b71c447 0x56188b7244b5 0x56188b71c9c8 0x56188b71cb37 0x56188b71dee4 0x56188b4eeca1 0x56188b6c6423 0x56188b4df2a7 0x56188b4e453a 0x7f1dbe237d84 0x7f1dbe3bc609 0x7f1dbdf25293
2021-05-16T12:50:05.474-0700 7f1dbdd1bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 1731f1a1-f92f-4c05-8af9-a0906b9ca87b -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55e3837b2000 @  0x7f471f914680 0x7f471f935824 0x55e379354447 0x55e37935c4b5 0x55e3793549c8 0x55e379354b37 0x55e379355ee4 0x55e379126ca1 0x55e3792fe423 0x55e3791172a7 0x55e37911c53a 0x7f471f467d84 0x7f471f5ec609 0x7f471f155293
2021-05-16T12:50:06.146-0700 7f471ef4bf00 -1 Falling back to public interface
2021-05-16T12:50:06.410-0700 7f471ef4bf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBtd6Fg2SPLMRAA5ix+ZVnuBQirzyLdjM+Irw== --osd-uuid 1731f1a1-f92f-4c05-8af9-a0906b9ca87b 
2021-05-16T12:50:06.518-0700 7f548e369f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:50:06.538-0700 7f548e369f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:50:06.538-0700 7f548e369f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55ac948da000 @  0x7f548ed32680 0x7f548ed53824 0x55ac88da8447 0x55ac88db04b5 0x55ac88da89c8 0x55ac88da8b37 0x55ac88da9ee4 0x55ac88b7aca1 0x55ac88d52423 0x55ac88b6b2a7 0x55ac88b7053a 0x7f548e885d84 0x7f548ea0a609 0x7f548e573293
2021-05-16T12:50:06.862-0700 7f548e369f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 78b6f90e-28a1-4ed8-a100-32e94f2efd99 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5632ca2e2000 @  0x7f4cd6c1b680 0x7f4cd6c3c824 0x5632c0391447 0x5632c03994b5 0x5632c03919c8 0x5632c0391b37 0x5632c0392ee4 0x5632c0163ca1 0x5632c033b423 0x5632c01542a7 0x5632c015953a 0x7f4cd676ed84 0x7f4cd68f3609 0x7f4cd645c293
2021-05-16T12:50:07.558-0700 7f4cd6252f00 -1 Falling back to public interface
2021-05-16T12:50:07.814-0700 7f4cd6252f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBvd6Fg5zC2DhAANnJgHHgOjDLkIFOrO6nJWQ== --osd-uuid 78b6f90e-28a1-4ed8-a100-32e94f2efd99 
2021-05-16T12:50:07.918-0700 7f22b5d2af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:50:07.938-0700 7f22b5d2af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:50:07.938-0700 7f22b5d2af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55ae3192e000 @  0x7f22b66f3680 0x7f22b6714824 0x55ae262dd447 0x55ae262e54b5 0x55ae262dd9c8 0x55ae262ddb37 0x55ae262deee4 0x55ae260afca1 0x55ae26287423 0x55ae260a02a7 0x55ae260a553a 0x7f22b6246d84 0x7f22b63cb609 0x7f22b5f34293
2021-05-16T12:50:08.266-0700 7f22b5d2af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x561a123a0000 @  0x7f2858d8e680 0x7f2858daf824 0x561a074c8447 0x561a074d04b5 0x561a074c89c8 0x561a074c8b37 0x561a074c9ee4 0x561a0729aca1 0x561a07472423 0x561a0728b2a7 0x561a0729053a 0x7f28588e1d84 0x7f2858a66609 0x7f28585cf293
2021-05-16T12:50:08.886-0700 7f28583c5f00 -1 Falling back to public interface
2021-05-16T12:50:09.154-0700 7f28583c5f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:11,404586777-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:11,415673980-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:11,457066676-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:11,460026778-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a80e3a42-206f-4522-84b0-f8bfe15b6443', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a80e3a42-206f-4522-84b0-f8bfe15b6443 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:15,026567560-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:15,030171943-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a80e3a42-206f-4522-84b0-f8bfe15b6443', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a80e3a42-206f-4522-84b0-f8bfe15b6443 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:18,666488223-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:18,670272183-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a80e3a42-206f-4522-84b0-f8bfe15b6443', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a80e3a42-206f-4522-84b0-f8bfe15b6443 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:22,323477056-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:22,327086848-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a80e3a42-206f-4522-84b0-f8bfe15b6443', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a80e3a42-206f-4522-84b0-f8bfe15b6443 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:29,573448465-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:29,577397064-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a80e3a42-206f-4522-84b0-f8bfe15b6443', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a80e3a42-206f-4522-84b0-f8bfe15b6443 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:34,083107478-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:34,086194518-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a80e3a42-206f-4522-84b0-f8bfe15b6443', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a80e3a42-206f-4522-84b0-f8bfe15b6443 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:38,562040636-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:38,565588503-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a80e3a42-206f-4522-84b0-f8bfe15b6443', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a80e3a42-206f-4522-84b0-f8bfe15b6443 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:42,710062100-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:42,713712799-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a80e3a42-206f-4522-84b0-f8bfe15b6443', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a80e3a42-206f-4522-84b0-f8bfe15b6443 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:46,971252485-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:46,975554218-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a80e3a42-206f-4522-84b0-f8bfe15b6443', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a80e3a42-206f-4522-84b0-f8bfe15b6443 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:51,652502052-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:51,656467353-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a80e3a42-206f-4522-84b0-f8bfe15b6443', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a80e3a42-206f-4522-84b0-f8bfe15b6443 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:55,986779487-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:55,990107871-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a80e3a42-206f-4522-84b0-f8bfe15b6443', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a80e3a42-206f-4522-84b0-f8bfe15b6443 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:59,542518575-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:50:59,546324195-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a80e3a42-206f-4522-84b0-f8bfe15b6443', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a80e3a42-206f-4522-84b0-f8bfe15b6443 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:51:03,172412437-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:51:26,688670786-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:51:35,308521703-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:51:43,915147772-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:51:52,533502469-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:01,055445801-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:09,579530758-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   245 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:09,588324792-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:18,241120904-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:18,250544952-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:26,731296497-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:26,741171101-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:35,392808698-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:35,402140522-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:44,015675175-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:44,025592259-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:44,032395412-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:44,036453848-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:44,045732832-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=215319
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:44,052407634-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:44,073603641-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:52:44,077535890-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a80e3a42-206f-4522-84b0-f8bfe15b6443', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a80e3a42-206f-4522-84b0-f8bfe15b6443 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:52:46.591+0000 7fa93e9b1d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:52:46.843+0000 7fa93e9b1d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:52:46.843+0000 7fa93e9b1d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:52:46.854471+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-16T19:52:46.854483+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T19:52:46.909684+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:52:46.909684+0000     0       0         0         0         0         0           -           0
2021-05-16T19:52:47.909788+0000     1      49        49         0         0         0           -           0
2021-05-16T19:52:48.909892+0000     2      95        95         0         0         0           -           0
2021-05-16T19:52:49.909984+0000     3      95        95         0         0         0           -           0
2021-05-16T19:52:50.910089+0000     4     153       153         0         0         0           -           0
2021-05-16T19:52:51.910203+0000     5     153       153         0         0         0           -           0
2021-05-16T19:52:52.910279+0000     6     195       195         0         0         0           -           0
2021-05-16T19:52:53.910373+0000     7     195       195         0         0         0           -           0
2021-05-16T19:52:54.910483+0000     8     242       242         0         0         0           -           0
2021-05-16T19:52:55.910574+0000     9     242       242         0         0         0           -           0
2021-05-16T19:52:56.910638+0000    10     255       266        11    1.0999       1.1     9.99167      9.9853
2021-05-16T19:52:57.910778+0000    11     255       266        11  0.999906         0           -      9.9853
2021-05-16T19:52:58.910878+0000    12     255       266        11   0.91658         0           -      9.9853
2021-05-16T19:52:59.910951+0000    13     255       280        25    1.9229   4.66667     12.1397     11.1832
2021-05-16T19:53:00.911024+0000    14     255       280        25   1.78555         0           -     11.1832
2021-05-16T19:53:01.911254+0000    15     255       346        91   6.06606        33     12.9242      12.822
2021-05-16T19:53:02.911525+0000    16     255       346        91   5.68687         0           -      12.822
2021-05-16T19:53:03.911606+0000    17     255       397       142   8.35203      25.5     12.8992     12.9194
2021-05-16T19:53:04.911701+0000    18     255       397       142   7.88803         0           -     12.9194
2021-05-16T19:53:05.911778+0000    19     255       447       192   10.1042        25     12.8892     13.0424
2021-05-16T19:53:06.911905+0000 min lat: 9.98206 max lat: 15.015 avg lat: 13.0424
2021-05-16T19:53:06.911905+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:53:06.911905+0000    20     255       447       192   9.59897         0           -     13.0424
2021-05-16T19:53:07.912019+0000    21       5       448       443    21.093     125.5     2.12601     9.66344
2021-05-16T19:53:08.912134+0000    22       5       448       443   20.1342         0           -     9.66344
2021-05-16T19:53:09.912274+0000 Total time run:         22.8574
Total writes made:      448
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     19.5997
Stddev Bandwidth:       27.6379
Max bandwidth (MB/sec): 125.5
Min bandwidth (MB/sec): 0
Average IOPS:           19
Stddev IOPS:            27.5325
Max IOPS:               125
Min IOPS:               0
Average Latency(s):     9.603
Stddev Latency(s):      4.21615
Max latency(s):         15.015
Min latency(s):         2.10679

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:53:10,461683748-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 215319


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:53:10,466976142-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:53:34,039416363-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 449 objects, 448 MiB
    usage:   896 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:53:34,049178957-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:53:42,690187485-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 449 objects, 448 MiB
    usage:   896 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:53:42,699313653-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:53:51,288264687-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 449 objects, 448 MiB
    usage:   896 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:53:51,297391265-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:53:59,891494881-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 449 objects, 448 MiB
    usage:   896 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:53:59,901053321-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:54:08,529628078-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 449 objects, 448 MiB
    usage:   896 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:54:08,538874191-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:54:08,546408248-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:54:08,550859241-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:54:08,560512449-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=216702
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:54:08,567532329-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:54:08,585561856-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:54:08,587539422-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a80e3a42-206f-4522-84b0-f8bfe15b6443', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a80e3a42-206f-4522-84b0-f8bfe15b6443 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.595E5a:/tmp/ceph-asok.595E5a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:54:10.895+0000 7f2f64cced00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:54:11.171+0000 7f2f64cced00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:54:11.171+0000 7f2f64cced00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:54:11.190028+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:54:11.190028+0000     0       0         0         0         0         0           -           0
2021-05-16T19:54:12.190116+0000     1     140       140         0         0         0           -           0
2021-05-16T19:54:13.190181+0000     2     187       187         0         0         0           -           0
2021-05-16T19:54:14.190245+0000     3     200       200         0         0         0           -           0
2021-05-16T19:54:15.190487+0000     4     200       200         0         0         0           -           0
2021-05-16T19:54:16.190552+0000     5     221       221         0         0         0           -           0
2021-05-16T19:54:17.190616+0000     6     234       234         0         0         0           -           0
2021-05-16T19:54:18.190682+0000     7     242       242         0         0         0           -           0
2021-05-16T19:54:19.190775+0000     8     255       255         0         0         0           -           0
2021-05-16T19:54:20.190839+0000     9     255       283        28   3.11081   3.11111     8.33796     8.33269
2021-05-16T19:54:21.190923+0000    10     255       301        46   4.59955        18     9.40731      8.7521
2021-05-16T19:54:22.190990+0000    11     255       304        49   4.45413         3     10.4761     8.85762
2021-05-16T19:54:23.191231+0000    12     255       322        67   5.58274        18     11.5548     9.58147
2021-05-16T19:54:24.191300+0000    13     255       369       114   8.76832        47     12.6424     10.8394
2021-05-16T19:54:25.191367+0000    14     255       388       133   9.49904        19     13.6921     11.2468
2021-05-16T19:54:26.191582+0000    15     255       412       157   10.4655        24     12.8877     11.6155
2021-05-16T19:54:27.191830+0000    16     255       429       174   10.8737        17     13.9252     11.8418
2021-05-16T19:54:28.191899+0000    17      78       448       370   21.7622       196     2.16391     10.1862
2021-05-16T19:54:29.192062+0000    18      52       448       396   21.9974        26     3.22317     9.79106
2021-05-16T19:54:30.192131+0000    19      51       448       397   20.8923         1     2.15507     9.77183
2021-05-16T19:54:31.192227+0000 min lat: 2.15271 max lat: 14.9939 avg lat: 9.68492
2021-05-16T19:54:31.192227+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:54:31.192227+0000    20      45       448       403   20.1477         6     3.22156     9.68492
2021-05-16T19:54:32.192305+0000    21      40       448       408   19.4264         5     3.21942     9.61093
2021-05-16T19:54:33.192372+0000    22      29       448       419   19.0434        11      4.2936     9.48669
2021-05-16T19:54:34.192441+0000    23      21       448       427   18.5632         8     8.58777     9.45476
2021-05-16T19:54:35.192536+0000    24      18       448       430   17.9147         3     6.43585      9.4337
2021-05-16T19:54:36.192603+0000    25       2       448       446   17.8381        16      7.5184      9.4203
2021-05-16T19:54:37.192671+0000    26       2       448       446   17.1521         0           -      9.4203
2021-05-16T19:54:38.192802+0000 Total time run:       26.5819
Total reads made:     448
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   16.8536
Average IOPS:         16
Stddev IOPS:          38.4125
Max IOPS:             196
Min IOPS:             0
Average Latency(s):   9.42136
Max latency(s):       14.9939
Min latency(s):       2.15271

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:54:38,772895898-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 216702


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:54:38,777798820-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:55:02,393133060-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 449 objects, 448 MiB
    usage:   896 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:55:02,402774335-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:55:10,948567250-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 449 objects, 448 MiB
    usage:   896 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:55:10,957125240-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:55:19,492422207-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 449 objects, 448 MiB
    usage:   896 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:55:19,501891920-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:55:27,996492547-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 449 objects, 448 MiB
    usage:   896 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:55:28,005829581-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:55:36,728209998-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 449 objects, 448 MiB
    usage:   896 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:55:36,737724084-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:55:36,745068835-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T15:55:36,747604680-04:00][RUNNING][ROUND 5/5/40] object_size=1MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:55:36,751645442-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:55:36,761890502-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40965\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.165147\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8bb14344-e6cc-4ea7-a4c4-1ddc478ed928\nsetting min_mon_release = octopus\nepoch 0\nfsid 8bb14344-e6cc-4ea7-a4c4-1ddc478ed928\nlast_changed 2021-05-16T12:55:46.663412-0700\ncreated 2021-05-16T12:55:46.663412-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40965/0,v1:10.10.1.2:40966/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.165147 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 794dc65b-dbdf-4af0-8146-d7c0b224db42\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 71a63b08-efb6-4622-a11d-0a21b71fc5dc\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 fd50aedf-6cc5-4949-a4fe-7cde1b2a0603\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42965\n  w/ user/pass: admin / 049681af-57e0-4f38-9ccd-94dd2d8e4dfe\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:56:04 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40965
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.165147
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8bb14344-e6cc-4ea7-a4c4-1ddc478ed928
setting min_mon_release = octopus
epoch 0
fsid 8bb14344-e6cc-4ea7-a4c4-1ddc478ed928
last_changed 2021-05-16T12:55:46.663412-0700
created 2021-05-16T12:55:46.663412-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40965/0,v1:10.10.1.2:40966/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.165147 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 794dc65b-dbdf-4af0-8146-d7c0b224db42
0
start osd.0
add osd1 71a63b08-efb6-4622-a11d-0a21b71fc5dc
1
start osd.1
add osd2 fd50aedf-6cc5-4949-a4fe-7cde1b2a0603
2
start osd.2


restful urls: https://10.10.1.2:42965
  w/ user/pass: admin / 049681af-57e0-4f38-9ccd-94dd2d8e4dfe


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T12:55:38.589-0700 7fb9c96e71c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:55:38.593-0700 7fb9c96e71c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:55:38.609-0700 7f2ebc9c31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T12:55:38.609-0700 7f2ebc9c31c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40965,v1:10.10.1.2:40966] --print /tmp/ceph_monmap.165147 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.165147 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.165147 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42965 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x56516d446000 @  0x7f5d7932b680 0x7f5d7934c824 0x7f5d79ae7187 0x7f5d79aef355 0x7f5d79ae7708 0x7f5d79ae7877 0x7f5d79ae8c24 0x7f5d79b00ec1 0x7f5d79a735f3 0x7f5d79ad4e97 0x7f5d79adcb1a 0x7f5d791dfd84 0x7f5d792fb609 0x7f5d78ecf293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.vp0242sfta 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 794dc65b-dbdf-4af0-8146-d7c0b224db42 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDNeKFgxhVZDxAAfw723dov6ukdElsPvG+LsA== --osd-uuid 794dc65b-dbdf-4af0-8146-d7c0b224db42 
2021-05-16T12:55:57.913-0700 7f3a3ee19f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:55:57.933-0700 7f3a3ee19f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T12:55:57.933-0700 7f3a3ee19f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55ba990c6000 @  0x7f3a3f7e2680 0x7f3a3f803824 0x55ba8f17c447 0x55ba8f1844b5 0x55ba8f17c9c8 0x55ba8f17cb37 0x55ba8f17dee4 0x55ba8ef4eca1 0x55ba8f126423 0x55ba8ef3f2a7 0x55ba8ef4453a 0x7f3a3f335d84 0x7f3a3f4ba609 0x7f3a3f023293
2021-05-16T12:55:58.253-0700 7f3a3ee19f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 71a63b08-efb6-4622-a11d-0a21b71fc5dc -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55c45284a000 @  0x7f3024e84680 0x7f3024ea5824 0x55c447784447 0x55c44778c4b5 0x55c4477849c8 0x55c447784b37 0x55c447785ee4 0x55c447556ca1 0x55c44772e423 0x55c4475472a7 0x55c44754c53a 0x7f30249d7d84 0x7f3024b5c609 0x7f30246c5293
2021-05-16T12:55:58.865-0700 7f30244bbf00 -1 Falling back to public interface
2021-05-16T12:55:59.125-0700 7f30244bbf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDOeKFg2mYjIRAAbTpjkQ4m21GeLTJk2bCTVw== --osd-uuid 71a63b08-efb6-4622-a11d-0a21b71fc5dc 
2021-05-16T12:55:59.237-0700 7f06d4112f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:55:59.257-0700 7f06d4112f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T12:55:59.257-0700 7f06d4112f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x563ed72c6000 @  0x7f06d4adb680 0x7f06d4afc824 0x563ecb58a447 0x563ecb5924b5 0x563ecb58a9c8 0x563ecb58ab37 0x563ecb58bee4 0x563ecb35cca1 0x563ecb534423 0x563ecb34d2a7 0x563ecb35253a 0x7f06d462ed84 0x7f06d47b3609 0x7f06d431c293
2021-05-16T12:55:59.581-0700 7f06d4112f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new fd50aedf-6cc5-4949-a4fe-7cde1b2a0603 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5635b15a6000 @  0x7f31b4307680 0x7f31b4328824 0x5635a7136447 0x5635a713e4b5 0x5635a71369c8 0x5635a7136b37 0x5635a7137ee4 0x5635a6f08ca1 0x5635a70e0423 0x5635a6ef92a7 0x5635a6efe53a 0x7f31b3e5ad84 0x7f31b3fdf609 0x7f31b3b48293
2021-05-16T12:56:00.237-0700 7f31b393ef00 -1 Falling back to public interface
2021-05-16T12:56:00.497-0700 7f31b393ef00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDPeKFgaAJTNxAAuMYTv2uN2TCN9mrsQpQamQ== --osd-uuid fd50aedf-6cc5-4949-a4fe-7cde1b2a0603 
2021-05-16T12:56:00.601-0700 7ff442da9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:56:00.621-0700 7ff442da9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T12:56:00.621-0700 7ff442da9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55bdefec2000 @  0x7ff443772680 0x7ff443793824 0x55bde5be0447 0x55bde5be84b5 0x55bde5be09c8 0x55bde5be0b37 0x55bde5be1ee4 0x55bde59b2ca1 0x55bde5b8a423 0x55bde59a32a7 0x55bde59a853a 0x7ff4432c5d84 0x7ff44344a609 0x7ff442fb3293
2021-05-16T12:56:00.945-0700 7ff442da9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55d91a46a000 @  0x7fcd6d530680 0x7fcd6d551824 0x55d90fd60447 0x55d90fd684b5 0x55d90fd609c8 0x55d90fd60b37 0x55d90fd61ee4 0x55d90fb32ca1 0x55d90fd0a423 0x55d90fb232a7 0x55d90fb2853a 0x7fcd6d083d84 0x7fcd6d208609 0x7fcd6cd71293
2021-05-16T12:56:01.625-0700 7fcd6cb67f00 -1 Falling back to public interface
2021-05-16T12:56:01.901-0700 7fcd6cb67f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:04,099328322-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:04,110583129-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:04,152197210-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:04,155559538-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '64db90af-86de-4b7b-a927-9255641bf6f7', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 64db90af-86de-4b7b-a927-9255641bf6f7 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:07,807414944-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:07,810784305-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '64db90af-86de-4b7b-a927-9255641bf6f7', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 64db90af-86de-4b7b-a927-9255641bf6f7 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:11,478135156-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:11,481493276-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '64db90af-86de-4b7b-a927-9255641bf6f7', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 64db90af-86de-4b7b-a927-9255641bf6f7 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:15,117729936-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:15,121330461-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '64db90af-86de-4b7b-a927-9255641bf6f7', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 64db90af-86de-4b7b-a927-9255641bf6f7 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:22,614085089-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:22,617448579-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '64db90af-86de-4b7b-a927-9255641bf6f7', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 64db90af-86de-4b7b-a927-9255641bf6f7 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:27,022194664-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:27,026351775-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '64db90af-86de-4b7b-a927-9255641bf6f7', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 64db90af-86de-4b7b-a927-9255641bf6f7 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:31,577840614-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:31,581311866-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '64db90af-86de-4b7b-a927-9255641bf6f7', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 64db90af-86de-4b7b-a927-9255641bf6f7 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:35,959305205-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:35,962873941-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '64db90af-86de-4b7b-a927-9255641bf6f7', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 64db90af-86de-4b7b-a927-9255641bf6f7 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:39,702075146-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:39,705819572-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '64db90af-86de-4b7b-a927-9255641bf6f7', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 64db90af-86de-4b7b-a927-9255641bf6f7 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:43,515265826-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:43,518525591-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '64db90af-86de-4b7b-a927-9255641bf6f7', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 64db90af-86de-4b7b-a927-9255641bf6f7 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:47,800123878-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:47,803676283-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '64db90af-86de-4b7b-a927-9255641bf6f7', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 64db90af-86de-4b7b-a927-9255641bf6f7 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:51,373698109-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:51,377167127-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '64db90af-86de-4b7b-a927-9255641bf6f7', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 64db90af-86de-4b7b-a927-9255641bf6f7 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:56:54,837383784-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:57:18,362579430-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:57:26,875935720-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:57:35,449662855-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:57:43,985479419-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:57:52,578496850-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:01,142894989-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   238 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:01,152457827-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:09,892390348-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:09,902382513-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:18,545033878-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:18,553488073-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:27,168536133-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:27,177523780-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:35,836183334-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:35,844825031-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:35,852280119-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:35,856423204-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:35,865916451-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=223157
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:35,872459254-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:35,891748468-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:58:35,894841460-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '64db90af-86de-4b7b-a927-9255641bf6f7', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 64db90af-86de-4b7b-a927-9255641bf6f7 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T19:58:38.580+0000 7f4f6733ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:58:38.888+0000 7f4f6733ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T19:58:38.888+0000 7f4f6733ed00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T19:58:38.904980+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-16T19:58:38.904993+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T19:58:38.960297+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:58:38.960297+0000     0      30        30         0         0         0           -           0
2021-05-16T19:58:39.960428+0000     1      62        62         0         0         0           -           0
2021-05-16T19:58:40.960503+0000     2      98        98         0         0         0           -           0
2021-05-16T19:58:41.960596+0000     3      98        98         0         0         0           -           0
2021-05-16T19:58:42.960701+0000     4     150       150         0         0         0           -           0
2021-05-16T19:58:43.960769+0000     5     150       150         0         0         0           -           0
2021-05-16T19:58:44.960956+0000     6     195       195         0         0         0           -           0
2021-05-16T19:58:45.961036+0000     7     195       195         0         0         0           -           0
2021-05-16T19:58:46.961140+0000     8     229       229         0         0         0           -           0
2021-05-16T19:58:47.961248+0000     9     240       240         0         0         0           -           0
2021-05-16T19:58:48.961319+0000    10     240       240         0         0         0           -           0
2021-05-16T19:58:49.961561+0000    11     251       251         0         0         0           -           0
2021-05-16T19:58:50.961694+0000    12     251       251         0         0         0           -           0
2021-05-16T19:58:51.961848+0000    13     255       272        17   1.30749   1.30769     12.2758       12.27
2021-05-16T19:58:52.961917+0000    14     255       272        17   1.21411         0           -       12.27
2021-05-16T19:58:53.961984+0000    15     255       288        33   2.19969         8     14.4226     13.3086
2021-05-16T19:58:54.962134+0000    16     255       288        33   2.06221         0           -     13.3086
2021-05-16T19:58:55.962212+0000    17     255       346        91    5.3522        29     15.0705     14.9269
2021-05-16T19:58:56.962283+0000    18     255       346        91   5.05488         0           -     14.9269
2021-05-16T19:58:57.962390+0000    19     255       368       113   5.94658        11     15.0216      15.095
2021-05-16T19:58:58.962499+0000 min lat: 12.2669 max lat: 17.1522 avg lat: 15.095
2021-05-16T19:58:58.962499+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T19:58:58.962499+0000    20     255       368       113   5.64926         0           -      15.095
2021-05-16T19:58:59.962616+0000    21       1       369       368   17.5215     127.5     2.18162     11.6993
2021-05-16T19:59:00.962694+0000    22       1       369       368   16.7251         0           -     11.6993
2021-05-16T19:59:01.962853+0000 Total time run:         22.9896
Total writes made:      369
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     16.0507
Stddev Bandwidth:       27.4842
Max bandwidth (MB/sec): 127.5
Min bandwidth (MB/sec): 0
Average IOPS:           16
Stddev IOPS:            27.3844
Max IOPS:               127
Min IOPS:               0
Average Latency(s):     11.6792
Stddev Latency(s):      4.9601
Max latency(s):         17.1524
Min latency(s):         2.13559

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:59:02,594180073-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 223157


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:59:02,599145022-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:59:26,213438522-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 370 objects, 369 MiB
    usage:   738 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:59:26,222459572-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:59:34,836882674-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 370 objects, 369 MiB
    usage:   738 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:59:34,846480668-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:59:43,340911911-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 370 objects, 369 MiB
    usage:   738 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:59:43,350009045-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:59:51,863484622-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 370 objects, 369 MiB
    usage:   738 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T15:59:51,872383733-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:00:00,363955665-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 370 objects, 369 MiB
    usage:   738 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:00:00,373601489-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:00:00,380907818-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:00:00,384659798-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:00:00,394572633-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=224551
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:00:00,401568769-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:00:00,420913177-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:00:00,424086889-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '64db90af-86de-4b7b-a927-9255641bf6f7', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 64db90af-86de-4b7b-a927-9255641bf6f7 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UDpUeO:/tmp/ceph-asok.UDpUeO -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T20:00:02.948+0000 7f5672d62d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:00:03.208+0000 7f5672d62d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:00:03.208+0000 7f5672d62d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T20:00:03.228123+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:00:03.228123+0000     0       0         0         0         0         0           -           0
2021-05-16T20:00:04.228214+0000     1     169       169         0         0         0           -           0
2021-05-16T20:00:05.228277+0000     2     169       169         0         0         0           -           0
2021-05-16T20:00:06.228340+0000     3     180       180         0         0         0           -           0
2021-05-16T20:00:07.228446+0000     4     180       180         0         0         0           -           0
2021-05-16T20:00:08.228511+0000     5     193       193         0         0         0           -           0
2021-05-16T20:00:09.228576+0000     6     193       193         0         0         0           -           0
2021-05-16T20:00:10.228640+0000     7     210       210         0         0         0           -           0
2021-05-16T20:00:11.228721+0000     8     210       210         0         0         0           -           0
2021-05-16T20:00:12.228952+0000     9     218       218         0         0         0           -           0
2021-05-16T20:00:13.229016+0000    10     218       218         0         0         0           -           0
2021-05-16T20:00:14.229080+0000    11     225       225         0         0         0           -           0
2021-05-16T20:00:15.229248+0000    12     225       225         0         0         0           -           0
2021-05-16T20:00:16.229312+0000    13     255       257         2  0.153831  0.153846     12.7835     12.7834
2021-05-16T20:00:17.229388+0000    14     255       257         2  0.142843         0           -     12.7834
2021-05-16T20:00:18.229561+0000    15     255       273        18   1.19988         8     14.9273     14.6857
2021-05-16T20:00:19.229646+0000    16     255       273        18   1.12489         0           -     14.6857
2021-05-16T20:00:20.229863+0000    17     255       273        18   1.05871         0           -     14.6857
2021-05-16T20:00:21.229977+0000    18     255       311        56   3.11078   12.6667     17.0821     16.3074
2021-05-16T20:00:22.230055+0000    19     255       311        56   2.94706         0           -     16.3074
2021-05-16T20:00:23.230271+0000 min lat: 12.7833 max lat: 19.2237 avg lat: 17.0739
2021-05-16T20:00:23.230271+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:00:23.230271+0000    20     255       331        76   3.79958        10     19.2237     17.0739
2021-05-16T20:00:24.230349+0000    21     255       331        76   3.61865         0           -     17.0739
2021-05-16T20:00:25.230560+0000    22     255       350        95   4.31769       9.5     21.3693     17.9323
2021-05-16T20:00:26.230776+0000    23     255       350        95   4.12994         0           -     17.9323
2021-05-16T20:00:27.230972+0000    24     255       361       106   4.41613       5.5     23.5074     18.5109
2021-05-16T20:00:28.231042+0000    25     255       361       106   4.23949         0           -     18.5109
2021-05-16T20:00:29.231108+0000    26      91       369       278    10.691        86       25.66     17.5409
2021-05-16T20:00:30.231261+0000    27      91       369       278   10.2951         0           -     17.5409
2021-05-16T20:00:31.231344+0000    28      88       369       281   10.0345       1.5     27.8023     17.5744
2021-05-16T20:00:32.231434+0000    29      88       369       281   9.68852         0           -     17.5744
2021-05-16T20:00:33.231627+0000    30      49       369       320   10.6654      19.5     17.1821     17.9737
2021-05-16T20:00:34.231842+0000    31      49       369       320   10.3213         0           -     17.9737
2021-05-16T20:00:35.231926+0000    32      49       369       320   9.99879         0           -     17.9737
2021-05-16T20:00:36.231992+0000    33      33       369       336   10.1806   5.33333     6.44788       17.61
2021-05-16T20:00:37.232058+0000    34      33       369       336   9.88119         0           -       17.61
2021-05-16T20:00:38.232125+0000    35      23       369       346   9.88456         5     17.1792     17.5975
2021-05-16T20:00:39.232206+0000    36      23       369       346      9.61         0           -     17.5975
2021-05-16T20:00:40.232272+0000    37      18       369       351    9.4854       2.5     19.3192     17.5976
2021-05-16T20:00:41.232339+0000    38      18       369       351    9.2358         0           -     17.5976
2021-05-16T20:00:42.232515+0000    39      14       369       355   9.10152         2     19.3271     17.6351
2021-05-16T20:00:43.232598+0000 min lat: 0.0135022 max lat: 29.7476 avg lat: 17.6351
2021-05-16T20:00:43.232598+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:00:43.232598+0000    40      14       369       355   8.87399         0           -     17.6351
2021-05-16T20:00:44.232675+0000    41       3       369       366   8.92582       5.5     17.1829     17.7094
2021-05-16T20:00:45.232741+0000    42       3       369       366   8.71331         0           -     17.7094
2021-05-16T20:00:46.233019+0000 Total time run:       42.8393
Total reads made:     369
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   8.61359
Average IOPS:         8
Stddev IOPS:          13.5925
Max IOPS:             86
Min IOPS:             0
Average Latency(s):   17.7225
Max latency(s):       29.7476
Min latency(s):       0.0135022

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:00:46,813262694-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 224551


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:00:46,817963347-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:01:10,434572243-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 370 objects, 369 MiB
    usage:   738 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:01:10,444241521-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:01:18,959604933-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 370 objects, 369 MiB
    usage:   738 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:01:18,969461885-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:01:27,562176077-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 370 objects, 369 MiB
    usage:   738 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:01:27,570999996-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:01:36,152919867-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 370 objects, 369 MiB
    usage:   738 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:01:36,162468319-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:01:44,799227240-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 370 objects, 369 MiB
    usage:   738 MiB used, 299 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:01:44,808719305-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:01:44,815897984-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T16:01:44,820101192-04:00][RUNNING][ROUND 1/6/40] object_size=4MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:01:44,824137706-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:01:44,833398547-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40727\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.166229\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fa6d39c7-fbc0-4003-ba93-8edbd7acef41\nsetting min_mon_release = octopus\nepoch 0\nfsid fa6d39c7-fbc0-4003-ba93-8edbd7acef41\nlast_changed 2021-05-16T13:01:54.764851-0700\ncreated 2021-05-16T13:01:54.764851-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40727/0,v1:10.10.1.2:40728/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.166229 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 eaff1462-f82b-4bd0-a517-98d74cc98c28\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 c349c75e-106e-4b3f-817d-880385b988da\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 8034e426-cc86-4638-addd-e07ad2217162\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42727\n  w/ user/pass: admin / 4ef85f01-f69f-4e2d-87de-f9a742efcbd7\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:02:12 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40727
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.166229
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fa6d39c7-fbc0-4003-ba93-8edbd7acef41
setting min_mon_release = octopus
epoch 0
fsid fa6d39c7-fbc0-4003-ba93-8edbd7acef41
last_changed 2021-05-16T13:01:54.764851-0700
created 2021-05-16T13:01:54.764851-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40727/0,v1:10.10.1.2:40728/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.166229 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 eaff1462-f82b-4bd0-a517-98d74cc98c28
0
start osd.0
add osd1 c349c75e-106e-4b3f-817d-880385b988da
1
start osd.1
add osd2 8034e426-cc86-4638-addd-e07ad2217162
2
start osd.2


restful urls: https://10.10.1.2:42727
  w/ user/pass: admin / 4ef85f01-f69f-4e2d-87de-f9a742efcbd7


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T13:01:46.716-0700 7f59954721c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:01:46.716-0700 7f59954721c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:01:46.736-0700 7f19e61a41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:01:46.736-0700 7f19e61a41c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40727,v1:10.10.1.2:40728] --print /tmp/ceph_monmap.166229 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.166229 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.166229 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42727 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x555c8b324000 @  0x7f923f772680 0x7f923f793824 0x7f923ff2e187 0x7f923ff36355 0x7f923ff2e708 0x7f923ff2e877 0x7f923ff2fc24 0x7f923ff47ec1 0x7f923feba5f3 0x7f923ff1be97 0x7f923ff23b1a 0x7f923f626d84 0x7f923f742609 0x7f923f316293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.gWzfY0Ydlc 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new eaff1462-f82b-4bd0-a517-98d74cc98c28 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA9eqFg+/DbIBAAkImHctmy0hxzkunDU/PApg== --osd-uuid eaff1462-f82b-4bd0-a517-98d74cc98c28 
2021-05-16T13:02:06.184-0700 7fd9a485bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:02:06.204-0700 7fd9a485bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:02:06.204-0700 7fd9a485bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55b6bba78000 @  0x7fd9a5224680 0x7fd9a5245824 0x55b6b146c447 0x55b6b14744b5 0x55b6b146c9c8 0x55b6b146cb37 0x55b6b146dee4 0x55b6b123eca1 0x55b6b1416423 0x55b6b122f2a7 0x55b6b123453a 0x7fd9a4d77d84 0x7fd9a4efc609 0x7fd9a4a65293
2021-05-16T13:02:06.528-0700 7fd9a485bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c349c75e-106e-4b3f-817d-880385b988da -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5599574e6000 @  0x7f5d039c4680 0x7f5d039e5824 0x55994b640447 0x55994b6484b5 0x55994b6409c8 0x55994b640b37 0x55994b641ee4 0x55994b412ca1 0x55994b5ea423 0x55994b4032a7 0x55994b40853a 0x7f5d03517d84 0x7f5d0369c609 0x7f5d03205293
2021-05-16T13:02:07.188-0700 7f5d02ffbf00 -1 Falling back to public interface
2021-05-16T13:02:07.452-0700 7f5d02ffbf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA+eqFgykyaNBAAggQUiiHOQQ3iSlI085O92Q== --osd-uuid c349c75e-106e-4b3f-817d-880385b988da 
2021-05-16T13:02:07.564-0700 7f6ebbdc1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:02:07.584-0700 7f6ebbdc1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:02:07.584-0700 7f6ebbdc1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5574c04cc000 @  0x7f6ebc78a680 0x7f6ebc7ab824 0x5574b62e9447 0x5574b62f14b5 0x5574b62e99c8 0x5574b62e9b37 0x5574b62eaee4 0x5574b60bbca1 0x5574b6293423 0x5574b60ac2a7 0x5574b60b153a 0x7f6ebc2ddd84 0x7f6ebc462609 0x7f6ebbfcb293
2021-05-16T13:02:07.908-0700 7f6ebbdc1f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8034e426-cc86-4638-addd-e07ad2217162 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55a66f012000 @  0x7f6d981c3680 0x7f6d981e4824 0x55a663cda447 0x55a663ce24b5 0x55a663cda9c8 0x55a663cdab37 0x55a663cdbee4 0x55a663aacca1 0x55a663c84423 0x55a663a9d2a7 0x55a663aa253a 0x7f6d97d16d84 0x7f6d97e9b609 0x7f6d97a04293
2021-05-16T13:02:08.584-0700 7f6d977faf00 -1 Falling back to public interface
2021-05-16T13:02:08.844-0700 7f6d977faf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBAeqFgqi/HDhAAkKYjH7CMt6W1qigymdbzkg== --osd-uuid 8034e426-cc86-4638-addd-e07ad2217162 
2021-05-16T13:02:08.924-0700 7f863f723f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:02:08.948-0700 7f863f723f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:02:08.948-0700 7f863f723f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5575344b4000 @  0x7f86400ec680 0x7f864010d824 0x55752a467447 0x55752a46f4b5 0x55752a4679c8 0x55752a467b37 0x55752a468ee4 0x55752a239ca1 0x55752a411423 0x55752a22a2a7 0x55752a22f53a 0x7f863fc3fd84 0x7f863fdc4609 0x7f863f92d293
2021-05-16T13:02:09.276-0700 7f863f723f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55cc1ca6a000 @  0x7f0f9baeb680 0x7f0f9bb0c824 0x55cc11ce9447 0x55cc11cf14b5 0x55cc11ce99c8 0x55cc11ce9b37 0x55cc11ceaee4 0x55cc11abbca1 0x55cc11c93423 0x55cc11aac2a7 0x55cc11ab153a 0x7f0f9b63ed84 0x7f0f9b7c3609 0x7f0f9b32c293
2021-05-16T13:02:09.944-0700 7f0f9b122f00 -1 Falling back to public interface
2021-05-16T13:02:10.220-0700 7f0f9b122f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:12,411897991-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:12,423332607-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:12,465584045-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:12,469298625-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '892dc0cc-acde-4093-8daf-d27d7eace465', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 892dc0cc-acde-4093-8daf-d27d7eace465 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:16,071069468-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:16,074806350-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '892dc0cc-acde-4093-8daf-d27d7eace465', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 892dc0cc-acde-4093-8daf-d27d7eace465 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:19,819157767-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:19,822375653-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '892dc0cc-acde-4093-8daf-d27d7eace465', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 892dc0cc-acde-4093-8daf-d27d7eace465 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:23,374079252-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:23,378309090-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '892dc0cc-acde-4093-8daf-d27d7eace465', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 892dc0cc-acde-4093-8daf-d27d7eace465 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:30,814006069-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:30,817995905-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '892dc0cc-acde-4093-8daf-d27d7eace465', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 892dc0cc-acde-4093-8daf-d27d7eace465 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:34,933471410-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:34,936732918-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '892dc0cc-acde-4093-8daf-d27d7eace465', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 892dc0cc-acde-4093-8daf-d27d7eace465 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:39,456512234-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:39,459661301-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '892dc0cc-acde-4093-8daf-d27d7eace465', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 892dc0cc-acde-4093-8daf-d27d7eace465 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:43,685629418-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:43,688928527-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '892dc0cc-acde-4093-8daf-d27d7eace465', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 892dc0cc-acde-4093-8daf-d27d7eace465 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:48,059176469-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:48,062324935-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '892dc0cc-acde-4093-8daf-d27d7eace465', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 892dc0cc-acde-4093-8daf-d27d7eace465 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:52,688758301-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:52,692544715-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '892dc0cc-acde-4093-8daf-d27d7eace465', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 892dc0cc-acde-4093-8daf-d27d7eace465 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:56,963061227-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:02:56,966576842-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '892dc0cc-acde-4093-8daf-d27d7eace465', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 892dc0cc-acde-4093-8daf-d27d7eace465 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:03:00,667284015-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:03:00,670867638-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '892dc0cc-acde-4093-8daf-d27d7eace465', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 892dc0cc-acde-4093-8daf-d27d7eace465 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:03:04,215273858-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:03:27,781122301-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:03:36,353608366-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:03:44,921197432-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:03:53,608974366-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:02,333109456-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:11,018243847-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   245 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:11,026948793-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:19,682835730-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:19,692139812-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:28,341741499-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:28,350729718-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:36,885634054-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:36,895237008-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:45,532383103-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:45,541434660-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:45,549009153-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:45,553416133-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:45,563080893-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=230975
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:45,569924532-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:45,591201611-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:04:45,594425959-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '892dc0cc-acde-4093-8daf-d27d7eace465', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 892dc0cc-acde-4093-8daf-d27d7eace465 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47 -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T20:04:47.933+0000 7f94b50d5d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:04:48.209+0000 7f94b50d5d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:04:48.209+0000 7f94b50d5d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T20:04:48.225873+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-16T20:04:48.225886+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T20:04:48.442820+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:04:48.442820+0000     0       0         0         0         0         0           -           0
2021-05-16T20:04:49.442935+0000     1      14        14         0         0         0           -           0
2021-05-16T20:04:50.443190+0000     2      30        30         0         0         0           -           0
2021-05-16T20:04:51.443327+0000     3      30        30         0         0         0           -           0
2021-05-16T20:04:52.443431+0000     4      59        59         0         0         0           -           0
2021-05-16T20:04:53.443570+0000     5      59        59         0         0         0           -           0
2021-05-16T20:04:54.443841+0000     6      79        79         0         0         0           -           0
2021-05-16T20:04:55.443984+0000     7      79        79         0         0         0           -           0
2021-05-16T20:04:56.444109+0000     8      92        92         0         0         0           -           0
2021-05-16T20:04:57.444250+0000     9      92        92         0         0         0           -           0
2021-05-16T20:04:58.444349+0000    10      92        92         0         0         0           -           0
2021-05-16T20:04:59.444486+0000    11     105       105         0         0         0           -           0
2021-05-16T20:05:00.444616+0000    12     105       105         0         0         0           -           0
2021-05-16T20:05:01.444720+0000    13     120       120         0         0         0           -           0
2021-05-16T20:05:02.444989+0000    14     120       120         0         0         0           -           0
2021-05-16T20:05:03.445238+0000    15     122       122         0         0         0           -           0
2021-05-16T20:05:04.445503+0000    16     122       122         0         0         0           -           0
2021-05-16T20:05:05.445777+0000    17     133       133         0         0         0           -           0
2021-05-16T20:05:06.445913+0000    18     133       133         0         0         0           -           0
2021-05-16T20:05:07.446016+0000    19     144       144         0         0         0           -           0
2021-05-16T20:05:08.446147+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:05:08.446147+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:05:08.446147+0000    20     144       144         0         0         0           -           0
2021-05-16T20:05:09.446259+0000    21     151       151         0         0         0           -           0
2021-05-16T20:05:10.446375+0000    22     151       151         0         0         0           -           0
2021-05-16T20:05:11.446487+0000    23     168       168         0         0         0           -           0
2021-05-16T20:05:12.446625+0000    24     168       168         0         0         0           -           0
2021-05-16T20:05:13.446730+0000    25     168       168         0         0         0           -           0
2021-05-16T20:05:14.446980+0000    26     183       183         0         0         0           -           0
2021-05-16T20:05:15.447262+0000    27     183       183         0         0         0           -           0
2021-05-16T20:05:16.447519+0000    28     190       190         0         0         0           -           0
2021-05-16T20:05:17.447825+0000    29     190       190         0         0         0           -           0
2021-05-16T20:05:18.447928+0000    30     192       192         0         0         0           -           0
2021-05-16T20:05:19.448027+0000    31     192       192         0         0         0           -           0
2021-05-16T20:05:20.448131+0000    32     210       210         0         0         0           -           0
2021-05-16T20:05:21.448265+0000    33     210       210         0         0         0           -           0
2021-05-16T20:05:22.448369+0000    34     228       228         0         0         0           -           0
2021-05-16T20:05:23.448504+0000    35     228       228         0         0         0           -           0
2021-05-16T20:05:24.448622+0000    36     250       250         0         0         0           -           0
2021-05-16T20:05:25.448765+0000    37     250       250         0         0         0           -           0
2021-05-16T20:05:26.448988+0000    38       4       256       252   26.5221   26.5263   0.0205747     21.1295
2021-05-16T20:05:27.449270+0000    39       4       256       252   25.8419         0           -     21.1295
2021-05-16T20:05:28.449521+0000 min lat: 0.0162368 max lat: 37.9426 avg lat: 21.1295
2021-05-16T20:05:28.449521+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:05:28.449521+0000    40       4       256       252   25.1958         0           -     21.1295
2021-05-16T20:05:29.449712+0000 Total time run:         40.0765
Total writes made:      256
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     25.5511
Stddev Bandwidth:       4.19418
Max bandwidth (MB/sec): 26.5263
Min bandwidth (MB/sec): 0
Average IOPS:           6
Stddev IOPS:            0.948683
Max IOPS:               6
Min IOPS:               0
Average Latency(s):     20.8406
Stddev Latency(s):      12.3926
Max latency(s):         37.9426
Min latency(s):         0.0162368

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:05:30,022819343-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 230975


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:05:30,027946377-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:05:53,595303709-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:05:53,604958350-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:06:02,116399256-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:06:02,125949411-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:06:10,728500330-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:06:10,737799022-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:06:19,343308602-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:06:19,352811948-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:06:27,973222777-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:06:27,982739078-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:06:27,990453032-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:06:27,994605725-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:06:28,003850957-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=232373
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:06:28,010454935-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:06:28,030534574-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:06:28,033940173-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '892dc0cc-acde-4093-8daf-d27d7eace465', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 892dc0cc-acde-4093-8daf-d27d7eace465 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.f3bB47:/tmp/ceph-asok.f3bB47 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T20:06:30.482+0000 7fedef560d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:06:30.750+0000 7fedef560d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:06:30.750+0000 7fedef560d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T20:06:30.769738+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:06:30.769738+0000     0       0         0         0         0         0           -           0
2021-05-16T20:06:31.769876+0000     1      28        28         0         0         0           -           0
2021-05-16T20:06:32.769945+0000     2      28        28         0         0         0           -           0
2021-05-16T20:06:33.770030+0000     3      38        38         0         0         0           -           0
2021-05-16T20:06:34.770101+0000     4      38        38         0         0         0           -           0
2021-05-16T20:06:35.770210+0000     5      41        41         0         0         0           -           0
2021-05-16T20:06:36.770275+0000     6      41        41         0         0         0           -           0
2021-05-16T20:06:37.770345+0000     7      46        46         0         0         0           -           0
2021-05-16T20:06:38.770419+0000     8      46        46         0         0         0           -           0
2021-05-16T20:06:39.770489+0000     9      53        53         0         0         0           -           0
2021-05-16T20:06:40.770617+0000    10      53        53         0         0         0           -           0
2021-05-16T20:06:41.770684+0000    11      60        60         0         0         0           -           0
2021-05-16T20:06:42.770747+0000    12      60        60         0         0         0           -           0
2021-05-16T20:06:43.770965+0000    13      61        61         0         0         0           -           0
2021-05-16T20:06:44.771030+0000    14      61        61         0         0         0           -           0
2021-05-16T20:06:45.771097+0000    15      62        62         0         0         0           -           0
2021-05-16T20:06:46.771188+0000    16      62        62         0         0         0           -           0
2021-05-16T20:06:47.771258+0000    17      62        62         0         0         0           -           0
2021-05-16T20:06:48.771322+0000    18      63        63         0         0         0           -           0
2021-05-16T20:06:49.771534+0000    19      63        63         0         0         0           -           0
2021-05-16T20:06:50.771751+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:06:50.771751+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:06:50.771751+0000    20      66        66         0         0         0           -           0
2021-05-16T20:06:51.771846+0000    21      66        66         0         0         0           -           0
2021-05-16T20:06:52.771911+0000    22      78        78         0         0         0           -           0
2021-05-16T20:06:53.771977+0000    23      78        78         0         0         0           -           0
2021-05-16T20:06:54.772042+0000    24      79        79         0         0         0           -           0
2021-05-16T20:06:55.772110+0000    25      79        79         0         0         0           -           0
2021-05-16T20:06:56.772177+0000    26      86        86         0         0         0           -           0
2021-05-16T20:06:57.772248+0000    27      86        86         0         0         0           -           0
2021-05-16T20:06:58.772318+0000    28      90        90         0         0         0           -           0
2021-05-16T20:06:59.772385+0000    29      90        90         0         0         0           -           0
2021-05-16T20:07:00.772449+0000    30      90        90         0         0         0           -           0
2021-05-16T20:07:01.772513+0000    31      90        90         0         0         0           -           0
2021-05-16T20:07:02.772578+0000    32      90        90         0         0         0           -           0
2021-05-16T20:07:03.772646+0000    33      90        90         0         0         0           -           0
2021-05-16T20:07:04.772714+0000    34      90        90         0         0         0           -           0
2021-05-16T20:07:05.772779+0000    35      95        95         0         0         0           -           0
2021-05-16T20:07:06.772947+0000    36      95        95         0         0         0           -           0
2021-05-16T20:07:07.773015+0000    37      95        95         0         0         0           -           0
2021-05-16T20:07:08.773247+0000    38      95        95         0         0         0           -           0
2021-05-16T20:07:09.773315+0000    39      98        98         0         0         0           -           0
2021-05-16T20:07:10.773490+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:07:10.773490+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:07:10.773490+0000    40      98        98         0         0         0           -           0
2021-05-16T20:07:11.773570+0000    41     101       101         0         0         0           -           0
2021-05-16T20:07:12.773636+0000    42     101       101         0         0         0           -           0
2021-05-16T20:07:13.773702+0000    43     109       109         0         0         0           -           0
2021-05-16T20:07:14.773884+0000    44     109       109         0         0         0           -           0
2021-05-16T20:07:15.773954+0000    45     109       109         0         0         0           -           0
2021-05-16T20:07:16.774020+0000    46     130       130         0         0         0           -           0
2021-05-16T20:07:17.774087+0000    47     130       130         0         0         0           -           0
2021-05-16T20:07:18.774236+0000    48     130       130         0         0         0           -           0
2021-05-16T20:07:19.774304+0000    49     130       130         0         0         0           -           0
2021-05-16T20:07:20.774508+0000    50     142       142         0         0         0           -           0
2021-05-16T20:07:21.774575+0000    51     142       142         0         0         0           -           0
2021-05-16T20:07:22.774644+0000    52     144       144         0         0         0           -           0
2021-05-16T20:07:23.774713+0000    53     144       144         0         0         0           -           0
2021-05-16T20:07:24.774778+0000    54     150       150         0         0         0           -           0
2021-05-16T20:07:25.774986+0000    55     150       150         0         0         0           -           0
2021-05-16T20:07:26.775215+0000    56     160       160         0         0         0           -           0
2021-05-16T20:07:27.775282+0000    57     160       160         0         0         0           -           0
2021-05-16T20:07:28.775493+0000    58     160       160         0         0         0           -           0
2021-05-16T20:07:29.775557+0000    59     160       160         0         0         0           -           0
2021-05-16T20:07:30.775673+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:07:30.775673+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:07:30.775673+0000    60     160       160         0         0         0           -           0
2021-05-16T20:07:31.775884+0000    61     166       166         0         0         0           -           0
2021-05-16T20:07:32.775948+0000    62     166       166         0         0         0           -           0
2021-05-16T20:07:33.776012+0000    63     170       170         0         0         0           -           0
2021-05-16T20:07:34.776077+0000    64     170       170         0         0         0           -           0
2021-05-16T20:07:35.776144+0000    65     176       176         0         0         0           -           0
2021-05-16T20:07:36.776209+0000    66     176       176         0         0         0           -           0
2021-05-16T20:07:37.776275+0000    67     182       182         0         0         0           -           0
2021-05-16T20:07:38.776354+0000    68     182       182         0         0         0           -           0
2021-05-16T20:07:39.776442+0000    69     190       190         0         0         0           -           0
2021-05-16T20:07:40.776518+0000    70     190       190         0         0         0           -           0
2021-05-16T20:07:41.776585+0000    71     190       190         0         0         0           -           0
2021-05-16T20:07:42.776652+0000    72     190       190         0         0         0           -           0
2021-05-16T20:07:43.776721+0000    73     195       195         0         0         0           -           0
2021-05-16T20:07:44.776965+0000    74     195       195         0         0         0           -           0
2021-05-16T20:07:45.777179+0000    75     195       195         0         0         0           -           0
2021-05-16T20:07:46.777263+0000    76     199       199         0         0         0           -           0
2021-05-16T20:07:47.777480+0000    77     199       199         0         0         0           -           0
2021-05-16T20:07:48.777552+0000    78     205       205         0         0         0           -           0
2021-05-16T20:07:49.777618+0000    79     205       205         0         0         0           -           0
2021-05-16T20:07:50.777686+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:07:50.777686+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:07:50.777686+0000    80     207       207         0         0         0           -           0
2021-05-16T20:07:51.777869+0000    81     207       207         0         0         0           -           0
2021-05-16T20:07:52.778085+0000    82     209       209         0         0         0           -           0
2021-05-16T20:07:53.778304+0000    83     209       209         0         0         0           -           0
2021-05-16T20:07:54.778482+0000    84     210       210         0         0         0           -           0
2021-05-16T20:07:55.778549+0000    85     210       210         0         0         0           -           0
2021-05-16T20:07:56.778615+0000    86     220       220         0         0         0           -           0
2021-05-16T20:07:57.778823+0000    87     220       220         0         0         0           -           0
2021-05-16T20:07:58.778888+0000    88     220       220         0         0         0           -           0
2021-05-16T20:07:59.778988+0000    89     221       221         0         0         0           -           0
2021-05-16T20:08:00.779060+0000    90     221       221         0         0         0           -           0
2021-05-16T20:08:01.779230+0000    91     225       225         0         0         0           -           0
2021-05-16T20:08:02.779296+0000    92     225       225         0         0         0           -           0
2021-05-16T20:08:03.779365+0000    93     228       228         0         0         0           -           0
2021-05-16T20:08:04.779430+0000    94     228       228         0         0         0           -           0
2021-05-16T20:08:05.779495+0000    95      20       256       236   9.93581   9.93684   0.0719258     52.6511
2021-05-16T20:08:06.779708+0000    96      20       256       236    9.8323         0           -     52.6511
2021-05-16T20:08:07.779861+0000    97      11       256       245    10.102        18     2.12703     50.7955
2021-05-16T20:08:08.779930+0000    98      11       256       245   9.99895         0           -     50.7955
2021-05-16T20:08:09.779996+0000    99       8       256       248   10.0192         6      8.5893     50.2501
2021-05-16T20:08:10.780065+0000 min lat: 0.0719258 max lat: 94.5004 avg lat: 50.2501
2021-05-16T20:08:10.780065+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:08:10.780065+0000   100       8       256       248   9.91897         0           -     50.2501
2021-05-16T20:08:11.780148+0000   101       7       256       249   9.86036         2     6.41513      50.074
2021-05-16T20:08:12.780220+0000   102       7       256       249   9.76369         0           -      50.074
2021-05-16T20:08:13.780293+0000   103       7       256       249    9.6689         0           -      50.074
2021-05-16T20:08:14.780363+0000   104       5       256       251   9.65285   2.66667     8.58546     49.7435
2021-05-16T20:08:15.780433+0000   105       5       256       251   9.56092         0           -     49.7435
2021-05-16T20:08:16.780500+0000   106       2       256       254   9.58393         6     10.7203     49.2825
2021-05-16T20:08:17.780568+0000   107       2       256       254   9.49436         0           -     49.2825
2021-05-16T20:08:18.780641+0000   108       1       256       255   9.44348         2     12.8627     49.1397
2021-05-16T20:08:19.780710+0000   109       1       256       255   9.35685         0           -     49.1397
2021-05-16T20:08:20.780860+0000 Total time run:       109.492
Total reads made:     256
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   9.35227
Average IOPS:         2
Stddev IOPS:          0.445272
Max IOPS:             4
Min IOPS:             0
Average Latency(s):   49.0063
Max latency(s):       94.5004
Min latency(s):       0.0719258

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:08:21,452029622-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 232373


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:08:21,457018686-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:08:45,084586180-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:08:45,094602510-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:08:53,709743570-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:08:53,719346132-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:09:02,496577114-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:09:02,505971515-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:09:11,245375712-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:09:11,254491791-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:09:19,787934193-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:09:19,797401542-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:09:19,804538272-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T16:09:19,807581830-04:00][RUNNING][ROUND 2/6/40] object_size=4MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:09:19,811796099-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:09:19,820911737-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40983\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.167318\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 4e644659-5d1c-4588-b57d-f7889c46b2af\nsetting min_mon_release = octopus\nepoch 0\nfsid 4e644659-5d1c-4588-b57d-f7889c46b2af\nlast_changed 2021-05-16T13:09:36.148140-0700\ncreated 2021-05-16T13:09:36.148140-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40983/0,v1:10.10.1.2:40984/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.167318 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 de011969-b08d-4ffb-80bf-6963ad5eed4b\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 43e83262-9f9c-43d2-94cb-8891b9459a18\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 45df57d2-1e49-4ed7-aa08-e9ec31869374\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42983\n  w/ user/pass: admin / 8feb8ff1-a85d-4476-9671-d3cb3508c84c\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 16:09:53 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40983
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.167318
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 4e644659-5d1c-4588-b57d-f7889c46b2af
setting min_mon_release = octopus
epoch 0
fsid 4e644659-5d1c-4588-b57d-f7889c46b2af
last_changed 2021-05-16T13:09:36.148140-0700
created 2021-05-16T13:09:36.148140-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40983/0,v1:10.10.1.2:40984/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.167318 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 de011969-b08d-4ffb-80bf-6963ad5eed4b
0
start osd.0
add osd1 43e83262-9f9c-43d2-94cb-8891b9459a18
1
start osd.1
add osd2 45df57d2-1e49-4ed7-aa08-e9ec31869374
2
start osd.2


restful urls: https://10.10.1.2:42983
  w/ user/pass: admin / 8feb8ff1-a85d-4476-9671-d3cb3508c84c


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T13:09:21.660-0700 7fd4ebe4b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:09:21.660-0700 7fd4ebe4b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:09:21.676-0700 7fcf03d9a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:09:21.676-0700 7fcf03d9a1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40983,v1:10.10.1.2:40984] --print /tmp/ceph_monmap.167318 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.167318 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.167318 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42983 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55764bde8000 @  0x7fbfa1097680 0x7fbfa10b8824 0x7fbfa1853187 0x7fbfa185b355 0x7fbfa1853708 0x7fbfa1853877 0x7fbfa1854c24 0x7fbfa186cec1 0x7fbfa17df5f3 0x7fbfa1840e97 0x7fbfa1848b1a 0x7fbfa0f4bd84 0x7fbfa1067609 0x7fbfa0c3b293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.aFokM8l7mG 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new de011969-b08d-4ffb-80bf-6963ad5eed4b -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQALfKFg/BRQAhAAvC9wXQkFdFaVpC1cae78jg== --osd-uuid de011969-b08d-4ffb-80bf-6963ad5eed4b 
2021-05-16T13:09:47.696-0700 7f2a617eff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:09:47.716-0700 7f2a617eff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:09:47.716-0700 7f2a617eff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x555b21f0e000 @  0x7f2a621b8680 0x7f2a621d9824 0x555b17a60447 0x555b17a684b5 0x555b17a609c8 0x555b17a60b37 0x555b17a61ee4 0x555b17832ca1 0x555b17a0a423 0x555b178232a7 0x555b1782853a 0x7f2a61d0bd84 0x7f2a61e90609 0x7f2a619f9293
2021-05-16T13:09:48.060-0700 7f2a617eff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 43e83262-9f9c-43d2-94cb-8891b9459a18 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x562bbb212000 @  0x7f14e94d6680 0x7f14e94f7824 0x562bb0648447 0x562bb06504b5 0x562bb06489c8 0x562bb0648b37 0x562bb0649ee4 0x562bb041aca1 0x562bb05f2423 0x562bb040b2a7 0x562bb041053a 0x7f14e9029d84 0x7f14e91ae609 0x7f14e8d17293
2021-05-16T13:09:48.696-0700 7f14e8b0df00 -1 Falling back to public interface
2021-05-16T13:09:48.960-0700 7f14e8b0df00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAMfKFg8pQsFxAAdQgx96JjyUhwqluIpjDnrA== --osd-uuid 43e83262-9f9c-43d2-94cb-8891b9459a18 
2021-05-16T13:09:49.040-0700 7fbad42a8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:09:49.060-0700 7fbad42a8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:09:49.060-0700 7fbad42a8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55f1fb902000 @  0x7fbad4c71680 0x7fbad4c92824 0x55f1efab3447 0x55f1efabb4b5 0x55f1efab39c8 0x55f1efab3b37 0x55f1efab4ee4 0x55f1ef885ca1 0x55f1efa5d423 0x55f1ef8762a7 0x55f1ef87b53a 0x7fbad47c4d84 0x7fbad4949609 0x7fbad44b2293
2021-05-16T13:09:49.388-0700 7fbad42a8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 45df57d2-1e49-4ed7-aa08-e9ec31869374 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5606fd328000 @  0x7fd459dba680 0x7fd459ddb824 0x5606f32ce447 0x5606f32d64b5 0x5606f32ce9c8 0x5606f32ceb37 0x5606f32cfee4 0x5606f30a0ca1 0x5606f3278423 0x5606f30912a7 0x5606f309653a 0x7fd45990dd84 0x7fd459a92609 0x7fd4595fb293
2021-05-16T13:09:50.104-0700 7fd4593f1f00 -1 Falling back to public interface
2021-05-16T13:09:50.360-0700 7fd4593f1f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQANfKFgvb+GLxAALPa7nbxxyQU661VJLj1tTQ== --osd-uuid 45df57d2-1e49-4ed7-aa08-e9ec31869374 
2021-05-16T13:09:50.476-0700 7f97fb70af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:09:50.500-0700 7f97fb70af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:09:50.500-0700 7f97fb70af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x555c23bcc000 @  0x7f97fc0d3680 0x7f97fc0f4824 0x555c18c57447 0x555c18c5f4b5 0x555c18c579c8 0x555c18c57b37 0x555c18c58ee4 0x555c18a29ca1 0x555c18c01423 0x555c18a1a2a7 0x555c18a1f53a 0x7f97fbc26d84 0x7f97fbdab609 0x7f97fb914293
2021-05-16T13:09:50.848-0700 7f97fb70af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5560aeed0000 @  0x7f1bcb94c680 0x7f1bcb96d824 0x5560a3460447 0x5560a34684b5 0x5560a34609c8 0x5560a3460b37 0x5560a3461ee4 0x5560a3232ca1 0x5560a340a423 0x5560a32232a7 0x5560a322853a 0x7f1bcb49fd84 0x7f1bcb624609 0x7f1bcb18d293
2021-05-16T13:09:51.492-0700 7f1bcaf83f00 -1 Falling back to public interface
2021-05-16T13:09:51.764-0700 7f1bcaf83f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:09:53,959121026-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:09:53,970354534-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:09:54,011785861-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:09:54,014764739-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aa5a0494-66cc-484d-9cf9-c7ff73b29c36', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aa5a0494-66cc-484d-9cf9-c7ff73b29c36 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:09:57,799731317-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:09:57,803404038-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aa5a0494-66cc-484d-9cf9-c7ff73b29c36', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aa5a0494-66cc-484d-9cf9-c7ff73b29c36 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:01,572409476-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:01,576163710-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aa5a0494-66cc-484d-9cf9-c7ff73b29c36', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aa5a0494-66cc-484d-9cf9-c7ff73b29c36 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:05,299249166-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:05,302755955-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aa5a0494-66cc-484d-9cf9-c7ff73b29c36', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aa5a0494-66cc-484d-9cf9-c7ff73b29c36 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:12,622265324-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:12,625698946-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aa5a0494-66cc-484d-9cf9-c7ff73b29c36', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aa5a0494-66cc-484d-9cf9-c7ff73b29c36 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:17,173882923-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:17,177641726-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aa5a0494-66cc-484d-9cf9-c7ff73b29c36', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aa5a0494-66cc-484d-9cf9-c7ff73b29c36 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:21,721780172-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:21,725573149-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aa5a0494-66cc-484d-9cf9-c7ff73b29c36', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aa5a0494-66cc-484d-9cf9-c7ff73b29c36 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:26,290884619-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:26,294874455-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aa5a0494-66cc-484d-9cf9-c7ff73b29c36', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aa5a0494-66cc-484d-9cf9-c7ff73b29c36 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:30,769909900-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:30,773710951-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aa5a0494-66cc-484d-9cf9-c7ff73b29c36', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aa5a0494-66cc-484d-9cf9-c7ff73b29c36 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:35,294738093-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:35,298221478-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aa5a0494-66cc-484d-9cf9-c7ff73b29c36', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aa5a0494-66cc-484d-9cf9-c7ff73b29c36 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:39,167495769-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:39,171621732-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aa5a0494-66cc-484d-9cf9-c7ff73b29c36', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aa5a0494-66cc-484d-9cf9-c7ff73b29c36 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:42,770055622-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:42,773486038-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aa5a0494-66cc-484d-9cf9-c7ff73b29c36', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aa5a0494-66cc-484d-9cf9-c7ff73b29c36 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:10:46,301905174-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:11:10,030742943-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:11:18,736692860-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:11:27,556484253-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:11:36,210827655-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:11:44,925972291-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:11:53,574860674-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:11:53,584202618-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:12:02,157099276-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:12:02,166396716-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:12:10,794584755-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:12:10,803732834-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:12:19,427889510-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:12:19,437732565-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:12:28,090583356-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:12:28,099959954-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:12:28,107069924-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:12:28,111404057-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:12:28,121707437-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=238789
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:12:28,129349386-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:12:28,149264847-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:12:28,152226992-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aa5a0494-66cc-484d-9cf9-c7ff73b29c36', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aa5a0494-66cc-484d-9cf9-c7ff73b29c36 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9 -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T20:12:30.759+0000 7f87d5e58d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:12:31.087+0000 7f87d5e58d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:12:31.087+0000 7f87d5e58d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T20:12:31.100259+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-16T20:12:31.100270+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_8
2021-05-16T20:12:31.314285+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:12:31.314285+0000     0       0         0         0         0         0           -           0
2021-05-16T20:12:32.314388+0000     1      12        12         0         0         0           -           0
2021-05-16T20:12:33.314454+0000     2      33        33         0         0         0           -           0
2021-05-16T20:12:34.314523+0000     3      33        33         0         0         0           -           0
2021-05-16T20:12:35.314602+0000     4      45        45         0         0         0           -           0
2021-05-16T20:12:36.314667+0000     5      45        45         0         0         0           -           0
2021-05-16T20:12:37.314732+0000     6      50        50         0         0         0           -           0
2021-05-16T20:12:38.314798+0000     7      50        50         0         0         0           -           0
2021-05-16T20:12:39.314880+0000     8      64        64         0         0         0           -           0
2021-05-16T20:12:40.314946+0000     9      64        64         0         0         0           -           0
2021-05-16T20:12:41.315183+0000    10      76        76         0         0         0           -           0
2021-05-16T20:12:42.315271+0000    11      76        76         0         0         0           -           0
2021-05-16T20:12:43.315517+0000    12      76        76         0         0         0           -           0
2021-05-16T20:12:44.315590+0000    13      93        93         0         0         0           -           0
2021-05-16T20:12:45.315808+0000    14      93        93         0         0         0           -           0
2021-05-16T20:12:46.315875+0000    15     101       101         0         0         0           -           0
2021-05-16T20:12:47.315952+0000    16     101       101         0         0         0           -           0
2021-05-16T20:12:48.316019+0000    17     107       107         0         0         0           -           0
2021-05-16T20:12:49.316084+0000    18     107       107         0         0         0           -           0
2021-05-16T20:12:50.316151+0000    19     113       113         0         0         0           -           0
2021-05-16T20:12:51.316228+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:12:51.316228+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:12:51.316228+0000    20     113       113         0         0         0           -           0
2021-05-16T20:12:52.316304+0000    21     124       124         0         0         0           -           0
2021-05-16T20:12:53.316373+0000    22     124       124         0         0         0           -           0
2021-05-16T20:12:54.316439+0000    23     124       124         0         0         0           -           0
2021-05-16T20:12:55.316516+0000    24     124       124         0         0         0           -           0
2021-05-16T20:12:56.316579+0000    25     133       133         0         0         0           -           0
2021-05-16T20:12:57.316647+0000    26     133       133         0         0         0           -           0
2021-05-16T20:12:58.316713+0000    27     133       133         0         0         0           -           0
2021-05-16T20:12:59.316952+0000    28     141       141         0         0         0           -           0
2021-05-16T20:13:00.317182+0000    29     141       141         0         0         0           -           0
2021-05-16T20:13:01.317268+0000    30     146       146         0         0         0           -           0
2021-05-16T20:13:02.317333+0000    31     146       146         0         0         0           -           0
2021-05-16T20:13:03.317580+0000    32     160       160         0         0         0           -           0
2021-05-16T20:13:04.317785+0000    33     160       160         0         0         0           -           0
2021-05-16T20:13:05.317852+0000    34     178       178         0         0         0           -           0
2021-05-16T20:13:06.317920+0000    35     178       178         0         0         0           -           0
2021-05-16T20:13:07.317999+0000    36     181       181         0         0         0           -           0
2021-05-16T20:13:08.318065+0000    37     181       181         0         0         0           -           0
2021-05-16T20:13:09.318131+0000    38     186       186         0         0         0           -           0
2021-05-16T20:13:10.318197+0000    39     186       186         0         0         0           -           0
2021-05-16T20:13:11.318276+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:13:11.318276+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:13:11.318276+0000    40     192       192         0         0         0           -           0
2021-05-16T20:13:12.318351+0000    41     201       201         0         0         0           -           0
2021-05-16T20:13:13.318564+0000    42     201       201         0         0         0           -           0
2021-05-16T20:13:14.318695+0000    43     201       201         0         0         0           -           0
2021-05-16T20:13:15.318775+0000    44     201       201         0         0         0           -           0
2021-05-16T20:13:16.318989+0000    45     206       206         0         0         0           -           0
2021-05-16T20:13:17.319122+0000    46     206       206         0         0         0           -           0
2021-05-16T20:13:18.319191+0000    47     225       225         0         0         0           -           0
2021-05-16T20:13:19.319269+0000    48     225       225         0         0         0           -           0
2021-05-16T20:13:20.319334+0000    49     247       247         0         0         0           -           0
2021-05-16T20:13:21.319439+0000    50     247       247         0         0         0           -           0
2021-05-16T20:13:22.319505+0000    51       3       256       253   19.8411   19.8431   0.0243407     26.8776
2021-05-16T20:13:23.319587+0000    52       3       256       253   19.4596         0           -     26.8776
2021-05-16T20:13:24.319862+0000 Total time run:         52.8683
Total writes made:      256
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     19.3689
Stddev Bandwidth:       2.75175
Max bandwidth (MB/sec): 19.8431
Min bandwidth (MB/sec): 0
Average IOPS:           4
Stddev IOPS:            0.5547
Max IOPS:               4
Min IOPS:               0
Average Latency(s):     26.5875
Stddev Latency(s):      16.9251
Max latency(s):         50.7575
Min latency(s):         0.0197226

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:13:24,898167142-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 238789


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:13:24,903226308-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:13:48,472463878-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:13:48,481871264-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:13:56,993711731-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:13:57,002612034-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:14:05,511423240-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:14:05,520925154-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:14:14,292136751-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:14:14,301630660-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:14:22,942002071-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:14:22,950874140-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:14:22,958207119-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:14:22,962454288-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:14:22,972188609-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=240179
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:14:22,978943791-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:14:22,998089996-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:14:23,001488411-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aa5a0494-66cc-484d-9cf9-c7ff73b29c36', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aa5a0494-66cc-484d-9cf9-c7ff73b29c36 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rmIyY9:/tmp/ceph-asok.rmIyY9 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T20:14:25.431+0000 7f0fef3cad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:14:25.719+0000 7f0fef3cad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:14:25.719+0000 7f0fef3cad00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T20:14:25.737747+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:14:25.737747+0000     0       0         0         0         0         0           -           0
2021-05-16T20:14:26.737848+0000     1      41        41         0         0         0           -           0
2021-05-16T20:14:27.737967+0000     2      50        50         0         0         0           -           0
2021-05-16T20:14:28.738035+0000     3      50        50         0         0         0           -           0
2021-05-16T20:14:29.738104+0000     4      60        60         0         0         0           -           0
2021-05-16T20:14:30.738204+0000     5      60        60         0         0         0           -           0
2021-05-16T20:14:31.738271+0000     6      60        60         0         0         0           -           0
2021-05-16T20:14:32.738337+0000     7      70        70         0         0         0           -           0
2021-05-16T20:14:33.738406+0000     8      70        70         0         0         0           -           0
2021-05-16T20:14:34.738509+0000     9      73        73         0         0         0           -           0
2021-05-16T20:14:35.738575+0000    10      73        73         0         0         0           -           0
2021-05-16T20:14:36.738642+0000    11      75        75         0         0         0           -           0
2021-05-16T20:14:37.738731+0000    12      75        75         0         0         0           -           0
2021-05-16T20:14:38.738879+0000    13      83        83         0         0         0           -           0
2021-05-16T20:14:39.739047+0000    14      83        83         0         0         0           -           0
2021-05-16T20:14:40.739115+0000    15      83        83         0         0         0           -           0
2021-05-16T20:14:41.739189+0000    16      83        83         0         0         0           -           0
2021-05-16T20:14:42.739291+0000    17      86        86         0         0         0           -           0
2021-05-16T20:14:43.739478+0000    18      86        86         0         0         0           -           0
2021-05-16T20:14:44.739547+0000    19      89        89         0         0         0           -           0
2021-05-16T20:14:45.739614+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:14:45.739614+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:14:45.739614+0000    20      89        89         0         0         0           -           0
2021-05-16T20:14:46.739793+0000    21      89        89         0         0         0           -           0
2021-05-16T20:14:47.739879+0000    22      98        98         0         0         0           -           0
2021-05-16T20:14:48.739944+0000    23      98        98         0         0         0           -           0
2021-05-16T20:14:49.740010+0000    24     100       100         0         0         0           -           0
2021-05-16T20:14:50.740109+0000    25     100       100         0         0         0           -           0
2021-05-16T20:14:51.740175+0000    26     112       112         0         0         0           -           0
2021-05-16T20:14:52.740241+0000    27     112       112         0         0         0           -           0
2021-05-16T20:14:53.740310+0000    28     113       113         0         0         0           -           0
2021-05-16T20:14:54.740414+0000    29     113       113         0         0         0           -           0
2021-05-16T20:14:55.740480+0000    30     131       131         0         0         0           -           0
2021-05-16T20:14:56.740545+0000    31     131       131         0         0         0           -           0
2021-05-16T20:14:57.740610+0000    32     131       131         0         0         0           -           0
2021-05-16T20:14:58.740710+0000    33     131       131         0         0         0           -           0
2021-05-16T20:14:59.740774+0000    34     139       139         0         0         0           -           0
2021-05-16T20:15:00.740953+0000    35     139       139         0         0         0           -           0
2021-05-16T20:15:01.741176+0000    36     139       139         0         0         0           -           0
2021-05-16T20:15:02.741422+0000    37     139       139         0         0         0           -           0
2021-05-16T20:15:03.741488+0000    38     139       139         0         0         0           -           0
2021-05-16T20:15:04.741571+0000    39     142       142         0         0         0           -           0
2021-05-16T20:15:05.741636+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:15:05.741636+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:15:05.741636+0000    40     142       142         0         0         0           -           0
2021-05-16T20:15:06.741748+0000    41     145       145         0         0         0           -           0
2021-05-16T20:15:07.741817+0000    42     145       145         0         0         0           -           0
2021-05-16T20:15:08.741883+0000    43     146       146         0         0         0           -           0
2021-05-16T20:15:09.741949+0000    44     146       146         0         0         0           -           0
2021-05-16T20:15:10.742061+0000    45     165       165         0         0         0           -           0
2021-05-16T20:15:11.742131+0000    46     165       165         0         0         0           -           0
2021-05-16T20:15:12.742209+0000    47     165       165         0         0         0           -           0
2021-05-16T20:15:13.742276+0000    48     165       165         0         0         0           -           0
2021-05-16T20:15:14.742376+0000    49     175       175         0         0         0           -           0
2021-05-16T20:15:15.742441+0000    50     186       186         0         0         0           -           0
2021-05-16T20:15:16.742507+0000    51     186       186         0         0         0           -           0
2021-05-16T20:15:17.742573+0000    52     199       199         0         0         0           -           0
2021-05-16T20:15:18.742676+0000    53     199       199         0         0         0           -           0
2021-05-16T20:15:19.742742+0000    54     201       201         0         0         0           -           0
2021-05-16T20:15:20.742951+0000    55     201       201         0         0         0           -           0
2021-05-16T20:15:21.743016+0000    56     202       202         0         0         0           -           0
2021-05-16T20:15:22.743119+0000    57     202       202         0         0         0           -           0
2021-05-16T20:15:23.743218+0000    58     205       205         0         0         0           -           0
2021-05-16T20:15:24.743431+0000    59     205       205         0         0         0           -           0
2021-05-16T20:15:25.743515+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:15:25.743515+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:15:25.743515+0000    60     216       216         0         0         0           -           0
2021-05-16T20:15:26.743626+0000    61     216       216         0         0         0           -           0
2021-05-16T20:15:27.743691+0000    62     230       230         0         0         0           -           0
2021-05-16T20:15:28.743877+0000    63     230       230         0         0         0           -           0
2021-05-16T20:15:29.743943+0000    64     230       230         0         0         0           -           0
2021-05-16T20:15:30.744044+0000    65     231       231         0         0         0           -           0
2021-05-16T20:15:31.744109+0000    66     231       231         0         0         0           -           0
2021-05-16T20:15:32.744174+0000    67     237       237         0         0         0           -           0
2021-05-16T20:15:33.744239+0000    68     237       237         0         0         0           -           0
2021-05-16T20:15:34.744339+0000    69     237       237         0         0         0           -           0
2021-05-16T20:15:35.744415+0000    70     237       237         0         0         0           -           0
2021-05-16T20:15:36.744480+0000    71     252       252         0         0         0           -           0
2021-05-16T20:15:37.744545+0000    72     252       252         0         0         0           -           0
2021-05-16T20:15:38.744646+0000    73     252       252         0         0         0           -           0
2021-05-16T20:15:39.744717+0000    74     252       252         0         0         0           -           0
2021-05-16T20:15:40.744950+0000    75     255       255         0         0         0           -           0
2021-05-16T20:15:41.745020+0000    76     255       255         0         0         0           -           0
2021-05-16T20:15:42.745144+0000    77      22       256       234   12.1547   12.1558     2.20453     48.2943
2021-05-16T20:15:43.745291+0000    78      22       256       234   11.9988         0           -     48.2943
2021-05-16T20:15:44.745357+0000    79      22       256       234    11.847         0           -     48.2943
2021-05-16T20:15:45.745573+0000 min lat: 2.20453 max lat: 76.9121 avg lat: 47.9577
2021-05-16T20:15:45.745573+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:15:45.745573+0000    80      20       256       236   11.7988   2.66667     8.57655     47.9577
2021-05-16T20:15:46.745754+0000    81      20       256       236   11.6532         0           -     47.9577
2021-05-16T20:15:47.745879+0000    82      10       256       246   11.9988        20      19.322     46.7851
2021-05-16T20:15:48.745987+0000    83      10       256       246   11.8542         0           -     46.7851
2021-05-16T20:15:49.746063+0000    84       6       256       250   11.9036         8     12.8866     46.2771
2021-05-16T20:15:50.746285+0000    85       6       256       250   11.7635         0           -     46.2771
2021-05-16T20:15:51.746357+0000    86       2       256       254   11.8128         8     15.0235     45.7849
2021-05-16T20:15:52.746449+0000    87       2       256       254    11.677         0           -     45.7849
2021-05-16T20:15:53.746517+0000    88       1       256       255   11.5897         2     17.1637     45.6727
2021-05-16T20:15:54.746622+0000    89       1       256       255   11.4595         0           -     45.6727
2021-05-16T20:15:55.746758+0000 Total time run:       89.792
Total reads made:     256
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   11.4041
Average IOPS:         2
Stddev IOPS:          0.677412
Max IOPS:             5
Min IOPS:             0
Average Latency(s):   45.553
Max latency(s):       76.9121
Min latency(s):       2.20453

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:15:56,317092678-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 240179


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:15:56,322234931-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:16:19,740571589-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:16:19,749910365-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:16:28,297268943-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:16:28,307219138-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:16:36,678648870-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:16:36,688076565-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:16:45,091336364-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:16:45,100934478-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:16:53,629165941-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:16:53,638490411-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:16:53,645661595-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T16:16:53,648409658-04:00][RUNNING][ROUND 3/6/40] object_size=4MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:16:53,652308424-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:16:53,662271183-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40493\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.168410\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ce58bd0c-b669-43fb-9e59-03dba75bb8d5\nsetting min_mon_release = octopus\nepoch 0\nfsid ce58bd0c-b669-43fb-9e59-03dba75bb8d5\nlast_changed 2021-05-16T13:17:04.007518-0700\ncreated 2021-05-16T13:17:04.007518-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40493/0,v1:10.10.1.2:40494/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.168410 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 70701a6d-c5f1-409e-8571-618b754515a6\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 22968a08-a83a-4eba-b5e8-28ce46450056\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 92cedb20-cb1d-476b-9d83-eb9101a7c35c\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42493\n'
10.10.1.2: b'  w/ user/pass: admin / 68dd292c-84d2-47d8-ae1f-722e53d43d0d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:17:21 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40493
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.168410
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ce58bd0c-b669-43fb-9e59-03dba75bb8d5
setting min_mon_release = octopus
epoch 0
fsid ce58bd0c-b669-43fb-9e59-03dba75bb8d5
last_changed 2021-05-16T13:17:04.007518-0700
created 2021-05-16T13:17:04.007518-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40493/0,v1:10.10.1.2:40494/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.168410 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 70701a6d-c5f1-409e-8571-618b754515a6
0
start osd.0
add osd1 22968a08-a83a-4eba-b5e8-28ce46450056
1
start osd.1
add osd2 92cedb20-cb1d-476b-9d83-eb9101a7c35c
2
start osd.2


restful urls: https://10.10.1.2:42493
  w/ user/pass: admin / 68dd292c-84d2-47d8-ae1f-722e53d43d0d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T13:16:55.504-0700 7f57107b71c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:16:55.504-0700 7f57107b71c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:16:55.520-0700 7fa3593161c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:16:55.520-0700 7fa3593161c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40493,v1:10.10.1.2:40494] --print /tmp/ceph_monmap.168410 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.168410 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.168410 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42493 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55e7da484000 @  0x7f08eb189680 0x7f08eb1aa824 0x7f08eb945187 0x7f08eb94d355 0x7f08eb945708 0x7f08eb945877 0x7f08eb946c24 0x7f08eb95eec1 0x7f08eb8d15f3 0x7f08eb932e97 0x7f08eb93ab1a 0x7f08eb03dd84 0x7f08eb159609 0x7f08ead2d293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.q6hnhAH7vY 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 70701a6d-c5f1-409e-8571-618b754515a6 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDKfaFgfTc0NRAAo6dlEPiXWUxdac1KjZ3F1g== --osd-uuid 70701a6d-c5f1-409e-8571-618b754515a6 
2021-05-16T13:17:15.524-0700 7f98564f4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:17:15.544-0700 7f98564f4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:17:15.544-0700 7f98564f4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56270d8f2000 @  0x7f9856ebd680 0x7f9856ede824 0x562702e6d447 0x562702e754b5 0x562702e6d9c8 0x562702e6db37 0x562702e6eee4 0x562702c3fca1 0x562702e17423 0x562702c302a7 0x562702c3553a 0x7f9856a10d84 0x7f9856b95609 0x7f98566fe293
2021-05-16T13:17:15.856-0700 7f98564f4f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 22968a08-a83a-4eba-b5e8-28ce46450056 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55b3b3ed6000 @  0x7f7ff61c3680 0x7f7ff61e4824 0x55b3a9108447 0x55b3a91104b5 0x55b3a91089c8 0x55b3a9108b37 0x55b3a9109ee4 0x55b3a8edaca1 0x55b3a90b2423 0x55b3a8ecb2a7 0x55b3a8ed053a 0x7f7ff5d16d84 0x7f7ff5e9b609 0x7f7ff5a04293
2021-05-16T13:17:16.496-0700 7f7ff57faf00 -1 Falling back to public interface
2021-05-16T13:17:16.760-0700 7f7ff57faf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDMfaFgM4WECxAAB7LJyj5g+YczhSU3QEQmJQ== --osd-uuid 22968a08-a83a-4eba-b5e8-28ce46450056 
2021-05-16T13:17:16.864-0700 7f178f01ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:17:16.884-0700 7f178f01ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:17:16.884-0700 7f178f01ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5596b4f86000 @  0x7f178f9e8680 0x7f178fa09824 0x5596a91ca447 0x5596a91d24b5 0x5596a91ca9c8 0x5596a91cab37 0x5596a91cbee4 0x5596a8f9cca1 0x5596a9174423 0x5596a8f8d2a7 0x5596a8f9253a 0x7f178f53bd84 0x7f178f6c0609 0x7f178f229293
2021-05-16T13:17:17.208-0700 7f178f01ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 92cedb20-cb1d-476b-9d83-eb9101a7c35c -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55a4e9e80000 @  0x7f5f2680d680 0x7f5f2682e824 0x55a4dfd14447 0x55a4dfd1c4b5 0x55a4dfd149c8 0x55a4dfd14b37 0x55a4dfd15ee4 0x55a4dfae6ca1 0x55a4dfcbe423 0x55a4dfad72a7 0x55a4dfadc53a 0x7f5f26360d84 0x7f5f264e5609 0x7f5f2604e293
2021-05-16T13:17:17.904-0700 7f5f25e44f00 -1 Falling back to public interface
2021-05-16T13:17:18.160-0700 7f5f25e44f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDNfaFgQPizIxAAoAFoF41RHeBOg534gDC8mg== --osd-uuid 92cedb20-cb1d-476b-9d83-eb9101a7c35c 
2021-05-16T13:17:18.280-0700 7f628abcef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:17:18.300-0700 7f628abcef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:17:18.300-0700 7f628abcef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x557bd7e96000 @  0x7f628b597680 0x7f628b5b8824 0x557bcdbbb447 0x557bcdbc34b5 0x557bcdbbb9c8 0x557bcdbbbb37 0x557bcdbbcee4 0x557bcd98dca1 0x557bcdb65423 0x557bcd97e2a7 0x557bcd98353a 0x7f628b0ead84 0x7f628b26f609 0x7f628add8293
2021-05-16T13:17:18.628-0700 7f628abcef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55f8333ca000 @  0x7f03af115680 0x7f03af136824 0x55f829407447 0x55f82940f4b5 0x55f8294079c8 0x55f829407b37 0x55f829408ee4 0x55f8291d9ca1 0x55f8293b1423 0x55f8291ca2a7 0x55f8291cf53a 0x7f03aec68d84 0x7f03aeded609 0x7f03ae956293
2021-05-16T13:17:19.304-0700 7f03ae74cf00 -1 Falling back to public interface
2021-05-16T13:17:19.584-0700 7f03ae74cf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:21,794863920-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:21,806043696-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:21,846047810-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:21,849155369-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88503473-0b71-432a-a4a4-98e09c6c1e03', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88503473-0b71-432a-a4a4-98e09c6c1e03 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:25,400250551-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:25,404310840-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88503473-0b71-432a-a4a4-98e09c6c1e03', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88503473-0b71-432a-a4a4-98e09c6c1e03 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:28,996089263-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:28,999358255-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88503473-0b71-432a-a4a4-98e09c6c1e03', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88503473-0b71-432a-a4a4-98e09c6c1e03 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:32,515022305-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:32,518494860-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88503473-0b71-432a-a4a4-98e09c6c1e03', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88503473-0b71-432a-a4a4-98e09c6c1e03 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:39,795555954-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:39,799462143-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88503473-0b71-432a-a4a4-98e09c6c1e03', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88503473-0b71-432a-a4a4-98e09c6c1e03 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:44,315672747-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:44,319215834-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88503473-0b71-432a-a4a4-98e09c6c1e03', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88503473-0b71-432a-a4a4-98e09c6c1e03 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:48,823770337-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:48,828029419-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88503473-0b71-432a-a4a4-98e09c6c1e03', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88503473-0b71-432a-a4a4-98e09c6c1e03 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:53,102971948-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:53,106403095-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88503473-0b71-432a-a4a4-98e09c6c1e03', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88503473-0b71-432a-a4a4-98e09c6c1e03 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:57,386290706-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:17:57,390052624-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88503473-0b71-432a-a4a4-98e09c6c1e03', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88503473-0b71-432a-a4a4-98e09c6c1e03 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:18:02,099299129-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:18:02,102823461-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88503473-0b71-432a-a4a4-98e09c6c1e03', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88503473-0b71-432a-a4a4-98e09c6c1e03 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:18:06,555252735-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:18:06,559334996-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88503473-0b71-432a-a4a4-98e09c6c1e03', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88503473-0b71-432a-a4a4-98e09c6c1e03 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:18:10,301844634-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:18:10,305655264-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88503473-0b71-432a-a4a4-98e09c6c1e03', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88503473-0b71-432a-a4a4-98e09c6c1e03 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:18:13,819528221-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:18:37,598580928-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:18:46,332398560-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:18:54,869570571-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:03,515636186-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:12,168747511-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:20,945881657-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:20,955067677-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:29,587947030-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:29,597443433-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:38,327010236-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:38,336723767-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:46,933650109-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:46,942839015-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:55,514623419-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:55,524344023-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:55,531528332-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:55,536159344-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:55,545794767-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=246621
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:55,552719038-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:55,572862245-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:19:55,576486906-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88503473-0b71-432a-a4a4-98e09c6c1e03', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88503473-0b71-432a-a4a4-98e09c6c1e03 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T20:19:57.981+0000 7f3846729d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:19:58.285+0000 7f3846729d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:19:58.285+0000 7f3846729d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T20:19:58.301229+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-16T20:19:58.301244+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T20:19:58.518509+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:19:58.518509+0000     0       0         0         0         0         0           -           0
2021-05-16T20:19:59.518630+0000     1      12        12         0         0         0           -           0
2021-05-16T20:20:00.518840+0000     2      19        19         0         0         0           -           0
2021-05-16T20:20:01.518919+0000     3      19        19         0         0         0           -           0
2021-05-16T20:20:02.519180+0000     4      19        19         0         0         0           -           0
2021-05-16T20:20:03.519259+0000     5      31        31         0         0         0           -           0
2021-05-16T20:20:04.519326+0000     6      31        31         0         0         0           -           0
2021-05-16T20:20:05.519550+0000     7      39        39         0         0         0           -           0
2021-05-16T20:20:06.519616+0000     8      39        39         0         0         0           -           0
2021-05-16T20:20:07.519702+0000     9      49        49         0         0         0           -           0
2021-05-16T20:20:08.519857+0000    10      49        49         0         0         0           -           0
2021-05-16T20:20:09.519949+0000    11      50        50         0         0         0           -           0
2021-05-16T20:20:10.520017+0000    12      50        50         0         0         0           -           0
2021-05-16T20:20:11.520115+0000    13      74        74         0         0         0           -           0
2021-05-16T20:20:12.520213+0000    14      74        74         0         0         0           -           0
2021-05-16T20:20:13.520317+0000    15      89        89         0         0         0           -           0
2021-05-16T20:20:14.520384+0000    16      89        89         0         0         0           -           0
2021-05-16T20:20:15.520478+0000    17      95        95         0         0         0           -           0
2021-05-16T20:20:16.520548+0000    18      95        95         0         0         0           -           0
2021-05-16T20:20:17.520614+0000    19      95        95         0         0         0           -           0
2021-05-16T20:20:18.520690+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:20:18.520690+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:20:18.520690+0000    20     122       122         0         0         0           -           0
2021-05-16T20:20:19.520779+0000    21     122       122         0         0         0           -           0
2021-05-16T20:20:20.521021+0000    22     151       151         0         0         0           -           0
2021-05-16T20:20:21.521181+0000    23     151       151         0         0         0           -           0
2021-05-16T20:20:22.521279+0000    24     188       188         0         0         0           -           0
2021-05-16T20:20:23.521505+0000    25     188       188         0         0         0           -           0
2021-05-16T20:20:24.521747+0000    26     210       210         0         0         0           -           0
2021-05-16T20:20:25.521869+0000    27     210       210         0         0         0           -           0
2021-05-16T20:20:26.521967+0000    28     220       220         0         0         0           -           0
2021-05-16T20:20:27.522046+0000    29     220       220         0         0         0           -           0
2021-05-16T20:20:28.522156+0000    30     236       236         0         0         0           -           0
2021-05-16T20:20:29.522221+0000    31     236       236         0         0         0           -           0
2021-05-16T20:20:30.522315+0000    32     249       249         0         0         0           -           0
2021-05-16T20:20:31.522394+0000    33       5       256       251   30.4207   30.4242   0.0268923     13.7095
2021-05-16T20:20:32.522496+0000    34       5       256       251    29.526         0           -     13.7095
2021-05-16T20:20:33.522614+0000 Total time run:         34.1086
Total writes made:      256
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     30.0218
Stddev Bandwidth:       5.21771
Max bandwidth (MB/sec): 30.4242
Min bandwidth (MB/sec): 0
Average IOPS:           7
Stddev IOPS:            1.20049
Max IOPS:               7
Min IOPS:               0
Average Latency(s):     13.4828
Stddev Latency(s):      8.97941
Max latency(s):         32.0412
Min latency(s):         0.0268923

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:20:34,156868424-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 246621


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:20:34,161607769-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:20:57,973015457-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:20:57,981748587-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:21:06,636889473-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:21:06,646706398-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:21:15,153984792-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:21:15,164380945-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:21:23,718683486-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:21:23,728401705-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:21:32,349365519-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:21:32,358988199-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:21:32,366618045-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:21:32,370950205-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:21:32,380886955-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=248012
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:21:32,387834890-04:00] INFO: > Run rados bench[0m
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.3
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:21:32,406523244-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:21:32,409755667-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '88503473-0b71-432a-a4a4-98e09c6c1e03', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 88503473-0b71-432a-a4a4-98e09c6c1e03 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CGVE0K:/tmp/ceph-asok.CGVE0K -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T20:21:34.797+0000 7f752de1ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:21:35.073+0000 7f752de1ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:21:35.073+0000 7f752de1ed00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T20:21:35.092034+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:21:35.092034+0000     0       0         0         0         0         0           -           0
2021-05-16T20:21:36.092142+0000     1      27        27         0         0         0           -           0
2021-05-16T20:21:37.092208+0000     2      37        37         0         0         0           -           0
2021-05-16T20:21:38.092271+0000     3      37        37         0         0         0           -           0
2021-05-16T20:21:39.092378+0000     4      37        37         0         0         0           -           0
2021-05-16T20:21:40.092446+0000     5      41        41         0         0         0           -           0
2021-05-16T20:21:41.092511+0000     6      41        41         0         0         0           -           0
2021-05-16T20:21:42.092578+0000     7      57        57         0         0         0           -           0
2021-05-16T20:21:43.092686+0000     8      57        57         0         0         0           -           0
2021-05-16T20:21:44.092752+0000     9      60        60         0         0         0           -           0
2021-05-16T20:21:45.092962+0000    10      60        60         0         0         0           -           0
2021-05-16T20:21:46.093249+0000    11      67        67         0         0         0           -           0
2021-05-16T20:21:47.093493+0000    12      67        67         0         0         0           -           0
2021-05-16T20:21:48.093560+0000    13      80        80         0         0         0           -           0
2021-05-16T20:21:49.093789+0000    14      80        80         0         0         0           -           0
2021-05-16T20:21:50.093856+0000    15      82        82         0         0         0           -           0
2021-05-16T20:21:51.093953+0000    16      82        82         0         0         0           -           0
2021-05-16T20:21:52.094031+0000    17      83        83         0         0         0           -           0
2021-05-16T20:21:53.094096+0000    18      83        83         0         0         0           -           0
2021-05-16T20:21:54.094163+0000    19      83        83         0         0         0           -           0
2021-05-16T20:21:55.094263+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:21:55.094263+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:21:55.094263+0000    20     101       101         0         0         0           -           0
2021-05-16T20:21:56.094339+0000    21     101       101         0         0         0           -           0
2021-05-16T20:21:57.094404+0000    22     102       102         0         0         0           -           0
2021-05-16T20:21:58.094469+0000    23     102       102         0         0         0           -           0
2021-05-16T20:21:59.094565+0000    24     108       108         0         0         0           -           0
2021-05-16T20:22:00.094776+0000    25     108       108         0         0         0           -           0
2021-05-16T20:22:01.094860+0000    26     110       110         0         0         0           -           0
2021-05-16T20:22:02.094982+0000    27     110       110         0         0         0           -           0
2021-05-16T20:22:03.095079+0000    28     120       120         0         0         0           -           0
2021-05-16T20:22:04.095272+0000    29     120       120         0         0         0           -           0
2021-05-16T20:22:05.095337+0000    30     127       127         0         0         0           -           0
2021-05-16T20:22:06.095549+0000    31     127       127         0         0         0           -           0
2021-05-16T20:22:07.095646+0000    32     132       132         0         0         0           -           0
2021-05-16T20:22:08.095725+0000    33     132       132         0         0         0           -           0
2021-05-16T20:22:09.095792+0000    34     132       132         0         0         0           -           0
2021-05-16T20:22:10.095882+0000    35     136       136         0         0         0           -           0
2021-05-16T20:22:11.095978+0000    36     136       136         0         0         0           -           0
2021-05-16T20:22:12.096045+0000    37     139       139         0         0         0           -           0
2021-05-16T20:22:13.096111+0000    38     139       139         0         0         0           -           0
2021-05-16T20:22:14.096177+0000    39     142       142         0         0         0           -           0
2021-05-16T20:22:15.096278+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:22:15.096278+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:22:15.096278+0000    40     142       142         0         0         0           -           0
2021-05-16T20:22:16.096369+0000    41     160       160         0         0         0           -           0
2021-05-16T20:22:17.096444+0000    42     160       160         0         0         0           -           0
2021-05-16T20:22:18.096510+0000    43     171       171         0         0         0           -           0
2021-05-16T20:22:19.096607+0000    44     171       171         0         0         0           -           0
2021-05-16T20:22:20.096702+0000    45     173       173         0         0         0           -           0
2021-05-16T20:22:21.096774+0000    46     173       173         0         0         0           -           0
2021-05-16T20:22:22.096840+0000    47     188       188         0         0         0           -           0
2021-05-16T20:22:23.096973+0000    48     198       198         0         0         0           -           0
2021-05-16T20:22:24.097188+0000    49     198       198         0         0         0           -           0
2021-05-16T20:22:25.097263+0000    50     206       206         0         0         0           -           0
2021-05-16T20:22:26.097329+0000    51     206       206         0         0         0           -           0
2021-05-16T20:22:27.097426+0000    52     208       208         0         0         0           -           0
2021-05-16T20:22:28.097576+0000    53     208       208         0         0         0           -           0
2021-05-16T20:22:29.097641+0000    54     216       216         0         0         0           -           0
2021-05-16T20:22:30.097706+0000    55     216       216         0         0         0           -           0
2021-05-16T20:22:31.097803+0000    56     232       232         0         0         0           -           0
2021-05-16T20:22:32.097868+0000    57     232       232         0         0         0           -           0
2021-05-16T20:22:33.097933+0000    58     239       239         0         0         0           -           0
2021-05-16T20:22:34.097997+0000    59     239       239         0         0         0           -           0
2021-05-16T20:22:35.098093+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:22:35.098093+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:22:35.098093+0000    60     247       247         0         0         0           -           0
2021-05-16T20:22:36.098297+0000    61     247       247         0         0         0           -           0
2021-05-16T20:22:37.098362+0000    62     247       247         0         0         0           -           0
2021-05-16T20:22:38.098428+0000    63     250       250         0         0         0           -           0
2021-05-16T20:22:39.098526+0000    64     250       250         0         0         0           -           0
2021-05-16T20:22:40.098589+0000    65      18       256       238   14.6447   14.6462     4.34906     36.5597
2021-05-16T20:22:41.098660+0000    66      18       256       238   14.4228         0           -     36.5597
2021-05-16T20:22:42.098727+0000    67      13       256       243    14.506        10     8.59553     35.9489
2021-05-16T20:22:43.098954+0000    68      13       256       243   14.2926         0           -     35.9489
2021-05-16T20:22:44.099071+0000    69      12       256       244   14.1435         2     4.29287     35.8191
2021-05-16T20:22:45.099293+0000    70      12       256       244   13.9414         0           -     35.8191
2021-05-16T20:22:46.099507+0000    71       7       256       249   14.0267        10     6.44634     35.3068
2021-05-16T20:22:47.099607+0000    72       7       256       249   13.8319         0           -     35.3068
2021-05-16T20:22:48.099820+0000    73       6       256       250   13.6972         2     15.0261     35.2257
2021-05-16T20:22:49.099889+0000    74       6       256       250   13.5121         0           -     35.2257
2021-05-16T20:22:50.099956+0000    75       1       256       255   13.5986        10     10.7398     34.7961
2021-05-16T20:22:51.100055+0000    76       1       256       255   13.4196         0           -     34.7961
2021-05-16T20:22:52.100125+0000    77       1       256       255   13.2453         0           -     34.7961
2021-05-16T20:22:53.100255+0000 Total time run:       77.0504
Total reads made:     256
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   13.29
Average IOPS:         3
Stddev IOPS:          0.512323
Max IOPS:             3
Min IOPS:             0
Average Latency(s):   34.7104
Max latency(s):       64.1779
Min latency(s):       2.15331

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:22:53,757637587-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 248012


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:22:53,762743902-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:23:17,232845477-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:23:17,242167803-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:23:25,738883410-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:23:25,748621898-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:23:34,388424977-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:23:34,397749026-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:23:42,994593450-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:23:43,004291653-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:23:51,649285166-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:23:51,658518886-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:23:51,666186914-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T16:23:51,669103904-04:00][RUNNING][ROUND 4/6/40] object_size=4MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:23:51,673379268-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:23:51,682054808-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40009\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.169517\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 37225ef7-5a9c-45a9-8d07-48c8d86d2e90\nsetting min_mon_release = octopus\nepoch 0\nfsid 37225ef7-5a9c-45a9-8d07-48c8d86d2e90\nlast_changed 2021-05-16T13:24:01.972386-0700\ncreated 2021-05-16T13:24:01.972386-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40009/0,v1:10.10.1.2:40010/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.169517 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 e0308b90-5294-4d8b-ac8a-801f329750fb\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 47ae3ac1-b791-4266-9041-a2acd16907de\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 308cd423-2fe0-434a-b46f-f7165d576876\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42009\n  w/ user/pass: admin / af9e82bd-ce12-468c-ae24-62a112b8085e\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 16:24:20 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40009
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.169517
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 37225ef7-5a9c-45a9-8d07-48c8d86d2e90
setting min_mon_release = octopus
epoch 0
fsid 37225ef7-5a9c-45a9-8d07-48c8d86d2e90
last_changed 2021-05-16T13:24:01.972386-0700
created 2021-05-16T13:24:01.972386-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40009/0,v1:10.10.1.2:40010/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.169517 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 e0308b90-5294-4d8b-ac8a-801f329750fb
0
start osd.0
add osd1 47ae3ac1-b791-4266-9041-a2acd16907de
1
start osd.1
add osd2 308cd423-2fe0-434a-b46f-f7165d576876
2
start osd.2


restful urls: https://10.10.1.2:42009
  w/ user/pass: admin / af9e82bd-ce12-468c-ae24-62a112b8085e


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T13:23:53.500-0700 7f78dd4991c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:23:53.500-0700 7f78dd4991c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:23:53.520-0700 7f1ba4bb81c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:23:53.520-0700 7f1ba4bb81c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40009,v1:10.10.1.2:40010] --print /tmp/ceph_monmap.169517 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.169517 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.169517 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42009 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5607a224e000 @  0x7fc2ed490680 0x7fc2ed4b1824 0x7fc2edc4c187 0x7fc2edc54355 0x7fc2edc4c708 0x7fc2edc4c877 0x7fc2edc4dc24 0x7fc2edc65ec1 0x7fc2edbd85f3 0x7fc2edc39e97 0x7fc2edc41b1a 0x7fc2ed344d84 0x7fc2ed460609 0x7fc2ed034293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.3ojs4RdZMi 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e0308b90-5294-4d8b-ac8a-801f329750fb -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBtf6FgTu5fGBAAodrvmGAJBZsbrSh2B9N5FQ== --osd-uuid e0308b90-5294-4d8b-ac8a-801f329750fb 
2021-05-16T13:24:14.040-0700 7f8e83d76f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:24:14.060-0700 7f8e83d76f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:24:14.060-0700 7f8e83d76f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x56396b7ba000 @  0x7f8e8473f680 0x7f8e84760824 0x563960b38447 0x563960b404b5 0x563960b389c8 0x563960b38b37 0x563960b39ee4 0x56396090aca1 0x563960ae2423 0x5639608fb2a7 0x56396090053a 0x7f8e84292d84 0x7f8e84417609 0x7f8e83f80293
2021-05-16T13:24:14.388-0700 7f8e83d76f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 47ae3ac1-b791-4266-9041-a2acd16907de -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5618d8b16000 @  0x7f4fb915c680 0x7f4fb917d824 0x5618cde2b447 0x5618cde334b5 0x5618cde2b9c8 0x5618cde2bb37 0x5618cde2cee4 0x5618cdbfdca1 0x5618cddd5423 0x5618cdbee2a7 0x5618cdbf353a 0x7f4fb8cafd84 0x7f4fb8e34609 0x7f4fb899d293
2021-05-16T13:24:15.008-0700 7f4fb8793f00 -1 Falling back to public interface
2021-05-16T13:24:15.272-0700 7f4fb8793f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBuf6FgTL8kKhAAZpN/WQQ6UnUixDvGnjwcXA== --osd-uuid 47ae3ac1-b791-4266-9041-a2acd16907de 
2021-05-16T13:24:15.396-0700 7f0df6b46f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:24:15.416-0700 7f0df6b46f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:24:15.416-0700 7f0df6b46f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55fe7a28a000 @  0x7f0df750f680 0x7f0df7530824 0x55fe70238447 0x55fe702404b5 0x55fe702389c8 0x55fe70238b37 0x55fe70239ee4 0x55fe7000aca1 0x55fe701e2423 0x55fe6fffb2a7 0x55fe7000053a 0x7f0df7062d84 0x7f0df71e7609 0x7f0df6d50293
2021-05-16T13:24:15.740-0700 7f0df6b46f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 308cd423-2fe0-434a-b46f-f7165d576876 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55df3a750000 @  0x7f6974e83680 0x7f6974ea4824 0x55df2fb02447 0x55df2fb0a4b5 0x55df2fb029c8 0x55df2fb02b37 0x55df2fb03ee4 0x55df2f8d4ca1 0x55df2faac423 0x55df2f8c52a7 0x55df2f8ca53a 0x7f69749d6d84 0x7f6974b5b609 0x7f69746c4293
2021-05-16T13:24:16.436-0700 7f69744baf00 -1 Falling back to public interface
2021-05-16T13:24:16.692-0700 7f69744baf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBwf6FgtFkbCBAAdNXmN8dpzBUhPl2HasZGtw== --osd-uuid 308cd423-2fe0-434a-b46f-f7165d576876 
2021-05-16T13:24:16.820-0700 7fbbc3506f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:24:16.844-0700 7fbbc3506f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:24:16.844-0700 7fbbc3506f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55f69e354000 @  0x7fbbc3ecf680 0x7fbbc3ef0824 0x55f693823447 0x55f69382b4b5 0x55f6938239c8 0x55f693823b37 0x55f693824ee4 0x55f6935f5ca1 0x55f6937cd423 0x55f6935e62a7 0x55f6935eb53a 0x7fbbc3a22d84 0x7fbbc3ba7609 0x7fbbc3710293
2021-05-16T13:24:17.160-0700 7fbbc3506f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x5602e72d6000 @  0x7fd65575f680 0x7fd655780824 0x5602dc50c447 0x5602dc5144b5 0x5602dc50c9c8 0x5602dc50cb37 0x5602dc50dee4 0x5602dc2deca1 0x5602dc4b6423 0x5602dc2cf2a7 0x5602dc2d453a 0x7fd6552b2d84 0x7fd655437609 0x7fd654fa0293
2021-05-16T13:24:17.840-0700 7fd654d96f00 -1 Falling back to public interface
2021-05-16T13:24:18.112-0700 7fd654d96f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:20,332116583-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:20,342449197-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:20,382936179-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:20,386600504-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1b8c5355-6648-41b8-ac93-4afc2cdc2a9a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1b8c5355-6648-41b8-ac93-4afc2cdc2a9a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:24,062926953-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:24,066261679-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1b8c5355-6648-41b8-ac93-4afc2cdc2a9a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1b8c5355-6648-41b8-ac93-4afc2cdc2a9a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:27,737614273-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:27,741324985-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1b8c5355-6648-41b8-ac93-4afc2cdc2a9a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1b8c5355-6648-41b8-ac93-4afc2cdc2a9a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:31,401890331-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:31,405242019-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1b8c5355-6648-41b8-ac93-4afc2cdc2a9a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1b8c5355-6648-41b8-ac93-4afc2cdc2a9a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:38,844242720-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:38,848173416-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1b8c5355-6648-41b8-ac93-4afc2cdc2a9a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1b8c5355-6648-41b8-ac93-4afc2cdc2a9a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:43,175452843-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:43,179538299-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1b8c5355-6648-41b8-ac93-4afc2cdc2a9a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1b8c5355-6648-41b8-ac93-4afc2cdc2a9a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:47,783615942-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:47,787017803-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1b8c5355-6648-41b8-ac93-4afc2cdc2a9a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1b8c5355-6648-41b8-ac93-4afc2cdc2a9a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:51,633804758-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:51,637715356-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1b8c5355-6648-41b8-ac93-4afc2cdc2a9a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1b8c5355-6648-41b8-ac93-4afc2cdc2a9a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:56,091109876-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:56,094394878-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1b8c5355-6648-41b8-ac93-4afc2cdc2a9a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1b8c5355-6648-41b8-ac93-4afc2cdc2a9a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:59,965712215-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:24:59,969274307-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1b8c5355-6648-41b8-ac93-4afc2cdc2a9a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1b8c5355-6648-41b8-ac93-4afc2cdc2a9a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:25:03,780667610-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:25:03,784779356-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1b8c5355-6648-41b8-ac93-4afc2cdc2a9a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1b8c5355-6648-41b8-ac93-4afc2cdc2a9a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:25:07,405607701-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:25:07,409085095-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1b8c5355-6648-41b8-ac93-4afc2cdc2a9a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1b8c5355-6648-41b8-ac93-4afc2cdc2a9a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:25:10,995855748-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:25:34,520926160-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:25:43,064681731-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:25:51,665622109-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:00,328035162-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:08,820816069-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:17,392925318-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:17,402452900-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:26,217408634-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:26,226934032-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:34,823964159-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:34,833411851-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:43,472462443-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:43,482060818-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:52,203060137-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:52,212275683-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:52,219598462-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:52,224026281-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:52,233393372-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=254484
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:52,241045740-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:52,260715879-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:26:52,264176541-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1b8c5355-6648-41b8-ac93-4afc2cdc2a9a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1b8c5355-6648-41b8-ac93-4afc2cdc2a9a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T20:26:54.774+0000 7febb597dd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:26:55.050+0000 7febb597dd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:26:55.054+0000 7febb597dd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T20:26:55.066846+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-16T20:26:55.066857+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T20:26:55.283895+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:26:55.283895+0000     0       0         0         0         0         0           -           0
2021-05-16T20:26:56.284005+0000     1       9         9         0         0         0           -           0
2021-05-16T20:26:57.284074+0000     2      19        19         0         0         0           -           0
2021-05-16T20:26:58.284142+0000     3      19        19         0         0         0           -           0
2021-05-16T20:26:59.284244+0000     4      38        38         0         0         0           -           0
2021-05-16T20:27:00.284312+0000     5      43        43         0         0         0           -           0
2021-05-16T20:27:01.284380+0000     6      43        43         0         0         0           -           0
2021-05-16T20:27:02.284449+0000     7      58        58         0         0         0           -           0
2021-05-16T20:27:03.284533+0000     8      58        58         0         0         0           -           0
2021-05-16T20:27:04.284601+0000     9      68        68         0         0         0           -           0
2021-05-16T20:27:05.284670+0000    10      68        68         0         0         0           -           0
2021-05-16T20:27:06.284752+0000    11     106       106         0         0         0           -           0
2021-05-16T20:27:07.284960+0000    12     106       106         0         0         0           -           0
2021-05-16T20:27:08.285035+0000    13     126       126         0         0         0           -           0
2021-05-16T20:27:09.285250+0000    14     126       126         0         0         0           -           0
2021-05-16T20:27:10.285359+0000    15     146       146         0         0         0           -           0
2021-05-16T20:27:11.285496+0000    16     146       146         0         0         0           -           0
2021-05-16T20:27:12.285564+0000    17     159       159         0         0         0           -           0
2021-05-16T20:27:13.285775+0000    18     159       159         0         0         0           -           0
2021-05-16T20:27:14.285844+0000    19     170       170         0         0         0           -           0
2021-05-16T20:27:15.285926+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:27:15.285926+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:27:15.285926+0000    20     170       170         0         0         0           -           0
2021-05-16T20:27:16.286007+0000    21     170       170         0         0         0           -           0
2021-05-16T20:27:17.286077+0000    22     187       187         0         0         0           -           0
2021-05-16T20:27:18.286145+0000    23     187       187         0         0         0           -           0
2021-05-16T20:27:19.286227+0000    24     205       205         0         0         0           -           0
2021-05-16T20:27:20.286295+0000    25     205       205         0         0         0           -           0
2021-05-16T20:27:21.286366+0000    26     214       214         0         0         0           -           0
2021-05-16T20:27:22.286434+0000    27     214       214         0         0         0           -           0
2021-05-16T20:27:23.286515+0000    28     247       247         0         0         0           -           0
2021-05-16T20:27:24.286621+0000    29     247       247         0         0         0           -           0
2021-05-16T20:27:25.286833+0000    30       3       256       253   33.7301   33.7333   0.0195017     15.0032
2021-05-16T20:27:26.286904+0000    31       3       256       253   32.6421         0           -     15.0032
2021-05-16T20:27:27.287049+0000 Total time run:         31.7894
Total writes made:      256
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     32.212
Stddev Bandwidth:       6.05869
Max bandwidth (MB/sec): 33.7333
Min bandwidth (MB/sec): 0
Average IOPS:           8
Stddev IOPS:            1.43684
Max IOPS:               8
Min IOPS:               0
Average Latency(s):     14.8523
Stddev Latency(s):      8.70516
Max latency(s):         29.6786
Min latency(s):         0.0195017

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:27:27,823499937-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 254484


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:27:27,828447012-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:27:51,477641448-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:27:51,487064473-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:28:00,227274057-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:28:00,237198385-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:28:08,812549451-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:28:08,822449964-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:28:17,417798435-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:28:17,426552714-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:28:26,124441554-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:28:26,133999382-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:28:26,141608630-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:28:26,146174048-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:28:26,156100289-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=255898
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:28:26,163923458-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:28:26,183695017-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:28:26,187201356-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1b8c5355-6648-41b8-ac93-4afc2cdc2a9a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1b8c5355-6648-41b8-ac93-4afc2cdc2a9a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.GycsLT:/tmp/ceph-asok.GycsLT -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T20:28:28.518+0000 7fa03b4e8d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:28:28.782+0000 7fa03b4e8d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:28:28.782+0000 7fa03b4e8d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T20:28:28.800206+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:28:28.800206+0000     0       0         0         0         0         0           -           0
2021-05-16T20:28:29.800305+0000     1      33        33         0         0         0           -           0
2021-05-16T20:28:30.800415+0000     2      54        54         0         0         0           -           0
2021-05-16T20:28:31.800549+0000     3      54        54         0         0         0           -           0
2021-05-16T20:28:32.800658+0000     4      61        61         0         0         0           -           0
2021-05-16T20:28:33.800739+0000     5      61        61         0         0         0           -           0
2021-05-16T20:28:34.800979+0000     6      73        73         0         0         0           -           0
2021-05-16T20:28:35.801127+0000     7      73        73         0         0         0           -           0
2021-05-16T20:28:36.801237+0000     8      73        73         0         0         0           -           0
2021-05-16T20:28:37.801556+0000     9      79        79         0         0         0           -           0
2021-05-16T20:28:38.801805+0000    10      79        79         0         0         0           -           0
2021-05-16T20:28:39.801884+0000    11      82        82         0         0         0           -           0
2021-05-16T20:28:40.801952+0000    12      82        82         0         0         0           -           0
2021-05-16T20:28:41.802057+0000    13      85        85         0         0         0           -           0
2021-05-16T20:28:42.802162+0000    14      85        85         0         0         0           -           0
2021-05-16T20:28:43.802246+0000    15      89        89         0         0         0           -           0
2021-05-16T20:28:44.802316+0000    16      89        89         0         0         0           -           0
2021-05-16T20:28:45.802575+0000    17      94        94         0         0         0           -           0
2021-05-16T20:28:46.802717+0000    18      94        94         0         0         0           -           0
2021-05-16T20:28:47.802809+0000    19      95        95         0         0         0           -           0
2021-05-16T20:28:48.802954+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:28:48.802954+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:28:48.802954+0000    20      95        95         0         0         0           -           0
2021-05-16T20:28:49.803070+0000    21      99        99         0         0         0           -           0
2021-05-16T20:28:50.803230+0000    22      99        99         0         0         0           -           0
2021-05-16T20:28:51.803310+0000    23      99        99         0         0         0           -           0
2021-05-16T20:28:52.803527+0000    24     109       109         0         0         0           -           0
2021-05-16T20:28:53.803765+0000    25     109       109         0         0         0           -           0
2021-05-16T20:28:54.803891+0000    26     114       114         0         0         0           -           0
2021-05-16T20:28:55.803996+0000    27     114       114         0         0         0           -           0
2021-05-16T20:28:56.804070+0000    28     119       119         0         0         0           -           0
2021-05-16T20:28:57.804166+0000    29     119       119         0         0         0           -           0
2021-05-16T20:28:58.804268+0000    30     123       123         0         0         0           -           0
2021-05-16T20:28:59.804335+0000    31     123       123         0         0         0           -           0
2021-05-16T20:29:00.804414+0000    32     123       123         0         0         0           -           0
2021-05-16T20:29:01.804483+0000    33     123       123         0         0         0           -           0
2021-05-16T20:29:02.804585+0000    34     126       126         0         0         0           -           0
2021-05-16T20:29:03.804655+0000    35     126       126         0         0         0           -           0
2021-05-16T20:29:04.804723+0000    36     130       130         0         0         0           -           0
2021-05-16T20:29:05.804791+0000    37     130       130         0         0         0           -           0
2021-05-16T20:29:06.804961+0000    38     130       130         0         0         0           -           0
2021-05-16T20:29:07.805030+0000    39     133       133         0         0         0           -           0
2021-05-16T20:29:08.805247+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:29:08.805247+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:29:08.805247+0000    40     133       133         0         0         0           -           0
2021-05-16T20:29:09.805326+0000    41     134       134         0         0         0           -           0
2021-05-16T20:29:10.805426+0000    42     134       134         0         0         0           -           0
2021-05-16T20:29:11.805486+0000    43     152       152         0         0         0           -           0
2021-05-16T20:29:12.805589+0000    44     152       152         0         0         0           -           0
2021-05-16T20:29:13.805657+0000    45     158       158         0         0         0           -           0
2021-05-16T20:29:14.805850+0000    46     158       158         0         0         0           -           0
2021-05-16T20:29:15.805917+0000    47     165       165         0         0         0           -           0
2021-05-16T20:29:16.805984+0000    48     165       165         0         0         0           -           0
2021-05-16T20:29:17.806051+0000    49     165       165         0         0         0           -           0
2021-05-16T20:29:18.806154+0000    50     165       165         0         0         0           -           0
2021-05-16T20:29:19.806223+0000    51     166       166         0         0         0           -           0
2021-05-16T20:29:20.806293+0000    52     166       166         0         0         0           -           0
2021-05-16T20:29:21.806484+0000    53     166       166         0         0         0           -           0
2021-05-16T20:29:22.806593+0000    54     178       178         0         0         0           -           0
2021-05-16T20:29:23.806690+0000    55     178       178         0         0         0           -           0
2021-05-16T20:29:24.806796+0000    56     180       180         0         0         0           -           0
2021-05-16T20:29:25.807028+0000    57     180       180         0         0         0           -           0
2021-05-16T20:29:26.807259+0000    58     180       180         0         0         0           -           0
2021-05-16T20:29:27.807494+0000    59     180       180         0         0         0           -           0
2021-05-16T20:29:28.807575+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:29:28.807575+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:29:28.807575+0000    60     186       186         0         0         0           -           0
2021-05-16T20:29:29.807800+0000    61     186       186         0         0         0           -           0
2021-05-16T20:29:30.807909+0000    62     189       189         0         0         0           -           0
2021-05-16T20:29:31.807999+0000    63     189       189         0         0         0           -           0
2021-05-16T20:29:32.808070+0000    64     197       197         0         0         0           -           0
2021-05-16T20:29:33.808139+0000    65     197       197         0         0         0           -           0
2021-05-16T20:29:34.808265+0000    66     206       206         0         0         0           -           0
2021-05-16T20:29:35.808378+0000    67     220       220         0         0         0           -           0
2021-05-16T20:29:36.808481+0000    68     220       220         0         0         0           -           0
2021-05-16T20:29:37.808550+0000    69     242       242         0         0         0           -           0
2021-05-16T20:29:38.808650+0000    70     242       242         0         0         0           -           0
2021-05-16T20:29:39.808726+0000    71     249       249         0         0         0           -           0
2021-05-16T20:29:40.808954+0000    72     249       249         0         0         0           -           0
2021-05-16T20:29:41.809184+0000    73     251       251         0         0         0           -           0
2021-05-16T20:29:42.809287+0000    74     251       251         0         0         0           -           0
2021-05-16T20:29:43.809355+0000    75      22       256       234   12.4785     12.48     4.34995     44.1537
2021-05-16T20:29:44.809576+0000    76      22       256       234   12.3143         0           -     44.1537
2021-05-16T20:29:45.809784+0000    77      21       256       235   12.2063         2     2.14836      43.975
2021-05-16T20:29:46.809894+0000    78      21       256       235   12.0498         0           -      43.975
2021-05-16T20:29:47.809963+0000    79      19       256       237   11.9985         4     12.8615     43.7124
2021-05-16T20:29:48.810037+0000 min lat: 2.14836 max lat: 74.5968 avg lat: 43.7124
2021-05-16T20:29:48.810037+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:29:48.810037+0000    80      19       256       237   11.8485         0           -     43.7124
2021-05-16T20:29:49.810118+0000    81      19       256       237   11.7023         0           -     43.7124
2021-05-16T20:29:50.810231+0000    82      18       256       238   11.6083   1.33333      12.859     43.5828
2021-05-16T20:29:51.810302+0000    83      18       256       238   11.4685         0           -     43.5828
2021-05-16T20:29:52.810373+0000    84      14       256       242   11.5224         8     10.7401     43.0841
2021-05-16T20:29:53.810444+0000    85      14       256       242   11.3869         0           -     43.0841
2021-05-16T20:29:54.810546+0000    86      10       256       246   11.4405         8     17.1788     42.6366
2021-05-16T20:29:55.810616+0000    87      10       256       246    11.309         0           -     42.6366
2021-05-16T20:29:56.810687+0000    88       4       256       252   11.4532        12     17.1835     42.0728
2021-05-16T20:29:57.810758+0000    89       4       256       252   11.3245         0           -     42.0728
2021-05-16T20:29:58.810956+0000    90       2       256       254   11.2875         4     19.3184     41.8937
2021-05-16T20:29:59.811030+0000    91       2       256       254   11.1635         0           -     41.8937
2021-05-16T20:30:00.811246+0000    92       1       256       255   11.0856         2     17.1789     41.7967
2021-05-16T20:30:01.811316+0000    93       1       256       255   10.9664         0           -     41.7967
2021-05-16T20:30:02.811604+0000 Total time run:       93.9141
Total reads made:     256
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   10.9036
Average IOPS:         2
Stddev IOPS:          0.536207
Max IOPS:             3
Min IOPS:             0
Average Latency(s):   41.709
Max latency(s):       74.5968
Min latency(s):       2.14836

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:30:03,466577464-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 255898


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:30:03,471586977-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:30:27,106102269-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:30:27,115637826-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:30:35,852255923-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:30:35,862099719-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:30:44,549746142-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:30:44,559344166-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:30:53,147833651-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:30:53,157745475-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:02,060665066-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:02,070302414-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:02,078001931-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T16:31:02,080924222-04:00][RUNNING][ROUND 5/6/40] object_size=4MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:02,085361289-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:02,093806937-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40824\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.170603\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 147045f9-721e-4717-9a28-bd04a4a5c5c8\nsetting min_mon_release = octopus\nepoch 0\nfsid 147045f9-721e-4717-9a28-bd04a4a5c5c8\nlast_changed 2021-05-16T13:31:12.553603-0700\ncreated 2021-05-16T13:31:12.553603-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40824/0,v1:10.10.1.2:40825/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.170603 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 80023973-7bae-481e-9673-07d75d662467\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 efe098f6-99dc-48f1-9ba4-2ef542833149\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 5955b26a-9456-4104-b857-9f2f5d559034\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42824\n  w/ user/pass: admin / 1ec1b730-f242-4503-b143-d4df57941e86\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:31:30 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40824
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.170603
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 147045f9-721e-4717-9a28-bd04a4a5c5c8
setting min_mon_release = octopus
epoch 0
fsid 147045f9-721e-4717-9a28-bd04a4a5c5c8
last_changed 2021-05-16T13:31:12.553603-0700
created 2021-05-16T13:31:12.553603-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40824/0,v1:10.10.1.2:40825/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.170603 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 80023973-7bae-481e-9673-07d75d662467
0
start osd.0
add osd1 efe098f6-99dc-48f1-9ba4-2ef542833149
1
start osd.1
add osd2 5955b26a-9456-4104-b857-9f2f5d559034
2
start osd.2


restful urls: https://10.10.1.2:42824
  w/ user/pass: admin / 1ec1b730-f242-4503-b143-d4df57941e86


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T13:31:03.936-0700 7ffb293831c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:31:03.936-0700 7ffb293831c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:31:03.952-0700 7fdcfcff61c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:31:03.952-0700 7fdcfcff61c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40824,v1:10.10.1.2:40825] --print /tmp/ceph_monmap.170603 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.170603 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.170603 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42824 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55e542394000 @  0x7f957b151680 0x7f957b172824 0x7f957b90d187 0x7f957b915355 0x7f957b90d708 0x7f957b90d877 0x7f957b90ec24 0x7f957b926ec1 0x7f957b8995f3 0x7f957b8fae97 0x7f957b902b1a 0x7f957b005d84 0x7f957b121609 0x7f957acf5293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.kphpgVnXce 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 80023973-7bae-481e-9673-07d75d662467 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAbgaFg1kiOERAArG2Usheav0HRLEQ4p2vtdg== --osd-uuid 80023973-7bae-481e-9673-07d75d662467 
2021-05-16T13:31:23.964-0700 7fe4fdd71f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:31:23.984-0700 7fe4fdd71f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:31:23.988-0700 7fe4fdd71f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5572c75bc000 @  0x7fe4fe73a680 0x7fe4fe75b824 0x5572bd4da447 0x5572bd4e24b5 0x5572bd4da9c8 0x5572bd4dab37 0x5572bd4dbee4 0x5572bd2acca1 0x5572bd484423 0x5572bd29d2a7 0x5572bd2a253a 0x7fe4fe28dd84 0x7fe4fe412609 0x7fe4fdf7b293
2021-05-16T13:31:24.304-0700 7fe4fdd71f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new efe098f6-99dc-48f1-9ba4-2ef542833149 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5585c6fce000 @  0x7fc1a66e3680 0x7fc1a6704824 0x5585bc0dd447 0x5585bc0e54b5 0x5585bc0dd9c8 0x5585bc0ddb37 0x5585bc0deee4 0x5585bbeafca1 0x5585bc087423 0x5585bbea02a7 0x5585bbea553a 0x7fc1a6236d84 0x7fc1a63bb609 0x7fc1a5f24293
2021-05-16T13:31:24.956-0700 7fc1a5d1af00 -1 Falling back to public interface
2021-05-16T13:31:25.220-0700 7fc1a5d1af00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAcgaFg/B2/JhAAKN1PjVdFHTczMy5nCELTgw== --osd-uuid efe098f6-99dc-48f1-9ba4-2ef542833149 
2021-05-16T13:31:25.332-0700 7efd30410f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:31:25.352-0700 7efd30410f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:31:25.352-0700 7efd30410f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5616658c0000 @  0x7efd30dd9680 0x7efd30dfa824 0x56165b63f447 0x56165b6474b5 0x56165b63f9c8 0x56165b63fb37 0x56165b640ee4 0x56165b411ca1 0x56165b5e9423 0x56165b4022a7 0x56165b40753a 0x7efd3092cd84 0x7efd30ab1609 0x7efd3061a293
2021-05-16T13:31:25.668-0700 7efd30410f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5955b26a-9456-4104-b857-9f2f5d559034 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x560487e40000 @  0x7fbbcc206680 0x7fbbcc227824 0x56047d046447 0x56047d04e4b5 0x56047d0469c8 0x56047d046b37 0x56047d047ee4 0x56047ce18ca1 0x56047cff0423 0x56047ce092a7 0x56047ce0e53a 0x7fbbcbd59d84 0x7fbbcbede609 0x7fbbcba47293
2021-05-16T13:31:26.308-0700 7fbbcb83df00 -1 Falling back to public interface
2021-05-16T13:31:26.564-0700 7fbbcb83df00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAegaFg2aA5ABAA/jXqBrGyosnbDu/6sXhD2Q== --osd-uuid 5955b26a-9456-4104-b857-9f2f5d559034 
2021-05-16T13:31:26.696-0700 7fb1db085f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:31:26.716-0700 7fb1db085f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:31:26.716-0700 7fb1db085f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55b567c9e000 @  0x7fb1dba4e680 0x7fb1dba6f824 0x55b55d813447 0x55b55d81b4b5 0x55b55d8139c8 0x55b55d813b37 0x55b55d814ee4 0x55b55d5e5ca1 0x55b55d7bd423 0x55b55d5d62a7 0x55b55d5db53a 0x7fb1db5a1d84 0x7fb1db726609 0x7fb1db28f293
2021-05-16T13:31:27.036-0700 7fb1db085f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55dfdc2a0000 @  0x7f6a34d21680 0x7f6a34d42824 0x55dfd12bd447 0x55dfd12c54b5 0x55dfd12bd9c8 0x55dfd12bdb37 0x55dfd12beee4 0x55dfd108fca1 0x55dfd1267423 0x55dfd10802a7 0x55dfd108553a 0x7f6a34874d84 0x7f6a349f9609 0x7f6a34562293
2021-05-16T13:31:27.696-0700 7f6a34358f00 -1 Falling back to public interface
2021-05-16T13:31:27.964-0700 7f6a34358f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:30,163157837-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:30,174172684-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:30,214744315-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:30,218942102-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '12a1ba4a-653e-42f4-afee-20f35c075217', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 12a1ba4a-653e-42f4-afee-20f35c075217 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:33,869403398-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:33,872615343-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '12a1ba4a-653e-42f4-afee-20f35c075217', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 12a1ba4a-653e-42f4-afee-20f35c075217 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:37,487802200-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:37,490872959-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '12a1ba4a-653e-42f4-afee-20f35c075217', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 12a1ba4a-653e-42f4-afee-20f35c075217 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:40,978267933-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:40,982184071-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '12a1ba4a-653e-42f4-afee-20f35c075217', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 12a1ba4a-653e-42f4-afee-20f35c075217 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:48,217352566-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:48,221305683-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '12a1ba4a-653e-42f4-afee-20f35c075217', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 12a1ba4a-653e-42f4-afee-20f35c075217 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:52,457632305-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:52,461355651-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '12a1ba4a-653e-42f4-afee-20f35c075217', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 12a1ba4a-653e-42f4-afee-20f35c075217 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:57,002202227-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:31:57,006292413-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '12a1ba4a-653e-42f4-afee-20f35c075217', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 12a1ba4a-653e-42f4-afee-20f35c075217 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:32:01,734036759-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:32:01,737991159-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '12a1ba4a-653e-42f4-afee-20f35c075217', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 12a1ba4a-653e-42f4-afee-20f35c075217 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:32:05,500369722-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:32:05,503849931-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '12a1ba4a-653e-42f4-afee-20f35c075217', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 12a1ba4a-653e-42f4-afee-20f35c075217 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:32:09,846637608-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:32:09,850421487-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '12a1ba4a-653e-42f4-afee-20f35c075217', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 12a1ba4a-653e-42f4-afee-20f35c075217 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:32:14,551970576-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:32:14,555508634-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '12a1ba4a-653e-42f4-afee-20f35c075217', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 12a1ba4a-653e-42f4-afee-20f35c075217 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:32:18,123924904-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:32:18,127502285-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '12a1ba4a-653e-42f4-afee-20f35c075217', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 12a1ba4a-653e-42f4-afee-20f35c075217 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:32:21,716989266-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:32:45,229175558-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:32:53,834934677-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:33:02,491073187-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:33:11,057679162-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:33:19,701874043-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:33:28,382115815-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   248 KiB used, 300 GiB / 300 GiB avail
    pgs:     1.042% pgs not active
             190 active+clean
             2   peering
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:33:28,392183802-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:33:36,948511438-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:33:36,958754964-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:33:45,880881245-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:33:45,890493616-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:33:54,554609612-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:33:54,564323715-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:34:03,386152514-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:34:03,396177761-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:34:03,404497484-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:34:03,409734093-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:34:03,419699487-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=262378
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:34:03,427920093-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:34:03,449800245-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:34:03,453534281-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '12a1ba4a-653e-42f4-afee-20f35c075217', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 12a1ba4a-653e-42f4-afee-20f35c075217 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T20:34:05.920+0000 7f2613d75d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:34:06.228+0000 7f2613d75d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:34:06.228+0000 7f2613d75d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T20:34:06.240931+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-16T20:34:06.240942+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T20:34:06.458527+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:34:06.458527+0000     0       0         0         0         0         0           -           0
2021-05-16T20:34:07.458649+0000     1      17        17         0         0         0           -           0
2021-05-16T20:34:08.458764+0000     2      36        36         0         0         0           -           0
2021-05-16T20:34:09.458862+0000     3      36        36         0         0         0           -           0
2021-05-16T20:34:10.459004+0000     4      57        57         0         0         0           -           0
2021-05-16T20:34:11.459117+0000     5      57        57         0         0         0           -           0
2021-05-16T20:34:12.459204+0000     6      73        73         0         0         0           -           0
2021-05-16T20:34:13.459310+0000     7      73        73         0         0         0           -           0
2021-05-16T20:34:14.459391+0000     8      85        85         0         0         0           -           0
2021-05-16T20:34:15.459504+0000     9      85        85         0         0         0           -           0
2021-05-16T20:34:16.459676+0000    10      95        95         0         0         0           -           0
2021-05-16T20:34:17.459774+0000    11      95        95         0         0         0           -           0
2021-05-16T20:34:18.459864+0000    12     112       112         0         0         0           -           0
2021-05-16T20:34:19.459983+0000    13     112       112         0         0         0           -           0
2021-05-16T20:34:20.460073+0000    14     113       113         0         0         0           -           0
2021-05-16T20:34:21.460174+0000    15     125       125         0         0         0           -           0
2021-05-16T20:34:22.460268+0000    16     125       125         0         0         0           -           0
2021-05-16T20:34:23.460390+0000    17     145       145         0         0         0           -           0
2021-05-16T20:34:24.460486+0000    18     145       145         0         0         0           -           0
2021-05-16T20:34:25.460557+0000    19     157       157         0         0         0           -           0
2021-05-16T20:34:26.460656+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:34:26.460656+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:34:26.460656+0000    20     157       157         0         0         0           -           0
2021-05-16T20:34:27.460757+0000    21     173       173         0         0         0           -           0
2021-05-16T20:34:28.460857+0000    22     173       173         0         0         0           -           0
2021-05-16T20:34:29.460940+0000    23     188       188         0         0         0           -           0
2021-05-16T20:34:30.461203+0000    24     188       188         0         0         0           -           0
2021-05-16T20:34:31.461298+0000    25     203       203         0         0         0           -           0
2021-05-16T20:34:32.461511+0000    26     203       203         0         0         0           -           0
2021-05-16T20:34:33.461600+0000    27     213       213         0         0         0           -           0
2021-05-16T20:34:34.461850+0000    28     213       213         0         0         0           -           0
2021-05-16T20:34:35.461930+0000    29     213       213         0         0         0           -           0
2021-05-16T20:34:36.461995+0000    30     217       217         0         0         0           -           0
2021-05-16T20:34:37.462060+0000    31     217       217         0         0         0           -           0
2021-05-16T20:34:38.462145+0000    32     238       238         0         0         0           -           0
2021-05-16T20:34:39.462261+0000    33     238       238         0         0         0           -           0
2021-05-16T20:34:40.462348+0000    34       7       256       249   29.2909   29.2941   0.0288121     18.5241
2021-05-16T20:34:41.462454+0000    35       7       256       249    28.454         0           -     18.5241
2021-05-16T20:34:42.462584+0000 Total time run:         35.5029
Total writes made:      256
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     28.8427
Stddev Bandwidth:       4.95161
Max bandwidth (MB/sec): 29.2941
Min bandwidth (MB/sec): 0
Average IOPS:           7
Stddev IOPS:            1.18322
Max IOPS:               7
Min IOPS:               0
Average Latency(s):     18.0758
Stddev Latency(s):      10.6645
Max latency(s):         33.4254
Min latency(s):         0.0259222

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:34:43,112809773-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 262378


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:34:43,117819225-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:06,614513107-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:06,624484022-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:14,892247924-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:14,902303959-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:23,257057412-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:23,266953315-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:31,723296450-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:31,732891088-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:39,969153154-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:39,978191436-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:39,985893808-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:39,990126421-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:39,999424582-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=263765
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:40,007189652-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:40,026865953-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:35:40,030300135-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '12a1ba4a-653e-42f4-afee-20f35c075217', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 12a1ba4a-653e-42f4-afee-20f35c075217 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kS3B6B:/tmp/ceph-asok.kS3B6B -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T20:35:42.272+0000 7fca819b2d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:35:42.524+0000 7fca819b2d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:35:42.524+0000 7fca819b2d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T20:35:42.545446+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:35:42.545446+0000     0       0         0         0         0         0           -           0
2021-05-16T20:35:43.545663+0000     1      33        33         0         0         0           -           0
2021-05-16T20:35:44.545761+0000     2      34        34         0         0         0           -           0
2021-05-16T20:35:45.545832+0000     3      34        34         0         0         0           -           0
2021-05-16T20:35:46.545978+0000     4      37        37         0         0         0           -           0
2021-05-16T20:35:47.546215+0000     5      37        37         0         0         0           -           0
2021-05-16T20:35:48.546280+0000     6      53        53         0         0         0           -           0
2021-05-16T20:35:49.546347+0000     7      53        53         0         0         0           -           0
2021-05-16T20:35:50.546412+0000     8      53        53         0         0         0           -           0
2021-05-16T20:35:51.546495+0000     9      59        59         0         0         0           -           0
2021-05-16T20:35:52.546595+0000    10      59        59         0         0         0           -           0
2021-05-16T20:35:53.546669+0000    11      67        67         0         0         0           -           0
2021-05-16T20:35:54.546765+0000    12      67        67         0         0         0           -           0
2021-05-16T20:35:55.546862+0000    13      69        69         0         0         0           -           0
2021-05-16T20:35:56.546928+0000    14      69        69         0         0         0           -           0
2021-05-16T20:35:57.546999+0000    15      86        86         0         0         0           -           0
2021-05-16T20:35:58.547240+0000    16      86        86         0         0         0           -           0
2021-05-16T20:35:59.547318+0000    17      88        88         0         0         0           -           0
2021-05-16T20:36:00.547539+0000    18      88        88         0         0         0           -           0
2021-05-16T20:36:01.547750+0000    19      95        95         0         0         0           -           0
2021-05-16T20:36:02.547890+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:36:02.547890+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:36:02.547890+0000    20      95        95         0         0         0           -           0
2021-05-16T20:36:03.547977+0000    21      96        96         0         0         0           -           0
2021-05-16T20:36:04.548041+0000    22      96        96         0         0         0           -           0
2021-05-16T20:36:05.548107+0000    23      96        96         0         0         0           -           0
2021-05-16T20:36:06.548204+0000    24     100       100         0         0         0           -           0
2021-05-16T20:36:07.548282+0000    25     100       100         0         0         0           -           0
2021-05-16T20:36:08.548346+0000    26     104       104         0         0         0           -           0
2021-05-16T20:36:09.548422+0000    27     104       104         0         0         0           -           0
2021-05-16T20:36:10.548488+0000    28     110       110         0         0         0           -           0
2021-05-16T20:36:11.548564+0000    29     110       110         0         0         0           -           0
2021-05-16T20:36:12.548630+0000    30     111       111         0         0         0           -           0
2021-05-16T20:36:13.548696+0000    31     111       111         0         0         0           -           0
2021-05-16T20:36:14.548762+0000    32     113       113         0         0         0           -           0
2021-05-16T20:36:15.548951+0000    33     113       113         0         0         0           -           0
2021-05-16T20:36:16.549052+0000    34     118       118         0         0         0           -           0
2021-05-16T20:36:17.549118+0000    35     118       118         0         0         0           -           0
2021-05-16T20:36:18.549185+0000    36     122       122         0         0         0           -           0
2021-05-16T20:36:19.549273+0000    37     122       122         0         0         0           -           0
2021-05-16T20:36:20.549340+0000    38     122       122         0         0         0           -           0
2021-05-16T20:36:21.549407+0000    39     125       125         0         0         0           -           0
2021-05-16T20:36:22.549587+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:36:22.549587+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:36:22.549587+0000    40     125       125         0         0         0           -           0
2021-05-16T20:36:23.549830+0000    41     126       126         0         0         0           -           0
2021-05-16T20:36:24.549895+0000    42     126       126         0         0         0           -           0
2021-05-16T20:36:25.549978+0000    43     133       133         0         0         0           -           0
2021-05-16T20:36:26.550046+0000    44     133       133         0         0         0           -           0
2021-05-16T20:36:27.550137+0000    45     135       135         0         0         0           -           0
2021-05-16T20:36:28.550222+0000    46     135       135         0         0         0           -           0
2021-05-16T20:36:29.550287+0000    47     135       135         0         0         0           -           0
2021-05-16T20:36:30.550350+0000    48     135       135         0         0         0           -           0
2021-05-16T20:36:31.550426+0000    49     137       137         0         0         0           -           0
2021-05-16T20:36:32.550492+0000    50     137       137         0         0         0           -           0
2021-05-16T20:36:33.550558+0000    51     137       137         0         0         0           -           0
2021-05-16T20:36:34.550622+0000    52     157       157         0         0         0           -           0
2021-05-16T20:36:35.550700+0000    53     157       157         0         0         0           -           0
2021-05-16T20:36:36.550765+0000    54     161       161         0         0         0           -           0
2021-05-16T20:36:37.550952+0000    55     161       161         0         0         0           -           0
2021-05-16T20:36:38.551028+0000    56     175       175         0         0         0           -           0
2021-05-16T20:36:39.551189+0000    57     175       175         0         0         0           -           0
2021-05-16T20:36:40.551260+0000    58     179       179         0         0         0           -           0
2021-05-16T20:36:41.551333+0000    59     179       179         0         0         0           -           0
2021-05-16T20:36:42.551513+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:36:42.551513+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:36:42.551513+0000    60     189       189         0         0         0           -           0
2021-05-16T20:36:43.551778+0000    61     189       189         0         0         0           -           0
2021-05-16T20:36:44.551855+0000    62     197       197         0         0         0           -           0
2021-05-16T20:36:45.551951+0000    63     197       197         0         0         0           -           0
2021-05-16T20:36:46.552016+0000    64     199       199         0         0         0           -           0
2021-05-16T20:36:47.552093+0000    65     199       199         0         0         0           -           0
2021-05-16T20:36:48.552159+0000    66     199       199         0         0         0           -           0
2021-05-16T20:36:49.552234+0000    67     206       206         0         0         0           -           0
2021-05-16T20:36:50.552325+0000    68     206       206         0         0         0           -           0
2021-05-16T20:36:51.552407+0000    69     211       211         0         0         0           -           0
2021-05-16T20:36:52.552497+0000    70     211       211         0         0         0           -           0
2021-05-16T20:36:53.552563+0000    71     214       214         0         0         0           -           0
2021-05-16T20:36:54.552630+0000    72     214       214         0         0         0           -           0
2021-05-16T20:36:55.552706+0000    73     225       225         0         0         0           -           0
2021-05-16T20:36:56.552773+0000    74     225       225         0         0         0           -           0
2021-05-16T20:36:57.552973+0000    75     227       227         0         0         0           -           0
2021-05-16T20:36:58.553054+0000    76     227       227         0         0         0           -           0
2021-05-16T20:36:59.553265+0000    77     234       234         0         0         0           -           0
2021-05-16T20:37:00.553339+0000    78     234       234         0         0         0           -           0
2021-05-16T20:37:01.553406+0000    79     240       240         0         0         0           -           0
2021-05-16T20:37:02.553474+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:37:02.553474+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:37:02.553474+0000    80     240       240         0         0         0           -           0
2021-05-16T20:37:03.553565+0000    81     240       240         0         0         0           -           0
2021-05-16T20:37:04.553789+0000    82     243       243         0         0         0           -           0
2021-05-16T20:37:05.553883+0000    83     243       243         0         0         0           -           0
2021-05-16T20:37:06.553951+0000    84     249       249         0         0         0           -           0
2021-05-16T20:37:07.554056+0000    85     249       249         0         0         0           -           0
2021-05-16T20:37:08.554121+0000    86     250       250         0         0         0           -           0
2021-05-16T20:37:09.554272+0000    87     250       250         0         0         0           -           0
2021-05-16T20:37:10.554370+0000    88     250       250         0         0         0           -           0
2021-05-16T20:37:11.554448+0000    89     250       250         0         0         0           -           0
2021-05-16T20:37:12.554526+0000    90     253       253         0         0         0           -           0
2021-05-16T20:37:13.554592+0000    91     253       253         0         0         0           -           0
2021-05-16T20:37:14.554666+0000    92     253       253         0         0         0           -           0
2021-05-16T20:37:15.554743+0000    93     253       253         0         0         0           -           0
2021-05-16T20:37:16.568005+0000    94      16       256       240   10.2103   10.2128     4.35569     58.4159
2021-05-16T20:37:17.568062+0000    95      16       256       240   10.1028         0           -     58.4159
2021-05-16T20:37:18.568128+0000    96      16       256       240   9.99763         0           -     58.4159
2021-05-16T20:37:19.568204+0000    97      16       256       240   9.89458         0           -     58.4159
2021-05-16T20:37:20.568270+0000    98      16       256       240   9.79363         0           -     58.4159
2021-05-16T20:37:21.568336+0000    99       1       256       255   10.3006        12     4.31359     56.1421
2021-05-16T20:37:22.568415+0000 min lat: 4.31359 max lat: 93.9687 avg lat: 56.1421
2021-05-16T20:37:22.568415+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:37:22.568415+0000   100       1       256       255   10.1976         0           -     56.1421
2021-05-16T20:37:23.568575+0000 Total time run:       100.404
Total reads made:     256
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   10.1988
Average IOPS:         2
Stddev IOPS:          0.35887
Max IOPS:             3
Min IOPS:             0
Average Latency(s):   55.9479
Max latency(s):       93.9687
Min latency(s):       4.31359

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:37:24,145087073-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 263765


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:37:24,149829154-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:37:47,543753741-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:37:47,553718614-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:37:55,973648968-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:37:55,983472606-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:38:04,561753553-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:38:04,571926207-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:38:13,099455084-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:38:13,109140924-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:38:21,466016845-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 1.0 GiB
    usage:   2.0 GiB used, 298 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:38:21,475548354-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:38:21,483784489-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T16:38:21,487874163-04:00][RUNNING][ROUND 1/7/40] object_size=16MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:38:21,491657973-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:38:21,501312083-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40673\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.177418\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 16ce45f8-8e80-4ba1-b83c-e52d970d3654\nsetting min_mon_release = octopus\nepoch 0\nfsid 16ce45f8-8e80-4ba1-b83c-e52d970d3654\nlast_changed 2021-05-16T13:38:36.834136-0700\ncreated 2021-05-16T13:38:36.834136-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40673/0,v1:10.10.1.2:40674/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.177418 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 72345498-df0e-40c3-9299-e6330d9b6dc7\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 5f065918-431f-495c-9fa9-2baf1693bb82\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 5e451d87-e467-4813-9dce-c40cd6bc98d7\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42673\n  w/ user/pass: admin / 88ca8bbe-9420-4553-973d-b5018e4a2af8\n\n'
10.10.1.2: b'\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:38:55 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40673
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.177418
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 16ce45f8-8e80-4ba1-b83c-e52d970d3654
setting min_mon_release = octopus
epoch 0
fsid 16ce45f8-8e80-4ba1-b83c-e52d970d3654
last_changed 2021-05-16T13:38:36.834136-0700
created 2021-05-16T13:38:36.834136-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40673/0,v1:10.10.1.2:40674/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.177418 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 72345498-df0e-40c3-9299-e6330d9b6dc7
0
start osd.0
add osd1 5f065918-431f-495c-9fa9-2baf1693bb82
1
start osd.1
add osd2 5e451d87-e467-4813-9dce-c40cd6bc98d7
2
start osd.2


restful urls: https://10.10.1.2:42673
  w/ user/pass: admin / 88ca8bbe-9420-4553-973d-b5018e4a2af8


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T13:38:23.340-0700 7f12f92401c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:38:23.340-0700 7f12f92401c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:38:23.360-0700 7fd9836931c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:38:23.360-0700 7fd9836931c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40673,v1:10.10.1.2:40674] --print /tmp/ceph_monmap.177418 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.177418 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.177418 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42673 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x560dfd5f4000 @  0x7fa1be0c7680 0x7fa1be0e8824 0x7fa1be883187 0x7fa1be88b355 0x7fa1be883708 0x7fa1be883877 0x7fa1be884c24 0x7fa1be89cec1 0x7fa1be80f5f3 0x7fa1be870e97 0x7fa1be878b1a 0x7fa1bdf7bd84 0x7fa1be097609 0x7fa1bdc6b293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.1RvhcRgOmt 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 72345498-df0e-40c3-9299-e6330d9b6dc7 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDYgqFg+3eGFBAAKeLOsv2EXT6lmpplsE9c7Q== --osd-uuid 72345498-df0e-40c3-9299-e6330d9b6dc7 
2021-05-16T13:38:49.000-0700 7f390d1d4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:38:49.020-0700 7f390d1d4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:38:49.020-0700 7f390d1d4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x562d4069c000 @  0x7f390db9d680 0x7f390dbbe824 0x562d34783447 0x562d3478b4b5 0x562d347839c8 0x562d34783b37 0x562d34784ee4 0x562d34555ca1 0x562d3472d423 0x562d345462a7 0x562d3454b53a 0x7f390d6f0d84 0x7f390d875609 0x7f390d3de293
2021-05-16T13:38:49.340-0700 7f390d1d4f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5f065918-431f-495c-9fa9-2baf1693bb82 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5605d09d6000 @  0x7fce99329680 0x7fce9934a824 0x5605c5b45447 0x5605c5b4d4b5 0x5605c5b459c8 0x5605c5b45b37 0x5605c5b46ee4 0x5605c5917ca1 0x5605c5aef423 0x5605c59082a7 0x5605c590d53a 0x7fce98e7cd84 0x7fce99001609 0x7fce98b6a293
2021-05-16T13:38:49.952-0700 7fce98960f00 -1 Falling back to public interface
2021-05-16T13:38:50.216-0700 7fce98960f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDZgqFgTqa4JhAAyYG+tA279Cc+MsE7QpT+Uw== --osd-uuid 5f065918-431f-495c-9fa9-2baf1693bb82 
2021-05-16T13:38:50.328-0700 7f96cb29bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:38:50.352-0700 7f96cb29bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:38:50.352-0700 7f96cb29bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55649bd7a000 @  0x7f96cbc64680 0x7f96cbc85824 0x55649078d447 0x5564907954b5 0x55649078d9c8 0x55649078db37 0x55649078eee4 0x55649055fca1 0x556490737423 0x5564905502a7 0x55649055553a 0x7f96cb7b7d84 0x7f96cb93c609 0x7f96cb4a5293
2021-05-16T13:38:50.676-0700 7f96cb29bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5e451d87-e467-4813-9dce-c40cd6bc98d7 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x564ebe67e000 @  0x7f967ac9a680 0x7f967acbb824 0x564eb3abc447 0x564eb3ac44b5 0x564eb3abc9c8 0x564eb3abcb37 0x564eb3abdee4 0x564eb388eca1 0x564eb3a66423 0x564eb387f2a7 0x564eb388453a 0x7f967a7edd84 0x7f967a972609 0x7f967a4db293
2021-05-16T13:38:51.372-0700 7f967a2d1f00 -1 Falling back to public interface
2021-05-16T13:38:51.628-0700 7f967a2d1f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDbgqFgiZ+9AxAAvDeVMlW/nmj1VTU89YvE7A== --osd-uuid 5e451d87-e467-4813-9dce-c40cd6bc98d7 
2021-05-16T13:38:51.752-0700 7facec253f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:38:51.772-0700 7facec253f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:38:51.772-0700 7facec253f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55e9f7bf0000 @  0x7facecc1c680 0x7facecc3d824 0x55e9eca16447 0x55e9eca1e4b5 0x55e9eca169c8 0x55e9eca16b37 0x55e9eca17ee4 0x55e9ec7e8ca1 0x55e9ec9c0423 0x55e9ec7d92a7 0x55e9ec7de53a 0x7facec76fd84 0x7facec8f4609 0x7facec45d293
2021-05-16T13:38:52.088-0700 7facec253f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55d9d6fbe000 @  0x7f79aee91680 0x7f79aeeb2824 0x55d9cba69447 0x55d9cba714b5 0x55d9cba699c8 0x55d9cba69b37 0x55d9cba6aee4 0x55d9cb83bca1 0x55d9cba13423 0x55d9cb82c2a7 0x55d9cb83153a 0x7f79ae9e4d84 0x7f79aeb69609 0x7f79ae6d2293
2021-05-16T13:38:52.784-0700 7f79ae4c8f00 -1 Falling back to public interface
2021-05-16T13:38:53.060-0700 7f79ae4c8f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:38:55,223447694-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:38:55,234595300-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:38:55,275422562-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:38:55,278553665-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c90f2fe-076d-40b0-bbeb-62997dfca6b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c90f2fe-076d-40b0-bbeb-62997dfca6b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:38:58,875200970-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:38:58,878954643-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c90f2fe-076d-40b0-bbeb-62997dfca6b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c90f2fe-076d-40b0-bbeb-62997dfca6b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:02,615879041-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:02,619632954-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c90f2fe-076d-40b0-bbeb-62997dfca6b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c90f2fe-076d-40b0-bbeb-62997dfca6b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:05,987151819-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:05,990664820-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c90f2fe-076d-40b0-bbeb-62997dfca6b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c90f2fe-076d-40b0-bbeb-62997dfca6b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:12,827323962-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:12,830999518-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c90f2fe-076d-40b0-bbeb-62997dfca6b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c90f2fe-076d-40b0-bbeb-62997dfca6b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:16,985375747-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:16,988920046-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c90f2fe-076d-40b0-bbeb-62997dfca6b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c90f2fe-076d-40b0-bbeb-62997dfca6b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:21,499957792-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:21,503388018-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c90f2fe-076d-40b0-bbeb-62997dfca6b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c90f2fe-076d-40b0-bbeb-62997dfca6b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:25,138003233-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:25,141699739-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c90f2fe-076d-40b0-bbeb-62997dfca6b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c90f2fe-076d-40b0-bbeb-62997dfca6b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:29,609767136-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:29,613389953-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c90f2fe-076d-40b0-bbeb-62997dfca6b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c90f2fe-076d-40b0-bbeb-62997dfca6b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:34,053566796-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:34,057122668-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c90f2fe-076d-40b0-bbeb-62997dfca6b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c90f2fe-076d-40b0-bbeb-62997dfca6b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:37,617468516-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:37,621458994-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c90f2fe-076d-40b0-bbeb-62997dfca6b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c90f2fe-076d-40b0-bbeb-62997dfca6b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:41,042944231-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:41,046553562-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c90f2fe-076d-40b0-bbeb-62997dfca6b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c90f2fe-076d-40b0-bbeb-62997dfca6b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:39:44,498683629-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:40:08,027512749-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:40:16,436004556-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:40:25,001884248-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:40:33,597460910-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:40:42,176652938-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:40:50,756933761-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:40:50,766215912-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:40:59,371202327-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:40:59,381449401-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:41:07,868989872-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:41:07,878456500-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:41:16,440490251-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:41:16,450665930-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:41:24,970665954-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:41:24,979719304-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:41:24,987345524-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:41:24,991657816-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:41:25,001598294-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=270238
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:41:25,009006834-04:00] INFO: > Run rados bench[0m
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.1
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:41:25,027346053-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:41:25,030758013-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c90f2fe-076d-40b0-bbeb-62997dfca6b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c90f2fe-076d-40b0-bbeb-62997dfca6b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T20:41:27.317+0000 7f3e99e76d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:41:27.593+0000 7f3e99e76d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:41:27.593+0000 7f3e99e76d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T20:41:27.612869+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-16T20:41:27.612901+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T20:41:28.613864+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:41:28.613864+0000     0       0         0         0         0         0           -           0
2021-05-16T20:41:29.613982+0000     1       4         4         0         0         0           -           0
2021-05-16T20:41:30.614084+0000     2       5         5         0         0         0           -           0
2021-05-16T20:41:31.614163+0000     3       5         5         0         0         0           -           0
2021-05-16T20:41:32.614273+0000     4      12        12         0         0         0           -           0
2021-05-16T20:41:33.614378+0000     5      12        12         0         0         0           -           0
2021-05-16T20:41:34.614475+0000     6      22        22         0         0         0           -           0
2021-05-16T20:41:35.614618+0000     7      36        36         0         0         0           -           0
2021-05-16T20:41:36.614730+0000     8      40        40         0         0         0           -           0
2021-05-16T20:41:37.614989+0000     9      43        43         0         0         0           -           0
2021-05-16T20:41:38.615097+0000    10      45        45         0         0         0           -           0
2021-05-16T20:41:39.615304+0000    11      68        68         0         0         0           -           0
2021-05-16T20:41:40.615550+0000    12      68        68         0         0         0           -           0
2021-05-16T20:41:41.615652+0000    13      94        94         0         0         0           -           0
2021-05-16T20:41:42.615801+0000    14     111       111         0         0         0           -           0
2021-05-16T20:41:43.615920+0000    15     111       111         0         0         0           -           0
2021-05-16T20:41:44.616020+0000    16     124       124         0         0         0           -           0
2021-05-16T20:41:45.616134+0000    17     124       124         0         0         0           -           0
2021-05-16T20:41:46.616227+0000    18     139       139         0         0         0           -           0
2021-05-16T20:41:47.616372+0000    19     142       142         0         0         0           -           0
2021-05-16T20:41:48.616495+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:41:48.616495+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:41:48.616495+0000    20     153       153         0         0         0           -           0
2021-05-16T20:41:49.616631+0000    21     155       155         0         0         0           -           0
2021-05-16T20:41:50.616740+0000    22     155       155         0         0         0           -           0
2021-05-16T20:41:51.616953+0000    23     155       155         0         0         0           -           0
2021-05-16T20:41:52.617201+0000    24     155       155         0         0         0           -           0
2021-05-16T20:41:53.617275+0000    25     155       155         0         0         0           -           0
2021-05-16T20:41:54.617341+0000    26     155       155         0         0         0           -           0
2021-05-16T20:41:55.617589+0000    27     156       156         0         0         0           -           0
2021-05-16T20:41:56.617796+0000    28     156       156         0         0         0           -           0
2021-05-16T20:41:57.617907+0000    29     179       179         0         0         0           -           0
2021-05-16T20:41:58.618045+0000    30     198       198         0         0         0           -           0
2021-05-16T20:41:59.618180+0000    31     217       217         0         0         0           -           0
2021-05-16T20:42:00.618283+0000    32     217       217         0         0         0           -           0
2021-05-16T20:42:01.618378+0000    33     232       232         0         0         0           -           0
2021-05-16T20:42:02.618528+0000    34       1       256       255   119.984       120    0.115132     14.7561
2021-05-16T20:42:03.618795+0000    35       1       256       255   116.555         0           -     14.7561
2021-05-16T20:42:04.619044+0000 Total time run:         35.7315
Total writes made:      256
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     114.633
Stddev Bandwidth:       20.2837
Max bandwidth (MB/sec): 120
Min bandwidth (MB/sec): 0
Average IOPS:           7
Stddev IOPS:            1.18322
Max IOPS:               7
Min IOPS:               0
Average Latency(s):     14.7063
Stddev Latency(s):      10.2142
Max latency(s):         33.8388
Min latency(s):         0.115132

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:42:05,270500943-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 270238


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:42:05,275806682-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:42:28,690905301-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:42:28,700505670-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:42:37,128847460-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:42:37,138307124-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:42:45,585029481-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:42:45,595087600-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:42:54,271171236-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:42:54,280979305-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:43:02,934557255-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:43:02,944424474-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:43:02,952429406-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:43:02,957162258-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:43:02,966701001-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=271632
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:43:02,974166518-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:43:02,994696293-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:43:02,998117411-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c90f2fe-076d-40b0-bbeb-62997dfca6b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c90f2fe-076d-40b0-bbeb-62997dfca6b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3YKDNV:/tmp/ceph-asok.3YKDNV -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T20:43:05.338+0000 7faf62c20d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:43:05.594+0000 7faf62c20d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:43:05.594+0000 7faf62c20d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T20:43:05.617117+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:43:05.617117+0000     0       0         0         0         0         0           -           0
2021-05-16T20:43:06.617227+0000     1       7         7         0         0         0           -           0
2021-05-16T20:43:07.617489+0000     2       7         7         0         0         0           -           0
2021-05-16T20:43:08.617569+0000     3       9         9         0         0         0           -           0
2021-05-16T20:43:09.617674+0000     4       9         9         0         0         0           -           0
2021-05-16T20:43:10.617768+0000     5      10        10         0         0         0           -           0
2021-05-16T20:43:11.617879+0000     6      10        10         0         0         0           -           0
2021-05-16T20:43:12.617982+0000     7      12        12         0         0         0           -           0
2021-05-16T20:43:13.618086+0000     8      12        12         0         0         0           -           0
2021-05-16T20:43:14.618183+0000     9      13        13         0         0         0           -           0
2021-05-16T20:43:15.618275+0000    10      13        13         0         0         0           -           0
2021-05-16T20:43:16.618375+0000    11      14        14         0         0         0           -           0
2021-05-16T20:43:17.618442+0000    12      14        14         0         0         0           -           0
2021-05-16T20:43:18.618592+0000    13      14        14         0         0         0           -           0
2021-05-16T20:43:19.618668+0000    14      14        14         0         0         0           -           0
2021-05-16T20:43:20.618778+0000    15      16        16         0         0         0           -           0
2021-05-16T20:43:21.618955+0000    16      16        16         0         0         0           -           0
2021-05-16T20:43:22.619210+0000    17      16        16         0         0         0           -           0
2021-05-16T20:43:23.619448+0000    18      17        17         0         0         0           -           0
2021-05-16T20:43:24.619535+0000    19      17        17         0         0         0           -           0
2021-05-16T20:43:25.619626+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:43:25.619626+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:43:25.619626+0000    20      19        19         0         0         0           -           0
2021-05-16T20:43:26.619809+0000    21      19        19         0         0         0           -           0
2021-05-16T20:43:27.619901+0000    22      19        19         0         0         0           -           0
2021-05-16T20:43:28.619967+0000    23      19        19         0         0         0           -           0
2021-05-16T20:43:29.620067+0000    24      25        25         0         0         0           -           0
2021-05-16T20:43:30.620157+0000    25      25        25         0         0         0           -           0
2021-05-16T20:43:31.620261+0000    26      25        25         0         0         0           -           0
2021-05-16T20:43:32.620369+0000    27      25        25         0         0         0           -           0
2021-05-16T20:43:33.620455+0000    28      30        30         0         0         0           -           0
2021-05-16T20:43:34.620556+0000    29      30        30         0         0         0           -           0
2021-05-16T20:43:35.620647+0000    30      30        30         0         0         0           -           0
2021-05-16T20:43:36.620723+0000    31      30        30         0         0         0           -           0
2021-05-16T20:43:37.620953+0000    32      30        30         0         0         0           -           0
2021-05-16T20:43:38.621201+0000    33      31        31         0         0         0           -           0
2021-05-16T20:43:39.621424+0000    34      31        31         0         0         0           -           0
2021-05-16T20:43:40.621528+0000    35      34        34         0         0         0           -           0
2021-05-16T20:43:41.621621+0000    36      34        34         0         0         0           -           0
2021-05-16T20:43:42.621856+0000    37      34        34         0         0         0           -           0
2021-05-16T20:43:43.621933+0000    38      34        34         0         0         0           -           0
2021-05-16T20:43:44.622044+0000    39      40        40         0         0         0           -           0
2021-05-16T20:43:45.622164+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:43:45.622164+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:43:45.622164+0000    40      40        40         0         0         0           -           0
2021-05-16T20:43:46.622298+0000    41      43        43         0         0         0           -           0
2021-05-16T20:43:47.622413+0000    42      43        43         0         0         0           -           0
2021-05-16T20:43:48.622518+0000    43      45        45         0         0         0           -           0
2021-05-16T20:43:49.622598+0000    44      45        45         0         0         0           -           0
2021-05-16T20:43:50.622712+0000    45      46        46         0         0         0           -           0
2021-05-16T20:43:51.622976+0000    46      46        46         0         0         0           -           0
2021-05-16T20:43:52.623071+0000    47      46        46         0         0         0           -           0
2021-05-16T20:43:53.623159+0000    48      47        47         0         0         0           -           0
2021-05-16T20:43:54.623254+0000    49      47        47         0         0         0           -           0
2021-05-16T20:43:55.623527+0000    50      51        51         0         0         0           -           0
2021-05-16T20:43:56.623631+0000    51      51        51         0         0         0           -           0
2021-05-16T20:43:57.623740+0000    52      52        52         0         0         0           -           0
2021-05-16T20:43:58.623846+0000    53      52        52         0         0         0           -           0
2021-05-16T20:43:59.623955+0000    54      52        52         0         0         0           -           0
2021-05-16T20:44:00.624047+0000    55      52        52         0         0         0           -           0
2021-05-16T20:44:01.624159+0000    56      59        59         0         0         0           -           0
2021-05-16T20:44:02.624292+0000    57      59        59         0         0         0           -           0
2021-05-16T20:44:03.624391+0000    58      59        59         0         0         0           -           0
2021-05-16T20:44:04.624470+0000    59      59        59         0         0         0           -           0
2021-05-16T20:44:05.624579+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:44:05.624579+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:44:05.624579+0000    60      62        62         0         0         0           -           0
2021-05-16T20:44:06.624697+0000    61      62        62         0         0         0           -           0
2021-05-16T20:44:07.624955+0000    62      62        62         0         0         0           -           0
2021-05-16T20:44:08.625046+0000    63      63        63         0         0         0           -           0
2021-05-16T20:44:09.625258+0000    64      63        63         0         0         0           -           0
2021-05-16T20:44:10.625524+0000    65      64        64         0         0         0           -           0
2021-05-16T20:44:11.625747+0000    66      64        64         0         0         0           -           0
2021-05-16T20:44:12.625850+0000    67      65        65         0         0         0           -           0
2021-05-16T20:44:13.625917+0000    68      65        65         0         0         0           -           0
2021-05-16T20:44:14.626023+0000    69      71        71         0         0         0           -           0
2021-05-16T20:44:15.626129+0000    70      71        71         0         0         0           -           0
2021-05-16T20:44:16.626225+0000    71      72        72         0         0         0           -           0
2021-05-16T20:44:17.626301+0000    72      72        72         0         0         0           -           0
2021-05-16T20:44:18.626542+0000    73      74        74         0         0         0           -           0
2021-05-16T20:44:19.626637+0000    74      74        74         0         0         0           -           0
2021-05-16T20:44:20.626727+0000    75      74        74         0         0         0           -           0
2021-05-16T20:44:21.626951+0000    76      74        74         0         0         0           -           0
2021-05-16T20:44:22.627052+0000    77      74        74         0         0         0           -           0
2021-05-16T20:44:23.627272+0000    78      75        75         0         0         0           -           0
2021-05-16T20:44:24.627521+0000    79      75        75         0         0         0           -           0
2021-05-16T20:44:25.627623+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:44:25.627623+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:44:25.627623+0000    80      76        76         0         0         0           -           0
2021-05-16T20:44:26.627700+0000    81      76        76         0         0         0           -           0
2021-05-16T20:44:27.627858+0000    82      76        76         0         0         0           -           0
2021-05-16T20:44:28.627927+0000    83      76        76         0         0         0           -           0
2021-05-16T20:44:29.628016+0000    84      79        79         0         0         0           -           0
2021-05-16T20:44:30.628135+0000    85      79        79         0         0         0           -           0
2021-05-16T20:44:31.628239+0000    86      80        80         0         0         0           -           0
2021-05-16T20:44:32.628333+0000    87      80        80         0         0         0           -           0
2021-05-16T20:44:33.628463+0000    88      86        86         0         0         0           -           0
2021-05-16T20:44:34.628585+0000    89      86        86         0         0         0           -           0
2021-05-16T20:44:35.628674+0000    90      86        86         0         0         0           -           0
2021-05-16T20:44:36.628776+0000    91      87        87         0         0         0           -           0
2021-05-16T20:44:37.628989+0000    92      87        87         0         0         0           -           0
2021-05-16T20:44:38.629091+0000    93      89        89         0         0         0           -           0
2021-05-16T20:44:39.629176+0000    94      89        89         0         0         0           -           0
2021-05-16T20:44:40.629281+0000    95      89        89         0         0         0           -           0
2021-05-16T20:44:41.629483+0000    96      89        89         0         0         0           -           0
2021-05-16T20:44:42.629581+0000    97      89        89         0         0         0           -           0
2021-05-16T20:44:43.629660+0000    98      89        89         0         0         0           -           0
2021-05-16T20:44:44.629832+0000    99      93        93         0         0         0           -           0
2021-05-16T20:44:45.629956+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:44:45.629956+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:44:45.629956+0000   100      93        93         0         0         0           -           0
2021-05-16T20:44:46.630049+0000   101     101       101         0         0         0           -           0
2021-05-16T20:44:47.630161+0000   102     101       101         0         0         0           -           0
2021-05-16T20:44:48.630244+0000   103     104       104         0         0         0           -           0
2021-05-16T20:44:49.630368+0000   104     104       104         0         0         0           -           0
2021-05-16T20:44:50.630458+0000   105     104       104         0         0         0           -           0
2021-05-16T20:44:51.630556+0000   106     112       112         0         0         0           -           0
2021-05-16T20:44:52.630653+0000   107     112       112         0         0         0           -           0
2021-05-16T20:44:53.630755+0000   108     113       113         0         0         0           -           0
2021-05-16T20:44:54.630949+0000   109     113       113         0         0         0           -           0
2021-05-16T20:44:55.631047+0000   110     113       113         0         0         0           -           0
2021-05-16T20:44:56.631259+0000   111     113       113         0         0         0           -           0
2021-05-16T20:44:57.631408+0000   112     115       115         0         0         0           -           0
2021-05-16T20:44:58.631613+0000   113     115       115         0         0         0           -           0
2021-05-16T20:44:59.631865+0000   114     121       121         0         0         0           -           0
2021-05-16T20:45:00.631993+0000   115     121       121         0         0         0           -           0
2021-05-16T20:45:01.632107+0000   116     122       122         0         0         0           -           0
2021-05-16T20:45:02.632226+0000   117     122       122         0         0         0           -           0
2021-05-16T20:45:03.632302+0000   118     123       123         0         0         0           -           0
2021-05-16T20:45:04.632398+0000   119     123       123         0         0         0           -           0
2021-05-16T20:45:05.632513+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:45:05.632513+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:45:05.632513+0000   120     123       123         0         0         0           -           0
2021-05-16T20:45:06.632627+0000   121     123       123         0         0         0           -           0
2021-05-16T20:45:07.632716+0000   122     123       123         0         0         0           -           0
2021-05-16T20:45:08.632979+0000   123     128       128         0         0         0           -           0
2021-05-16T20:45:09.633081+0000   124     128       128         0         0         0           -           0
2021-05-16T20:45:10.633181+0000   125     130       130         0         0         0           -           0
2021-05-16T20:45:11.633342+0000   126     130       130         0         0         0           -           0
2021-05-16T20:45:12.633441+0000   127     132       132         0         0         0           -           0
2021-05-16T20:45:13.633533+0000   128     132       132         0         0         0           -           0
2021-05-16T20:45:14.633722+0000   129     133       133         0         0         0           -           0
2021-05-16T20:45:15.633813+0000   130     133       133         0         0         0           -           0
2021-05-16T20:45:16.633966+0000   131     134       134         0         0         0           -           0
2021-05-16T20:45:17.634059+0000   132     134       134         0         0         0           -           0
2021-05-16T20:45:18.634126+0000   133     137       137         0         0         0           -           0
2021-05-16T20:45:19.634224+0000   134     139       139         0         0         0           -           0
2021-05-16T20:45:20.634313+0000   135     139       139         0         0         0           -           0
2021-05-16T20:45:21.634416+0000   136     139       139         0         0         0           -           0
2021-05-16T20:45:22.634573+0000   137     139       139         0         0         0           -           0
2021-05-16T20:45:23.634655+0000   138     140       140         0         0         0           -           0
2021-05-16T20:45:24.634866+0000   139     140       140         0         0         0           -           0
2021-05-16T20:45:25.634964+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:45:25.634964+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:45:25.634964+0000   140     144       144         0         0         0           -           0
2021-05-16T20:45:26.635087+0000   141     144       144         0         0         0           -           0
2021-05-16T20:45:27.635279+0000   142     144       144         0         0         0           -           0
2021-05-16T20:45:28.635508+0000   143     144       144         0         0         0           -           0
2021-05-16T20:45:29.635604+0000   144     147       147         0         0         0           -           0
2021-05-16T20:45:30.635798+0000   145     147       147         0         0         0           -           0
2021-05-16T20:45:31.635899+0000   146     156       156         0         0         0           -           0
2021-05-16T20:45:32.636031+0000   147     156       156         0         0         0           -           0
2021-05-16T20:45:33.636098+0000   148     156       156         0         0         0           -           0
2021-05-16T20:45:34.636201+0000   149     159       159         0         0         0           -           0
2021-05-16T20:45:35.636297+0000   150     159       159         0         0         0           -           0
2021-05-16T20:45:36.636388+0000   151     160       160         0         0         0           -           0
2021-05-16T20:45:37.636461+0000   152     160       160         0         0         0           -           0
2021-05-16T20:45:38.636536+0000   153     160       160         0         0         0           -           0
2021-05-16T20:45:39.636613+0000   154     160       160         0         0         0           -           0
2021-05-16T20:45:40.636713+0000   155     163       163         0         0         0           -           0
2021-05-16T20:45:41.636980+0000   156     163       163         0         0         0           -           0
2021-05-16T20:45:42.637219+0000   157     163       163         0         0         0           -           0
2021-05-16T20:45:43.637304+0000   158     163       163         0         0         0           -           0
2021-05-16T20:45:44.637497+0000   159     163       163         0         0         0           -           0
2021-05-16T20:45:45.637742+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:45:45.637742+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:45:45.637742+0000   160     163       163         0         0         0           -           0
2021-05-16T20:45:46.637826+0000   161     170       170         0         0         0           -           0
2021-05-16T20:45:47.638097+0000   162     170       170         0         0         0           -           0
2021-05-16T20:45:48.638162+0000   163     170       170         0         0         0           -           0
2021-05-16T20:45:49.638268+0000   164     174       174         0         0         0           -           0
2021-05-16T20:45:50.638375+0000   165     174       174         0         0         0           -           0
2021-05-16T20:45:51.638449+0000   166     175       175         0         0         0           -           0
2021-05-16T20:45:52.638543+0000   167     175       175         0         0         0           -           0
2021-05-16T20:45:53.638623+0000   168     179       179         0         0         0           -           0
2021-05-16T20:45:54.638733+0000   169     179       179         0         0         0           -           0
2021-05-16T20:45:55.638978+0000   170     182       182         0         0         0           -           0
2021-05-16T20:45:56.639091+0000   171     182       182         0         0         0           -           0
2021-05-16T20:45:57.639298+0000   172     182       182         0         0         0           -           0
2021-05-16T20:45:58.639391+0000   173     182       182         0         0         0           -           0
2021-05-16T20:45:59.639513+0000   174     183       183         0         0         0           -           0
2021-05-16T20:46:00.639609+0000   175     183       183         0         0         0           -           0
2021-05-16T20:46:01.639695+0000   176     190       190         0         0         0           -           0
2021-05-16T20:46:02.639832+0000   177     190       190         0         0         0           -           0
2021-05-16T20:46:03.639908+0000   178     190       190         0         0         0           -           0
2021-05-16T20:46:04.640011+0000   179     192       192         0         0         0           -           0
2021-05-16T20:46:05.640132+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:46:05.640132+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:46:05.640132+0000   180     192       192         0         0         0           -           0
2021-05-16T20:46:06.640219+0000   181     193       193         0         0         0           -           0
2021-05-16T20:46:07.640299+0000   182     193       193         0         0         0           -           0
2021-05-16T20:46:08.640367+0000   183     193       193         0         0         0           -           0
2021-05-16T20:46:09.640449+0000   184     193       193         0         0         0           -           0
2021-05-16T20:46:10.640539+0000   185     193       193         0         0         0           -           0
2021-05-16T20:46:11.640625+0000   186     193       193         0         0         0           -           0
2021-05-16T20:46:12.640729+0000   187     196       196         0         0         0           -           0
2021-05-16T20:46:13.640962+0000   188     196       196         0         0         0           -           0
2021-05-16T20:46:14.641042+0000   189     201       201         0         0         0           -           0
2021-05-16T20:46:15.641250+0000   190     201       201         0         0         0           -           0
2021-05-16T20:46:16.641332+0000   191     205       205         0         0         0           -           0
2021-05-16T20:46:17.641548+0000   192     205       205         0         0         0           -           0
2021-05-16T20:46:18.641670+0000   193     205       205         0         0         0           -           0
2021-05-16T20:46:19.641750+0000   194     207       207         0         0         0           -           0
2021-05-16T20:46:20.641861+0000   195     207       207         0         0         0           -           0
2021-05-16T20:46:21.641927+0000   196     209       209         0         0         0           -           0
2021-05-16T20:46:22.642023+0000   197     209       209         0         0         0           -           0
2021-05-16T20:46:23.642098+0000   198     210       210         0         0         0           -           0
2021-05-16T20:46:24.642290+0000   199     210       210         0         0         0           -           0
2021-05-16T20:46:25.642385+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:46:25.642385+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:46:25.642385+0000   200     212       212         0         0         0           -           0
2021-05-16T20:46:26.642587+0000   201     212       212         0         0         0           -           0
2021-05-16T20:46:27.642661+0000   202     212       212         0         0         0           -           0
2021-05-16T20:46:28.642879+0000   203     212       212         0         0         0           -           0
2021-05-16T20:46:29.642976+0000   204     215       215         0         0         0           -           0
2021-05-16T20:46:30.643273+0000   205     215       215         0         0         0           -           0
2021-05-16T20:46:31.643348+0000   206     215       215         0         0         0           -           0
2021-05-16T20:46:32.643564+0000   207     215       215         0         0         0           -           0
2021-05-16T20:46:33.643634+0000   208     215       215         0         0         0           -           0
2021-05-16T20:46:34.643699+0000   209     217       217         0         0         0           -           0
2021-05-16T20:46:35.643872+0000   210     217       217         0         0         0           -           0
2021-05-16T20:46:36.643965+0000   211     219       219         0         0         0           -           0
2021-05-16T20:46:37.644084+0000   212     219       219         0         0         0           -           0
2021-05-16T20:46:38.644150+0000   213     219       219         0         0         0           -           0
2021-05-16T20:46:39.644254+0000   214     219       219         0         0         0           -           0
2021-05-16T20:46:40.644346+0000   215     219       219         0         0         0           -           0
2021-05-16T20:46:41.644456+0000   216     219       219         0         0         0           -           0
2021-05-16T20:46:42.644560+0000   217     225       225         0         0         0           -           0
2021-05-16T20:46:43.644688+0000   218     225       225         0         0         0           -           0
2021-05-16T20:46:44.644785+0000   219     229       229         0         0         0           -           0
2021-05-16T20:46:45.644983+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:46:45.644983+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:46:45.644983+0000   220     229       229         0         0         0           -           0
2021-05-16T20:46:46.645208+0000   221     229       229         0         0         0           -           0
2021-05-16T20:46:47.645293+0000   222     234       234         0         0         0           -           0
2021-05-16T20:46:48.645549+0000   223     234       234         0         0         0           -           0
2021-05-16T20:46:49.645614+0000   224     234       234         0         0         0           -           0
2021-05-16T20:46:50.645704+0000   225     234       234         0         0         0           -           0
2021-05-16T20:46:51.645816+0000   226     241       241         0         0         0           -           0
2021-05-16T20:46:52.645949+0000   227     241       241         0         0         0           -           0
2021-05-16T20:46:53.646189+0000   228     243       243         0         0         0           -           0
2021-05-16T20:46:54.646296+0000   229     243       243         0         0         0           -           0
2021-05-16T20:46:55.646408+0000   230     244       244         0         0         0           -           0
2021-05-16T20:46:56.646546+0000   231     244       244         0         0         0           -           0
2021-05-16T20:46:57.646620+0000   232     247       247         0         0         0           -           0
2021-05-16T20:46:58.646739+0000   233     247       247         0         0         0           -           0
2021-05-16T20:46:59.646836+0000   234     250       250         0         0         0           -           0
2021-05-16T20:47:00.646997+0000   235     250       250         0         0         0           -           0
2021-05-16T20:47:01.647068+0000   236     250       250         0         0         0           -           0
2021-05-16T20:47:02.647171+0000   237     255       255         0         0         0           -           0
2021-05-16T20:47:03.647286+0000   238     255       255         0         0         0           -           0
2021-05-16T20:47:04.647493+0000   239       6       256       250   16.7343   16.7364     2.50358      118.41
2021-05-16T20:47:05.647594+0000 min lat: 2.50358 max lat: 238.24 avg lat: 118.41
2021-05-16T20:47:05.647594+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:47:05.647594+0000   240       6       256       250   16.6645         0           -      118.41
2021-05-16T20:47:06.647689+0000   241       3       256       253   16.7945        24     4.27965     117.065
2021-05-16T20:47:07.647856+0000   242       3       256       253   16.7251         0           -     117.065
2021-05-16T20:47:08.647931+0000   243       2       256       254   16.7221         8     8.57291     116.638
2021-05-16T20:47:09.648032+0000   244       2       256       254   16.6536         0           -     116.638
2021-05-16T20:47:10.648123+0000   245       1       256       255   16.6509         8     8.58941     116.214
2021-05-16T20:47:11.648229+0000   246       1       256       255   16.5833         0           -     116.214
2021-05-16T20:47:12.648326+0000   247       1       256       255   16.5161         0           -     116.214
2021-05-16T20:47:13.648395+0000   248       1       256       255   16.4495         0           -     116.214
2021-05-16T20:47:14.648544+0000 Total time run:       248.969
Total reads made:     256
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   16.4518
Average IOPS:         1
Stddev IOPS:          0.0896207
Max IOPS:             1
Min IOPS:             0
Average Latency(s):   115.811
Max latency(s):       238.24
Min latency(s):       2.50358

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:47:15,331583548-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 271632


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:47:15,337210641-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:47:38,743412433-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:47:38,753802335-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:47:47,160412044-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:47:47,170611398-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:47:55,751449896-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:47:55,761785936-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:04,117782733-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:04,127248928-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:12,515019562-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:12,524592317-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:12,532192787-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T16:48:12,535047090-04:00][RUNNING][ROUND 2/7/40] object_size=16MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:12,539563786-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:12,549633314-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40225\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.182381\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a7a7b564-1665-41e2-a158-a19678de6d24\nsetting min_mon_release = octopus\nepoch 0\nfsid a7a7b564-1665-41e2-a158-a19678de6d24\nlast_changed 2021-05-16T13:48:31.977450-0700\ncreated 2021-05-16T13:48:31.977450-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40225/0,v1:10.10.1.2:40226/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.182381 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 b28d380f-192d-429f-bdbb-c232b92cb7f2\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 ede9e805-9caa-4541-b99b-4ace59cf48ba\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 60432e68-3681-4d42-8e5f-5ce762fff094\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42225\n  w/ user/pass: admin / 2ec82914-947d-4f0e-ad8d-1aea0edba668\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 16:48:49 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40225
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.182381
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a7a7b564-1665-41e2-a158-a19678de6d24
setting min_mon_release = octopus
epoch 0
fsid a7a7b564-1665-41e2-a158-a19678de6d24
last_changed 2021-05-16T13:48:31.977450-0700
created 2021-05-16T13:48:31.977450-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40225/0,v1:10.10.1.2:40226/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.182381 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 b28d380f-192d-429f-bdbb-c232b92cb7f2
0
start osd.0
add osd1 ede9e805-9caa-4541-b99b-4ace59cf48ba
1
start osd.1
add osd2 60432e68-3681-4d42-8e5f-5ce762fff094
2
start osd.2


restful urls: https://10.10.1.2:42225
  w/ user/pass: admin / 2ec82914-947d-4f0e-ad8d-1aea0edba668


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T13:48:14.409-0700 7f509f01e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:48:14.409-0700 7f509f01e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:48:14.425-0700 7f77626241c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:48:14.425-0700 7f77626241c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40225,v1:10.10.1.2:40226] --print /tmp/ceph_monmap.182381 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.182381 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.182381 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42225 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55c96a2a4000 @  0x7f285fd17680 0x7f285fd38824 0x7f28604d3187 0x7f28604db355 0x7f28604d3708 0x7f28604d3877 0x7f28604d4c24 0x7f28604ecec1 0x7f286045f5f3 0x7f28604c0e97 0x7f28604c8b1a 0x7f285fbcbd84 0x7f285fce7609 0x7f285f8bb293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.W79N2nOwcS 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b28d380f-192d-429f-bdbb-c232b92cb7f2 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAqhaFg/RrPKxAA2QbqiyS7tf8yznuskZNsJw== --osd-uuid b28d380f-192d-429f-bdbb-c232b92cb7f2 
2021-05-16T13:48:43.402-0700 7fa38ca0cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:48:43.422-0700 7fa38ca0cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:48:43.422-0700 7fa38ca0cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x556c8a280000 @  0x7fa38d3d5680 0x7fa38d3f6824 0x556c80217447 0x556c8021f4b5 0x556c802179c8 0x556c80217b37 0x556c80218ee4 0x556c7ffe9ca1 0x556c801c1423 0x556c7ffda2a7 0x556c7ffdf53a 0x7fa38cf28d84 0x7fa38d0ad609 0x7fa38cc16293
2021-05-16T13:48:43.734-0700 7fa38ca0cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ede9e805-9caa-4541-b99b-4ace59cf48ba -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5558f4bc0000 @  0x7f46f5d64680 0x7f46f5d85824 0x5558eac09447 0x5558eac114b5 0x5558eac099c8 0x5558eac09b37 0x5558eac0aee4 0x5558ea9dbca1 0x5558eabb3423 0x5558ea9cc2a7 0x5558ea9d153a 0x7f46f58b7d84 0x7f46f5a3c609 0x7f46f55a5293
2021-05-16T13:48:44.402-0700 7f46f539bf00 -1 Falling back to public interface
2021-05-16T13:48:44.670-0700 7f46f539bf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAshaFgGDv/BRAAebt9fXNRh42zg3a8wJBlKg== --osd-uuid ede9e805-9caa-4541-b99b-4ace59cf48ba 
2021-05-16T13:48:44.790-0700 7fe3b8a8ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:48:44.810-0700 7fe3b8a8ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:48:44.810-0700 7fe3b8a8ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x562db8abc000 @  0x7fe3b9458680 0x7fe3b9479824 0x562dade6f447 0x562dade774b5 0x562dade6f9c8 0x562dade6fb37 0x562dade70ee4 0x562dadc41ca1 0x562dade19423 0x562dadc322a7 0x562dadc3753a 0x7fe3b8fabd84 0x7fe3b9130609 0x7fe3b8c99293
2021-05-16T13:48:45.126-0700 7fe3b8a8ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 60432e68-3681-4d42-8e5f-5ce762fff094 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5616d4b74000 @  0x7f9865977680 0x7f9865998824 0x5616c9917447 0x5616c991f4b5 0x5616c99179c8 0x5616c9917b37 0x5616c9918ee4 0x5616c96e9ca1 0x5616c98c1423 0x5616c96da2a7 0x5616c96df53a 0x7f98654cad84 0x7f986564f609 0x7f98651b8293
2021-05-16T13:48:45.826-0700 7f9864faef00 -1 Falling back to public interface
2021-05-16T13:48:46.086-0700 7f9864faef00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAthaFg+XAqHxAAGeRVk1EUlGwPN1unOh7IpQ== --osd-uuid 60432e68-3681-4d42-8e5f-5ce762fff094 
2021-05-16T13:48:46.182-0700 7f20b8d15f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:48:46.202-0700 7f20b8d15f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:48:46.202-0700 7f20b8d15f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x559745a1a000 @  0x7f20b96de680 0x7f20b96ff824 0x55973a5cb447 0x55973a5d34b5 0x55973a5cb9c8 0x55973a5cbb37 0x55973a5ccee4 0x55973a39dca1 0x55973a575423 0x55973a38e2a7 0x55973a39353a 0x7f20b9231d84 0x7f20b93b6609 0x7f20b8f1f293
2021-05-16T13:48:46.530-0700 7f20b8d15f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55a37bf38000 @  0x7fc894e8a680 0x7fc894eab824 0x55a3713aa447 0x55a3713b24b5 0x55a3713aa9c8 0x55a3713aab37 0x55a3713abee4 0x55a37117cca1 0x55a371354423 0x55a37116d2a7 0x55a37117253a 0x7fc8949ddd84 0x7fc894b62609 0x7fc8946cb293
2021-05-16T13:48:47.238-0700 7fc8944c1f00 -1 Falling back to public interface
2021-05-16T13:48:47.506-0700 7fc8944c1f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:49,683149258-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:49,694461892-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:49,734190143-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:49,737746104-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:53,197915771-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:53,201652070-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:56,645438196-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:56,649442670-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:59,990844417-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:48:59,994409706-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:07,054159134-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:07,058023274-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:10,950631453-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:10,954267926-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:15,397992347-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:15,402032567-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:19,020005025-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:19,024458162-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:23,351614477-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:23,355820941-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:26,898076704-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:26,901383727-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:31,285649941-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:31,289659524-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:34,717851205-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:34,721439036-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:49:38,006481401-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:50:01,552645327-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:50:10,057695909-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:50:18,577150605-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:50:27,059078529-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:50:35,703576581-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:50:44,175278617-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:50:44,185408459-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:50:52,649741132-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:50:52,658997914-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:51:01,303791047-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:51:01,313539743-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:51:09,757981069-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:51:09,767456763-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:51:18,260905065-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:51:18,270598176-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:51:18,278330103-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:51:18,282602470-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:51:18,292084735-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=278105
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:51:18,299555491-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:51:18,320111802-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:51:18,323395060-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0 -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T20:51:20.716+0000 7fa9bc791d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:51:20.968+0000 7fa9bc791d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:51:20.968+0000 7fa9bc791d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T20:51:20.982338+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-16T20:51:20.982370+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T20:51:21.985268+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:51:21.985268+0000     0       0         0         0         0         0           -           0
2021-05-16T20:51:22.985408+0000     1      19        19         0         0         0           -           0
2021-05-16T20:51:23.985507+0000     2      20        20         0         0         0           -           0
2021-05-16T20:51:24.985609+0000     3      20        20         0         0         0           -           0
2021-05-16T20:51:25.985675+0000     4      22        22         0         0         0           -           0
2021-05-16T20:51:26.985824+0000     5      26        26         0         0         0           -           0
2021-05-16T20:51:27.985927+0000     6      26        26         0         0         0           -           0
2021-05-16T20:51:28.986010+0000     7      50        50         0         0         0           -           0
2021-05-16T20:51:29.986130+0000     8      52        52         0         0         0           -           0
2021-05-16T20:51:30.986274+0000     9      62        62         0         0         0           -           0
2021-05-16T20:51:31.986377+0000    10      62        62         0         0         0           -           0
2021-05-16T20:51:32.986472+0000    11      81        81         0         0         0           -           0
2021-05-16T20:51:33.986610+0000    12     108       108         0         0         0           -           0
2021-05-16T20:51:34.986765+0000    13     136       136         0         0         0           -           0
2021-05-16T20:51:35.987115+0000    14     146       146         0         0         0           -           0
2021-05-16T20:51:36.987243+0000    15     156       156         0         0         0           -           0
2021-05-16T20:51:37.987364+0000    16     177       177         0         0         0           -           0
2021-05-16T20:51:38.987508+0000    17     177       177         0         0         0           -           0
2021-05-16T20:51:39.987813+0000    18     177       177         0         0         0           -           0
2021-05-16T20:51:40.987878+0000    19     177       177         0         0         0           -           0
2021-05-16T20:51:41.987944+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:51:41.987944+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:51:41.987944+0000    20     177       177         0         0         0           -           0
2021-05-16T20:51:42.988055+0000    21     178       178         0         0         0           -           0
2021-05-16T20:51:43.988121+0000    22     178       178         0         0         0           -           0
2021-05-16T20:51:44.988189+0000    23     188       188         0         0         0           -           0
2021-05-16T20:51:45.988254+0000    24     188       188         0         0         0           -           0
2021-05-16T20:51:46.988377+0000    25     206       206         0         0         0           -           0
2021-05-16T20:51:47.988494+0000    26     210       210         0         0         0           -           0
2021-05-16T20:51:48.988609+0000    27     218       218         0         0         0           -           0
2021-05-16T20:51:49.988713+0000    28     218       218         0         0         0           -           0
2021-05-16T20:51:50.989054+0000    29     219       219         0         0         0           -           0
2021-05-16T20:51:51.989120+0000    30     219       219         0         0         0           -           0
2021-05-16T20:51:52.989216+0000    31     226       226         0         0         0           -           0
2021-05-16T20:51:53.989506+0000    32     233       233         0         0         0           -           0
2021-05-16T20:51:54.989886+0000    33     235       235         0         0         0           -           0
2021-05-16T20:51:55.989972+0000    34     251       251         0         0         0           -           0
2021-05-16T20:51:56.990080+0000    35     251       251         0         0         0           -           0
2021-05-16T20:51:57.990157+0000    36     255       255         0         0         0           -           0
2021-05-16T20:51:58.990296+0000    37     255       255         0         0         0           -           0
2021-05-16T20:51:59.990434+0000 Total time run:         37.298
Total writes made:      256
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     109.818
Stddev Bandwidth:       0
Max bandwidth (MB/sec): 0
Min bandwidth (MB/sec): 0
Average IOPS:           6
Stddev IOPS:            0
Max IOPS:               0
Min IOPS:               0
Average Latency(s):     21.7973
Stddev Latency(s):      9.59417
Max latency(s):         37.2397
Min latency(s):         2.05852

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:00,867480413-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 278105


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:00,872690822-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:24,253560875-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:24,262984501-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:32,708791184-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:32,718466663-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:41,097879679-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:41,107796911-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:49,582894452-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:49,592434626-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:58,036710482-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:58,046574043-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:58,054242020-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:58,058618513-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:58,068894218-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=279500
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:58,076228548-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:58,097483692-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:52:58,100982015-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b1b48f6f-03ca-49d0-b6f3-e67daa0fc2e0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rjK5J0:/tmp/ceph-asok.rjK5J0 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T20:53:00.444+0000 7f283c53dd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:53:00.700+0000 7f283c53dd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T20:53:00.700+0000 7f283c53dd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T20:53:00.725396+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:53:00.725396+0000     0       0         0         0         0         0           -           0
2021-05-16T20:53:01.725758+0000     1       7         7         0         0         0           -           0
2021-05-16T20:53:02.725975+0000     2       8         8         0         0         0           -           0
2021-05-16T20:53:03.726072+0000     3       8         8         0         0         0           -           0
2021-05-16T20:53:04.726167+0000     4      11        11         0         0         0           -           0
2021-05-16T20:53:05.726260+0000     5      11        11         0         0         0           -           0
2021-05-16T20:53:06.726365+0000     6      11        11         0         0         0           -           0
2021-05-16T20:53:07.726458+0000     7      13        13         0         0         0           -           0
2021-05-16T20:53:08.726555+0000     8      13        13         0         0         0           -           0
2021-05-16T20:53:09.726662+0000     9      13        13         0         0         0           -           0
2021-05-16T20:53:10.726770+0000    10      13        13         0         0         0           -           0
2021-05-16T20:53:11.727103+0000    11      14        14         0         0         0           -           0
2021-05-16T20:53:12.727198+0000    12      14        14         0         0         0           -           0
2021-05-16T20:53:13.727441+0000    13      16        16         0         0         0           -           0
2021-05-16T20:53:14.727761+0000    14      16        16         0         0         0           -           0
2021-05-16T20:53:15.727854+0000    15      16        16         0         0         0           -           0
2021-05-16T20:53:16.727954+0000    16      16        16         0         0         0           -           0
2021-05-16T20:53:17.728057+0000    17      17        17         0         0         0           -           0
2021-05-16T20:53:18.728157+0000    18      17        17         0         0         0           -           0
2021-05-16T20:53:19.728251+0000    19      18        18         0         0         0           -           0
2021-05-16T20:53:20.728344+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:53:20.728344+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:53:20.728344+0000    20      18        18         0         0         0           -           0
2021-05-16T20:53:21.728460+0000    21      18        18         0         0         0           -           0
2021-05-16T20:53:22.728560+0000    22      22        22         0         0         0           -           0
2021-05-16T20:53:23.728653+0000    23      22        22         0         0         0           -           0
2021-05-16T20:53:24.728746+0000    24      23        23         0         0         0           -           0
2021-05-16T20:53:25.729080+0000    25      23        23         0         0         0           -           0
2021-05-16T20:53:26.729181+0000    26      24        24         0         0         0           -           0
2021-05-16T20:53:27.729429+0000    27      24        24         0         0         0           -           0
2021-05-16T20:53:28.729763+0000    28      25        25         0         0         0           -           0
2021-05-16T20:53:29.729925+0000    29      25        25         0         0         0           -           0
2021-05-16T20:53:30.730026+0000    30      29        29         0         0         0           -           0
2021-05-16T20:53:31.730120+0000    31      29        29         0         0         0           -           0
2021-05-16T20:53:32.730215+0000    32      33        33         0         0         0           -           0
2021-05-16T20:53:33.730309+0000    33      33        33         0         0         0           -           0
2021-05-16T20:53:34.730412+0000    34      39        39         0         0         0           -           0
2021-05-16T20:53:35.730508+0000    35      39        39         0         0         0           -           0
2021-05-16T20:53:36.730603+0000    36      39        39         0         0         0           -           0
2021-05-16T20:53:37.730706+0000    37      43        43         0         0         0           -           0
2021-05-16T20:53:38.731048+0000    38      43        43         0         0         0           -           0
2021-05-16T20:53:39.731381+0000    39      44        44         0         0         0           -           0
2021-05-16T20:53:40.731495+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:53:40.731495+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:53:40.731495+0000    40      44        44         0         0         0           -           0
2021-05-16T20:53:41.731838+0000    41      44        44         0         0         0           -           0
2021-05-16T20:53:42.731939+0000    42      44        44         0         0         0           -           0
2021-05-16T20:53:43.732037+0000    43      44        44         0         0         0           -           0
2021-05-16T20:53:44.732135+0000    44      44        44         0         0         0           -           0
2021-05-16T20:53:45.732246+0000    45      44        44         0         0         0           -           0
2021-05-16T20:53:46.732346+0000    46      44        44         0         0         0           -           0
2021-05-16T20:53:47.732452+0000    47      45        45         0         0         0           -           0
2021-05-16T20:53:48.732544+0000    48      45        45         0         0         0           -           0
2021-05-16T20:53:49.732640+0000    49      47        47         0         0         0           -           0
2021-05-16T20:53:50.732742+0000    50      56        56         0         0         0           -           0
2021-05-16T20:53:51.733074+0000    51      56        56         0         0         0           -           0
2021-05-16T20:53:52.733410+0000    52      57        57         0         0         0           -           0
2021-05-16T20:53:53.733507+0000    53      57        57         0         0         0           -           0
2021-05-16T20:53:54.733608+0000    54      57        57         0         0         0           -           0
2021-05-16T20:53:55.733854+0000    55      57        57         0         0         0           -           0
2021-05-16T20:53:56.734047+0000    56      59        59         0         0         0           -           0
2021-05-16T20:53:57.734142+0000    57      59        59         0         0         0           -           0
2021-05-16T20:53:58.734245+0000    58      59        59         0         0         0           -           0
2021-05-16T20:53:59.734340+0000    59      59        59         0         0         0           -           0
2021-05-16T20:54:00.734434+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:54:00.734434+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:54:00.734434+0000    60      59        59         0         0         0           -           0
2021-05-16T20:54:01.734539+0000    61      59        59         0         0         0           -           0
2021-05-16T20:54:02.734640+0000    62      64        64         0         0         0           -           0
2021-05-16T20:54:03.734735+0000    63      64        64         0         0         0           -           0
2021-05-16T20:54:04.734836+0000    64      64        64         0         0         0           -           0
2021-05-16T20:54:05.735163+0000    65      66        66         0         0         0           -           0
2021-05-16T20:54:06.735383+0000    66      66        66         0         0         0           -           0
2021-05-16T20:54:07.735502+0000    67      75        75         0         0         0           -           0
2021-05-16T20:54:08.735837+0000    68      75        75         0         0         0           -           0
2021-05-16T20:54:09.735946+0000    69      78        78         0         0         0           -           0
2021-05-16T20:54:10.736048+0000    70      78        78         0         0         0           -           0
2021-05-16T20:54:11.736148+0000    71      80        80         0         0         0           -           0
2021-05-16T20:54:12.736243+0000    72      80        80         0         0         0           -           0
2021-05-16T20:54:13.736336+0000    73      80        80         0         0         0           -           0
2021-05-16T20:54:14.736451+0000    74      80        80         0         0         0           -           0
2021-05-16T20:54:15.736549+0000    75      86        86         0         0         0           -           0
2021-05-16T20:54:16.736655+0000    76      86        86         0         0         0           -           0
2021-05-16T20:54:17.736754+0000    77      92        92         0         0         0           -           0
2021-05-16T20:54:18.737049+0000    78      92        92         0         0         0           -           0
2021-05-16T20:54:19.737148+0000    79      92        92         0         0         0           -           0
2021-05-16T20:54:20.737469+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:54:20.737469+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:54:20.737469+0000    80      94        94         0         0         0           -           0
2021-05-16T20:54:21.737812+0000    81      94        94         0         0         0           -           0
2021-05-16T20:54:22.738055+0000    82      95        95         0         0         0           -           0
2021-05-16T20:54:23.738380+0000    83      95        95         0         0         0           -           0
2021-05-16T20:54:24.738478+0000    84      97        97         0         0         0           -           0
2021-05-16T20:54:25.738573+0000    85      97        97         0         0         0           -           0
2021-05-16T20:54:26.738677+0000    86     102       102         0         0         0           -           0
2021-05-16T20:54:27.738770+0000    87     102       102         0         0         0           -           0
2021-05-16T20:54:28.739085+0000    88     104       104         0         0         0           -           0
2021-05-16T20:54:29.739421+0000    89     104       104         0         0         0           -           0
2021-05-16T20:54:30.739521+0000    90     104       104         0         0         0           -           0
2021-05-16T20:54:31.739846+0000    91     104       104         0         0         0           -           0
2021-05-16T20:54:32.739939+0000    92     105       105         0         0         0           -           0
2021-05-16T20:54:33.740032+0000    93     105       105         0         0         0           -           0
2021-05-16T20:54:34.740132+0000    94     105       105         0         0         0           -           0
2021-05-16T20:54:35.740225+0000    95     108       108         0         0         0           -           0
2021-05-16T20:54:36.740319+0000    96     108       108         0         0         0           -           0
2021-05-16T20:54:37.740416+0000    97     109       109         0         0         0           -           0
2021-05-16T20:54:38.740515+0000    98     109       109         0         0         0           -           0
2021-05-16T20:54:39.740620+0000    99     112       112         0         0         0           -           0
2021-05-16T20:54:40.740718+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:54:40.740718+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:54:40.740718+0000   100     112       112         0         0         0           -           0
2021-05-16T20:54:41.741067+0000   101     114       114         0         0         0           -           0
2021-05-16T20:54:42.741380+0000   102     114       114         0         0         0           -           0
2021-05-16T20:54:43.741495+0000   103     115       115         0         0         0           -           0
2021-05-16T20:54:44.741590+0000   104     115       115         0         0         0           -           0
2021-05-16T20:54:45.741782+0000   105     117       117         0         0         0           -           0
2021-05-16T20:54:46.741885+0000   106     117       117         0         0         0           -           0
2021-05-16T20:54:47.741993+0000   107     117       117         0         0         0           -           0
2021-05-16T20:54:48.742149+0000   108     117       117         0         0         0           -           0
2021-05-16T20:54:49.742242+0000   109     117       117         0         0         0           -           0
2021-05-16T20:54:50.742341+0000   110     117       117         0         0         0           -           0
2021-05-16T20:54:51.742436+0000   111     117       117         0         0         0           -           0
2021-05-16T20:54:52.742544+0000   112     117       117         0         0         0           -           0
2021-05-16T20:54:53.742640+0000   113     117       117         0         0         0           -           0
2021-05-16T20:54:54.742739+0000   114     121       121         0         0         0           -           0
2021-05-16T20:54:55.742833+0000   115     121       121         0         0         0           -           0
2021-05-16T20:54:56.743110+0000   116     123       123         0         0         0           -           0
2021-05-16T20:54:57.743444+0000   117     123       123         0         0         0           -           0
2021-05-16T20:54:58.743544+0000   118     126       126         0         0         0           -           0
2021-05-16T20:54:59.743696+0000   119     126       126         0         0         0           -           0
2021-05-16T20:55:00.743957+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:55:00.743957+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:55:00.743957+0000   120     126       126         0         0         0           -           0
2021-05-16T20:55:01.744064+0000   121     126       126         0         0         0           -           0
2021-05-16T20:55:02.744164+0000   122     127       127         0         0         0           -           0
2021-05-16T20:55:03.744258+0000   123     134       134         0         0         0           -           0
2021-05-16T20:55:04.744352+0000   124     134       134         0         0         0           -           0
2021-05-16T20:55:05.744461+0000   125     134       134         0         0         0           -           0
2021-05-16T20:55:06.744568+0000   126     134       134         0         0         0           -           0
2021-05-16T20:55:07.744665+0000   127     136       136         0         0         0           -           0
2021-05-16T20:55:08.744759+0000   128     136       136         0         0         0           -           0
2021-05-16T20:55:09.745095+0000   129     137       137         0         0         0           -           0
2021-05-16T20:55:10.745196+0000   130     137       137         0         0         0           -           0
2021-05-16T20:55:11.745422+0000   131     137       137         0         0         0           -           0
2021-05-16T20:55:12.745515+0000   132     137       137         0         0         0           -           0
2021-05-16T20:55:13.745847+0000   133     137       137         0         0         0           -           0
2021-05-16T20:55:14.745976+0000   134     137       137         0         0         0           -           0
2021-05-16T20:55:15.746068+0000   135     137       137         0         0         0           -           0
2021-05-16T20:55:16.746161+0000   136     137       137         0         0         0           -           0
2021-05-16T20:55:17.746254+0000   137     137       137         0         0         0           -           0
2021-05-16T20:55:18.746355+0000   138     139       139         0         0         0           -           0
2021-05-16T20:55:19.746449+0000   139     139       139         0         0         0           -           0
2021-05-16T20:55:20.746552+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:55:20.746552+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:55:20.746552+0000   140     139       139         0         0         0           -           0
2021-05-16T20:55:21.746673+0000   141     139       139         0         0         0           -           0
2021-05-16T20:55:22.746775+0000   142     140       140         0         0         0           -           0
2021-05-16T20:55:23.747109+0000   143     140       140         0         0         0           -           0
2021-05-16T20:55:24.747387+0000   144     142       142         0         0         0           -           0
2021-05-16T20:55:25.747481+0000   145     142       142         0         0         0           -           0
2021-05-16T20:55:26.747586+0000   146     144       144         0         0         0           -           0
2021-05-16T20:55:27.747920+0000   147     144       144         0         0         0           -           0
2021-05-16T20:55:28.748014+0000   148     145       145         0         0         0           -           0
2021-05-16T20:55:29.748108+0000   149     145       145         0         0         0           -           0
2021-05-16T20:55:30.748211+0000   150     145       145         0         0         0           -           0
2021-05-16T20:55:31.748306+0000   151     145       145         0         0         0           -           0
2021-05-16T20:55:32.748404+0000   152     145       145         0         0         0           -           0
2021-05-16T20:55:33.748499+0000   153     145       145         0         0         0           -           0
2021-05-16T20:55:34.748601+0000   154     145       145         0         0         0           -           0
2021-05-16T20:55:35.748694+0000   155     145       145         0         0         0           -           0
2021-05-16T20:55:36.748788+0000   156     145       145         0         0         0           -           0
2021-05-16T20:55:37.749103+0000   157     146       146         0         0         0           -           0
2021-05-16T20:55:38.749377+0000   158     146       146         0         0         0           -           0
2021-05-16T20:55:39.749544+0000   159     149       149         0         0         0           -           0
2021-05-16T20:55:40.749817+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:55:40.749817+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:55:40.749817+0000   160     149       149         0         0         0           -           0
2021-05-16T20:55:41.749921+0000   161     149       149         0         0         0           -           0
2021-05-16T20:55:42.750023+0000   162     149       149         0         0         0           -           0
2021-05-16T20:55:43.750117+0000   163     150       150         0         0         0           -           0
2021-05-16T20:55:44.750211+0000   164     150       150         0         0         0           -           0
2021-05-16T20:55:45.750541+0000   165     151       151         0         0         0           -           0
2021-05-16T20:55:46.750642+0000   166     151       151         0         0         0           -           0
2021-05-16T20:55:47.750813+0000   167     151       151         0         0         0           -           0
2021-05-16T20:55:48.751079+0000   168     152       152         0         0         0           -           0
2021-05-16T20:55:49.751174+0000   169     152       152         0         0         0           -           0
2021-05-16T20:55:50.751520+0000   170     157       157         0         0         0           -           0
2021-05-16T20:55:51.751793+0000   171     157       157         0         0         0           -           0
2021-05-16T20:55:52.751886+0000   172     163       163         0         0         0           -           0
2021-05-16T20:55:53.751980+0000   173     163       163         0         0         0           -           0
2021-05-16T20:55:54.752082+0000   174     167       167         0         0         0           -           0
2021-05-16T20:55:55.752176+0000   175     167       167         0         0         0           -           0
2021-05-16T20:55:56.752276+0000   176     169       169         0         0         0           -           0
2021-05-16T20:55:57.752379+0000   177     169       169         0         0         0           -           0
2021-05-16T20:55:58.752487+0000   178     170       170         0         0         0           -           0
2021-05-16T20:55:59.752585+0000   179     170       170         0         0         0           -           0
2021-05-16T20:56:00.752679+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:56:00.752679+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:56:00.752679+0000   180     171       171         0         0         0           -           0
2021-05-16T20:56:01.752793+0000   181     171       171         0         0         0           -           0
2021-05-16T20:56:02.753118+0000   182     171       171         0         0         0           -           0
2021-05-16T20:56:03.753452+0000   183     173       173         0         0         0           -           0
2021-05-16T20:56:04.753546+0000   184     173       173         0         0         0           -           0
2021-05-16T20:56:05.753641+0000   185     174       174         0         0         0           -           0
2021-05-16T20:56:06.753750+0000   186     174       174         0         0         0           -           0
2021-05-16T20:56:07.753845+0000   187     178       178         0         0         0           -           0
2021-05-16T20:56:08.754145+0000   188     178       178         0         0         0           -           0
2021-05-16T20:56:09.754239+0000   189     178       178         0         0         0           -           0
2021-05-16T20:56:10.754339+0000   190     178       178         0         0         0           -           0
2021-05-16T20:56:11.754434+0000   191     178       178         0         0         0           -           0
2021-05-16T20:56:12.754767+0000   192     178       178         0         0         0           -           0
2021-05-16T20:56:13.755100+0000   193     179       179         0         0         0           -           0
2021-05-16T20:56:14.755203+0000   194     179       179         0         0         0           -           0
2021-05-16T20:56:15.755298+0000   195     181       181         0         0         0           -           0
2021-05-16T20:56:16.755458+0000   196     181       181         0         0         0           -           0
2021-05-16T20:56:17.755791+0000   197     181       181         0         0         0           -           0
2021-05-16T20:56:18.755891+0000   198     181       181         0         0         0           -           0
2021-05-16T20:56:19.755985+0000   199     181       181         0         0         0           -           0
2021-05-16T20:56:20.756080+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:56:20.756080+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:56:20.756080+0000   200     185       185         0         0         0           -           0
2021-05-16T20:56:21.756187+0000   201     185       185         0         0         0           -           0
2021-05-16T20:56:22.756292+0000   202     189       189         0         0         0           -           0
2021-05-16T20:56:23.756386+0000   203     189       189         0         0         0           -           0
2021-05-16T20:56:24.756480+0000   204     189       189         0         0         0           -           0
2021-05-16T20:56:25.756583+0000   205     189       189         0         0         0           -           0
2021-05-16T20:56:26.756688+0000   206     192       192         0         0         0           -           0
2021-05-16T20:56:27.756799+0000   207     192       192         0         0         0           -           0
2021-05-16T20:56:28.757079+0000   208     192       192         0         0         0           -           0
2021-05-16T20:56:29.757375+0000   209     192       192         0         0         0           -           0
2021-05-16T20:56:30.757546+0000   210     192       192         0         0         0           -           0
2021-05-16T20:56:31.757638+0000   211     192       192         0         0         0           -           0
2021-05-16T20:56:32.757791+0000   212     192       192         0         0         0           -           0
2021-05-16T20:56:33.757965+0000   213     194       194         0         0         0           -           0
2021-05-16T20:56:34.758065+0000   214     194       194         0         0         0           -           0
2021-05-16T20:56:35.758164+0000   215     196       196         0         0         0           -           0
2021-05-16T20:56:36.758266+0000   216     196       196         0         0         0           -           0
2021-05-16T20:56:37.758360+0000   217     197       197         0         0         0           -           0
2021-05-16T20:56:38.758462+0000   218     197       197         0         0         0           -           0
2021-05-16T20:56:39.758556+0000   219     197       197         0         0         0           -           0
2021-05-16T20:56:40.758653+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:56:40.758653+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:56:40.758653+0000   220     197       197         0         0         0           -           0
2021-05-16T20:56:41.758763+0000   221     199       199         0         0         0           -           0
2021-05-16T20:56:42.759098+0000   222     199       199         0         0         0           -           0
2021-05-16T20:56:43.759193+0000   223     199       199         0         0         0           -           0
2021-05-16T20:56:44.759488+0000   224     199       199         0         0         0           -           0
2021-05-16T20:56:45.759805+0000   225     199       199         0         0         0           -           0
2021-05-16T20:56:46.759909+0000   226     202       202         0         0         0           -           0
2021-05-16T20:56:47.760002+0000   227     202       202         0         0         0           -           0
2021-05-16T20:56:48.760096+0000   228     204       204         0         0         0           -           0
2021-05-16T20:56:49.760191+0000   229     204       204         0         0         0           -           0
2021-05-16T20:56:50.760291+0000   230     210       210         0         0         0           -           0
2021-05-16T20:56:51.760386+0000   231     210       210         0         0         0           -           0
2021-05-16T20:56:52.760479+0000   232     212       212         0         0         0           -           0
2021-05-16T20:56:53.760573+0000   233     212       212         0         0         0           -           0
2021-05-16T20:56:54.760673+0000   234     214       214         0         0         0           -           0
2021-05-16T20:56:55.760774+0000   235     214       214         0         0         0           -           0
2021-05-16T20:56:56.761053+0000   236     216       216         0         0         0           -           0
2021-05-16T20:56:57.761146+0000   237     216       216         0         0         0           -           0
2021-05-16T20:56:58.761251+0000   238     224       224         0         0         0           -           0
2021-05-16T20:56:59.761394+0000   239     225       225         0         0         0           -           0
2021-05-16T20:57:00.761567+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:57:00.761567+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:57:00.761567+0000   240     225       225         0         0         0           -           0
2021-05-16T20:57:01.761786+0000   241     227       227         0         0         0           -           0
2021-05-16T20:57:02.761890+0000   242     227       227         0         0         0           -           0
2021-05-16T20:57:03.761988+0000   243     229       229         0         0         0           -           0
2021-05-16T20:57:04.762082+0000   244     229       229         0         0         0           -           0
2021-05-16T20:57:05.762178+0000   245     232       232         0         0         0           -           0
2021-05-16T20:57:06.762282+0000   246     232       232         0         0         0           -           0
2021-05-16T20:57:07.762377+0000   247     233       233         0         0         0           -           0
2021-05-16T20:57:08.762476+0000   248     233       233         0         0         0           -           0
2021-05-16T20:57:09.762570+0000   249     235       235         0         0         0           -           0
2021-05-16T20:57:10.762669+0000   250     235       235         0         0         0           -           0
2021-05-16T20:57:11.762763+0000   251     235       235         0         0         0           -           0
2021-05-16T20:57:12.763095+0000   252     235       235         0         0         0           -           0
2021-05-16T20:57:13.763423+0000   253     235       235         0         0         0           -           0
2021-05-16T20:57:14.763551+0000   254     235       235         0         0         0           -           0
2021-05-16T20:57:15.763770+0000   255     235       235         0         0         0           -           0
2021-05-16T20:57:16.763863+0000   256     238       238         0         0         0           -           0
2021-05-16T20:57:17.763956+0000   257     238       238         0         0         0           -           0
2021-05-16T20:57:18.764055+0000   258     238       238         0         0         0           -           0
2021-05-16T20:57:19.764147+0000   259     238       238         0         0         0           -           0
2021-05-16T20:57:20.764240+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T20:57:20.764240+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:57:20.764240+0000   260     239       239         0         0         0           -           0
2021-05-16T20:57:21.764345+0000   261     239       239         0         0         0           -           0
2021-05-16T20:57:22.764460+0000   262     239       239         0         0         0           -           0
2021-05-16T20:57:23.764554+0000   263     239       239         0         0         0           -           0
2021-05-16T20:57:24.764651+0000   264     239       239         0         0         0           -           0
2021-05-16T20:57:25.764747+0000   265     239       239         0         0         0           -           0
2021-05-16T20:57:26.765053+0000   266     244       244         0         0         0           -           0
2021-05-16T20:57:27.765147+0000   267     244       244         0         0         0           -           0
2021-05-16T20:57:28.765242+0000   268     244       244         0         0         0           -           0
2021-05-16T20:57:29.765580+0000   269     244       244         0         0         0           -           0
2021-05-16T20:57:30.765780+0000   270     244       244         0         0         0           -           0
2021-05-16T20:57:31.765874+0000   271     251       251         0         0         0           -           0
2021-05-16T20:57:32.765968+0000   272     251       251         0         0         0           -           0
2021-05-16T20:57:33.766063+0000   273     253       253         0         0         0           -           0
2021-05-16T20:57:34.766164+0000   274     253       253         0         0         0           -           0
2021-05-16T20:57:35.766258+0000   275     253       253         0         0         0           -           0
2021-05-16T20:57:36.766350+0000   276     253       253         0         0         0           -           0
2021-05-16T20:57:37.766448+0000   277       6       256       250   14.4383   14.4404     4.66896     148.938
2021-05-16T20:57:38.766554+0000   278       6       256       250   14.3864         0           -     148.938
2021-05-16T20:57:39.766649+0000   279       4       256       252   14.4495        16     8.56783     147.841
2021-05-16T20:57:40.766746+0000 min lat: 4.66896 max lat: 276.65 avg lat: 147.841
2021-05-16T20:57:40.766746+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T20:57:40.766746+0000   280       4       256       252   14.3979         0           -     147.841
2021-05-16T20:57:41.766853+0000   281       3       256       253   14.4036         8      10.705     147.299
2021-05-16T20:57:42.766999+0000   282       3       256       253   14.3525         0           -     147.299
2021-05-16T20:57:43.767152+0000   283       3       256       253   14.3018         0           -     147.299
2021-05-16T20:57:44.767426+0000   284       1       256       255   14.3641   10.6667     6.44522     146.211
2021-05-16T20:57:45.767761+0000   285       1       256       255   14.3137         0           -     146.211
2021-05-16T20:57:46.767961+0000   286       1       256       255   14.2636         0           -     146.211
2021-05-16T20:57:47.768057+0000   287       1       256       255   14.2139         0           -     146.211
2021-05-16T20:57:48.768270+0000 Total time run:       287.37
Total reads made:     256
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   14.2534
Average IOPS:         0
Stddev IOPS:          0.0590281
Max IOPS:             1
Min IOPS:             0
Average Latency(s):   145.682
Max latency(s):       276.65
Min latency(s):       4.66896

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:57:49,574723241-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 279500


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:57:49,579320117-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:58:13,155832587-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:58:13,165718941-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:58:21,596967704-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:58:21,606585083-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:58:30,167651792-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:58:30,177870581-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:58:38,731204463-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:58:38,741487423-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:58:47,269475367-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:58:47,278697484-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:58:47,286437576-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T16:58:47,289481324-04:00][RUNNING][ROUND 3/7/40] object_size=16MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:58:47,293750225-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:58:47,302266064-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40755\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.187541\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 28701d51-c710-496b-94f3-170c55ebd889\nsetting min_mon_release = octopus\nepoch 0\nfsid 28701d51-c710-496b-94f3-170c55ebd889\nlast_changed 2021-05-16T13:59:05.840522-0700\ncreated 2021-05-16T13:59:05.840522-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40755/0,v1:10.10.1.2:40756/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.187541 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 025966ca-bdf2-4365-82fd-0476ef9ee8f6\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 cadefbf2-470e-4553-adae-5dd1c4836185\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 fc9ee9d2-a582-427d-bef9-3cccd310b103\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42755\n  w/ user/pass: admin / 0ebee263-a180-4e7c-a2c0-7b10bf336fbd\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 16:59:23 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40755
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.187541
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 28701d51-c710-496b-94f3-170c55ebd889
setting min_mon_release = octopus
epoch 0
fsid 28701d51-c710-496b-94f3-170c55ebd889
last_changed 2021-05-16T13:59:05.840522-0700
created 2021-05-16T13:59:05.840522-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40755/0,v1:10.10.1.2:40756/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.187541 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 025966ca-bdf2-4365-82fd-0476ef9ee8f6
0
start osd.0
add osd1 cadefbf2-470e-4553-adae-5dd1c4836185
1
start osd.1
add osd2 fc9ee9d2-a582-427d-bef9-3cccd310b103
2
start osd.2


restful urls: https://10.10.1.2:42755
  w/ user/pass: admin / 0ebee263-a180-4e7c-a2c0-7b10bf336fbd


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T13:58:49.139-0700 7fc17dc231c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:58:49.139-0700 7fc17dc231c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:58:49.159-0700 7f478ce061c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T13:58:49.159-0700 7f478ce061c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40755,v1:10.10.1.2:40756] --print /tmp/ceph_monmap.187541 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.187541 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.187541 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42755 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x557a1bf1e000 @  0x7fe997467680 0x7fe997488824 0x7fe997c23187 0x7fe997c2b355 0x7fe997c23708 0x7fe997c23877 0x7fe997c24c24 0x7fe997c3cec1 0x7fe997baf5f3 0x7fe997c10e97 0x7fe997c18b1a 0x7fe99731bd84 0x7fe997437609 0x7fe99700b293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.mPlxL8ao0E 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 025966ca-bdf2-4365-82fd-0476ef9ee8f6 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCkh6Fgwe4sGBAAIo33EMRfqQm8P1et+yr6Vw== --osd-uuid 025966ca-bdf2-4365-82fd-0476ef9ee8f6 
2021-05-16T13:59:17.059-0700 7fa66f53df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:59:17.079-0700 7fa66f53df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T13:59:17.079-0700 7fa66f53df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55f8d00fa000 @  0x7fa66ff06680 0x7fa66ff27824 0x55f8c50de447 0x55f8c50e64b5 0x55f8c50de9c8 0x55f8c50deb37 0x55f8c50dfee4 0x55f8c4eb0ca1 0x55f8c5088423 0x55f8c4ea12a7 0x55f8c4ea653a 0x7fa66fa59d84 0x7fa66fbde609 0x7fa66f747293
2021-05-16T13:59:17.399-0700 7fa66f53df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new cadefbf2-470e-4553-adae-5dd1c4836185 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x558daad0c000 @  0x7fed52911680 0x7fed52932824 0x558d9fb1c447 0x558d9fb244b5 0x558d9fb1c9c8 0x558d9fb1cb37 0x558d9fb1dee4 0x558d9f8eeca1 0x558d9fac6423 0x558d9f8df2a7 0x558d9f8e453a 0x7fed52464d84 0x7fed525e9609 0x7fed52152293
2021-05-16T13:59:18.055-0700 7fed51f48f00 -1 Falling back to public interface
2021-05-16T13:59:18.319-0700 7fed51f48f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQClh6Fgh4bWLBAAXkkjxdpZ2ijcGF0WsEyvFQ== --osd-uuid cadefbf2-470e-4553-adae-5dd1c4836185 
2021-05-16T13:59:18.431-0700 7fdf75cf5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:59:18.451-0700 7fdf75cf5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T13:59:18.451-0700 7fdf75cf5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55bac63c8000 @  0x7fdf766be680 0x7fdf766df824 0x55baba99f447 0x55baba9a74b5 0x55baba99f9c8 0x55baba99fb37 0x55baba9a0ee4 0x55baba771ca1 0x55baba949423 0x55baba7622a7 0x55baba76753a 0x7fdf76211d84 0x7fdf76396609 0x7fdf75eff293
2021-05-16T13:59:18.767-0700 7fdf75cf5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new fc9ee9d2-a582-427d-bef9-3cccd310b103 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x556b6ce22000 @  0x7f74b448e680 0x7f74b44af824 0x556b61960447 0x556b619684b5 0x556b619609c8 0x556b61960b37 0x556b61961ee4 0x556b61732ca1 0x556b6190a423 0x556b617232a7 0x556b6172853a 0x7f74b3fe1d84 0x7f74b4166609 0x7f74b3ccf293
2021-05-16T13:59:19.459-0700 7f74b3ac5f00 -1 Falling back to public interface
2021-05-16T13:59:19.719-0700 7f74b3ac5f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCnh6FgIAk+CRAA0kf8yYzEnV2h7xpmKXSDkw== --osd-uuid fc9ee9d2-a582-427d-bef9-3cccd310b103 
2021-05-16T13:59:19.811-0700 7f2fcb96cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:59:19.831-0700 7f2fcb96cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T13:59:19.831-0700 7f2fcb96cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x559f8746c000 @  0x7f2fcc335680 0x7f2fcc356824 0x559f7d091447 0x559f7d0994b5 0x559f7d0919c8 0x559f7d091b37 0x559f7d092ee4 0x559f7ce63ca1 0x559f7d03b423 0x559f7ce542a7 0x559f7ce5953a 0x7f2fcbe88d84 0x7f2fcc00d609 0x7f2fcbb76293
2021-05-16T13:59:20.147-0700 7f2fcb96cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55c168a5c000 @  0x7f0b96837680 0x7f0b96858824 0x55c15ced7447 0x55c15cedf4b5 0x55c15ced79c8 0x55c15ced7b37 0x55c15ced8ee4 0x55c15cca9ca1 0x55c15ce81423 0x55c15cc9a2a7 0x55c15cc9f53a 0x7f0b9638ad84 0x7f0b9650f609 0x7f0b96078293
2021-05-16T13:59:20.839-0700 7f0b95e6ef00 -1 Falling back to public interface
2021-05-16T13:59:21.111-0700 7f0b95e6ef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:23,331474480-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:23,342733624-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:23,384006678-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:23,387572167-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a249b2c5-e6d6-48ec-b1fa-8d06a70a997e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a249b2c5-e6d6-48ec-b1fa-8d06a70a997e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:26,999316392-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:27,003344439-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a249b2c5-e6d6-48ec-b1fa-8d06a70a997e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a249b2c5-e6d6-48ec-b1fa-8d06a70a997e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:30,562672355-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:30,566338823-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a249b2c5-e6d6-48ec-b1fa-8d06a70a997e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a249b2c5-e6d6-48ec-b1fa-8d06a70a997e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:34,036187495-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:34,040270065-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a249b2c5-e6d6-48ec-b1fa-8d06a70a997e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a249b2c5-e6d6-48ec-b1fa-8d06a70a997e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:40,801020121-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:40,804571073-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a249b2c5-e6d6-48ec-b1fa-8d06a70a997e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a249b2c5-e6d6-48ec-b1fa-8d06a70a997e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:44,665373452-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:44,668822572-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a249b2c5-e6d6-48ec-b1fa-8d06a70a997e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a249b2c5-e6d6-48ec-b1fa-8d06a70a997e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:48,879694566-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:48,883259885-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a249b2c5-e6d6-48ec-b1fa-8d06a70a997e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a249b2c5-e6d6-48ec-b1fa-8d06a70a997e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:52,893783553-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:52,897691625-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a249b2c5-e6d6-48ec-b1fa-8d06a70a997e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a249b2c5-e6d6-48ec-b1fa-8d06a70a997e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:56,539196538-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T16:59:56,543096225-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a249b2c5-e6d6-48ec-b1fa-8d06a70a997e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a249b2c5-e6d6-48ec-b1fa-8d06a70a997e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:00:01,123152978-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:00:01,126784370-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a249b2c5-e6d6-48ec-b1fa-8d06a70a997e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a249b2c5-e6d6-48ec-b1fa-8d06a70a997e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:00:05,006427741-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:00:05,010265041-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a249b2c5-e6d6-48ec-b1fa-8d06a70a997e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a249b2c5-e6d6-48ec-b1fa-8d06a70a997e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:00:08,445600511-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:00:08,449413816-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a249b2c5-e6d6-48ec-b1fa-8d06a70a997e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a249b2c5-e6d6-48ec-b1fa-8d06a70a997e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  161 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  161 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  162 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.02  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:00:11,796484074-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:00:35,323062905-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:00:43,918762004-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:00:52,278557838-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:00,750755426-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:09,273730727-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:17,660293287-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:17,670327739-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:26,267016398-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:26,277026916-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:34,898345071-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:34,908127891-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:43,251427163-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:43,261634771-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:51,718075707-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:51,728081857-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:51,735640137-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:51,739921091-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:51,750217916-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=285938
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:51,757951156-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:51,778842516-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:01:51,782373601-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a249b2c5-e6d6-48ec-b1fa-8d06a70a997e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a249b2c5-e6d6-48ec-b1fa-8d06a70a997e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T21:01:54.054+0000 7f1b4c2b4d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T21:01:54.310+0000 7f1b4c2b4d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T21:01:54.310+0000 7f1b4c2b4d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T21:01:54.327310+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-16T21:01:54.327337+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T21:01:55.341848+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:01:55.341848+0000     0       0         0         0         0         0           -           0
2021-05-16T21:01:56.341944+0000     1      15        15         0         0         0           -           0
2021-05-16T21:01:57.342009+0000     2      18        18         0         0         0           -           0
2021-05-16T21:01:58.342105+0000     3      42        42         0         0         0           -           0
2021-05-16T21:01:59.342169+0000     4      42        42         0         0         0           -           0
2021-05-16T21:02:00.342228+0000     5      66        66         0         0         0           -           0
2021-05-16T21:02:01.342301+0000     6      94        94         0         0         0           -           0
2021-05-16T21:02:02.342375+0000     7      97        97         0         0         0           -           0
2021-05-16T21:02:03.342455+0000     8     117       117         0         0         0           -           0
2021-05-16T21:02:04.342519+0000     9     125       125         0         0         0           -           0
2021-05-16T21:02:05.342585+0000    10     129       129         0         0         0           -           0
2021-05-16T21:02:06.342652+0000    11     129       129         0         0         0           -           0
2021-05-16T21:02:07.342730+0000    12     142       142         0         0         0           -           0
2021-05-16T21:02:08.343036+0000    13     164       164         0         0         0           -           0
2021-05-16T21:02:09.343153+0000    14     164       164         0         0         0           -           0
2021-05-16T21:02:10.343456+0000    15     165       165         0         0         0           -           0
2021-05-16T21:02:11.343530+0000    16     165       165         0         0         0           -           0
2021-05-16T21:02:12.343595+0000    17     167       167         0         0         0           -           0
2021-05-16T21:02:13.343660+0000    18     197       197         0         0         0           -           0
2021-05-16T21:02:14.343959+0000    19     220       220         0         0         0           -           0
2021-05-16T21:02:15.344034+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:02:15.344034+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:02:15.344034+0000    20     220       220         0         0         0           -           0
2021-05-16T21:02:16.344110+0000    21     222       222         0         0         0           -           0
2021-05-16T21:02:17.344174+0000    22     222       222         0         0         0           -           0
2021-05-16T21:02:18.344244+0000    23     241       241         0         0         0           -           0
2021-05-16T21:02:19.344418+0000 Total time run:         23.5709
Total writes made:      256
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     173.774
Stddev Bandwidth:       0
Max bandwidth (MB/sec): 0
Min bandwidth (MB/sec): 0
Average IOPS:           10
Stddev IOPS:            0
Max IOPS:               0
Min IOPS:               0
Average Latency(s):     12.4863
Stddev Latency(s):      7.31299
Max latency(s):         23.512
Min latency(s):         0.0759248

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:02:20,131203107-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 285938


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:02:20,136499698-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:02:43,553906140-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:02:43,563303857-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:02:51,957727788-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:02:51,968004386-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:03:00,345618266-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:03:00,355993851-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:03:08,781445421-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:03:08,791396377-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:03:17,153766231-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:03:17,163875865-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:03:17,171775397-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:03:17,176112986-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:03:17,186161325-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=287322
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:03:17,194131279-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:03:17,214630403-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:03:17,218248381-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a249b2c5-e6d6-48ec-b1fa-8d06a70a997e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a249b2c5-e6d6-48ec-b1fa-8d06a70a997e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ra8x5m:/tmp/ceph-asok.Ra8x5m -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T21:03:19.510+0000 7f3433d68d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T21:03:19.782+0000 7f3433d68d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T21:03:19.782+0000 7f3433d68d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T21:03:19.803431+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:03:19.803431+0000     0       0         0         0         0         0           -           0
2021-05-16T21:03:20.803536+0000     1      10        10         0         0         0           -           0
2021-05-16T21:03:21.803776+0000     2      10        10         0         0         0           -           0
2021-05-16T21:03:22.803881+0000     3      10        10         0         0         0           -           0
2021-05-16T21:03:23.803971+0000     4      12        12         0         0         0           -           0
2021-05-16T21:03:24.804110+0000     5      12        12         0         0         0           -           0
2021-05-16T21:03:25.804213+0000     6      14        14         0         0         0           -           0
2021-05-16T21:03:26.804337+0000     7      14        14         0         0         0           -           0
2021-05-16T21:03:27.804418+0000     8      16        16         0         0         0           -           0
2021-05-16T21:03:28.804519+0000     9      16        16         0         0         0           -           0
2021-05-16T21:03:29.804627+0000    10      17        17         0         0         0           -           0
2021-05-16T21:03:30.804765+0000    11      17        17         0         0         0           -           0
2021-05-16T21:03:31.804847+0000    12      17        17         0         0         0           -           0
2021-05-16T21:03:32.804949+0000    13      17        17         0         0         0           -           0
2021-05-16T21:03:33.805015+0000    14      17        17         0         0         0           -           0
2021-05-16T21:03:34.805149+0000    15      19        19         0         0         0           -           0
2021-05-16T21:03:35.805504+0000    16      19        19         0         0         0           -           0
2021-05-16T21:03:36.805613+0000    17      19        19         0         0         0           -           0
2021-05-16T21:03:37.805734+0000    18      19        19         0         0         0           -           0
2021-05-16T21:03:38.805839+0000    19      22        22         0         0         0           -           0
2021-05-16T21:03:39.805911+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:03:39.805911+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:03:39.805911+0000    20      22        22         0         0         0           -           0
2021-05-16T21:03:40.806033+0000    21      22        22         0         0         0           -           0
2021-05-16T21:03:41.806378+0000    22      22        22         0         0         0           -           0
2021-05-16T21:03:42.806489+0000    23      31        31         0         0         0           -           0
2021-05-16T21:03:43.806582+0000    24      31        31         0         0         0           -           0
2021-05-16T21:03:44.806647+0000    25      34        34         0         0         0           -           0
2021-05-16T21:03:45.806716+0000    26      34        34         0         0         0           -           0
2021-05-16T21:03:46.806797+0000    27      37        37         0         0         0           -           0
2021-05-16T21:03:47.807134+0000    28      37        37         0         0         0           -           0
2021-05-16T21:03:48.807385+0000    29      37        37         0         0         0           -           0
2021-05-16T21:03:49.807557+0000    30      37        37         0         0         0           -           0
2021-05-16T21:03:50.807642+0000    31      37        37         0         0         0           -           0
2021-05-16T21:03:51.807793+0000    32      39        39         0         0         0           -           0
2021-05-16T21:03:52.807897+0000    33      39        39         0         0         0           -           0
2021-05-16T21:03:53.807969+0000    34      41        41         0         0         0           -           0
2021-05-16T21:03:54.808060+0000    35      41        41         0         0         0           -           0
2021-05-16T21:03:55.808141+0000    36      46        46         0         0         0           -           0
2021-05-16T21:03:56.808243+0000    37      46        46         0         0         0           -           0
2021-05-16T21:03:57.808308+0000    38      47        47         0         0         0           -           0
2021-05-16T21:03:58.808374+0000    39      47        47         0         0         0           -           0
2021-05-16T21:03:59.808446+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:03:59.808446+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:03:59.808446+0000    40      48        48         0         0         0           -           0
2021-05-16T21:04:00.808524+0000    41      48        48         0         0         0           -           0
2021-05-16T21:04:01.808649+0000    42      50        50         0         0         0           -           0
2021-05-16T21:04:02.809038+0000    43      50        50         0         0         0           -           0
2021-05-16T21:04:03.809140+0000    44      50        50         0         0         0           -           0
2021-05-16T21:04:04.809468+0000    45      52        52         0         0         0           -           0
2021-05-16T21:04:05.809842+0000    46      52        52         0         0         0           -           0
2021-05-16T21:04:06.809935+0000    47      55        55         0         0         0           -           0
2021-05-16T21:04:07.810037+0000    48      55        55         0         0         0           -           0
2021-05-16T21:04:08.810102+0000    49      55        55         0         0         0           -           0
2021-05-16T21:04:09.810168+0000    50      55        55         0         0         0           -           0
2021-05-16T21:04:10.810234+0000    51      58        58         0         0         0           -           0
2021-05-16T21:04:11.810304+0000    52      58        58         0         0         0           -           0
2021-05-16T21:04:12.810390+0000    53      59        59         0         0         0           -           0
2021-05-16T21:04:13.810469+0000    54      59        59         0         0         0           -           0
2021-05-16T21:04:14.810541+0000    55      62        62         0         0         0           -           0
2021-05-16T21:04:15.810789+0000    56      62        62         0         0         0           -           0
2021-05-16T21:04:16.810887+0000    57      63        63         0         0         0           -           0
2021-05-16T21:04:17.811080+0000    58      63        63         0         0         0           -           0
2021-05-16T21:04:18.811145+0000    59      63        63         0         0         0           -           0
2021-05-16T21:04:19.811462+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:04:19.811462+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:04:19.811462+0000    60      63        63         0         0         0           -           0
2021-05-16T21:04:20.811552+0000    61      63        63         0         0         0           -           0
2021-05-16T21:04:21.811625+0000    62      64        64         0         0         0           -           0
2021-05-16T21:04:22.811935+0000    63      64        64         0         0         0           -           0
2021-05-16T21:04:23.812021+0000    64      66        66         0         0         0           -           0
2021-05-16T21:04:24.812114+0000    65      66        66         0         0         0           -           0
2021-05-16T21:04:25.812189+0000    66      71        71         0         0         0           -           0
2021-05-16T21:04:26.812283+0000    67      71        71         0         0         0           -           0
2021-05-16T21:04:27.812367+0000    68      73        73         0         0         0           -           0
2021-05-16T21:04:28.812466+0000    69      73        73         0         0         0           -           0
2021-05-16T21:04:29.812532+0000    70      73        73         0         0         0           -           0
2021-05-16T21:04:30.812599+0000    71      73        73         0         0         0           -           0
2021-05-16T21:04:31.812669+0000    72      73        73         0         0         0           -           0
2021-05-16T21:04:32.812743+0000    73      74        74         0         0         0           -           0
2021-05-16T21:04:33.813059+0000    74      74        74         0         0         0           -           0
2021-05-16T21:04:34.813132+0000    75      75        75         0         0         0           -           0
2021-05-16T21:04:35.813462+0000    76      75        75         0         0         0           -           0
2021-05-16T21:04:36.813721+0000    77      75        75         0         0         0           -           0
2021-05-16T21:04:37.813941+0000    78      75        75         0         0         0           -           0
2021-05-16T21:04:38.814014+0000    79      83        83         0         0         0           -           0
2021-05-16T21:04:39.814106+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:04:39.814106+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:04:39.814106+0000    80      83        83         0         0         0           -           0
2021-05-16T21:04:40.814414+0000    81      86        86         0         0         0           -           0
2021-05-16T21:04:41.814508+0000    82      86        86         0         0         0           -           0
2021-05-16T21:04:42.814584+0000    83      87        87         0         0         0           -           0
2021-05-16T21:04:43.814683+0000    84      87        87         0         0         0           -           0
2021-05-16T21:04:44.814753+0000    85      88        88         0         0         0           -           0
2021-05-16T21:04:45.815057+0000    86      88        88         0         0         0           -           0
2021-05-16T21:04:46.815360+0000    87      88        88         0         0         0           -           0
2021-05-16T21:04:47.815452+0000    88      90        90         0         0         0           -           0
2021-05-16T21:04:48.815781+0000    89      90        90         0         0         0           -           0
2021-05-16T21:04:49.815893+0000    90      90        90         0         0         0           -           0
2021-05-16T21:04:50.815970+0000    91      90        90         0         0         0           -           0
2021-05-16T21:04:51.816043+0000    92      90        90         0         0         0           -           0
2021-05-16T21:04:52.816112+0000    93      90        90         0         0         0           -           0
2021-05-16T21:04:53.816183+0000    94      90        90         0         0         0           -           0
2021-05-16T21:04:54.816248+0000    95      90        90         0         0         0           -           0
2021-05-16T21:04:55.816331+0000    96      98        98         0         0         0           -           0
2021-05-16T21:04:56.816463+0000    97      98        98         0         0         0           -           0
2021-05-16T21:04:57.816532+0000    98      98        98         0         0         0           -           0
2021-05-16T21:04:58.816610+0000    99      98        98         0         0         0           -           0
2021-05-16T21:04:59.816686+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:04:59.816686+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:04:59.816686+0000   100      99        99         0         0         0           -           0
2021-05-16T21:05:00.816786+0000   101      99        99         0         0         0           -           0
2021-05-16T21:05:01.817091+0000   102      99        99         0         0         0           -           0
2021-05-16T21:05:02.817167+0000   103     101       101         0         0         0           -           0
2021-05-16T21:05:03.817506+0000   104     101       101         0         0         0           -           0
2021-05-16T21:05:04.817571+0000   105     101       101         0         0         0           -           0
2021-05-16T21:05:05.817875+0000   106     101       101         0         0         0           -           0
2021-05-16T21:05:06.817948+0000   107     103       103         0         0         0           -           0
2021-05-16T21:05:07.818053+0000   108     103       103         0         0         0           -           0
2021-05-16T21:05:08.818365+0000   109     107       107         0         0         0           -           0
2021-05-16T21:05:09.818724+0000   110     107       107         0         0         0           -           0
2021-05-16T21:05:10.819044+0000   111     107       107         0         0         0           -           0
2021-05-16T21:05:11.819116+0000   112     107       107         0         0         0           -           0
2021-05-16T21:05:12.819183+0000   113     107       107         0         0         0           -           0
2021-05-16T21:05:13.819250+0000   114     107       107         0         0         0           -           0
2021-05-16T21:05:14.819318+0000   115     114       114         0         0         0           -           0
2021-05-16T21:05:15.819397+0000   116     116       116         0         0         0           -           0
2021-05-16T21:05:16.819462+0000   117     116       116         0         0         0           -           0
2021-05-16T21:05:17.819575+0000   118     117       117         0         0         0           -           0
2021-05-16T21:05:18.819902+0000   119     117       117         0         0         0           -           0
2021-05-16T21:05:19.820001+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:05:19.820001+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:05:19.820001+0000   120     119       119         0         0         0           -           0
2021-05-16T21:05:20.820106+0000   121     119       119         0         0         0           -           0
2021-05-16T21:05:21.820174+0000   122     119       119         0         0         0           -           0
2021-05-16T21:05:22.820241+0000   123     119       119         0         0         0           -           0
2021-05-16T21:05:23.820310+0000   124     119       119         0         0         0           -           0
2021-05-16T21:05:24.820378+0000   125     119       119         0         0         0           -           0
2021-05-16T21:05:25.820442+0000   126     119       119         0         0         0           -           0
2021-05-16T21:05:26.820512+0000   127     119       119         0         0         0           -           0
2021-05-16T21:05:27.820591+0000   128     121       121         0         0         0           -           0
2021-05-16T21:05:28.820683+0000   129     121       121         0         0         0           -           0
2021-05-16T21:05:29.820747+0000   130     121       121         0         0         0           -           0
2021-05-16T21:05:30.820816+0000   131     121       121         0         0         0           -           0
2021-05-16T21:05:31.820887+0000   132     121       121         0         0         0           -           0
2021-05-16T21:05:32.821134+0000   133     131       131         0         0         0           -           0
2021-05-16T21:05:33.821224+0000   134     131       131         0         0         0           -           0
2021-05-16T21:05:34.821474+0000   135     136       136         0         0         0           -           0
2021-05-16T21:05:35.821571+0000   136     136       136         0         0         0           -           0
2021-05-16T21:05:36.821818+0000   137     138       138         0         0         0           -           0
2021-05-16T21:05:37.821910+0000   138     138       138         0         0         0           -           0
2021-05-16T21:05:38.822012+0000   139     139       139         0         0         0           -           0
2021-05-16T21:05:39.822121+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:05:39.822121+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:05:39.822121+0000   140     139       139         0         0         0           -           0
2021-05-16T21:05:40.822207+0000   141     144       144         0         0         0           -           0
2021-05-16T21:05:41.822299+0000   142     144       144         0         0         0           -           0
2021-05-16T21:05:42.822365+0000   143     144       144         0         0         0           -           0
2021-05-16T21:05:43.822437+0000   144     144       144         0         0         0           -           0
2021-05-16T21:05:44.822503+0000   145     144       144         0         0         0           -           0
2021-05-16T21:05:45.822577+0000   146     148       148         0         0         0           -           0
2021-05-16T21:05:46.822668+0000   147     148       148         0         0         0           -           0
2021-05-16T21:05:47.822737+0000   148     148       148         0         0         0           -           0
2021-05-16T21:05:48.822804+0000   149     148       148         0         0         0           -           0
2021-05-16T21:05:49.823043+0000   150     148       148         0         0         0           -           0
2021-05-16T21:05:50.823201+0000   151     148       148         0         0         0           -           0
2021-05-16T21:05:51.823516+0000   152     149       149         0         0         0           -           0
2021-05-16T21:05:52.823853+0000   153     149       149         0         0         0           -           0
2021-05-16T21:05:53.823962+0000   154     151       151         0         0         0           -           0
2021-05-16T21:05:54.824051+0000   155     151       151         0         0         0           -           0
2021-05-16T21:05:55.824120+0000   156     151       151         0         0         0           -           0
2021-05-16T21:05:56.824186+0000   157     151       151         0         0         0           -           0
2021-05-16T21:05:57.824260+0000   158     155       155         0         0         0           -           0
2021-05-16T21:05:58.824352+0000   159     155       155         0         0         0           -           0
2021-05-16T21:05:59.824434+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:05:59.824434+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:05:59.824434+0000   160     155       155         0         0         0           -           0
2021-05-16T21:06:00.824517+0000   161     156       156         0         0         0           -           0
2021-05-16T21:06:01.824607+0000   162     156       156         0         0         0           -           0
2021-05-16T21:06:02.824675+0000   163     156       156         0         0         0           -           0
2021-05-16T21:06:03.824745+0000   164     156       156         0         0         0           -           0
2021-05-16T21:06:04.824816+0000   165     157       157         0         0         0           -           0
2021-05-16T21:06:05.824882+0000   166     157       157         0         0         0           -           0
2021-05-16T21:06:06.824954+0000   167     157       157         0         0         0           -           0
2021-05-16T21:06:07.825063+0000   168     157       157         0         0         0           -           0
2021-05-16T21:06:08.825129+0000   169     157       157         0         0         0           -           0
2021-05-16T21:06:09.825195+0000   170     157       157         0         0         0           -           0
2021-05-16T21:06:10.825501+0000   171     157       157         0         0         0           -           0
2021-05-16T21:06:11.825575+0000   172     157       157         0         0         0           -           0
2021-05-16T21:06:12.825649+0000   173     158       158         0         0         0           -           0
2021-05-16T21:06:13.825970+0000   174     158       158         0         0         0           -           0
2021-05-16T21:06:14.826065+0000   175     158       158         0         0         0           -           0
2021-05-16T21:06:15.826147+0000   176     160       160         0         0         0           -           0
2021-05-16T21:06:16.826239+0000   177     160       160         0         0         0           -           0
2021-05-16T21:06:17.826305+0000   178     160       160         0         0         0           -           0
2021-05-16T21:06:18.826570+0000   179     160       160         0         0         0           -           0
2021-05-16T21:06:19.826666+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:06:19.826666+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:06:19.826666+0000   180     163       163         0         0         0           -           0
2021-05-16T21:06:20.826767+0000   181     163       163         0         0         0           -           0
2021-05-16T21:06:21.826840+0000   182     176       176         0         0         0           -           0
2021-05-16T21:06:22.827157+0000   183     176       176         0         0         0           -           0
2021-05-16T21:06:23.827470+0000   184     178       178         0         0         0           -           0
2021-05-16T21:06:24.827810+0000   185     178       178         0         0         0           -           0
2021-05-16T21:06:25.827888+0000   186     182       182         0         0         0           -           0
2021-05-16T21:06:26.827986+0000   187     182       182         0         0         0           -           0
2021-05-16T21:06:27.828058+0000   188     182       182         0         0         0           -           0
2021-05-16T21:06:28.828125+0000   189     182       182         0         0         0           -           0
2021-05-16T21:06:29.828191+0000   190     182       182         0         0         0           -           0
2021-05-16T21:06:30.828264+0000   191     187       187         0         0         0           -           0
2021-05-16T21:06:31.828372+0000   192     187       187         0         0         0           -           0
2021-05-16T21:06:32.828447+0000   193     189       189         0         0         0           -           0
2021-05-16T21:06:33.828537+0000   194     189       189         0         0         0           -           0
2021-05-16T21:06:34.828611+0000   195     192       192         0         0         0           -           0
2021-05-16T21:06:35.828711+0000   196     192       192         0         0         0           -           0
2021-05-16T21:06:36.828785+0000   197     193       193         0         0         0           -           0
2021-05-16T21:06:37.829114+0000   198     193       193         0         0         0           -           0
2021-05-16T21:06:38.829188+0000   199     198       198         0         0         0           -           0
2021-05-16T21:06:39.829488+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:06:39.829488+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:06:39.829488+0000   200     198       198         0         0         0           -           0
2021-05-16T21:06:40.829575+0000   201     204       204         0         0         0           -           0
2021-05-16T21:06:41.829877+0000   202     204       204         0         0         0           -           0
2021-05-16T21:06:42.829941+0000   203     204       204         0         0         0           -           0
2021-05-16T21:06:43.830010+0000   204     204       204         0         0         0           -           0
2021-05-16T21:06:44.830112+0000   205     204       204         0         0         0           -           0
2021-05-16T21:06:45.830218+0000   206     204       204         0         0         0           -           0
2021-05-16T21:06:46.830321+0000   207     204       204         0         0         0           -           0
2021-05-16T21:06:47.830397+0000   208     209       209         0         0         0           -           0
2021-05-16T21:06:48.830512+0000   209     209       209         0         0         0           -           0
2021-05-16T21:06:49.830609+0000   210     212       212         0         0         0           -           0
2021-05-16T21:06:50.830734+0000   211     212       212         0         0         0           -           0
2021-05-16T21:06:51.831072+0000   212     213       213         0         0         0           -           0
2021-05-16T21:06:52.831378+0000   213     213       213         0         0         0           -           0
2021-05-16T21:06:53.831548+0000   214     213       213         0         0         0           -           0
2021-05-16T21:06:54.831672+0000   215     213       213         0         0         0           -           0
2021-05-16T21:06:55.831746+0000   216     216       216         0         0         0           -           0
2021-05-16T21:06:56.831908+0000   217     216       216         0         0         0           -           0
2021-05-16T21:06:57.832015+0000   218     216       216         0         0         0           -           0
2021-05-16T21:06:58.832108+0000   219     216       216         0         0         0           -           0
2021-05-16T21:06:59.832178+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:06:59.832178+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:06:59.832178+0000   220     216       216         0         0         0           -           0
2021-05-16T21:07:00.832295+0000   221     219       219         0         0         0           -           0
2021-05-16T21:07:01.832420+0000   222     219       219         0         0         0           -           0
2021-05-16T21:07:02.832512+0000   223     220       220         0         0         0           -           0
2021-05-16T21:07:03.832609+0000   224     220       220         0         0         0           -           0
2021-05-16T21:07:04.832688+0000   225     223       223         0         0         0           -           0
2021-05-16T21:07:05.832779+0000   226     223       223         0         0         0           -           0
2021-05-16T21:07:06.832852+0000   227     228       228         0         0         0           -           0
2021-05-16T21:07:07.832947+0000   228     228       228         0         0         0           -           0
2021-05-16T21:07:08.833073+0000   229     228       228         0         0         0           -           0
2021-05-16T21:07:09.833139+0000   230     228       228         0         0         0           -           0
2021-05-16T21:07:10.833452+0000   231     228       228         0         0         0           -           0
2021-05-16T21:07:11.833551+0000   232     228       228         0         0         0           -           0
2021-05-16T21:07:12.833791+0000   233     228       228         0         0         0           -           0
2021-05-16T21:07:13.834104+0000   234     230       230         0         0         0           -           0
2021-05-16T21:07:14.834379+0000   235     230       230         0         0         0           -           0
2021-05-16T21:07:15.834481+0000   236     231       231         0         0         0           -           0
2021-05-16T21:07:16.834592+0000   237     231       231         0         0         0           -           0
2021-05-16T21:07:17.834702+0000   238     231       231         0         0         0           -           0
2021-05-16T21:07:18.834915+0000   239     231       231         0         0         0           -           0
2021-05-16T21:07:19.835134+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:07:19.835134+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:07:19.835134+0000   240     231       231         0         0         0           -           0
2021-05-16T21:07:20.835451+0000   241     231       231         0         0         0           -           0
2021-05-16T21:07:21.835532+0000   242     234       234         0         0         0           -           0
2021-05-16T21:07:22.835846+0000   243     234       234         0         0         0           -           0
2021-05-16T21:07:23.835958+0000   244     235       235         0         0         0           -           0
2021-05-16T21:07:24.836092+0000   245     235       235         0         0         0           -           0
2021-05-16T21:07:25.836165+0000   246     236       236         0         0         0           -           0
2021-05-16T21:07:26.836285+0000   247     236       236         0         0         0           -           0
2021-05-16T21:07:27.836391+0000   248     236       236         0         0         0           -           0
2021-05-16T21:07:28.836493+0000   249     236       236         0         0         0           -           0
2021-05-16T21:07:29.836559+0000   250     236       236         0         0         0           -           0
2021-05-16T21:07:30.836637+0000   251     239       239         0         0         0           -           0
2021-05-16T21:07:31.836743+0000   252     239       239         0         0         0           -           0
2021-05-16T21:07:32.837047+0000   253     241       241         0         0         0           -           0
2021-05-16T21:07:33.837130+0000   254     241       241         0         0         0           -           0
2021-05-16T21:07:34.837211+0000   255     251       251         0         0         0           -           0
2021-05-16T21:07:35.837316+0000   256     251       251         0         0         0           -           0
2021-05-16T21:07:36.837423+0000   257     255       255         0         0         0           -           0
2021-05-16T21:07:37.837757+0000   258     255       255         0         0         0           -           0
2021-05-16T21:07:39.023056+0000   259       6       256       250   15.4309    15.444     2.51466     133.617
2021-05-16T21:07:40.023150+0000 min lat: 2.51466 max lat: 258.862 avg lat: 133.617
2021-05-16T21:07:40.023150+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:07:40.023150+0000   260       6       256       250   15.3716         0           -     133.617
2021-05-16T21:07:41.023257+0000   261       6       256       250   15.3128         0           -     133.617
2021-05-16T21:07:42.023525+0000   262       6       256       250   15.2544         0           -     133.617
2021-05-16T21:07:43.023636+0000   263       2       256       254   15.4395        16     6.46294     131.648
2021-05-16T21:07:44.023950+0000   264       2       256       254   15.3811         0           -     131.648
2021-05-16T21:07:45.024017+0000   265       2       256       254   15.3231         0           -     131.648
2021-05-16T21:07:46.024089+0000   266       1       256       255   15.3256   5.33333     8.57575     131.165
2021-05-16T21:07:47.024210+0000   267       1       256       255   15.2683         0           -     131.165
2021-05-16T21:07:48.024279+0000   268       1       256       255   15.2113         0           -     131.165
2021-05-16T21:07:49.024347+0000   269       1       256       255   15.1548         0           -     131.165
2021-05-16T21:07:50.024521+0000 Total time run:       269.589
Total reads made:     256
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   15.1935
Average IOPS:         0
Stddev IOPS:          0.0609711
Max IOPS:             1
Min IOPS:             0
Average Latency(s):   130.703
Max latency(s):       258.862
Min latency(s):       2.51466

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:07:50,794183907-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 287322


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:07:50,799225269-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:08:14,192163035-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:08:14,201643327-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:08:22,570507650-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:08:22,580799516-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:08:30,959892029-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:08:30,969763916-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:08:39,429685514-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:08:39,439404584-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:08:47,776432247-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:08:47,786242429-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:08:47,793981670-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T17:08:47,796987517-04:00][RUNNING][ROUND 4/7/40] object_size=16MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:08:47,801364280-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:08:47,810498622-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40534\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.190551\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c2fd6e77-9433-4286-aa23-53c066c7a8b4\nsetting min_mon_release = octopus\nepoch 0\nfsid c2fd6e77-9433-4286-aa23-53c066c7a8b4\nlast_changed 2021-05-16T14:09:14.011542-0700\ncreated 2021-05-16T14:09:14.011542-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40534/0,v1:10.10.1.2:40535/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.190551 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 0c07289d-3add-4748-ae26-66b6bb42206c\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 804ce3ed-66d0-4345-a055-3349407b85a3\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 6f1679af-901e-4dcb-a467-fcc41dc05b0d\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42534\n  w/ user/pass: admin / ab0b56c0-b333-4cab-afda-36a6ac8c9df3\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 17:09:33 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40534
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.190551
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c2fd6e77-9433-4286-aa23-53c066c7a8b4
setting min_mon_release = octopus
epoch 0
fsid c2fd6e77-9433-4286-aa23-53c066c7a8b4
last_changed 2021-05-16T14:09:14.011542-0700
created 2021-05-16T14:09:14.011542-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40534/0,v1:10.10.1.2:40535/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.190551 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 0c07289d-3add-4748-ae26-66b6bb42206c
0
start osd.0
add osd1 804ce3ed-66d0-4345-a055-3349407b85a3
1
start osd.1
add osd2 6f1679af-901e-4dcb-a467-fcc41dc05b0d
2
start osd.2


restful urls: https://10.10.1.2:42534
  w/ user/pass: admin / ab0b56c0-b333-4cab-afda-36a6ac8c9df3


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T14:08:49.629-0700 7f9c3eacd1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T14:08:49.629-0700 7f9c3eacd1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T14:08:49.645-0700 7ff3f047b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T14:08:49.645-0700 7ff3f047b1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40534,v1:10.10.1.2:40535] --print /tmp/ceph_monmap.190551 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.190551 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.190551 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42534 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55c6b2996000 @  0x7febc33b4680 0x7febc33d5824 0x7febc3b70187 0x7febc3b78355 0x7febc3b70708 0x7febc3b70877 0x7febc3b71c24 0x7febc3b89ec1 0x7febc3afc5f3 0x7febc3b5de97 0x7febc3b65b1a 0x7febc3268d84 0x7febc3384609 0x7febc2f58293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.bMgNJSwYM6 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0c07289d-3add-4748-ae26-66b6bb42206c -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAGiqFg0xd0DRAAMRuSVag3EUlxGt0sMsiR6Q== --osd-uuid 0c07289d-3add-4748-ae26-66b6bb42206c 
2021-05-16T14:09:26.881-0700 7f8c0cd10f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T14:09:26.901-0700 7f8c0cd10f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T14:09:26.901-0700 7f8c0cd10f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5600d9ee8000 @  0x7f8c0d6d9680 0x7f8c0d6fa824 0x5600cf9b6447 0x5600cf9be4b5 0x5600cf9b69c8 0x5600cf9b6b37 0x5600cf9b7ee4 0x5600cf788ca1 0x5600cf960423 0x5600cf7792a7 0x5600cf77e53a 0x7f8c0d22cd84 0x7f8c0d3b1609 0x7f8c0cf1a293
2021-05-16T14:09:27.213-0700 7f8c0cd10f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 804ce3ed-66d0-4345-a055-3349407b85a3 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x55bc86a44000 @  0x7f03ae523680 0x7f03ae544824 0x55bc7c291447 0x55bc7c2994b5 0x55bc7c2919c8 0x55bc7c291b37 0x55bc7c292ee4 0x55bc7c063ca1 0x55bc7c23b423 0x55bc7c0542a7 0x55bc7c05953a 0x7f03ae076d84 0x7f03ae1fb609 0x7f03add64293
2021-05-16T14:09:27.821-0700 7f03adb5af00 -1 Falling back to public interface
2021-05-16T14:09:28.085-0700 7f03adb5af00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAHiqFgDMCgHhAAa+BcezOWnnPY0J0bKXXeEw== --osd-uuid 804ce3ed-66d0-4345-a055-3349407b85a3 
2021-05-16T14:09:28.237-0700 7f55dd52df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T14:09:28.257-0700 7f55dd52df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T14:09:28.257-0700 7f55dd52df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x557eaa90a000 @  0x7f55ddef6680 0x7f55ddf17824 0x557ea06af447 0x557ea06b74b5 0x557ea06af9c8 0x557ea06afb37 0x557ea06b0ee4 0x557ea0481ca1 0x557ea0659423 0x557ea04722a7 0x557ea047753a 0x7f55dda49d84 0x7f55ddbce609 0x7f55dd737293
2021-05-16T14:09:28.581-0700 7f55dd52df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6f1679af-901e-4dcb-a467-fcc41dc05b0d -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5584f8832000 @  0x7f1b2ed8a680 0x7f1b2edab824 0x5584ede32447 0x5584ede3a4b5 0x5584ede329c8 0x5584ede32b37 0x5584ede33ee4 0x5584edc04ca1 0x5584edddc423 0x5584edbf52a7 0x5584edbfa53a 0x7f1b2e8ddd84 0x7f1b2ea62609 0x7f1b2e5cb293
2021-05-16T14:09:29.249-0700 7f1b2e3c1f00 -1 Falling back to public interface
2021-05-16T14:09:29.513-0700 7f1b2e3c1f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAIiqFgPX03OBAAQeSnid/A0QYTO6CS3xXBBw== --osd-uuid 6f1679af-901e-4dcb-a467-fcc41dc05b0d 
2021-05-16T14:09:29.621-0700 7f615ab85f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T14:09:29.641-0700 7f615ab85f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T14:09:29.641-0700 7f615ab85f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55f45806e000 @  0x7f615b54e680 0x7f615b56f824 0x55f44d4d3447 0x55f44d4db4b5 0x55f44d4d39c8 0x55f44d4d3b37 0x55f44d4d4ee4 0x55f44d2a5ca1 0x55f44d47d423 0x55f44d2962a7 0x55f44d29b53a 0x7f615b0a1d84 0x7f615b226609 0x7f615ad8f293
2021-05-16T14:09:29.961-0700 7f615ab85f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x556c64668000 @  0x7f5eecabb680 0x7f5eecadc824 0x556c58aa0447 0x556c58aa84b5 0x556c58aa09c8 0x556c58aa0b37 0x556c58aa1ee4 0x556c58872ca1 0x556c58a4a423 0x556c588632a7 0x556c5886853a 0x7f5eec60ed84 0x7f5eec793609 0x7f5eec2fc293
2021-05-16T14:09:30.653-0700 7f5eec0f2f00 -1 Falling back to public interface
2021-05-16T14:09:30.921-0700 7f5eec0f2f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:33,131723293-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:33,142442492-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:33,182596615-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:33,185739790-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:36,723516573-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:36,727543319-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:40,171979591-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:40,175755515-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:43,534291136-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:43,537836788-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:50,436479570-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:50,440095755-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:54,883858057-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:54,887939235-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:59,390963741-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:09:59,394881562-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:10:04,004042531-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:10:04,008201185-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:10:08,467835239-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:10:08,471760634-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:10:13,123924659-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:10:13,127920957-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:10:16,995464578-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:10:16,999695367-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:10:20,429721399-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:10:20,433404008-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:10:23,841491401-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:10:47,269246111-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:10:55,807573959-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:11:04,420963226-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:11:12,942000772-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:11:21,421955032-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:11:30,109072457-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   248 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:11:30,118844667-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:11:38,630988117-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:11:38,641538609-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:11:47,242686276-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:11:47,253065516-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:11:55,765524163-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:11:55,774918033-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:12:04,339523827-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:12:04,349747345-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:12:04,357379404-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:12:04,361770184-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:12:04,371560407-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=293717
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:12:04,379393425-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:12:04,399033435-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:12:04,402291526-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T21:12:06.700+0000 7f367774dd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T21:12:06.972+0000 7f367774dd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T21:12:06.972+0000 7f367774dd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T21:12:06.992834+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-16T21:12:06.992864+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T21:12:07.995538+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:12:07.995538+0000     0       0         0         0         0         0           -           0
2021-05-16T21:12:08.995808+0000     1       6         6         0         0         0           -           0
2021-05-16T21:12:09.995873+0000     2      29        29         0         0         0           -           0
2021-05-16T21:12:10.995979+0000     3      59        59         0         0         0           -           0
2021-05-16T21:12:11.996043+0000     4      61        61         0         0         0           -           0
2021-05-16T21:12:12.996109+0000     5      76        76         0         0         0           -           0
2021-05-16T21:12:13.996174+0000     6     106       106         0         0         0           -           0
2021-05-16T21:12:14.996281+0000     7     135       135         0         0         0           -           0
2021-05-16T21:12:15.996345+0000     8     167       167         0         0         0           -           0
2021-05-16T21:12:16.996433+0000     9     195       195         0         0         0           -           0
2021-05-16T21:12:17.996500+0000    10     224       224         0         0         0           -           0
2021-05-16T21:12:18.996603+0000    11     253       253         0         0         0           -           0
2021-05-16T21:12:19.996668+0000    12     255       282        27   35.9968        36     10.0804     10.3345
2021-05-16T21:12:20.996927+0000    13     255       313        58   71.3773       496     10.1011     10.2052
2021-05-16T21:12:21.996996+0000    14     255       343        88   100.561       480      8.6402     9.73987
2021-05-16T21:12:22.997102+0000    15     255       358       103   109.856       240      8.7011     9.58512
2021-05-16T21:12:23.997170+0000    16     255       358       103    102.99         0           -     9.58512
2021-05-16T21:12:24.997242+0000    17     255       376       121   113.871       144     10.5246     9.71966
2021-05-16T21:12:25.997309+0000    18     255       377       122   108.434        16     10.5848     9.72675
2021-05-16T21:12:26.997416+0000    19     255       393       138   116.199       256     11.9751     9.98696
2021-05-16T21:12:27.997495+0000 min lat: 8.62288 max lat: 12.1329 avg lat: 10.3048
2021-05-16T21:12:27.997495+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:12:27.997495+0000    20     255       419       164   131.188       416     12.1329     10.3048
2021-05-16T21:12:28.997796+0000    21       1       420       419   319.205      4080     0.12669     8.37916
2021-05-16T21:12:29.997943+0000 Total time run:         21.7144
Total writes made:      420
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     309.471
Stddev Bandwidth:       884.082
Max bandwidth (MB/sec): 4080
Min bandwidth (MB/sec): 0
Average IOPS:           19
Stddev IOPS:            55.2588
Max IOPS:               255
Min IOPS:               0
Average Latency(s):     8.36421
Stddev Latency(s):      3.24683
Max latency(s):         12.178
Min latency(s):         0.12669

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:12:30,674645690-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 293717


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:12:30,679741804-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:12:54,044428723-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 421 objects, 6.6 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:12:54,054553276-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:13:02,453700844-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 421 objects, 6.6 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:13:02,462868118-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:13:10,991729427-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 421 objects, 6.6 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:13:11,001824765-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:13:19,405546370-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 421 objects, 6.6 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:13:19,415918929-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:13:27,855507383-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 421 objects, 6.6 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:13:27,865758513-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:13:27,873794522-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:13:27,878158741-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:13:27,888763866-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=295126
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:13:27,895657288-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:13:27,914728560-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:13:27,918053988-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4f7bdbff-8901-46a2-b6dd-3ae3be3f6e28 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.AG1Nyu:/tmp/ceph-asok.AG1Nyu -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T21:13:30.232+0000 7f845ea67d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T21:13:30.516+0000 7f845ea67d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T21:13:30.516+0000 7f845ea67d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T21:13:30.539657+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:13:30.539657+0000     0       0         0         0         0         0           -           0
2021-05-16T21:13:31.539897+0000     1       6         6         0         0         0           -           0
2021-05-16T21:13:32.539967+0000     2       7         7         0         0         0           -           0
2021-05-16T21:13:33.540035+0000     3       7         7         0         0         0           -           0
2021-05-16T21:13:34.540103+0000     4       8         8         0         0         0           -           0
2021-05-16T21:13:35.540183+0000     5       8         8         0         0         0           -           0
2021-05-16T21:13:36.540250+0000     6      10        10         0         0         0           -           0
2021-05-16T21:13:37.540318+0000     7      10        10         0         0         0           -           0
2021-05-16T21:13:38.540387+0000     8      10        10         0         0         0           -           0
2021-05-16T21:13:39.540466+0000     9      10        10         0         0         0           -           0
2021-05-16T21:13:40.540533+0000    10      16        16         0         0         0           -           0
2021-05-16T21:13:41.540600+0000    11      17        17         0         0         0           -           0
2021-05-16T21:13:42.540668+0000    12      17        17         0         0         0           -           0
2021-05-16T21:13:43.540750+0000    13      20        20         0         0         0           -           0
2021-05-16T21:13:44.541068+0000    14      20        20         0         0         0           -           0
2021-05-16T21:13:45.541136+0000    15      22        22         0         0         0           -           0
2021-05-16T21:13:46.541203+0000    16      22        22         0         0         0           -           0
2021-05-16T21:13:47.541381+0000    17      24        24         0         0         0           -           0
2021-05-16T21:13:48.541449+0000    18      24        24         0         0         0           -           0
2021-05-16T21:13:49.541521+0000    19      29        29         0         0         0           -           0
2021-05-16T21:13:50.541828+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:13:50.541828+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:13:50.541828+0000    20      29        29         0         0         0           -           0
2021-05-16T21:13:51.541919+0000    21      32        32         0         0         0           -           0
2021-05-16T21:13:52.541990+0000    22      32        32         0         0         0           -           0
2021-05-16T21:13:53.542059+0000    23      33        33         0         0         0           -           0
2021-05-16T21:13:54.542127+0000    24      33        33         0         0         0           -           0
2021-05-16T21:13:55.542210+0000    25      33        33         0         0         0           -           0
2021-05-16T21:13:56.542278+0000    26      33        33         0         0         0           -           0
2021-05-16T21:13:57.542349+0000    27      33        33         0         0         0           -           0
2021-05-16T21:13:58.542416+0000    28      33        33         0         0         0           -           0
2021-05-16T21:13:59.542497+0000    29      33        33         0         0         0           -           0
2021-05-16T21:14:00.542565+0000    30      34        34         0         0         0           -           0
2021-05-16T21:14:01.542632+0000    31      34        34         0         0         0           -           0
2021-05-16T21:14:02.542700+0000    32      36        36         0         0         0           -           0
2021-05-16T21:14:03.542781+0000    33      36        36         0         0         0           -           0
2021-05-16T21:14:04.543050+0000    34      36        36         0         0         0           -           0
2021-05-16T21:14:05.543150+0000    35      36        36         0         0         0           -           0
2021-05-16T21:14:06.543218+0000    36      48        48         0         0         0           -           0
2021-05-16T21:14:07.543539+0000    37      48        48         0         0         0           -           0
2021-05-16T21:14:08.543607+0000    38      50        50         0         0         0           -           0
2021-05-16T21:14:09.543676+0000    39      50        50         0         0         0           -           0
2021-05-16T21:14:10.543823+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:14:10.543823+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:14:10.543823+0000    40      50        50         0         0         0           -           0
2021-05-16T21:14:11.543914+0000    41      51        51         0         0         0           -           0
2021-05-16T21:14:12.543986+0000    42      51        51         0         0         0           -           0
2021-05-16T21:14:13.544065+0000    43      52        52         0         0         0           -           0
2021-05-16T21:14:14.544133+0000    44      52        52         0         0         0           -           0
2021-05-16T21:14:15.544215+0000    45      54        54         0         0         0           -           0
2021-05-16T21:14:16.544284+0000    46      54        54         0         0         0           -           0
2021-05-16T21:14:17.544351+0000    47      54        54         0         0         0           -           0
2021-05-16T21:14:18.544436+0000    48      54        54         0         0         0           -           0
2021-05-16T21:14:19.544518+0000    49      57        57         0         0         0           -           0
2021-05-16T21:14:20.544586+0000    50      57        57         0         0         0           -           0
2021-05-16T21:14:21.544653+0000    51      57        57         0         0         0           -           0
2021-05-16T21:14:22.544721+0000    52      57        57         0         0         0           -           0
2021-05-16T21:14:23.545041+0000    53      60        60         0         0         0           -           0
2021-05-16T21:14:24.545350+0000    54      60        60         0         0         0           -           0
2021-05-16T21:14:25.545511+0000    55      60        60         0         0         0           -           0
2021-05-16T21:14:26.545820+0000    56      63        63         0         0         0           -           0
2021-05-16T21:14:27.545905+0000    57      63        63         0         0         0           -           0
2021-05-16T21:14:28.545973+0000    58      63        63         0         0         0           -           0
2021-05-16T21:14:29.546042+0000    59      63        63         0         0         0           -           0
2021-05-16T21:14:30.546108+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:14:30.546108+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:14:30.546108+0000    60      65        65         0         0         0           -           0
2021-05-16T21:14:31.546199+0000    61      65        65         0         0         0           -           0
2021-05-16T21:14:32.546267+0000    62      65        65         0         0         0           -           0
2021-05-16T21:14:33.546336+0000    63      65        65         0         0         0           -           0
2021-05-16T21:14:34.546436+0000    64      68        68         0         0         0           -           0
2021-05-16T21:14:35.546518+0000    65      68        68         0         0         0           -           0
2021-05-16T21:14:36.546586+0000    66      71        71         0         0         0           -           0
2021-05-16T21:14:37.546653+0000    67      71        71         0         0         0           -           0
2021-05-16T21:14:38.546721+0000    68      74        74         0         0         0           -           0
2021-05-16T21:14:39.547040+0000    69      74        74         0         0         0           -           0
2021-05-16T21:14:40.547109+0000    70      74        74         0         0         0           -           0
2021-05-16T21:14:41.547177+0000    71      76        76         0         0         0           -           0
2021-05-16T21:14:42.547247+0000    72      76        76         0         0         0           -           0
2021-05-16T21:14:43.547404+0000    73      78        78         0         0         0           -           0
2021-05-16T21:14:44.547472+0000    74      78        78         0         0         0           -           0
2021-05-16T21:14:45.547542+0000    75      82        82         0         0         0           -           0
2021-05-16T21:14:46.547612+0000    76      82        82         0         0         0           -           0
2021-05-16T21:14:47.547880+0000    77      82        82         0         0         0           -           0
2021-05-16T21:14:48.547950+0000    78      82        82         0         0         0           -           0
2021-05-16T21:14:49.548019+0000    79      83        83         0         0         0           -           0
2021-05-16T21:14:50.548087+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:14:50.548087+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:14:50.548087+0000    80      83        83         0         0         0           -           0
2021-05-16T21:14:51.548178+0000    81      86        86         0         0         0           -           0
2021-05-16T21:14:52.548246+0000    82      86        86         0         0         0           -           0
2021-05-16T21:14:53.548314+0000    83      90        90         0         0         0           -           0
2021-05-16T21:14:54.548382+0000    84      90        90         0         0         0           -           0
2021-05-16T21:14:55.548461+0000    85      90        90         0         0         0           -           0
2021-05-16T21:14:56.548529+0000    86      92        92         0         0         0           -           0
2021-05-16T21:14:57.548598+0000    87      92        92         0         0         0           -           0
2021-05-16T21:14:58.548667+0000    88      93        93         0         0         0           -           0
2021-05-16T21:14:59.548746+0000    89      93        93         0         0         0           -           0
2021-05-16T21:15:00.548817+0000    90      95        95         0         0         0           -           0
2021-05-16T21:15:01.549125+0000    91      95        95         0         0         0           -           0
2021-05-16T21:15:02.549378+0000    92     102       102         0         0         0           -           0
2021-05-16T21:15:03.549499+0000    93     102       102         0         0         0           -           0
2021-05-16T21:15:04.549566+0000    94     102       102         0         0         0           -           0
2021-05-16T21:15:05.549785+0000    95     102       102         0         0         0           -           0
2021-05-16T21:15:06.549854+0000    96     103       103         0         0         0           -           0
2021-05-16T21:15:07.549934+0000    97     103       103         0         0         0           -           0
2021-05-16T21:15:08.550004+0000    98     103       103         0         0         0           -           0
2021-05-16T21:15:09.550072+0000    99     103       103         0         0         0           -           0
2021-05-16T21:15:10.550140+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:15:10.550140+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:15:10.550140+0000   100     103       103         0         0         0           -           0
2021-05-16T21:15:11.550232+0000   101     104       104         0         0         0           -           0
2021-05-16T21:15:12.550300+0000   102     104       104         0         0         0           -           0
2021-05-16T21:15:13.550405+0000   103     104       104         0         0         0           -           0
2021-05-16T21:15:14.550475+0000   104     104       104         0         0         0           -           0
2021-05-16T21:15:15.550559+0000   105     108       108         0         0         0           -           0
2021-05-16T21:15:16.550641+0000   106     108       108         0         0         0           -           0
2021-05-16T21:15:17.550721+0000   107     109       109         0         0         0           -           0
2021-05-16T21:15:18.550792+0000   108     109       109         0         0         0           -           0
2021-05-16T21:15:19.551052+0000   109     117       117         0         0         0           -           0
2021-05-16T21:15:20.551357+0000   110     117       117         0         0         0           -           0
2021-05-16T21:15:21.551518+0000   111     118       118         0         0         0           -           0
2021-05-16T21:15:22.551787+0000   112     118       118         0         0         0           -           0
2021-05-16T21:15:23.551960+0000   113     118       118         0         0         0           -           0
2021-05-16T21:15:24.552030+0000   114     128       128         0         0         0           -           0
2021-05-16T21:15:25.552097+0000   115     128       128         0         0         0           -           0
2021-05-16T21:15:26.552166+0000   116     129       129         0         0         0           -           0
2021-05-16T21:15:27.552246+0000   117     129       129         0         0         0           -           0
2021-05-16T21:15:28.552314+0000   118     132       132         0         0         0           -           0
2021-05-16T21:15:29.552383+0000   119     132       132         0         0         0           -           0
2021-05-16T21:15:30.552453+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:15:30.552453+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:15:30.552453+0000   120     132       132         0         0         0           -           0
2021-05-16T21:15:31.552549+0000   121     132       132         0         0         0           -           0
2021-05-16T21:15:32.552617+0000   122     132       132         0         0         0           -           0
2021-05-16T21:15:33.552687+0000   123     132       132         0         0         0           -           0
2021-05-16T21:15:34.552755+0000   124     137       137         0         0         0           -           0
2021-05-16T21:15:35.553075+0000   125     137       137         0         0         0           -           0
2021-05-16T21:15:36.553386+0000   126     140       140         0         0         0           -           0
2021-05-16T21:15:37.553551+0000   127     140       140         0         0         0           -           0
2021-05-16T21:15:38.553767+0000   128     140       140         0         0         0           -           0
2021-05-16T21:15:39.553959+0000   129     142       142         0         0         0           -           0
2021-05-16T21:15:40.554027+0000   130     142       142         0         0         0           -           0
2021-05-16T21:15:41.554095+0000   131     142       142         0         0         0           -           0
2021-05-16T21:15:42.554161+0000   132     142       142         0         0         0           -           0
2021-05-16T21:15:43.554242+0000   133     142       142         0         0         0           -           0
2021-05-16T21:15:44.554312+0000   134     142       142         0         0         0           -           0
2021-05-16T21:15:45.554380+0000   135     146       146         0         0         0           -           0
2021-05-16T21:15:46.554535+0000   136     146       146         0         0         0           -           0
2021-05-16T21:15:47.554616+0000   137     146       146         0         0         0           -           0
2021-05-16T21:15:48.554684+0000   138     146       146         0         0         0           -           0
2021-05-16T21:15:49.554751+0000   139     146       146         0         0         0           -           0
2021-05-16T21:15:50.555082+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:15:50.555082+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:15:50.555082+0000   140     146       146         0         0         0           -           0
2021-05-16T21:15:51.555414+0000   141     149       149         0         0         0           -           0
2021-05-16T21:15:52.555523+0000   142     149       149         0         0         0           -           0
2021-05-16T21:15:53.555841+0000   143     149       149         0         0         0           -           0
2021-05-16T21:15:54.555909+0000   144     150       150         0         0         0           -           0
2021-05-16T21:15:55.555989+0000   145     150       150         0         0         0           -           0
2021-05-16T21:15:56.556057+0000   146     151       151         0         0         0           -           0
2021-05-16T21:15:57.556125+0000   147     151       151         0         0         0           -           0
2021-05-16T21:15:58.556192+0000   148     151       151         0         0         0           -           0
2021-05-16T21:15:59.556271+0000   149     151       151         0         0         0           -           0
2021-05-16T21:16:00.556339+0000   150     151       151         0         0         0           -           0
2021-05-16T21:16:01.556419+0000   151     151       151         0         0         0           -           0
2021-05-16T21:16:02.556488+0000   152     155       155         0         0         0           -           0
2021-05-16T21:16:03.556569+0000   153     155       155         0         0         0           -           0
2021-05-16T21:16:04.556637+0000   154     155       155         0         0         0           -           0
2021-05-16T21:16:05.556703+0000   155     155       155         0         0         0           -           0
2021-05-16T21:16:06.556931+0000   156     155       155         0         0         0           -           0
2021-05-16T21:16:07.557013+0000   157     155       155         0         0         0           -           0
2021-05-16T21:16:08.557082+0000   158     155       155         0         0         0           -           0
2021-05-16T21:16:09.557150+0000   159     159       159         0         0         0           -           0
2021-05-16T21:16:10.557455+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:16:10.557455+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:16:10.557455+0000   160     159       159         0         0         0           -           0
2021-05-16T21:16:11.557546+0000   161     171       171         0         0         0           -           0
2021-05-16T21:16:12.557814+0000   162     171       171         0         0         0           -           0
2021-05-16T21:16:13.557942+0000   163     173       173         0         0         0           -           0
2021-05-16T21:16:14.558014+0000   164     173       173         0         0         0           -           0
2021-05-16T21:16:15.558097+0000   165     175       175         0         0         0           -           0
2021-05-16T21:16:16.558165+0000   166     175       175         0         0         0           -           0
2021-05-16T21:16:17.558233+0000   167     175       175         0         0         0           -           0
2021-05-16T21:16:18.558301+0000   168     175       175         0         0         0           -           0
2021-05-16T21:16:19.558380+0000   169     178       178         0         0         0           -           0
2021-05-16T21:16:20.558473+0000   170     178       178         0         0         0           -           0
2021-05-16T21:16:21.558552+0000   171     178       178         0         0         0           -           0
2021-05-16T21:16:22.558625+0000   172     178       178         0         0         0           -           0
2021-05-16T21:16:23.558706+0000   173     178       178         0         0         0           -           0
2021-05-16T21:16:24.558774+0000   174     181       181         0         0         0           -           0
2021-05-16T21:16:25.559057+0000   175     181       181         0         0         0           -           0
2021-05-16T21:16:26.559124+0000   176     182       182         0         0         0           -           0
2021-05-16T21:16:27.559427+0000   177     182       182         0         0         0           -           0
2021-05-16T21:16:28.559498+0000   178     182       182         0         0         0           -           0
2021-05-16T21:16:29.559767+0000   179     182       182         0         0         0           -           0
2021-05-16T21:16:30.559834+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:16:30.559834+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:16:30.559834+0000   180     183       183         0         0         0           -           0
2021-05-16T21:16:31.559925+0000   181     183       183         0         0         0           -           0
2021-05-16T21:16:32.559992+0000   182     184       184         0         0         0           -           0
2021-05-16T21:16:33.560061+0000   183     184       184         0         0         0           -           0
2021-05-16T21:16:34.560132+0000   184     185       185         0         0         0           -           0
2021-05-16T21:16:35.560216+0000   185     185       185         0         0         0           -           0
2021-05-16T21:16:36.560287+0000   186     185       185         0         0         0           -           0
2021-05-16T21:16:37.560367+0000   187     186       186         0         0         0           -           0
2021-05-16T21:16:38.560434+0000   188     186       186         0         0         0           -           0
2021-05-16T21:16:39.560517+0000   189     186       186         0         0         0           -           0
2021-05-16T21:16:40.560584+0000   190     186       186         0         0         0           -           0
2021-05-16T21:16:41.560653+0000   191     189       189         0         0         0           -           0
2021-05-16T21:16:42.560751+0000   192     189       189         0         0         0           -           0
2021-05-16T21:16:43.561071+0000   193     189       189         0         0         0           -           0
2021-05-16T21:16:44.561377+0000   194     189       189         0         0         0           -           0
2021-05-16T21:16:45.561445+0000   195     189       189         0         0         0           -           0
2021-05-16T21:16:46.561514+0000   196     189       189         0         0         0           -           0
2021-05-16T21:16:47.561595+0000   197     191       191         0         0         0           -           0
2021-05-16T21:16:48.561809+0000   198     191       191         0         0         0           -           0
2021-05-16T21:16:49.561877+0000   199     191       191         0         0         0           -           0
2021-05-16T21:16:50.561948+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:16:50.561948+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:16:50.561948+0000   200     191       191         0         0         0           -           0
2021-05-16T21:16:51.562039+0000   201     191       191         0         0         0           -           0
2021-05-16T21:16:52.562107+0000   202     197       197         0         0         0           -           0
2021-05-16T21:16:53.562175+0000   203     197       197         0         0         0           -           0
2021-05-16T21:16:54.562242+0000   204     200       200         0         0         0           -           0
2021-05-16T21:16:55.562331+0000   205     200       200         0         0         0           -           0
2021-05-16T21:16:56.562405+0000   206     201       201         0         0         0           -           0
2021-05-16T21:16:57.562486+0000   207     201       201         0         0         0           -           0
2021-05-16T21:16:58.562559+0000   208     203       203         0         0         0           -           0
2021-05-16T21:16:59.562642+0000   209     203       203         0         0         0           -           0
2021-05-16T21:17:00.562711+0000   210     204       204         0         0         0           -           0
2021-05-16T21:17:01.562780+0000   211     204       204         0         0         0           -           0
2021-05-16T21:17:02.563088+0000   212     205       205         0         0         0           -           0
2021-05-16T21:17:03.563172+0000   213     205       205         0         0         0           -           0
2021-05-16T21:17:04.563480+0000   214     206       206         0         0         0           -           0
2021-05-16T21:17:05.563549+0000   215     206       206         0         0         0           -           0
2021-05-16T21:17:06.563617+0000   216     206       206         0         0         0           -           0
2021-05-16T21:17:07.563696+0000   217     206       206         0         0         0           -           0
2021-05-16T21:17:08.563821+0000   218     206       206         0         0         0           -           0
2021-05-16T21:17:09.563889+0000   219     206       206         0         0         0           -           0
2021-05-16T21:17:10.563965+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:17:10.563965+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:17:10.563965+0000   220     206       206         0         0         0           -           0
2021-05-16T21:17:11.564058+0000   221     208       208         0         0         0           -           0
2021-05-16T21:17:12.564127+0000   222     208       208         0         0         0           -           0
2021-05-16T21:17:13.564195+0000   223     208       208         0         0         0           -           0
2021-05-16T21:17:14.564262+0000   224     208       208         0         0         0           -           0
2021-05-16T21:17:15.564342+0000   225     209       209         0         0         0           -           0
2021-05-16T21:17:16.564421+0000   226     209       209         0         0         0           -           0
2021-05-16T21:17:17.564490+0000   227     213       213         0         0         0           -           0
2021-05-16T21:17:18.564560+0000   228     213       213         0         0         0           -           0
2021-05-16T21:17:19.564641+0000   229     216       216         0         0         0           -           0
2021-05-16T21:17:20.564710+0000   230     217       217         0         0         0           -           0
2021-05-16T21:17:21.564778+0000   231     217       217         0         0         0           -           0
2021-05-16T21:17:22.565061+0000   232     217       217         0         0         0           -           0
2021-05-16T21:17:23.565141+0000   233     217       217         0         0         0           -           0
2021-05-16T21:17:24.565211+0000   234     222       222         0         0         0           -           0
2021-05-16T21:17:25.565282+0000   235     222       222         0         0         0           -           0
2021-05-16T21:17:26.565374+0000   236     226       226         0         0         0           -           0
2021-05-16T21:17:27.565469+0000   237     226       226         0         0         0           -           0
2021-05-16T21:17:28.565782+0000   238     230       230         0         0         0           -           0
2021-05-16T21:17:29.565855+0000   239     230       230         0         0         0           -           0
2021-05-16T21:17:30.565923+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:17:30.565923+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:17:30.565923+0000   240     230       230         0         0         0           -           0
2021-05-16T21:17:31.566014+0000   241     230       230         0         0         0           -           0
2021-05-16T21:17:32.566081+0000   242     231       231         0         0         0           -           0
2021-05-16T21:17:33.566150+0000   243     231       231         0         0         0           -           0
2021-05-16T21:17:34.566220+0000   244     231       231         0         0         0           -           0
2021-05-16T21:17:35.566387+0000   245     231       231         0         0         0           -           0
2021-05-16T21:17:36.566457+0000   246     231       231         0         0         0           -           0
2021-05-16T21:17:37.566524+0000   247     236       236         0         0         0           -           0
2021-05-16T21:17:38.566594+0000   248     236       236         0         0         0           -           0
2021-05-16T21:17:39.566679+0000   249     240       240         0         0         0           -           0
2021-05-16T21:17:40.566749+0000   250     240       240         0         0         0           -           0
2021-05-16T21:17:41.566914+0000   251     242       242         0         0         0           -           0
2021-05-16T21:17:42.567079+0000   252     242       242         0         0         0           -           0
2021-05-16T21:17:43.567160+0000   253     242       242         0         0         0           -           0
2021-05-16T21:17:44.567423+0000   254     242       242         0         0         0           -           0
2021-05-16T21:17:45.567507+0000   255     246       246         0         0         0           -           0
2021-05-16T21:17:46.567606+0000   256     246       246         0         0         0           -           0
2021-05-16T21:17:47.567688+0000   257     255       255         0         0         0           -           0
2021-05-16T21:17:48.567756+0000   258     255       255         0         0         0           -           0
2021-05-16T21:17:49.567826+0000   259     255       255         0         0         0           -           0
2021-05-16T21:17:50.567896+0000 min lat: 257.736 max lat: 259.108 avg lat: 258.922
2021-05-16T21:17:50.567896+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:17:50.567896+0000   260     255       263         8  0.492254  0.492308     257.736     258.922
2021-05-16T21:17:51.567997+0000   261     255       263         8  0.490368         0           -     258.922
2021-05-16T21:17:52.568066+0000   262     255       266        11  0.671683        24     255.551     258.199
2021-05-16T21:17:53.568139+0000   263     255       266        11  0.669129         0           -     258.199
2021-05-16T21:17:54.568207+0000   264     255       270        15  0.908992        32     253.403      256.92
2021-05-16T21:17:55.568292+0000   265     255       270        15  0.905562         0           -      256.92
2021-05-16T21:17:56.568364+0000   266     255       272        17   1.02245        16      255.53     256.756
2021-05-16T21:17:57.568463+0000   267     255       272        17   1.01862         0           -     256.756
2021-05-16T21:17:58.568535+0000   268     255       272        17   1.01482         0           -     256.756
2021-05-16T21:17:59.568632+0000   269     255       272        17   1.01104         0           -     256.756
2021-05-16T21:18:00.568706+0000   270     255       274        19    1.1258         8     257.702     256.967
2021-05-16T21:18:01.568779+0000   271     255       274        19   1.12165         0           -     256.967
2021-05-16T21:18:02.569065+0000   272     255       275        20   1.17634         8     259.842     257.111
2021-05-16T21:18:03.569146+0000   273     255       275        20   1.17203         0           -     257.111
2021-05-16T21:18:04.569391+0000   274     255       275        20   1.16776         0           -     257.111
2021-05-16T21:18:05.569462+0000   275     255       277        22   1.27986   10.6667      259.85     257.456
2021-05-16T21:18:06.569772+0000   276     255       277        22   1.27522         0           -     257.456
2021-05-16T21:18:07.569857+0000   277     255       281        26   1.50164        32     257.733     257.827
2021-05-16T21:18:08.569971+0000   278     255       281        26   1.49624         0           -     257.827
2021-05-16T21:18:09.570063+0000   279     255       281        26   1.49088         0           -     257.827
2021-05-16T21:18:10.570136+0000 min lat: 253.4 max lat: 261.992 avg lat: 257.827
2021-05-16T21:18:10.570136+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:18:10.570136+0000   280     255       281        26   1.48555         0           -     257.827
2021-05-16T21:18:11.570228+0000   281     255       281        26   1.48027         0           -     257.827
2021-05-16T21:18:12.570296+0000   282     255       281        26   1.47502         0           -     257.827
2021-05-16T21:18:13.570378+0000   283     255       281        26    1.4698         0           -     257.827
2021-05-16T21:18:14.570448+0000   284     255       281        26   1.46463         0           -     257.827
2021-05-16T21:18:15.570533+0000   285     255       282        27   1.51562         2     266.287      258.14
2021-05-16T21:18:16.570603+0000   286     255       282        27   1.51033         0           -      258.14
2021-05-16T21:18:17.570671+0000   287     255       286        31   1.72804        32       266.3     259.399
2021-05-16T21:18:18.570742+0000   288     255       286        31   1.72204         0           -     259.399
2021-05-16T21:18:19.571065+0000   289     255       286        31   1.71608         0           -     259.399
2021-05-16T21:18:20.571136+0000   290     255       288        33   1.82049   10.6667     268.426     259.946
2021-05-16T21:18:21.571206+0000   291     255       288        33   1.81424         0           -     259.946
2021-05-16T21:18:22.571513+0000   292     255       289        34   1.86281         8     268.442     260.196
2021-05-16T21:18:23.571597+0000   293     255       289        34   1.85645         0           -     260.196
2021-05-16T21:18:24.571667+0000   294     255       293        38    2.0678        32     257.719     260.329
2021-05-16T21:18:25.571739+0000   295     255       293        38   2.06079         0           -     260.329
2021-05-16T21:18:26.571897+0000   296     255       295        40   2.16193        16     259.853     260.306
2021-05-16T21:18:27.571980+0000   297     255       295        40   2.15465         0           -     260.306
2021-05-16T21:18:28.572051+0000   298     255       296        41    2.2011         8     261.981     260.346
2021-05-16T21:18:29.572121+0000   299     255       296        41   2.19374         0           -     260.346
2021-05-16T21:18:30.572191+0000 min lat: 253.4 max lat: 268.442 avg lat: 260.346
2021-05-16T21:18:30.572191+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:18:30.572191+0000   300     255       296        41   2.18643         0           -     260.346
2021-05-16T21:18:31.572283+0000   301     255       296        41   2.17916         0           -     260.346
2021-05-16T21:18:32.572353+0000   302     255       297        42   2.22492         4     266.271     260.488
2021-05-16T21:18:33.572635+0000   303     255       298        43   2.27038        16     266.274     260.622
2021-05-16T21:18:34.572707+0000   304     255       298        43   2.26291         0           -     260.622
2021-05-16T21:18:35.573033+0000   305     255       299        44   2.30794         8     268.405     260.799
2021-05-16T21:18:36.573104+0000   306     255       299        44    2.3004         0           -     260.799
2021-05-16T21:18:37.573174+0000   307     255       300        45   2.34502         8     270.544     261.016
2021-05-16T21:18:38.573392+0000   308     255       300        45   2.33741         0           -     261.016
2021-05-16T21:18:39.573477+0000   309     255       303        48   2.48516        24     272.686     261.745
2021-05-16T21:18:40.573549+0000   310     255       303        48   2.47715         0           -     261.745
2021-05-16T21:18:41.573619+0000   311     255       309        54   2.77783        48     266.335     262.768
2021-05-16T21:18:42.573689+0000   312     255       309        54   2.76893         0           -     262.768
2021-05-16T21:18:43.573851+0000   313     255       310        55   2.81119         8     268.428     262.871
2021-05-16T21:18:44.573924+0000   314     255       310        55   2.80224         0           -     262.871
2021-05-16T21:18:45.574020+0000   315     255       310        55   2.79334         0           -     262.871
2021-05-16T21:18:46.574107+0000   316     255       310        55   2.78451         0           -     262.871
2021-05-16T21:18:47.574189+0000   317     255       310        55   2.77572         0           -     262.871
2021-05-16T21:18:48.574503+0000   318     255       311        56    2.8173       3.2     268.437      262.97
2021-05-16T21:18:49.574813+0000   319     255       311        56   2.80847         0           -      262.97
2021-05-16T21:18:50.575140+0000 min lat: 253.4 max lat: 274.813 avg lat: 263.104
2021-05-16T21:18:50.575140+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:18:50.575140+0000   320     255       312        57   2.84968         8     270.574     263.104
2021-05-16T21:18:51.575475+0000   321     255       312        57    2.8408         0           -     263.104
2021-05-16T21:18:52.575780+0000   322     255       313        58   2.88166         8     272.718      263.27
2021-05-16T21:18:53.575851+0000   323     255       313        58   2.87274         0           -      263.27
2021-05-16T21:18:54.575923+0000   324     255       314        59   2.91325         8     270.588     263.394
2021-05-16T21:18:55.576005+0000   325     255       314        59   2.90429         0           -     263.394
2021-05-16T21:18:56.576078+0000   326     255       316        61   2.99353        16     272.734       263.7
2021-05-16T21:18:57.576149+0000   327     255       316        61   2.98438         0           -       263.7
2021-05-16T21:18:58.576218+0000   328     255       320        65   3.17038        32     268.461      264.19
2021-05-16T21:18:59.576302+0000   329     255       320        65   3.16074         0           -      264.19
2021-05-16T21:19:00.576372+0000   330     255       322        67   3.24812        16     266.307     264.317
2021-05-16T21:19:01.576442+0000   331     255       322        67   3.23831         0           -     264.317
2021-05-16T21:19:02.576513+0000   332     255       322        67   3.22856         0           -     264.317
2021-05-16T21:19:03.576595+0000   333     255       324        69   3.31495   10.6667     268.445     264.437
2021-05-16T21:19:04.576682+0000   334     255       324        69   3.30502         0           -     264.437
2021-05-16T21:19:05.576753+0000   335     255       328        73   3.48618        32     266.318     264.627
2021-05-16T21:19:06.577063+0000   336     255       328        73    3.4758         0           -     264.627
2021-05-16T21:19:07.577382+0000   337     255       334        79   3.75032        48      264.18     264.755
2021-05-16T21:19:08.577577+0000   338     255       334        79   3.73922         0           -     264.755
2021-05-16T21:19:09.577835+0000   339     255       339        84   3.96415        40     259.871     264.667
2021-05-16T21:19:10.577905+0000 min lat: 253.4 max lat: 274.813 avg lat: 264.667
2021-05-16T21:19:10.577905+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:19:10.577905+0000   340     255       339        84    3.9525         0           -     264.667
2021-05-16T21:19:11.577996+0000   341     255       339        84    3.9409         0           -     264.667
2021-05-16T21:19:12.578064+0000   342     255       339        84   3.92938         0           -     264.667
2021-05-16T21:19:13.578137+0000   343     255       343        88   4.10449        16      259.86     264.521
2021-05-16T21:19:14.578210+0000   344     255       343        88   4.09256         0           -     264.521
2021-05-16T21:19:15.578530+0000   345     255       344        89   4.12707         8     261.993     264.493
2021-05-16T21:19:16.578830+0000   346     255       344        89   4.11514         0           -     264.493
2021-05-16T21:19:17.579103+0000   347     255       344        89   4.10328         0           -     264.493
2021-05-16T21:19:18.579410+0000   348     255       344        89   4.09149         0           -     264.493
2021-05-16T21:19:19.579575+0000   349     255       344        89   4.07976         0           -     264.493
2021-05-16T21:19:20.579887+0000   350     255       345        90   4.11381       3.2     266.278     264.513
2021-05-16T21:19:21.579973+0000   351     255       345        90   4.10209         0           -     264.513
2021-05-16T21:19:22.580046+0000   352     255       345        90   4.09044         0           -     264.513
2021-05-16T21:19:23.580129+0000   353     255       345        90   4.07885         0           -     264.513
2021-05-16T21:19:24.580198+0000   354     255       346        91   4.11252         4     270.564     264.579
2021-05-16T21:19:25.580269+0000   355     255       346        91   4.10094         0           -     264.579
2021-05-16T21:19:26.580339+0000   356     255       347        92   4.13436         8     270.585     264.644
2021-05-16T21:19:27.580433+0000   357     255       347        92   4.12278         0           -     264.644
2021-05-16T21:19:28.580504+0000   358     255       348        93   4.15595         8     272.726     264.731
2021-05-16T21:19:29.580575+0000   359     255       348        93   4.14437         0           -     264.731
2021-05-16T21:19:30.580644+0000 min lat: 253.4 max lat: 274.813 avg lat: 264.731
2021-05-16T21:19:30.580644+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:19:30.580644+0000   360     255       348        93   4.13286         0           -     264.731
2021-05-16T21:19:31.580735+0000   361     255       348        93   4.12141         0           -     264.731
2021-05-16T21:19:32.580804+0000   362     255       348        93   4.11003         0           -     264.731
2021-05-16T21:19:33.581112+0000   363     255       348        93    4.0987         0           -     264.731
2021-05-16T21:19:34.581182+0000   364     255       348        93   4.08744         0           -     264.731
2021-05-16T21:19:35.581422+0000   365     255       348        93   4.07624         0           -     264.731
2021-05-16T21:19:36.581731+0000   366     255       348        93   4.06511         0           -     264.731
2021-05-16T21:19:37.581913+0000   367     255       356       101   4.40276   14.2222     274.918     265.622
2021-05-16T21:19:38.581984+0000   368     255       356       101    4.3908         0           -     265.622
2021-05-16T21:19:39.582064+0000   369     255       361       106   4.59568        40     264.181     265.917
2021-05-16T21:19:40.582134+0000   370     255       361       106   4.58326         0           -     265.917
2021-05-16T21:19:41.582205+0000   371     255       364       109   4.70027        24     266.282     265.927
2021-05-16T21:19:42.582275+0000   372     255       364       109   4.68763         0           -     265.927
2021-05-16T21:19:43.582358+0000   373     255       366       111   4.76085        16     264.145     265.914
2021-05-16T21:19:44.582430+0000   374     255       366       111   4.74812         0           -     265.914
2021-05-16T21:19:45.582684+0000   375     255       366       111   4.73546         0           -     265.914
2021-05-16T21:19:46.582752+0000   376     255       369       114   4.85051        16     266.297     265.924
2021-05-16T21:19:47.583074+0000   377     255       369       114   4.83764         0           -     265.924
2021-05-16T21:19:48.583382+0000   378     255       377       122   5.16342        64     264.186     265.966
2021-05-16T21:19:49.583561+0000   379     255       377       122    5.1498         0           -     265.966
2021-05-16T21:19:50.583870+0000 min lat: 253.4 max lat: 279.174 avg lat: 265.978
2021-05-16T21:19:50.583870+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:19:50.583870+0000   380     255       382       127   5.34674        40     266.276     265.978
2021-05-16T21:19:51.583980+0000   381     255       382       127   5.33271         0           -     265.978
2021-05-16T21:19:52.584052+0000   382     255       386       131   5.48627        32      264.16     266.004
2021-05-16T21:19:53.584127+0000   383     255       386       131   5.47195         0           -     266.004
2021-05-16T21:19:54.584228+0000   384     255       392       137   5.70767        48     259.866     265.829
2021-05-16T21:19:55.584307+0000   385     255       392       137   5.69284         0           -     265.829
2021-05-16T21:19:56.584376+0000   386     255       392       137    5.6781         0           -     265.829
2021-05-16T21:19:57.584446+0000   387     255       392       137   5.66343         0           -     265.829
2021-05-16T21:19:58.584516+0000   388     255       392       137   5.64883         0           -     265.829
2021-05-16T21:19:59.584596+0000   389     255       392       137   5.63431         0           -     265.829
2021-05-16T21:20:00.584663+0000   390     255       392       137   5.61986         0           -     265.829
2021-05-16T21:20:01.584732+0000   391     255       395       140   5.72824   6.85714      264.15     265.808
2021-05-16T21:20:02.585043+0000   392     255       395       140   5.71362         0           -     265.808
2021-05-16T21:20:03.585154+0000   393     255       395       140   5.69908         0           -     265.808
2021-05-16T21:20:04.585422+0000   394     255       395       140   5.68462         0           -     265.808
2021-05-16T21:20:05.585493+0000   395     255       398       143   5.79173        12     266.279     265.833
2021-05-16T21:20:06.585692+0000   396     255       398       143    5.7771         0           -     265.833
2021-05-16T21:20:07.585775+0000   397     255       398       143   5.76255         0           -     265.833
2021-05-16T21:20:08.585843+0000   398     255       398       143   5.74807         0           -     265.833
2021-05-16T21:20:09.585912+0000   399     255       403       148   5.93415        20     257.731     265.733
2021-05-16T21:20:10.585985+0000 min lat: 253.4 max lat: 279.174 avg lat: 265.733
2021-05-16T21:20:10.585985+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:20:10.585985+0000   400     255       403       148   5.91931         0           -     265.733
2021-05-16T21:20:11.586076+0000   401     255       406       151   6.02424        24     257.712     265.601
2021-05-16T21:20:12.586149+0000   402     255       406       151   6.00925         0           -     265.601
2021-05-16T21:20:13.586224+0000   403     255       409       154   6.11344        24     251.268     265.364
2021-05-16T21:20:14.586300+0000   404     255       409       154    6.0983         0           -     265.364
2021-05-16T21:20:15.586380+0000   405     255       409       154   6.08325         0           -     265.364
2021-05-16T21:20:16.586450+0000   406     255       413       158   6.22588   21.3333     246.977     264.979
2021-05-16T21:20:17.586518+0000   407     255       413       158   6.21059         0           -     264.979
2021-05-16T21:20:18.586589+0000   408     255       413       158   6.19536         0           -     264.979
2021-05-16T21:20:19.586671+0000   409     255       413       158   6.18022         0           -     264.979
2021-05-16T21:20:20.586743+0000   410     255       414       159   6.20416         4     251.254     264.893
2021-05-16T21:20:21.587050+0000   411     255       414       159   6.18907         0           -     264.893
2021-05-16T21:20:22.587120+0000   412     255       416       161   6.25171        16     251.265     264.737
2021-05-16T21:20:23.587417+0000   413     255       416       161   6.23656         0           -     264.737
2021-05-16T21:20:24.587496+0000   414     255       419       164   6.33743        24     253.394      264.53
2021-05-16T21:20:25.587578+0000   415     255       419       164   6.32216         0           -      264.53
2021-05-16T21:20:26.828998+0000   416       7       420       413   15.8736      1992      11.165     180.201
2021-05-16T21:20:27.829074+0000   417       3       420       417   15.9889        64     2.59423     178.534
2021-05-16T21:20:28.829144+0000   418       2       420       418   15.9889        16     6.42969     178.122
2021-05-16T21:20:29.829214+0000   419       2       420       418   15.9508         0           -     178.122
2021-05-16T21:20:30.829286+0000 min lat: 2.59423 max lat: 279.174 avg lat: 177.712
2021-05-16T21:20:30.829286+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:20:30.829286+0000   420       1       420       419   15.9509         8     6.42901     177.712
2021-05-16T21:20:31.829607+0000   421       1       420       419    15.913         0           -     177.712
2021-05-16T21:20:32.829677+0000   422       1       420       419   15.8753         0           -     177.712
2021-05-16T21:20:33.829797+0000   423       1       420       419   15.8378         0           -     177.712
2021-05-16T21:20:34.829867+0000   424       1       420       419   15.8005         0           -     177.712
2021-05-16T21:20:35.829970+0000   425       1       420       419   15.7633         0           -     177.712
2021-05-16T21:20:36.830040+0000   426       1       420       419   15.7264         0           -     177.712
2021-05-16T21:20:37.830111+0000   427       1       420       419   15.6896         0           -     177.712
2021-05-16T21:20:38.830181+0000   428       1       420       419   15.6529         0           -     177.712
2021-05-16T21:20:39.830329+0000 Total time run:       428.73
Total reads made:     420
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   15.6742
Average IOPS:         0
Stddev IOPS:          6.009
Max IOPS:             124
Min IOPS:             0
Average Latency(s):   177.325
Max latency(s):       279.174
Min latency(s):       2.59423

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:20:40,554681641-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 295126


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:20:40,559982621-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:21:04,036500801-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 421 objects, 6.6 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:21:04,046291717-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:21:12,434986367-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 421 objects, 6.6 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:21:12,444914210-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:21:20,741676767-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 421 objects, 6.6 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:21:20,750891460-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:21:29,235326475-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 421 objects, 6.6 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:21:29,244986224-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:21:37,727812908-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 421 objects, 6.6 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:21:37,737522071-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:21:37,745098696-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T17:21:37,747999847-04:00][RUNNING][ROUND 5/7/40] object_size=16MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:21:37,751950700-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:21:37,761316987-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40876\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.194127\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 9deb3c80-5671-4b6d-ab6a-e274b331c534\nsetting min_mon_release = octopus\nepoch 0\nfsid 9deb3c80-5671-4b6d-ab6a-e274b331c534\nlast_changed 2021-05-16T14:22:05.738435-0700\ncreated 2021-05-16T14:22:05.738435-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40876/0,v1:10.10.1.2:40877/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.194127 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 08887d60-4ac7-49e4-ae7b-917a1cb9c0f2\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 723118c7-6d24-4425-bb4f-07e6bd7a78b8\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 1aea7b4c-b58c-4b28-8b14-3efa1f3d4830\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42876\n  w/ user/pass: admin / 5ef5582a-2313-43f0-8f99-5d8d9a2ac659\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 17:22:23 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40876
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.194127
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 9deb3c80-5671-4b6d-ab6a-e274b331c534
setting min_mon_release = octopus
epoch 0
fsid 9deb3c80-5671-4b6d-ab6a-e274b331c534
last_changed 2021-05-16T14:22:05.738435-0700
created 2021-05-16T14:22:05.738435-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40876/0,v1:10.10.1.2:40877/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.194127 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 08887d60-4ac7-49e4-ae7b-917a1cb9c0f2
0
start osd.0
add osd1 723118c7-6d24-4425-bb4f-07e6bd7a78b8
1
start osd.1
add osd2 1aea7b4c-b58c-4b28-8b14-3efa1f3d4830
2
start osd.2


restful urls: https://10.10.1.2:42876
  w/ user/pass: admin / 5ef5582a-2313-43f0-8f99-5d8d9a2ac659


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T14:21:39.604-0700 7f9e31bd01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T14:21:39.604-0700 7f9e31bd01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T14:21:39.620-0700 7fd34a8721c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T14:21:39.620-0700 7fd34a8721c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40876,v1:10.10.1.2:40877] --print /tmp/ceph_monmap.194127 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.194127 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.194127 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42876 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x56036c648000 @  0x7f218c0f7680 0x7f218c118824 0x7f218c8b3187 0x7f218c8bb355 0x7f218c8b3708 0x7f218c8b3877 0x7f218c8b4c24 0x7f218c8ccec1 0x7f218c83f5f3 0x7f218c8a0e97 0x7f218c8a8b1a 0x7f218bfabd84 0x7f218c0c7609 0x7f218bc9b293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.xVHTkK2dgp 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 08887d60-4ac7-49e4-ae7b-917a1cb9c0f2 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAIjaFgSYS+GxAAhqeNNZjiy+g3CBh3EciasQ== --osd-uuid 08887d60-4ac7-49e4-ae7b-917a1cb9c0f2 
2021-05-16T14:22:17.124-0700 7f6a2b6b6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T14:22:17.144-0700 7f6a2b6b6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T14:22:17.144-0700 7f6a2b6b6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x561690934000 @  0x7f6a2c07f680 0x7f6a2c0a0824 0x5616860ad447 0x5616860b54b5 0x5616860ad9c8 0x5616860adb37 0x5616860aeee4 0x561685e7fca1 0x561686057423 0x561685e702a7 0x561685e7553a 0x7f6a2bbd2d84 0x7f6a2bd57609 0x7f6a2b8c0293
2021-05-16T14:22:17.460-0700 7f6a2b6b6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 723118c7-6d24-4425-bb4f-07e6bd7a78b8 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x559f972c2000 @  0x7f15232c2680 0x7f15232e3824 0x559f8b68d447 0x559f8b6954b5 0x559f8b68d9c8 0x559f8b68db37 0x559f8b68eee4 0x559f8b45fca1 0x559f8b637423 0x559f8b4502a7 0x559f8b45553a 0x7f1522e15d84 0x7f1522f9a609 0x7f1522b03293
2021-05-16T14:22:18.116-0700 7f15228f9f00 -1 Falling back to public interface
2021-05-16T14:22:18.384-0700 7f15228f9f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAJjaFg5/58MBAAg37MBYYthw/+qg8UZ5cOCQ== --osd-uuid 723118c7-6d24-4425-bb4f-07e6bd7a78b8 
2021-05-16T14:22:18.496-0700 7f6b55bbff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T14:22:18.516-0700 7f6b55bbff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T14:22:18.516-0700 7f6b55bbff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x561612ffe000 @  0x7f6b56588680 0x7f6b565a9824 0x5616077fc447 0x5616078044b5 0x5616077fc9c8 0x5616077fcb37 0x5616077fdee4 0x5616075ceca1 0x5616077a6423 0x5616075bf2a7 0x5616075c453a 0x7f6b560dbd84 0x7f6b56260609 0x7f6b55dc9293
2021-05-16T14:22:18.832-0700 7f6b55bbff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 1aea7b4c-b58c-4b28-8b14-3efa1f3d4830 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x557e319ee000 @  0x7f167c242680 0x7f167c263824 0x557e27742447 0x557e2774a4b5 0x557e277429c8 0x557e27742b37 0x557e27743ee4 0x557e27514ca1 0x557e276ec423 0x557e275052a7 0x557e2750a53a 0x7f167bd95d84 0x7f167bf1a609 0x7f167ba83293
2021-05-16T14:22:19.464-0700 7f167b879f00 -1 Falling back to public interface
2021-05-16T14:22:19.724-0700 7f167b879f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQALjaFgtIiACRAAEOErNRs5R8jUcC4OKA5VYA== --osd-uuid 1aea7b4c-b58c-4b28-8b14-3efa1f3d4830 
2021-05-16T14:22:19.868-0700 7f05c3af4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T14:22:19.888-0700 7f05c3af4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T14:22:19.888-0700 7f05c3af4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5559f19c4000 @  0x7f05c44bd680 0x7f05c44de824 0x5559e783c447 0x5559e78444b5 0x5559e783c9c8 0x5559e783cb37 0x5559e783dee4 0x5559e760eca1 0x5559e77e6423 0x5559e75ff2a7 0x5559e760453a 0x7f05c4010d84 0x7f05c4195609 0x7f05c3cfe293
2021-05-16T14:22:20.208-0700 7f05c3af4f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55d05ac08000 @  0x7fab8cf20680 0x7fab8cf41824 0x55d04fdaa447 0x55d04fdb24b5 0x55d04fdaa9c8 0x55d04fdaab37 0x55d04fdabee4 0x55d04fb7cca1 0x55d04fd54423 0x55d04fb6d2a7 0x55d04fb7253a 0x7fab8ca73d84 0x7fab8cbf8609 0x7fab8c761293
2021-05-16T14:22:20.892-0700 7fab8c557f00 -1 Falling back to public interface
2021-05-16T14:22:21.164-0700 7fab8c557f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:23,366681182-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:23,377372921-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:23,418037022-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:23,421220944-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bbdc4ca2-fe3e-41df-84d7-ced059fd0804', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bbdc4ca2-fe3e-41df-84d7-ced059fd0804 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:26,955928582-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:26,959579722-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bbdc4ca2-fe3e-41df-84d7-ced059fd0804', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bbdc4ca2-fe3e-41df-84d7-ced059fd0804 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:30,391095169-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:30,395129699-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bbdc4ca2-fe3e-41df-84d7-ced059fd0804', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bbdc4ca2-fe3e-41df-84d7-ced059fd0804 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:33,764791533-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:33,768466167-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bbdc4ca2-fe3e-41df-84d7-ced059fd0804', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bbdc4ca2-fe3e-41df-84d7-ced059fd0804 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:40,887184285-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:40,890333581-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bbdc4ca2-fe3e-41df-84d7-ced059fd0804', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bbdc4ca2-fe3e-41df-84d7-ced059fd0804 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:45,058548348-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:45,062040239-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bbdc4ca2-fe3e-41df-84d7-ced059fd0804', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bbdc4ca2-fe3e-41df-84d7-ced059fd0804 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:48,807806449-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:48,810809330-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bbdc4ca2-fe3e-41df-84d7-ced059fd0804', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bbdc4ca2-fe3e-41df-84d7-ced059fd0804 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:52,223983553-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:52,227739490-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bbdc4ca2-fe3e-41df-84d7-ced059fd0804', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bbdc4ca2-fe3e-41df-84d7-ced059fd0804 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:56,805836937-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:22:56,809373983-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bbdc4ca2-fe3e-41df-84d7-ced059fd0804', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bbdc4ca2-fe3e-41df-84d7-ced059fd0804 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:23:00,511675362-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:23:00,515575350-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bbdc4ca2-fe3e-41df-84d7-ced059fd0804', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bbdc4ca2-fe3e-41df-84d7-ced059fd0804 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:23:04,918299951-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:23:04,922369868-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bbdc4ca2-fe3e-41df-84d7-ced059fd0804', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bbdc4ca2-fe3e-41df-84d7-ced059fd0804 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:23:08,356395257-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:23:08,360044062-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bbdc4ca2-fe3e-41df-84d7-ced059fd0804', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bbdc4ca2-fe3e-41df-84d7-ced059fd0804 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:23:11,864349702-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:23:35,252726661-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:23:43,810292032-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:23:52,346029345-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:00,790896724-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:09,317287110-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:17,806139240-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     1.042% pgs not active
             190 active+clean
             2   peering
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:17,815916720-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:26,253963510-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:26,263925677-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:34,844194873-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:34,853837650-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:43,313052974-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:43,323192434-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:51,770780105-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:51,780276797-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:51,788428803-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:51,793597243-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:51,803939203-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=301657
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:51,811670639-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:51,831405025-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:24:51,834600098-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bbdc4ca2-fe3e-41df-84d7-ced059fd0804', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bbdc4ca2-fe3e-41df-84d7-ced059fd0804 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T21:24:54.067+0000 7f41eadcdd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T21:24:54.323+0000 7f41eadcdd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T21:24:54.323+0000 7f41eadcdd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T21:24:54.337438+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-16T21:24:54.337469+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T21:24:55.347984+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:24:55.347984+0000     0       0         0         0         0         0           -           0
2021-05-16T21:24:56.348102+0000     1       5         5         0         0         0           -           0
2021-05-16T21:24:57.348184+0000     2      12        12         0         0         0           -           0
2021-05-16T21:24:58.348251+0000     3      12        12         0         0         0           -           0
2021-05-16T21:24:59.348330+0000     4      18        18         0         0         0           -           0
2021-05-16T21:25:00.348395+0000     5      48        48         0         0         0           -           0
2021-05-16T21:25:01.348462+0000     6      78        78         0         0         0           -           0
2021-05-16T21:25:02.348530+0000     7      89        89         0         0         0           -           0
2021-05-16T21:25:03.348609+0000     8      89        89         0         0         0           -           0
2021-05-16T21:25:04.348676+0000     9      94        94         0         0         0           -           0
2021-05-16T21:25:05.348745+0000    10      94        94         0         0         0           -           0
2021-05-16T21:25:06.348810+0000    11     117       117         0         0         0           -           0
2021-05-16T21:25:07.348890+0000    12     135       135         0         0         0           -           0
2021-05-16T21:25:08.349046+0000    13     135       135         0         0         0           -           0
2021-05-16T21:25:09.349143+0000    14     144       144         0         0         0           -           0
2021-05-16T21:25:10.349213+0000    15     144       144         0         0         0           -           0
2021-05-16T21:25:11.349291+0000    16     155       155         0         0         0           -           0
2021-05-16T21:25:12.349357+0000    17     184       184         0         0         0           -           0
2021-05-16T21:25:13.349544+0000    18     187       187         0         0         0           -           0
2021-05-16T21:25:14.349612+0000    19     192       192         0         0         0           -           0
2021-05-16T21:25:15.349690+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:25:15.349690+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:25:15.349690+0000    20     221       221         0         0         0           -           0
2021-05-16T21:25:16.349769+0000    21     250       250         0         0         0           -           0
2021-05-16T21:25:17.349972+0000 Total time run:         21.2606
Total writes made:      256
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     192.657
Stddev Bandwidth:       0
Max bandwidth (MB/sec): 0
Min bandwidth (MB/sec): 0
Average IOPS:           12
Stddev IOPS:            0
Max IOPS:               0
Min IOPS:               0
Average Latency(s):     9.16855
Stddev Latency(s):      6.4626
Max latency(s):         21.2018
Min latency(s):         0.0725192

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:25:18,043013468-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 301657


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:25:18,048016809-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:25:41,456795486-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:25:41,466928694-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:25:49,853971144-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:25:49,864089966-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:25:58,238113748-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:25:58,247576156-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:26:06,785604358-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:26:06,795922955-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:26:15,225845622-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:26:15,236034345-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:26:15,243802710-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:26:15,248324946-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:26:15,258580284-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=303061
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:26:15,266308123-04:00] INFO: > Run rados bench[0m
# ./bench-rados:196 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:199 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:26:15,287113762-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:26:15,290140518-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bbdc4ca2-fe3e-41df-84d7-ced059fd0804', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bbdc4ca2-fe3e-41df-84d7-ced059fd0804 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.SdVIyb:/tmp/ceph-asok.SdVIyb -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T21:26:17.595+0000 7f7b6503ad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T21:26:17.851+0000 7f7b6503ad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T21:26:17.851+0000 7f7b6503ad00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T21:26:17.905254+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:26:17.905254+0000     0       0         0         0         0         0           -           0
2021-05-16T21:26:18.905402+0000     1       9         9         0         0         0           -           0
2021-05-16T21:26:19.905479+0000     2      12        12         0         0         0           -           0
2021-05-16T21:26:20.905545+0000     3      12        12         0         0         0           -           0
2021-05-16T21:26:21.905798+0000     4      12        12         0         0         0           -           0
2021-05-16T21:26:22.905907+0000     5      12        12         0         0         0           -           0
2021-05-16T21:26:23.906060+0000     6      12        12         0         0         0           -           0
2021-05-16T21:26:24.906141+0000     7      12        12         0         0         0           -           0
2021-05-16T21:26:25.906244+0000     8      12        12         0         0         0           -           0
2021-05-16T21:26:26.906496+0000     9      13        13         0         0         0           -           0
2021-05-16T21:26:27.906561+0000    10      13        13         0         0         0           -           0
2021-05-16T21:26:28.906629+0000    11      13        13         0         0         0           -           0
2021-05-16T21:26:29.906696+0000    12      13        13         0         0         0           -           0
2021-05-16T21:26:30.906802+0000    13      21        21         0         0         0           -           0
2021-05-16T21:26:31.907069+0000    14      21        21         0         0         0           -           0
2021-05-16T21:26:32.907136+0000    15      23        23         0         0         0           -           0
2021-05-16T21:26:33.907391+0000    16      23        23         0         0         0           -           0
2021-05-16T21:26:34.907512+0000    17      24        24         0         0         0           -           0
2021-05-16T21:26:35.907814+0000    18      24        24         0         0         0           -           0
2021-05-16T21:26:36.907899+0000    19      28        28         0         0         0           -           0
2021-05-16T21:26:37.907975+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:26:37.907975+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:26:37.907975+0000    20      28        28         0         0         0           -           0
2021-05-16T21:26:38.908092+0000    21      28        28         0         0         0           -           0
2021-05-16T21:26:39.908158+0000    22      30        30         0         0         0           -           0
2021-05-16T21:26:40.908227+0000    23      30        30         0         0         0           -           0
2021-05-16T21:26:41.908293+0000    24      31        31         0         0         0           -           0
2021-05-16T21:26:42.908400+0000    25      31        31         0         0         0           -           0
2021-05-16T21:26:43.908471+0000    26      31        31         0         0         0           -           0
2021-05-16T21:26:44.908557+0000    27      31        31         0         0         0           -           0
2021-05-16T21:26:45.908623+0000    28      33        33         0         0         0           -           0
2021-05-16T21:26:46.908729+0000    29      33        33         0         0         0           -           0
2021-05-16T21:26:47.909034+0000    30      33        33         0         0         0           -           0
2021-05-16T21:26:48.909100+0000    31      33        33         0         0         0           -           0
2021-05-16T21:26:49.909167+0000    32      35        35         0         0         0           -           0
2021-05-16T21:26:50.909272+0000    33      35        35         0         0         0           -           0
2021-05-16T21:26:51.909340+0000    34      40        40         0         0         0           -           0
2021-05-16T21:26:52.909528+0000    35      40        40         0         0         0           -           0
2021-05-16T21:26:53.909802+0000    36      40        40         0         0         0           -           0
2021-05-16T21:26:54.909909+0000    37      42        42         0         0         0           -           0
2021-05-16T21:26:55.909974+0000    38      42        42         0         0         0           -           0
2021-05-16T21:26:56.910040+0000    39      42        42         0         0         0           -           0
2021-05-16T21:26:57.910107+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:26:57.910107+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:26:57.910107+0000    40      42        42         0         0         0           -           0
2021-05-16T21:26:58.910225+0000    41      44        44         0         0         0           -           0
2021-05-16T21:26:59.910294+0000    42      44        44         0         0         0           -           0
2021-05-16T21:27:00.910362+0000    43      58        58         0         0         0           -           0
2021-05-16T21:27:01.910428+0000    44      58        58         0         0         0           -           0
2021-05-16T21:27:02.910538+0000    45      59        59         0         0         0           -           0
2021-05-16T21:27:03.910611+0000    46      59        59         0         0         0           -           0
2021-05-16T21:27:04.910694+0000    47      62        62         0         0         0           -           0
2021-05-16T21:27:05.910760+0000    48      62        62         0         0         0           -           0
2021-05-16T21:27:06.910866+0000    49      62        62         0         0         0           -           0
2021-05-16T21:27:07.911090+0000    50      62        62         0         0         0           -           0
2021-05-16T21:27:08.911173+0000    51      62        62         0         0         0           -           0
2021-05-16T21:27:09.911436+0000    52      62        62         0         0         0           -           0
2021-05-16T21:27:10.911543+0000    53      62        62         0         0         0           -           0
2021-05-16T21:27:11.911848+0000    54      64        64         0         0         0           -           0
2021-05-16T21:27:12.911915+0000    55      64        64         0         0         0           -           0
2021-05-16T21:27:13.911981+0000    56      64        64         0         0         0           -           0
2021-05-16T21:27:14.912088+0000    57      64        64         0         0         0           -           0
2021-05-16T21:27:15.912153+0000    58      71        71         0         0         0           -           0
2021-05-16T21:27:16.912219+0000    59      71        71         0         0         0           -           0
2021-05-16T21:27:17.912284+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:27:17.912284+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:27:17.912284+0000    60      80        80         0         0         0           -           0
2021-05-16T21:27:18.912405+0000    61      80        80         0         0         0           -           0
2021-05-16T21:27:19.912471+0000    62      80        80         0         0         0           -           0
2021-05-16T21:27:20.912539+0000    63      80        80         0         0         0           -           0
2021-05-16T21:27:21.912606+0000    64      80        80         0         0         0           -           0
2021-05-16T21:27:22.912712+0000    65      83        83         0         0         0           -           0
2021-05-16T21:27:23.912777+0000    66      83        83         0         0         0           -           0
2021-05-16T21:27:24.912863+0000    67      84        84         0         0         0           -           0
2021-05-16T21:27:25.913110+0000    68      84        84         0         0         0           -           0
2021-05-16T21:27:26.913383+0000    69      84        84         0         0         0           -           0
2021-05-16T21:27:27.913450+0000    70      84        84         0         0         0           -           0
2021-05-16T21:27:28.913517+0000    71      84        84         0         0         0           -           0
2021-05-16T21:27:29.913841+0000    72      84        84         0         0         0           -           0
2021-05-16T21:27:30.913950+0000    73      85        85         0         0         0           -           0
2021-05-16T21:27:31.914018+0000    74      85        85         0         0         0           -           0
2021-05-16T21:27:32.914086+0000    75      90        90         0         0         0           -           0
2021-05-16T21:27:33.914153+0000    76      90        90         0         0         0           -           0
2021-05-16T21:27:34.914260+0000    77      91        91         0         0         0           -           0
2021-05-16T21:27:35.914333+0000    78      91        91         0         0         0           -           0
2021-05-16T21:27:36.914400+0000    79      91        91         0         0         0           -           0
2021-05-16T21:27:37.914467+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:27:37.914467+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:27:37.914467+0000    80      91        91         0         0         0           -           0
2021-05-16T21:27:38.914584+0000    81      91        91         0         0         0           -           0
2021-05-16T21:27:39.914649+0000    82      91        91         0         0         0           -           0
2021-05-16T21:27:40.914715+0000    83      91        91         0         0         0           -           0
2021-05-16T21:27:41.914790+0000    84      94        94         0         0         0           -           0
2021-05-16T21:27:42.915077+0000    85      94        94         0         0         0           -           0
2021-05-16T21:27:43.915385+0000    86      94        94         0         0         0           -           0
2021-05-16T21:27:44.915621+0000    87      94        94         0         0         0           -           0
2021-05-16T21:27:45.915869+0000    88      94        94         0         0         0           -           0
2021-05-16T21:27:46.915975+0000    89      94        94         0         0         0           -           0
2021-05-16T21:27:47.916042+0000    90      95        95         0         0         0           -           0
2021-05-16T21:27:48.916110+0000    91      95        95         0         0         0           -           0
2021-05-16T21:27:49.916178+0000    92      99        99         0         0         0           -           0
2021-05-16T21:27:50.916290+0000    93      99        99         0         0         0           -           0
2021-05-16T21:27:51.916354+0000    94      99        99         0         0         0           -           0
2021-05-16T21:27:52.916449+0000    95     100       100         0         0         0           -           0
2021-05-16T21:27:53.916515+0000    96     100       100         0         0         0           -           0
2021-05-16T21:27:54.916627+0000    97     102       102         0         0         0           -           0
2021-05-16T21:27:55.916707+0000    98     102       102         0         0         0           -           0
2021-05-16T21:27:56.916789+0000    99     102       102         0         0         0           -           0
2021-05-16T21:27:57.917092+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:27:57.917092+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:27:57.917092+0000   100     102       102         0         0         0           -           0
2021-05-16T21:27:58.917214+0000   101     104       104         0         0         0           -           0
2021-05-16T21:27:59.917283+0000   102     104       104         0         0         0           -           0
2021-05-16T21:28:00.917375+0000   103     105       105         0         0         0           -           0
2021-05-16T21:28:01.917441+0000   104     105       105         0         0         0           -           0
2021-05-16T21:28:02.917568+0000   105     106       106         0         0         0           -           0
2021-05-16T21:28:03.917841+0000   106     106       106         0         0         0           -           0
2021-05-16T21:28:04.917947+0000   107     106       106         0         0         0           -           0
2021-05-16T21:28:05.918025+0000   108     106       106         0         0         0           -           0
2021-05-16T21:28:06.918130+0000   109     106       106         0         0         0           -           0
2021-05-16T21:28:07.918202+0000   110     108       108         0         0         0           -           0
2021-05-16T21:28:08.918271+0000   111     108       108         0         0         0           -           0
2021-05-16T21:28:09.918350+0000   112     110       110         0         0         0           -           0
2021-05-16T21:28:10.918457+0000   113     110       110         0         0         0           -           0
2021-05-16T21:28:11.918543+0000   114     110       110         0         0         0           -           0
2021-05-16T21:28:12.918630+0000   115     110       110         0         0         0           -           0
2021-05-16T21:28:13.918727+0000   116     112       112         0         0         0           -           0
2021-05-16T21:28:14.918840+0000   117     112       112         0         0         0           -           0
2021-05-16T21:28:15.919059+0000   118     115       115         0         0         0           -           0
2021-05-16T21:28:16.919367+0000   119     115       115         0         0         0           -           0
2021-05-16T21:28:17.919434+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:28:17.919434+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:28:17.919434+0000   120     115       115         0         0         0           -           0
2021-05-16T21:28:18.919551+0000   121     115       115         0         0         0           -           0
2021-05-16T21:28:19.919766+0000   122     116       116         0         0         0           -           0
2021-05-16T21:28:20.919940+0000   123     116       116         0         0         0           -           0
2021-05-16T21:28:21.920007+0000   124     116       116         0         0         0           -           0
2021-05-16T21:28:22.920112+0000   125     117       117         0         0         0           -           0
2021-05-16T21:28:23.920179+0000   126     117       117         0         0         0           -           0
2021-05-16T21:28:24.920264+0000   127     119       119         0         0         0           -           0
2021-05-16T21:28:25.920331+0000   128     119       119         0         0         0           -           0
2021-05-16T21:28:26.920461+0000   129     123       123         0         0         0           -           0
2021-05-16T21:28:27.920529+0000   130     123       123         0         0         0           -           0
2021-05-16T21:28:28.920595+0000   131     124       124         0         0         0           -           0
2021-05-16T21:28:29.920661+0000   132     124       124         0         0         0           -           0
2021-05-16T21:28:30.920767+0000   133     127       127         0         0         0           -           0
2021-05-16T21:28:31.921074+0000   134     127       127         0         0         0           -           0
2021-05-16T21:28:32.921380+0000   135     129       129         0         0         0           -           0
2021-05-16T21:28:33.921509+0000   136     129       129         0         0         0           -           0
2021-05-16T21:28:34.921617+0000   137     129       129         0         0         0           -           0
2021-05-16T21:28:35.921797+0000   138     129       129         0         0         0           -           0
2021-05-16T21:28:36.921863+0000   139     129       129         0         0         0           -           0
2021-05-16T21:28:37.921930+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:28:37.921930+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:28:37.921930+0000   140     130       130         0         0         0           -           0
2021-05-16T21:28:38.922038+0000   141     130       130         0         0         0           -           0
2021-05-16T21:28:39.922096+0000   142     132       132         0         0         0           -           0
2021-05-16T21:28:40.922164+0000   143     132       132         0         0         0           -           0
2021-05-16T21:28:41.922231+0000   144     135       135         0         0         0           -           0
2021-05-16T21:28:42.922336+0000   145     135       135         0         0         0           -           0
2021-05-16T21:28:43.922402+0000   146     135       135         0         0         0           -           0
2021-05-16T21:28:44.922532+0000   147     135       135         0         0         0           -           0
2021-05-16T21:28:45.922801+0000   148     138       138         0         0         0           -           0
2021-05-16T21:28:46.923046+0000   149     138       138         0         0         0           -           0
2021-05-16T21:28:47.923112+0000   150     140       140         0         0         0           -           0
2021-05-16T21:28:48.923182+0000   151     140       140         0         0         0           -           0
2021-05-16T21:28:49.923402+0000   152     140       140         0         0         0           -           0
2021-05-16T21:28:50.923517+0000   153     145       145         0         0         0           -           0
2021-05-16T21:28:51.923826+0000   154     145       145         0         0         0           -           0
2021-05-16T21:28:52.923951+0000   155     149       149         0         0         0           -           0
2021-05-16T21:28:53.924018+0000   156     149       149         0         0         0           -           0
2021-05-16T21:28:54.924125+0000   157     162       162         0         0         0           -           0
2021-05-16T21:28:55.924190+0000   158     162       162         0         0         0           -           0
2021-05-16T21:28:56.924256+0000   159     166       166         0         0         0           -           0
2021-05-16T21:28:57.924324+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:28:57.924324+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:28:57.924324+0000   160     166       166         0         0         0           -           0
2021-05-16T21:28:58.924463+0000   161     169       169         0         0         0           -           0
2021-05-16T21:28:59.924532+0000   162     169       169         0         0         0           -           0
2021-05-16T21:29:00.924602+0000   163     172       172         0         0         0           -           0
2021-05-16T21:29:01.924670+0000   164     172       172         0         0         0           -           0
2021-05-16T21:29:02.924775+0000   165     172       172         0         0         0           -           0
2021-05-16T21:29:03.925048+0000   166     172       172         0         0         0           -           0
2021-05-16T21:29:04.925137+0000   167     172       172         0         0         0           -           0
2021-05-16T21:29:05.925424+0000   168     174       174         0         0         0           -           0
2021-05-16T21:29:06.925765+0000   169     174       174         0         0         0           -           0
2021-05-16T21:29:07.925960+0000   170     176       176         0         0         0           -           0
2021-05-16T21:29:08.926026+0000   171     176       176         0         0         0           -           0
2021-05-16T21:29:09.926094+0000   172     181       181         0         0         0           -           0
2021-05-16T21:29:10.926200+0000   173     181       181         0         0         0           -           0
2021-05-16T21:29:11.926268+0000   174     182       182         0         0         0           -           0
2021-05-16T21:29:12.926333+0000   175     182       182         0         0         0           -           0
2021-05-16T21:29:13.926400+0000   176     183       183         0         0         0           -           0
2021-05-16T21:29:14.926508+0000   177     183       183         0         0         0           -           0
2021-05-16T21:29:15.926575+0000   178     185       185         0         0         0           -           0
2021-05-16T21:29:16.926641+0000   179     185       185         0         0         0           -           0
2021-05-16T21:29:17.926708+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:29:17.926708+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:29:17.926708+0000   180     190       190         0         0         0           -           0
2021-05-16T21:29:18.927056+0000   181     190       190         0         0         0           -           0
2021-05-16T21:29:19.927383+0000   182     190       190         0         0         0           -           0
2021-05-16T21:29:20.927551+0000   183     190       190         0         0         0           -           0
2021-05-16T21:29:21.927796+0000   184     190       190         0         0         0           -           0
2021-05-16T21:29:22.927986+0000   185     191       191         0         0         0           -           0
2021-05-16T21:29:23.928054+0000   186     191       191         0         0         0           -           0
2021-05-16T21:29:24.928144+0000   187     191       191         0         0         0           -           0
2021-05-16T21:29:25.928222+0000   188     191       191         0         0         0           -           0
2021-05-16T21:29:26.928328+0000   189     191       191         0         0         0           -           0
2021-05-16T21:29:27.928396+0000   190     191       191         0         0         0           -           0
2021-05-16T21:29:28.928463+0000   191     192       192         0         0         0           -           0
2021-05-16T21:29:29.928529+0000   192     192       192         0         0         0           -           0
2021-05-16T21:29:30.928636+0000   193     192       192         0         0         0           -           0
2021-05-16T21:29:31.928701+0000   194     192       192         0         0         0           -           0
2021-05-16T21:29:32.928768+0000   195     195       195         0         0         0           -           0
2021-05-16T21:29:33.929045+0000   196     197       197         0         0         0           -           0
2021-05-16T21:29:34.929152+0000   197     197       197         0         0         0           -           0
2021-05-16T21:29:35.929228+0000   198     198       198         0         0         0           -           0
2021-05-16T21:29:36.929502+0000   199     198       198         0         0         0           -           0
2021-05-16T21:29:37.929845+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:29:37.929845+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:29:37.929845+0000   200     201       201         0         0         0           -           0
2021-05-16T21:29:38.929961+0000   201     201       201         0         0         0           -           0
2021-05-16T21:29:39.930027+0000   202     202       202         0         0         0           -           0
2021-05-16T21:29:40.930094+0000   203     202       202         0         0         0           -           0
2021-05-16T21:29:41.930162+0000   204     208       208         0         0         0           -           0
2021-05-16T21:29:42.930269+0000   205     208       208         0         0         0           -           0
2021-05-16T21:29:43.930337+0000   206     209       209         0         0         0           -           0
2021-05-16T21:29:44.930431+0000   207     209       209         0         0         0           -           0
2021-05-16T21:29:45.930721+0000   208     211       211         0         0         0           -           0
2021-05-16T21:29:46.931063+0000   209     211       211         0         0         0           -           0
2021-05-16T21:29:47.931129+0000   210     211       211         0         0         0           -           0
2021-05-16T21:29:48.931198+0000   211     212       212         0         0         0           -           0
2021-05-16T21:29:49.931501+0000   212     212       212         0         0         0           -           0
2021-05-16T21:29:50.931846+0000   213     212       212         0         0         0           -           0
2021-05-16T21:29:51.931913+0000   214     212       212         0         0         0           -           0
2021-05-16T21:29:52.932005+0000   215     215       215         0         0         0           -           0
2021-05-16T21:29:53.932075+0000   216     215       215         0         0         0           -           0
2021-05-16T21:29:54.932192+0000   217     215       215         0         0         0           -           0
2021-05-16T21:29:55.932259+0000   218     215       215         0         0         0           -           0
2021-05-16T21:29:56.932326+0000   219     215       215         0         0         0           -           0
2021-05-16T21:29:57.932392+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:29:57.932392+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:29:57.932392+0000   220     215       215         0         0         0           -           0
2021-05-16T21:29:58.932509+0000   221     216       216         0         0         0           -           0
2021-05-16T21:29:59.932578+0000   222     216       216         0         0         0           -           0
2021-05-16T21:30:00.932645+0000   223     216       216         0         0         0           -           0
2021-05-16T21:30:01.932711+0000   224     216       216         0         0         0           -           0
2021-05-16T21:30:02.933055+0000   225     216       216         0         0         0           -           0
2021-05-16T21:30:03.933163+0000   226     219       219         0         0         0           -           0
2021-05-16T21:30:04.933379+0000   227     219       219         0         0         0           -           0
2021-05-16T21:30:05.933554+0000   228     222       222         0         0         0           -           0
2021-05-16T21:30:06.933821+0000   229     222       222         0         0         0           -           0
2021-05-16T21:30:07.933967+0000   230     234       234         0         0         0           -           0
2021-05-16T21:30:08.934036+0000   231     234       234         0         0         0           -           0
2021-05-16T21:30:09.934102+0000   232     237       237         0         0         0           -           0
2021-05-16T21:30:10.934208+0000   233     237       237         0         0         0           -           0
2021-05-16T21:30:11.934277+0000   234     238       238         0         0         0           -           0
2021-05-16T21:30:12.934347+0000   235     238       238         0         0         0           -           0
2021-05-16T21:30:13.934415+0000   236     239       239         0         0         0           -           0
2021-05-16T21:30:14.934521+0000   237     239       239         0         0         0           -           0
2021-05-16T21:30:15.934587+0000   238     242       242         0         0         0           -           0
2021-05-16T21:30:16.934653+0000   239     242       242         0         0         0           -           0
2021-05-16T21:30:17.934722+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:30:17.934722+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:30:17.934722+0000   240     242       242         0         0         0           -           0
2021-05-16T21:30:18.935080+0000   241     242       242         0         0         0           -           0
2021-05-16T21:30:19.935383+0000   242     242       242         0         0         0           -           0
2021-05-16T21:30:20.935528+0000   243     248       248         0         0         0           -           0
2021-05-16T21:30:21.935757+0000   244     248       248         0         0         0           -           0
2021-05-16T21:30:22.935973+0000   245     249       249         0         0         0           -           0
2021-05-16T21:30:23.936047+0000   246     249       249         0         0         0           -           0
2021-05-16T21:30:24.936148+0000   247     250       250         0         0         0           -           0
2021-05-16T21:30:25.936230+0000   248     250       250         0         0         0           -           0
2021-05-16T21:30:26.936337+0000   249     250       250         0         0         0           -           0
2021-05-16T21:30:27.936433+0000   250     250       250         0         0         0           -           0
2021-05-16T21:30:28.936502+0000   251     251       251         0         0         0           -           0
2021-05-16T21:30:29.936571+0000   252     251       251         0         0         0           -           0
2021-05-16T21:30:30.936679+0000   253     251       251         0         0         0           -           0
2021-05-16T21:30:31.936746+0000   254     251       251         0         0         0           -           0
2021-05-16T21:30:32.937050+0000   255     251       251         0         0         0           -           0
2021-05-16T21:30:33.937354+0000   256     252       252         0         0         0           -           0
2021-05-16T21:30:34.937505+0000   257     252       252         0         0         0           -           0
2021-05-16T21:30:35.937580+0000   258     252       252         0         0         0           -           0
2021-05-16T21:30:36.937860+0000   259     252       252         0         0         0           -           0
2021-05-16T21:30:37.937925+0000 min lat: 8.95843 max lat: 259.455 avg lat: 138.442
2021-05-16T21:30:37.937925+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:30:37.937925+0000   260       6       256       250   15.3827   15.3846     8.95843     138.442
2021-05-16T21:30:38.938044+0000   261       6       256       250   15.3237         0           -     138.442
2021-05-16T21:30:39.938114+0000   262       6       256       250   15.2653         0           -     138.442
2021-05-16T21:30:40.938181+0000   263       6       256       250   15.2072         0           -     138.442
2021-05-16T21:30:41.938285+0000   264       4       256       252   15.2708         8     21.4554     137.445
2021-05-16T21:30:42.938393+0000   265       4       256       252   15.2132         0           -     137.445
2021-05-16T21:30:43.938462+0000   266       2       256       254   15.2763        16     21.4764      136.49
2021-05-16T21:30:44.938555+0000   267       2       256       254   15.2191         0           -      136.49
2021-05-16T21:30:45.938634+0000   268       1       256       255    15.222         8     8.59047     135.988
2021-05-16T21:30:46.938742+0000   269       1       256       255   15.1654         0           -     135.988
2021-05-16T21:30:47.939050+0000   270       1       256       255   15.1092         0           -     135.988
2021-05-16T21:30:48.939202+0000 Total time run:       270.176
Total reads made:     256
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   15.1605
Average IOPS:         0
Stddev IOPS:          0.0608581
Max IOPS:             1
Min IOPS:             0
Average Latency(s):   135.499
Max latency(s):       259.455
Min latency(s):       4.29483

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:30:49,681792577-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:207 - rados_bench() > kill -INT 303061


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:30:49,687254709-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:31:13,044724596-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:31:13,054939979-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:31:21,476567427-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:31:21,486640252-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:31:29,947417967-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:31:29,957636225-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:31:38,384908241-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:31:38,394767635-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:31:46,814030155-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 257 objects, 4.0 GiB
    usage:   8.0 GiB used, 292 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:31:46,823968537-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:31:46,832332191-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-16T17:31:46,837468982-04:00][RUNNING][ROUND 1/8/40] object_size=64MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:31:46,841772347-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:55 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:31:46,850776834-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40292\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.196135\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 3a5c533f-8f13-41c1-9c9c-33ac342ca7ba\nsetting min_mon_release = octopus\nepoch 0\nfsid 3a5c533f-8f13-41c1-9c9c-33ac342ca7ba\nlast_changed 2021-05-16T14:32:05.533018-0700\ncreated 2021-05-16T14:32:05.533018-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40292/0,v1:10.10.1.2:40293/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.196135 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 a35f6891-41fa-4dd4-9a18-1db04d92c157\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 902be73e-6223-43da-883d-91e8343293bb\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 252363ec-fd8b-4d8b-aa66-424f8c124af0\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42292\n  w/ user/pass: admin / c691a4ad-aef2-47e5-8067-88fff899f4d5\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 17:32:23 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40292
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.196135
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 3a5c533f-8f13-41c1-9c9c-33ac342ca7ba
setting min_mon_release = octopus
epoch 0
fsid 3a5c533f-8f13-41c1-9c9c-33ac342ca7ba
last_changed 2021-05-16T14:32:05.533018-0700
created 2021-05-16T14:32:05.533018-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40292/0,v1:10.10.1.2:40293/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.196135 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 a35f6891-41fa-4dd4-9a18-1db04d92c157
0
start osd.0
add osd1 902be73e-6223-43da-883d-91e8343293bb
1
start osd.1
add osd2 252363ec-fd8b-4d8b-aa66-424f8c124af0
2
start osd.2


restful urls: https://10.10.1.2:42292
  w/ user/pass: admin / c691a4ad-aef2-47e5-8067-88fff899f4d5


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-16T14:31:48.721-0700 7f7a3c5fb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T14:31:48.721-0700 7f7a3c5fb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T14:31:48.737-0700 7f969b5f61c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T14:31:48.737-0700 7f969b5f61c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40292,v1:10.10.1.2:40293] --print /tmp/ceph_monmap.196135 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.196135 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.196135 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42292 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x55ee489e6000 @  0x7fd593cf0680 0x7fd593d11824 0x7fd5944ac187 0x7fd5944b4355 0x7fd5944ac708 0x7fd5944ac877 0x7fd5944adc24 0x7fd5944c5ec1 0x7fd5944385f3 0x7fd594499e97 0x7fd5944a1b1a 0x7fd593ba4d84 0x7fd593cc0609 0x7fd593894293
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.NTbBGJWe3U 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a35f6891-41fa-4dd4-9a18-1db04d92c157 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBgj6Fg7M7AGhAAHGrpTP8S+F6bgzWZLbmZ0A== --osd-uuid a35f6891-41fa-4dd4-9a18-1db04d92c157 
2021-05-16T14:32:17.105-0700 7fa5f0a3df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T14:32:17.125-0700 7fa5f0a3df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-16T14:32:17.125-0700 7fa5f0a3df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55ed3979c000 @  0x7fa5f1406680 0x7fa5f1427824 0x55ed2f22c447 0x55ed2f2344b5 0x55ed2f22c9c8 0x55ed2f22cb37 0x55ed2f22dee4 0x55ed2effeca1 0x55ed2f1d6423 0x55ed2efef2a7 0x55ed2eff453a 0x7fa5f0f59d84 0x7fa5f10de609 0x7fa5f0c47293
2021-05-16T14:32:17.433-0700 7fa5f0a3df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 902be73e-6223-43da-883d-91e8343293bb -i /mnt/sda8/ceph/build/dev/osd1/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x5570789a6000 @  0x7fc3314ba680 0x7fc3314db824 0x55706cb18447 0x55706cb204b5 0x55706cb189c8 0x55706cb18b37 0x55706cb19ee4 0x55706c8eaca1 0x55706cac2423 0x55706c8db2a7 0x55706c8e053a 0x7fc33100dd84 0x7fc331192609 0x7fc330cfb293
2021-05-16T14:32:18.093-0700 7fc330af1f00 -1 Falling back to public interface
2021-05-16T14:32:18.349-0700 7fc330af1f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBhj6FguWxwLhAA3yTPuenX64fGoI70aWLT2g== --osd-uuid 902be73e-6223-43da-883d-91e8343293bb 
2021-05-16T14:32:18.449-0700 7fa2bb349f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T14:32:18.473-0700 7fa2bb349f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-16T14:32:18.473-0700 7fa2bb349f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x5614260f8000 @  0x7fa2bbd12680 0x7fa2bbd33824 0x56141a507447 0x56141a50f4b5 0x56141a5079c8 0x56141a507b37 0x56141a508ee4 0x56141a2d9ca1 0x56141a4b1423 0x56141a2ca2a7 0x56141a2cf53a 0x7fa2bb865d84 0x7fa2bb9ea609 0x7fa2bb553293
2021-05-16T14:32:18.789-0700 7fa2bb349f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 252363ec-fd8b-4d8b-aa66-424f8c124af0 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
tcmalloc: large alloc 1074077696 bytes == 0x558e7a41a000 @  0x7f91b12cf680 0x7f91b12f0824 0x558e6fa74447 0x558e6fa7c4b5 0x558e6fa749c8 0x558e6fa74b37 0x558e6fa75ee4 0x558e6f846ca1 0x558e6fa1e423 0x558e6f8372a7 0x558e6f83c53a 0x7f91b0e22d84 0x7f91b0fa7609 0x7f91b0b10293
2021-05-16T14:32:19.645-0700 7f91b0906f00 -1 Falling back to public interface
2021-05-16T14:32:19.905-0700 7f91b0906f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBjj6FgEQ3SExAA40R9KwsvMd+X11ryVzzZCQ== --osd-uuid 252363ec-fd8b-4d8b-aa66-424f8c124af0 
2021-05-16T14:32:20.021-0700 7fa4fb0c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T14:32:20.041-0700 7fa4fb0c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-16T14:32:20.041-0700 7fa4fb0c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
tcmalloc: large alloc 1074077696 bytes == 0x55b92495c000 @  0x7fa4fba8e680 0x7fa4fbaaf824 0x55b91a53b447 0x55b91a5434b5 0x55b91a53b9c8 0x55b91a53bb37 0x55b91a53cee4 0x55b91a30dca1 0x55b91a4e5423 0x55b91a2fe2a7 0x55b91a30353a 0x7fa4fb5e1d84 0x7fa4fb766609 0x7fa4fb2cf293
2021-05-16T14:32:20.377-0700 7fa4fb0c5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
tcmalloc: large alloc 1074077696 bytes == 0x56267dc9c000 @  0x7fc6b9d2f680 0x7fc6b9d50824 0x56267292d447 0x5626729354b5 0x56267292d9c8 0x56267292db37 0x56267292eee4 0x5626726ffca1 0x5626728d7423 0x5626726f02a7 0x5626726f553a 0x7fc6b9882d84 0x7fc6b9a07609 0x7fc6b9570293
2021-05-16T14:32:21.041-0700 7fc6b9366f00 -1 Falling back to public interface
2021-05-16T14:32:21.313-0700 7fc6b9366f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:23,498913743-04:00] STAGE: Update local ceph.conf as a client...[0m
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_device_name /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_device_name *=).*$/\1 mlx5_2/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:80 - update_local_ceph_conf() > grep --fixed-strings --word-regexp --quiet ms_async_rdma_local_gid /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf
# ./bench-rados:83 - update_local_ceph_conf() > sed --in-place --regexp-extended 's/(ms_async_rdma_local_gid *=).*$/\1 0000:0000:0000:0000:0000:ffff:0a0a:0101/g' /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:23,509762225-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:94 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:23,551989722-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:23,555211565-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e5794e8-5c84-4f9a-a391-f818029371f5', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e5794e8-5c84-4f9a-a391-f818029371f5 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV -- ceph config set osd osd_max_backfills 32
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:27,003871421-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:27,007239619-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e5794e8-5c84-4f9a-a391-f818029371f5', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e5794e8-5c84-4f9a-a391-f818029371f5 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:30,543483207-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:30,547517817-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e5794e8-5c84-4f9a-a391-f818029371f5', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e5794e8-5c84-4f9a-a391-f818029371f5 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:33,967576929-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:33,971457540-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e5794e8-5c84-4f9a-a391-f818029371f5', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e5794e8-5c84-4f9a-a391-f818029371f5 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:99 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:100 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:102 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:40,560020294-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:40,563813100-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e5794e8-5c84-4f9a-a391-f818029371f5', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e5794e8-5c84-4f9a-a391-f818029371f5 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:104 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:44,400594667-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:44,404458036-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e5794e8-5c84-4f9a-a391-f818029371f5', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e5794e8-5c84-4f9a-a391-f818029371f5 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:48,823910321-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:48,827630891-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e5794e8-5c84-4f9a-a391-f818029371f5', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e5794e8-5c84-4f9a-a391-f818029371f5 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:52,311568730-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:52,315190995-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e5794e8-5c84-4f9a-a391-f818029371f5', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e5794e8-5c84-4f9a-a391-f818029371f5 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:55,760661742-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:32:55,764229525-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e5794e8-5c84-4f9a-a391-f818029371f5', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e5794e8-5c84-4f9a-a391-f818029371f5 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:33:00,258787559-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:33:00,262233714-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e5794e8-5c84-4f9a-a391-f818029371f5', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e5794e8-5c84-4f9a-a391-f818029371f5 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:33:04,619561061-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:33:04,622669441-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e5794e8-5c84-4f9a-a391-f818029371f5', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e5794e8-5c84-4f9a-a391-f818029371f5 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 12 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:33:08,025375272-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:33:08,029469264-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e5794e8-5c84-4f9a-a391-f818029371f5', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e5794e8-5c84-4f9a-a391-f818029371f5 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  165 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   55 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  166 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:33:11,320738880-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:33:34,718761285-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:33:43,238086803-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:33:51,593500274-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:00,006685619-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:08,368523163-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:16,722792750-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:25,213749445-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:25,224585594-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:33,647484284-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:33,657707982-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:42,095325046-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:42,105535890-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:50,455043606-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:50,465381429-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:58,962959320-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   252 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:58,973452464-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:58,981588610-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:58,986874712-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:58,997853790-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:175 - rados_bench() > sar_pid=309750
# ./bench-rados:175 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:175 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64MB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:59,005519462-04:00] INFO: > Run rados bench[0m
# ./bench-rados:182 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 67108864 -O 67108864 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:191 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64MB_write.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:59,026178385-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-16T17:34:59,029480991-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e5794e8-5c84-4f9a-a391-f818029371f5', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '67108864', '-O', '67108864', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e5794e8-5c84-4f9a-a391-f818029371f5 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NWw7yV:/tmp/ceph-asok.NWw7yV -- rados bench 20 write --pool bench_rados -b 67108864 -O 67108864 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-16T21:35:01.281+0000 7fdace9e4d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T21:35:01.529+0000 7fdace9e4d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-16T21:35:01.529+0000 7fdace9e4d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-16T21:35:01.558858+0000 Maintaining 256 concurrent writes of 67108864 bytes to objects of size 67108864 for up to 20 seconds or 0 objects
2021-05-16T21:35:01.558921+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-16T21:35:05.591789+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:35:05.591789+0000     0       0         0         0         0         0           -           0
2021-05-16T21:35:06.591938+0000     1      12        12         0         0         0           -           0
2021-05-16T21:35:07.592015+0000     2      25        25         0         0         0           -           0
2021-05-16T21:35:08.592084+0000     3      38        38         0         0         0           -           0
2021-05-16T21:35:09.592154+0000     4      51        51         0         0         0           -           0
2021-05-16T21:35:10.592222+0000     5      65        65         0         0         0           -           0
2021-05-16T21:35:11.592299+0000     6      81        81         0         0         0           -           0
2021-05-16T21:35:12.592367+0000     7      96        96         0         0         0           -           0
2021-05-16T21:35:13.592438+0000     8     104       104         0         0         0           -           0
2021-05-16T21:35:14.592505+0000     9     107       107         0         0         0           -           0
2021-05-16T21:35:15.592582+0000    10     122       122         0         0         0           -           0
2021-05-16T21:35:16.592652+0000    11     136       136         0         0         0           -           0
2021-05-16T21:35:17.592721+0000    12     144       144         0         0         0           -           0
2021-05-16T21:35:18.593141+0000    13     144       144         0         0         0           -           0
2021-05-16T21:35:19.593217+0000    14     158       158         0         0         0           -           0
2021-05-16T21:35:20.593287+0000    15     174       174         0         0         0           -           0
2021-05-16T21:35:21.593356+0000    16     189       189         0         0         0           -           0
2021-05-16T21:35:22.593427+0000    17     202       202         0         0         0           -           0
2021-05-16T21:35:23.593504+0000    18     215       215         0         0         0           -           0
2021-05-16T21:35:24.593574+0000    19     230       230         0         0         0           -           0
2021-05-16T21:35:25.593656+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-16T21:35:25.593656+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-16T21:35:25.593656+0000    20     243       243         0         0         0           -           0
2021-05-16T21:35:26.593739+0000    21     243       243         0         0         0           -           0
2021-05-16T21:35:27.593815+0000    22     248       248         0         0         0           -           0
error during benchmark: (90) Message too long
error 90: (90) Message too long
